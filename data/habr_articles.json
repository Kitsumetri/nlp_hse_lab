[
  {
    "article_id": "https://habr.com/ru/articles/878658/",
    "title": "Многопользовательский рой агентов для Ollama",
    "category": "Программирование",
    "tags": "typescript, javascript, ollama, deepseek, openai, chatgpt, искусственный интеллект, llm, python, telegram",
    "text": "В данной статье осуществлен разбор многопользовательского телеграм чат бота на LLM, код которогоопубликован в этом репозитории Куда движется рынок Когда-то давным давно графический пользовательский интерфейс сменил консольный ввод. Казалось бы, проблему неудобства взаимодействия для неподготовленного пользователя мог бы решить псевдографический интерфейс, но есть фактор, который не все замечают Важно!Разрабатывать графический пользовательский интерфейс дешевле, чем псевдографический. В историческом процессе, сразу после выхода Next CUBE был представлен язык ObjC с графическим редактором форм, где страницы можно компоновать мышкой. В современном же мире, Frontend предоставляет графическую отладку форм через Dev Tools, что примерно то же самое: код номинальный без технических деталей, при проблемах есть GUI, удешевляющее поиск бага. Но ещё дешевле - не делать пользовательский интерфейс вообще. Тебе не нужен статический IP, PCI DSS, домен раскрученый в яндексе и гугле, highload, если ты решил не изобретать велосипед и не создавать очередной веб продукт, на привлечение посетителей в которого заплатить денег придется в три раза больше, чем на разработку. Телефон это от слова фон, фонетика - звук. Вместо того, чтобы учить огромное количество сочетаний кнопок для Figma, Blender, Photoshop, Unreal Engine, проще просто озвучить команду.Как повернуть чертёж в архикаде? LLM как новый вид пользовательского интерфейса Agent Swarmэто как фрагменты в android или роутер в react: они позволяют конкретизировать скоуп задач (кнопки на экране) исходя из предидущего пользовательского ввода. Например, когда поступил звонок на SIP телефон, сначала нужно понять, хочет ли человек купить или вернуть товар в принципе, а потом предложить ему список товаров в наличие для покупки Технические требования Налоговая в любом случае спросит дебет/кредит в табличном виде, по этому CRM системы никуда не денутся. Задача LLM - спарсить естественный текст или чата или распознавалки голоса и трансформировать его в сигнатуру функции с наименованием и аргументами, чтобы можно было осуществить вызов и записать данные в базу Для решения этой проблемы, важно знать чреду нюансов На момент 2025 года OpenSource языковые модели для офлайн запуска галлюцинируют в 40%-50% случаев, когда оригинальный ChatGPT в 5%-10%. На момент 2025 года OpenSource языковые модели для офлайн запуска галлюцинируют в 40%-50% случаев, когда оригинальный ChatGPT в 5%-10%. Если не разделять промпт на агенты, модель будет больше галюцинировать, так как предмет разговора становится расплывчатым Если не разделять промпт на агенты, модель будет больше галюцинировать, так как предмет разговора становится расплывчатым Каждый месяц появляются новые модели, которые галюцинируют меньше. Появляются альтернативные SaaS, код которых закрыт, но они дешевле ChatGPT, например, DeepSeek Каждый месяц появляются новые модели, которые галюцинируют меньше. Появляются альтернативные SaaS, код которых закрыт, но они дешевле ChatGPT, например, DeepSeek Как следствие Код чат бота должен быть обособленным от провайдера LLM с возможностью переключения на GPT4All, Ollama, OpenAI, DeepSeek и тд без редактирования бизнес логики Код чат бота должен быть обособленным от провайдера LLM с возможностью переключения на GPT4All, Ollama, OpenAI, DeepSeek и тд без редактирования бизнес логики Должен быть testbed, который позволяет оценить, какое количество промптов поломалось при смене языковой модели или провайдера, так как версионирование из-за SJW цензуры не предусмотрено: поведение промптов меняют в тихую не афишируя подробности Должен быть testbed, который позволяет оценить, какое количество промптов поломалось при смене языковой модели или провайдера, так как версионирование из-за SJW цензуры не предусмотрено: поведение промптов меняют в тихую не афишируя подробности Код для оркестрации сессий чатов с контекстом активного агента должен быть отделён от провайдера LLM или фреймворка, так как в любой момент выйдет что-то новое Код для оркестрации сессий чатов с контекстом активного агента должен быть отделён от провайдера LLM или фреймворка, так как в любой момент выйдет что-то новое Разбор кода На каждую открытую сессию чата нужно осуществить оркестрацию Swarm c деревом агентов, имеющих общую историю чата между собой и отдельную для разных пользователей. В данном коде, это реализовано под капотомagent-swarm-kit При создании агента мы указываем хотя бы одинsystem message, описывающий что он должен делать. Мы указываем коннектор к языковой модели, что позволит часть агентов обрабатывать бесплатно локально, сложные же делегировать в облачный червис openai. Если что-то не работает, добавляем промпты в массив system, например, фикс вызова функций для Ollama В данном примере я использую ollama для обработки запросов пользователей. Для не сведующих в терминологии: процесс, когда языковая модель на вход получает историю переписки с пользователем, а на выход выдаёт новое сообщение, называется completion. В agent-swarm-kit используется абстрактный интерфейс, который одинокого подходит к любому облачному провайдеру или локальной модели.Используйте этот материал,чтобы подключить deepseek Изменение активного агента и получение данных из БД осуществляется через вызовtools: языковая модель возвращает специальный XML, который обрабатывается фреймворком для локальных моделей или облачным провайдером для OpenAI, чтобы вызвать внешний код на python/js и тд. Результат исполнения кода записывается в историю переписки в виде{\"role\": \"tool\", \"content\": \"В базе данных найден продукт Парацетамол: жаропонижающее для борьбы с гриппом\"}. Со следующего сообщения от пользователя, языковая модель оперирует данными из инструмента Для того, чтобы не хардкодить первые сообщения от агентов, при переключении агента происходит эмуляция запроса со стороны пользователя с просьбой поздороваться. Языковые модели умеют формировать словарь именованных параметров для вызоваtools. Однако, OpenSource модели плохо справляются, если есть техническое требование закрытый контур, проще анализировать саму переписку. Принцип работы роя агентов Несколько сессий chatgpt (агентов) выполняют вызовы инструментов. Каждый агент может использовать разные модели, например,mistral 7bдля повседневного общения,nemotronдля деловых разговоров. Несколько сессий chatgpt (агентов) выполняют вызовы инструментов. Каждый агент может использовать разные модели, например,mistral 7bдля повседневного общения,nemotronдля деловых разговоров. Рой агентов направляет сообщения к активной сессии chatgpt (агенту) для каждого каналаWebSocket, используя параметр URLclientId. Для каждого нового чата с человеком создается новый канал со своим роем агентов Рой агентов направляет сообщения к активной сессии chatgpt (агенту) для каждого каналаWebSocket, используя параметр URLclientId. Для каждого нового чата с человеком создается новый канал со своим роем агентов Активная сессия chatgpt (агент) в рое может быть изменена путем выполнения инструмента. Активная сессия chatgpt (агент) в рое может быть изменена путем выполнения инструмента. Все клиентские сессии используют общую историю сообщений чата для всех агентов. История чата каждого клиента хранит последние 25 сообщений с ротацией. Между сессиями chatgpt (агентами) передаются только сообщения типаassistantиuser, а сообщения типаsystemиtoolограничены областью действия агента, поэтому каждый агент знает только те инструменты, которые относятся к нему. В результате каждая сессия chatgpt (агент) имеет свой уникальный системный промпт. Все клиентские сессии используют общую историю сообщений чата для всех агентов. История чата каждого клиента хранит последние 25 сообщений с ротацией. Между сессиями chatgpt (агентами) передаются только сообщения типаassistantиuser, а сообщения типаsystemиtoolограничены областью действия агента, поэтому каждый агент знает только те инструменты, которые относятся к нему. В результате каждая сессия chatgpt (агент) имеет свой уникальный системный промпт. Если вывод агента не прошел валидацию (несуществующий вызов инструмента, вызов инструмента с неверными аргументами, пустой вывод, XML-теги в выводе или JSON в выводе), алгоритм спасения попытается исправить модель. Сначала он скроет предыдущие сообщения от модели, если это не поможет, то вернет заглушку вида \"Извините, я не понял. Не могли бы вы повторить?\" Если вывод агента не прошел валидацию (несуществующий вызов инструмента, вызов инструмента с неверными аргументами, пустой вывод, XML-теги в выводе или JSON в выводе), алгоритм спасения попытается исправить модель. Сначала он скроет предыдущие сообщения от модели, если это не поможет, то вернет заглушку вида \"Извините, я не понял. Не могли бы вы повторить?\" Полезные функции для администрирования роя агентов addAgent- Регистрация нового агента addAgent- Регистрация нового агента addCompletion- Регистрация новой языковой модели: облачной, локальной или мок addCompletion- Регистрация новой языковой модели: облачной, локальной или мок addSwarm- Регистрация группы агентов для обработки чатов с пользователями addSwarm- Регистрация группы агентов для обработки чатов с пользователями addTool- Регистрация инструмента для интеграции языковых моделей во внешние системы addTool- Регистрация инструмента для интеграции языковых моделей во внешние системы changeAgent- Изменить активный агент в рое changeAgent- Изменить активный агент в рое complete- Запросить ответ на сообщение, переданное в аргументы рою агентов complete- Запросить ответ на сообщение, переданное в аргументы рою агентов session- Создать сессию для чата, дать коллбек на завершение сессии и отправку новых сообщений session- Создать сессию для чата, дать коллбек на завершение сессии и отправку новых сообщений getRawHistory- Получает необработанную историю системы для отладки getRawHistory- Получает необработанную историю системы для отладки getAgentHistory- Получает историю, которую видит агент с поправкой на механизм самовосстановления и получателей сообщений getAgentHistory- Получает историю, которую видит агент с поправкой на механизм самовосстановления и получателей сообщений commitToolOutput- Отправить в историю результат исполнения функции. Если была вызвана функция, агент замораживается до получения ответа commitToolOutput- Отправить в историю результат исполнения функции. Если была вызвана функция, агент замораживается до получения ответа commitSystemMessage- Дополнить системный промпт новыми вводными commitSystemMessage- Дополнить системный промпт новыми вводными commitFlush- Очистить переписку для агента, если были получены некорректные ответы или модель ошибочно рекурсивно вызывает инструмент commitFlush- Очистить переписку для агента, если были получены некорректные ответы или модель ошибочно рекурсивно вызывает инструмент execute- Попросить нейронку проявить инициативу и первой написать пользователю execute- Попросить нейронку проявить инициативу и первой написать пользователю emit- Отправить пользователю заранее заготовленное сообщение emit- Отправить пользователю заранее заготовленное сообщение getLastUserMessage- Получить последнее сообщение от пользователя (без учетаexecute) getLastUserMessage- Получить последнее сообщение от пользователя (без учетаexecute) commitUserMessage- Сохранить сообщение от пользователя в истории чата без ответа. Если пользователь спамит сообщения не дожидаясь обработки запроса commitUserMessage- Сохранить сообщение от пользователя в истории чата без ответа. Если пользователь спамит сообщения не дожидаясь обработки запроса getAgentName- Получить имя активного агента getAgentName- Получить имя активного агента getUserHistory- Получает историю сообщений от  пользователя getUserHistory- Получает историю сообщений от  пользователя getAssistantHistory- Получает историю сообщений от языковой модели getAssistantHistory- Получает историю сообщений от языковой модели getLastAssistantMessage- Получает последнее сообщение от языковой модели getLastAssistantMessage- Получает последнее сообщение от языковой модели getLastSystemMessage- Получает последнее дополнение системного промпта getLastSystemMessage- Получает последнее дополнение системного промпта "
  },
  {
    "article_id": "https://habr.com/ru/articles/871154/",
    "title": "Правильный инструмент для аналитики нагрузочного тестирования. Часть 2",
    "category": "Программирование",
    "tags": "нагрузочное тестирование, нагрузка на сервер, python, performance tests, performance analysis, reporting, сервис для аналитики, locust, load testing, metrics server",
    "text": "Вступление Недавно вышластатьяо сервисе аналитики нагрузочного тестирования, где я рассказывал о типичных проблемах, возникающих при проведении нагрузочного тестирования и последующем анализе его результатов. В той статье я также упоминал о сервисеload-testing-hub, который помогает решать проблемы аналитики и агрегации данных по нагрузочным тестам. На тот моментload-testing-hubнаходился в стадии MVP и использовался в тестовом режиме нашей командой. Однако в процессе длительного использования выявились недостатки, такие как отсутствие необходимой информации, ограниченные возможности для сравнения результатов, недостаточная гибкость настроек, субъективная оценка результатов в некоторых случаях, а также невозможность просмотреть детальные данные и графики по каждому нагружаемому методу. Помимо этого, с помощью сервиса удалось выявить некоторые аномалии в работе наших микросервисов. За время работы сload-testing-hubтакже были реализованы важные для нашей команды и компании фичи, каждая из которых проходила нагрузочное тестирование с использованием данного сервиса для анализа производительности. Одной из таких фичей стал поиск по операциям в ленте операций в мобильном приложении банка. Контекст: необходимо было реализовать поиск по операциям, которых в системе насчитывается несколько сотен миллионов. Возможно, это не кажется критичным объемом, но на самом деле речь идет о гигантских данных размером свыше сотни гигабайт. Эти данные нужно было \"засидить\" в базу данных, подготовить нагрузочное окружение и провести тестирование. С небольшими объемами данных (100–200 тыс. записей) работа обычно не вызывает проблем. Но когда речь заходит о сотнях миллионов или нескольких миллиардах записей, становится критически важным выбирать максимально оптимальные алгоритмы генерации данных и их вставки в базу. Анализ результатов нагрузочного тестирования поиска проводился черезload-testing-hub, что позволило получить массу полезной информации, выявить узкие места и, в конечном итоге, выпустить фичу с максимальной оптимизацией. При этом была проделана огромная работа как над самой фичей, так и над улучшениемload-testing-hub, о чем я расскажу в этой статье. Объем изменений вload-testing-hubбыл настолько значительным, что проще рассказать о сервисе заново, чем сравнивать его с версией, описанной впредыдущей статье. Поэтому я начну с самого начала, делая отсылки к MVP-версии лишь в ключевых местах. Также хочу отметить, что все данные на скриншотах ниже являются тестовыми, и в некоторых случаях они могут выглядеть нелогично или несвязно. Главная цель этих изображений — продемонстрировать отображение информации Сервисы Базовой сущностью в сервисеload-testing-hubявляютсяservices— то есть сервисы, которые подвергаются нагрузочному тестированию. В нашем случае используется протоколgRPC, в рамках которого один физический сервис может содержать несколько логических. Например, есть сервисmobile-gateway-service, внутри которого находятся логические сервисыaccount-service,cards-serviceиoperations-service. Мобильное приложение обращается к хостуmobile-gateway-service, используяgRPC-контрактылогических сервисов. В данном случае сущностьservicesвload-testing-hubпредставляет собой именно физический сервисmobile-gateway-service. В случае сHTTPможно встретить похожую архитектуру. Например, естьweb-app-gatewayс собственным хостом, но здесь отсутствуют контракты логических сервисов. Вместо этого логика сервисов (account-service,cards-service,operations-service) скрыта \"под капотом\"web-app-gateway, а доступ предоставляется черезREST API. В этом случае сущностьservicesвload-testing-hubтакже будет представлять физический сервисweb-app-gateway. В новой версииload-testing-hubпоявился функционал управления сервисами через UI, а все сервисы были вынесены на отдельную страницу. Каждый элемент списка содержит: Базовую информацию о сервисе. Базовую информацию о сервисе. Небольшую статистику: количество сценариев для данного сервиса и количество нагрузочных результатов. Небольшую статистику: количество сценариев для данного сервиса и количество нагрузочных результатов. Создание и редактирование сервисов теперь реализовано через модальные окна: Устранение конфликтов при работе с несколькими вкладкамиРанее выбор сервиса происходил через модальное окно, что приводило к проблемам при работе с несколькими вкладками. Данные о выбранном сервисе хранились вlocalStorage, который привязан к домену. В результате, если в одной вкладке выбрать один сервис, а в другой — другой, после перезагрузки первой вкладки данные из второй перезаписывали данные первой.Теперь идентификатор сервиса фиксируется в URI, например:/services/199/results.Это не только устраняет проблемы с несколькими вкладками, но и позволяет удобнее конструировать ссылки на различные части приложения.Такое решение кажется очевидным, но на этапе разработки MVP зачастую выбираются минимальные рабочие подходы с мыслью \"лишь бы работало\". Позже приходится перерабатывать многое, чтобы учесть реальный масштаб использования. Устранение конфликтов при работе с несколькими вкладкамиРанее выбор сервиса происходил через модальное окно, что приводило к проблемам при работе с несколькими вкладками. Данные о выбранном сервисе хранились вlocalStorage, который привязан к домену. В результате, если в одной вкладке выбрать один сервис, а в другой — другой, после перезагрузки первой вкладки данные из второй перезаписывали данные первой.Теперь идентификатор сервиса фиксируется в URI, например:/services/199/results.Это не только устраняет проблемы с несколькими вкладками, но и позволяет удобнее конструировать ссылки на различные части приложения. Такое решение кажется очевидным, но на этапе разработки MVP зачастую выбираются минимальные рабочие подходы с мыслью \"лишь бы работало\". Позже приходится перерабатывать многое, чтобы учесть реальный масштаб использования. МасштабируемостьРанее создание сервисов происходило вручную через базу данных, что было неудобным и нецелевым решением. Например, если в компании множество команд, каждая из которых используетload-testing-hub, важно, чтобы каждая команда могла самостоятельно управлять своими сервисами и результатами нагрузочных тестов.Теперь каждая команда может работать независимо, без необходимости \"лезть под капот\" и вносить изменения в базу данных. МасштабируемостьРанее создание сервисов происходило вручную через базу данных, что было неудобным и нецелевым решением. Например, если в компании множество команд, каждая из которых используетload-testing-hub, важно, чтобы каждая команда могла самостоятельно управлять своими сервисами и результатами нагрузочных тестов.Теперь каждая команда может работать независимо, без необходимости \"лезть под капот\" и вносить изменения в базу данных. В будущем планируется расширять использованиеload-testing-hubна другие команды и сервисы. В этом контексте редактирование сущностей напрямую через базу данных становится недопустимым. Результаты В данном разделе отображается список запусков нагрузочного тестирования (НТ). Каждая карточка содержит следующую информацию: Заголовок— идентификатор запуска, название сервиса, для которого проводилось тестирование, и название нагрузочного сценария. Заголовок— идентификатор запуска, название сервиса, для которого проводилось тестирование, и название нагрузочного сценария. Средний RPS(Requests Per Second) для данного запуска. Средний RPS(Requests Per Second) для данного запуска. Графическая шкала— отображает общее количество успешных запросов и количество запросов с ошибками. Графическая шкала— отображает общее количество успешных запросов и количество запросов с ошибками. Количество виртуальных пользователей, использованных в тестировании. Количество виртуальных пользователей, использованных в тестировании. Время запуска— отображает начальное и конечное время выполнения теста, а также его продолжительность (например,2m59s). Время запуска— отображает начальное и конечное время выполнения теста, а также его продолжительность (например,2m59s). Блок тегов:Версия нагружаемого сервиса (если была предоставлена).Теги сравнения текущего результата с предыдущим и средними значениями.Версия сценария и его теги (например,LATEST,EXPERIMENT,LEGACY). Блок тегов: Версия нагружаемого сервиса (если была предоставлена). Версия нагружаемого сервиса (если была предоставлена). Теги сравнения текущего результата с предыдущим и средними значениями. Теги сравнения текущего результата с предыдущим и средними значениями. Версия сценария и его теги (например,LATEST,EXPERIMENT,LEGACY). Версия сценария и его теги (например,LATEST,EXPERIMENT,LEGACY). Если в настройкахне выбран сценарий, система подбирает предыдущий результат по сервису, определяя его относительно текущего запуска. Еслисценарий выбран, сравнение производится среди запусков только этого сценария. Аналогичная логика применяется для вычислениясредних значений, но в этом случае берутся средние показатели либо по сервису, либо по сценарию. Иконка быстрого перехода— при нажатии на нее открывается джоба, в которой был запущен данный нагрузочный тест. Это удобно, если необходимо быстро проанализировать, что пошло не так.Load tests job URL Иконка быстрого перехода— при нажатии на нее открывается джоба, в которой был запущен данный нагрузочный тест. Это удобно, если необходимо быстро проанализировать, что пошло не так. Меню управления нагрузочным запуском, включающее:Ссылки на пайплайны и джобы тестов.Просмотр деталей сценария.Возможность оставить комментарий к результату.Удаление некорректного результата. Меню управления нагрузочным запуском, включающее: Ссылки на пайплайны и джобы тестов. Ссылки на пайплайны и джобы тестов. Просмотр деталей сценария. Просмотр деталей сценария. Возможность оставить комментарий к результату. Возможность оставить комментарий к результату. Удаление некорректного результата. Удаление некорректного результата. Удаление результатов реализовано черезстатусную модель, поэтому на самом деле данные не исчезают из системы, а просто исключаются из статистики. Это позволяет скрывать невалидные запуски, например, если тест был запущен с ошибками в конфигурации и не отражает реальную производительность. Визуально по сравнению с MVP-версией изменилось немногое, но \"под капотом\" произошли глобальные изменения. Самое важное из них — переработанный механизм сравнения результатов. Ранее показатели{percent}% better/worse than average/previousрассчитывались предельно просто: сравнение шло исключительно поRequests Per Second (RPS). Такой подход был примитивным и недостаточно информативным. Он не позволял гибко выбирать метрики для анализа, а также не учитывал приоритеты различных сервисов. Например: Для сервисов, отвечающих зазагрузку главного экрана мобильного приложения, критичновремя отклика, так как пользователи не любят долгого ожидания. Для сервисов, отвечающих зазагрузку главного экрана мобильного приложения, критичновремя отклика, так как пользователи не любят долгого ожидания. Для сервисов, работающих сключевой бизнес-логикой, важнеестабильность и отсутствие ошибок. В этом случае приоритетными метриками становятсяобщее количество ошибокиошибки в секунду. Для сервисов, работающих сключевой бизнес-логикой, важнеестабильность и отсутствие ошибок. В этом случае приоритетными метриками становятсяобщее количество ошибокиошибки в секунду. MVP-версия не позволяла настраивать такие приоритеты, а нам этого очень хотелось. Поэтому функционал сравнения былполностью переработан. Теперь: Сравнение выполняетсяпо всем метрикам одновременно. Сравнение выполняетсяпо всем метрикам одновременно. У каждой метрики есть свойвес. У каждой метрики есть свойвес. Учитываетсяконтекст метрики. Учитываетсяконтекст метрики. Новая формула выглядит так: Где: — итоговый процент. — итоговый процент. — количество метрик. — количество метрик. ​ — вес-й метрики (доля от 1, например, 0.2, 0.3 и т.д.).Важно, сумма весов не может быть больше 1 ​ — вес-й метрики (доля от 1, например, 0.2, 0.3 и т.д.).Важно, сумма весов не может быть больше 1 ​ — процент-й метрики (рассчитывается как разница между фактическим и ожидаемым значением, в процентах). ​ — процент-й метрики (рассчитывается как разница между фактическим и ожидаемым значением, в процентах). Пример:Если у нас есть три метрики: Метрика number of requests: Метрика number of requests: Метрика requests per second: Метрика requests per second: Метрика average response time (ms): Метрика average response time (ms): Тогда итоговый процент: Теперь у нас появилась возможностьгибко настраивать веса метрик. Можно распределять вес между разными метриками. Можно распределять вес между разными метриками. Можно задатьмаксимальный вес одной метрике, чтобы сравнение велось только по ней. Можно задатьмаксимальный вес одной метрике, чтобы сравнение велось только по ней. Это глобальное изменение, которое значительно расширяет возможности аналитики нагрузочного тестирования (НТ). Разные сервисы имеют разные приоритетные показатели, и теперь мы можем учитывать это при анализе. Подробнее о настройке весов расскажу в разделе\"Settings\". Я также упоминалконтекст метрик. Давайте разберем, что это значит. Некоторые метрикичем выше, тем лучше. Например,Requests Per Second (RPS): RPS = 100лучше, чемRPS = 37, так как сервис обрабатывает больше запросов. RPS = 100лучше, чемRPS = 37, так как сервис обрабатывает больше запросов. Другие метрикичем выше, тем хуже. Например,Max Response Time(максимальное время ответа): 700 msхуже, чем100 ms, потому что более долгий отклик негативно влияет на UX. 700 msхуже, чем100 ms, потому что более долгий отклик негативно влияет на UX. Таким образом,каждая метрика требует индивидуального подхода к сравнению. В новой версииload-testing-hubэто учтено: у каждой метрики есть свойконтекст, который автоматически учитывается при расчетах. Хотя вычисление процентного сравнения с учетомвесов и контекстанесложное,важно понимать, какие метрики участвуют в расчете. Чтобы не держать расчеты это в голове, добавиласьопция \"Explanation\". Если нажать налейбл сравнения, откроетсятултип. Если нажать налейбл сравнения, откроетсятултип. В нем будет подробно расписано,как именно рассчитан процент. В нем будет подробно расписано,как именно рассчитан процент. Эта возможность доступна для всех сравнений. Эта возможность доступна для всех сравнений. Фильтрация результатовТеперь можно фильтровать результаты подате и времени начала/окончания НТ, а также поверсии сервиса.Results filters Фильтрация результатовТеперь можно фильтровать результаты подате и времени начала/окончания НТ, а также поверсии сервиса. Пагинация результатовНа странице с результатами добавленапагинация, так как количество тестов может бытьочень большим, особенно если они запускаютсярегулярноилиавтоматически по событиям. Например, для одного сервиса может накапливаться200-300 запусковв среднем.Results pagination Пагинация результатовНа странице с результатами добавленапагинация, так как количество тестов может бытьочень большим, особенно если они запускаютсярегулярноилиавтоматически по событиям. Например, для одного сервиса может накапливаться200-300 запусковв среднем. Детали результата В разделе деталей результата представлена подробная информация о запуске нагрузочных тестов. На скриншоте выше отображены средние и суммарные метрики за весь запуск нагрузочного тестирования. Эти данные позволяют получить общее представление о производительности сервиса. Давайте разберем каждую метрику: Number of users— количество виртуальных пользователей. Number of users— количество виртуальных пользователей. Number of requests— общее количество отправленных запросов. Number of requests— общее количество отправленных запросов. Requests per second (RPS)— среднее количество запросов в секунду. Requests per second (RPS)— среднее количество запросов в секунду. Number of failures— общее количество запросов, завершившихся с ошибкой. Number of failures— общее количество запросов, завершившихся с ошибкой. Failures per second— среднее количество ошибочных запросов в секунду. Failures per second— среднее количество ошибочных запросов в секунду. Max response time (ms)— максимальное время ответа (в миллисекундах) среди всех нагружаемых методов.Например, если тестировались три метода:GetUser→ max response time =10 msGetUsers→ max response time =33 msGetAccount→ max response time =57 msТогда итоговыйMax response time=57 ms. Max response time (ms)— максимальное время ответа (в миллисекундах) среди всех нагружаемых методов.Например, если тестировались три метода: GetUser→ max response time =10 ms GetUser→ max response time =10 ms GetUsers→ max response time =33 ms GetUsers→ max response time =33 ms GetAccount→ max response time =57 msТогда итоговыйMax response time=57 ms. GetAccount→ max response time =57 msТогда итоговыйMax response time=57 ms. Min response time (ms)— минимальное время ответа среди всех нагружаемых методов (по аналогии сMax response time, но вычисляется минимальное значение). Min response time (ms)— минимальное время ответа среди всех нагружаемых методов (по аналогии сMax response time, но вычисляется минимальное значение). Median response time (ms)—медианное время ответав миллисекундах. Median response time (ms)—медианное время ответав миллисекундах. Average response time (ms)— среднее время ответа среди всех нагружаемых методов. Average response time (ms)— среднее время ответа среди всех нагружаемых методов. Перцентили (50%-ile, 60%-ile, 70%-ile, 80%-ile, 90%-ile, 95%-ile, 99%-ile, 100%-ile) (ms)— распределение времени ответа по персентилям. Перцентили (50%-ile, 60%-ile, 70%-ile, 80%-ile, 90%-ile, 95%-ile, 99%-ile, 100%-ile) (ms)— распределение времени ответа по персентилям. Теперь разберем доступные возможности на странице деталей результата, двигаясь слева направо. Комментарии к результатуК каждому запуску нагрузочного тестирования можно оставлять комментарии в произвольной форме. Эта функция особенно полезна при проведении экспериментов, например, при изменении ресурсовподов. В комментариях можно указать, на каком именно окружении выполнялся тест.Set load test result comment Комментарии к результатуК каждому запуску нагрузочного тестирования можно оставлять комментарии в произвольной форме. Эта функция особенно полезна при проведении экспериментов, например, при изменении ресурсовподов. В комментариях можно указать, на каком именно окружении выполнялся тест. Просмотр деталей сценарияМожно открыть подробную информацию о сценарии, по которому был запущен нагрузочный тест.Scenario details Просмотр деталей сценарияМожно открыть подробную информацию о сценарии, по которому был запущен нагрузочный тест. Сравнение результатовДоступен инструмент для сравнения текущего результата с другими запусками, сценариями или средними значениями. Подробнее об этом в разделе \"Сравнения\".Comparison of result Сравнение результатовДоступен инструмент для сравнения текущего результата с другими запусками, сценариями или средними значениями. Подробнее об этом в разделе \"Сравнения\". Просмотр дашбордов Grafana и KibanaЕсть возможность открыть дашборды вGrafanaиKibana, но эти опции требуют предварительной настройки. Для их работы необходимо указать ссылки на инстансы Grafana и Kibana в конфигурации сервера, а также выполнить настройки на странице\"Интеграции\".Integrations Просмотр дашбордов Grafana и KibanaЕсть возможность открыть дашборды вGrafanaиKibana, но эти опции требуют предварительной настройки. Для их работы необходимо указать ссылки на инстансы Grafana и Kibana в конфигурации сервера, а также выполнить настройки на странице\"Интеграции\". Ссылки на связанные результатыЗдесь доступны все ссылки, которые отображались на карточке результата НТ, но также добавлена возможность перейти к предыдущему результату. При нажатии на эту опцию откроется страница с предыдущим запуском нагрузочного тестирования.Result URLs Ссылки на связанные результатыЗдесь доступны все ссылки, которые отображались на карточке результата НТ, но также добавлена возможность перейти к предыдущему результату. При нажатии на эту опцию откроется страница с предыдущим запуском нагрузочного тестирования. Ниже представлен виджет, отображающий детализированные результаты по каждому нагружаемому методу. В отличие от предыдущего виджета, который содержал только средние и суммарные значения, здесь приведены фактические показатели для каждого метода. Также доступен поиск по методам. Функционал таблицы Statistics of methods СортировкаВ таблице доступна сортировка по каждому столбцу, что упрощает анализ данных. Этот функционал реализован практически во всех таблицах системы. СортировкаВ таблице доступна сортировка по каждому столбцу, что упрощает анализ данных. Этот функционал реализован практически во всех таблицах системы. Настройка отображаемых колонокВ правом верхнем углу виджетаStatistics of methods(а также на других подобных виджетах) находится кнопка с иконкой шестеренки. Нажав на неё, можно выбрать, какие колонки таблицы отображать. Это особенно удобно, если необходимо сфокусироваться на определенных метриках и скрыть остальные для лучшего восприятия.Выбранные настройки автоматически сохраняются, и после перезагрузки страницы таблица останется в том же виде.В MVP-версии данной опции не было, и приходилось прокручивать таблицу, чтобы найти 2-3 ключевые метрики. Настройка отображаемых колонокВ правом верхнем углу виджетаStatistics of methods(а также на других подобных виджетах) находится кнопка с иконкой шестеренки. Нажав на неё, можно выбрать, какие колонки таблицы отображать. Это особенно удобно, если необходимо сфокусироваться на определенных метриках и скрыть остальные для лучшего восприятия. Выбранные настройки автоматически сохраняются, и после перезагрузки страницы таблица останется в том же виде. Выбранные настройки автоматически сохраняются, и после перезагрузки страницы таблица останется в том же виде. В MVP-версии данной опции не было, и приходилось прокручивать таблицу, чтобы найти 2-3 ключевые метрики. В MVP-версии данной опции не было, и приходилось прокручивать таблицу, чтобы найти 2-3 ключевые метрики.  Под таблицейStatistics of methodsрасположена аналогичная таблица, но с распределением значений по персентилям. Детализация данных по каждому методу Рядом с каждым методом в таблице находится кнопка с иконкой графика. При нажатии на нее открывается модальное окно с подробной информацией по методу: Вся информация из строки таблицы представлена в удобном виджете. Вся информация из строки таблицы представлена в удобном виджете. Внутри модального окна также отображаются различные графики:RPS,Время ответа,Количество запросов,Ошибкии т. д. Внутри модального окна также отображаются различные графики: RPS, RPS, Время ответа, Время ответа, Количество запросов, Количество запросов, Ошибкии т. д. Ошибкии т. д. Этот функционал особенно полезен при анализе проблем с производительностью: Позволяет определить, какой метод оказал наибольшее влияние на просадку производительности. Позволяет определить, какой метод оказал наибольшее влияние на просадку производительности. Помогает отследить, в какой момент времени началось снижение показателей и проанализировать динамику. Помогает отследить, в какой момент времени началось снижение показателей и проанализировать динамику. Виджет с ошибками Если в процессе тестирования возникли ошибки, они отображаются в отдельном виджете. Графики с агрегированными данными Далее идут несколько графиков, отображающих динамику ключевых показателей: Total requests per second— изменение общего количества запросов в секунду по всем методам (включая количество ошибок). Total requests per second— изменение общего количества запросов в секунду по всем методам (включая количество ошибок). Total number of requests— динамика общего количества запросов по всем методам (включая ошибки). Total number of requests— динамика общего количества запросов по всем методам (включая ошибки). Response times (ms)— динамика времени ответа с несколькими метриками:min response time,max response time,average response time,median response time. Response times (ms)— динамика времени ответа с несколькими метриками: min response time, min response time, max response time, max response time, average response time, average response time, median response time. median response time. Number of users— изменение количества виртуальных пользователей во времени. Number of users— изменение количества виртуальных пользователей во времени. Эти графики уже встречались в деталях методов, но здесь они отображаютагрегированные данные по всем методам. Если просуммировать/усреднить значения из результатов отдельных методов, получатся именно те графики, которые приведены в деталях результата. Если просуммировать/усреднить значения из результатов отдельных методов, получатся именно те графики, которые приведены в деталях результата. Настройки отображаемых метрик на графиках В правом верхнем углу каждого графика находится кнопкашестеренки, открывающая меню настройки отображаемых метрик. Можно отключать ненужные метрики, чтобы сфокусироваться на анализе конкретных данных. Можно отключать ненужные метрики, чтобы сфокусироваться на анализе конкретных данных. Выбранные настройки сохраняются даже после перезагрузки страницы. Выбранные настройки сохраняются даже после перезагрузки страницы. Ранее в MVP-версии этого функционала не было, что создавало неудобства. Например: Если одна метрика имела очень большие значения, а другая была значительно меньше, то их было сложно анализировать на одном графике. Если одна метрика имела очень большие значения, а другая была значительно меньше, то их было сложно анализировать на одном графике. Особенно это проявлялось при анализе ошибок:Если ошибок было мало, то график выглядел \"плоским\", и их динамику можно было увидеть только по отдельным точкам. Особенно это проявлялось при анализе ошибок: Если ошибок было мало, то график выглядел \"плоским\", и их динамику можно было увидеть только по отдельным точкам. Если ошибок было мало, то график выглядел \"плоским\", и их динамику можно было увидеть только по отдельным точкам. Zoom и таймлайн Под каждым графиком расположена шкалаtimeline, позволяющая делать \"zoom\" и выделять конкретный временной интервал для детального анализа. В MVP-версии этой опции не было. В MVP-версии этой опции не было. Сейчас слайдерzoomдобавлен практически на все графики. Сейчас слайдерzoomдобавлен практически на все графики. Виджет распределения нагрузки по методам Самый последний виджет на странице отображаетраспределение нагрузки по методам. Этот виджетостался без измененийс MVP-версии. Этот виджетостался без измененийс MVP-версии. Сравнения Система предоставляетширокие возможности сравнения результатов нагрузочного тестирования. Доступны несколько типов сравнений: Позволяет анализировать разницу между несколькими тестовыми запусками. Например, можно сравнитьрезультат Aсрезультатами B, C и D, чтобы выявить изменения в производительности. Этот тип сравнения позволяет сопоставитьконкретный результат с усредненными показателямиза определенный период времени. Агрегация средних значений помогает выявитьтенденции и отклоненияот нормы. Агрегация средних значений помогает выявитьтенденции и отклоненияот нормы. Полезно прирегулярном мониторинге производительности, чтобы понять, насколько текущий запуск соответствует общим трендам. Полезно прирегулярном мониторинге производительности, чтобы понять, насколько текущий запуск соответствует общим трендам. Используется, когда необходимо проверить соответствие теста заданным требованиям. Например, в сценарии указано, что методGetUserдолжен отвечатьне дольше 1 секунды. Например, в сценарии указано, что методGetUserдолжен отвечатьне дольше 1 секунды. В настройках сценария можно задатьфиксированные/ожидаемые параметры. В настройках сценария можно задатьфиксированные/ожидаемые параметры. Затем система позволитсравнить фактический результат с ожидаемыми значениями. Затем система позволитсравнить фактический результат с ожидаемыми значениями. Помогает понять,насколько в среднем результаты тестов соответствуют заданным требованиям. Полезно при анализеобщей картины нагрузки. Полезно при анализеобщей картины нагрузки. Например, если один из тестовнеожиданно выбиваетсяиз ожидаемых значений (из-за временных сбоев), нов среднемрезультаты стабильны, это помогает избежать ложных тревог. Например, если один из тестовнеожиданно выбиваетсяиз ожидаемых значений (из-за временных сбоев), нов среднемрезультаты стабильны, это помогает избежать ложных тревог. Также этот вид сравнения подходит дляоценки новой функциональности— если для новой фичи проведено10 запусков, можно проверить, насколько средний результат соответствует ожидаемым параметрам. Также этот вид сравнения подходит дляоценки новой функциональности— если для новой фичи проведено10 запусков, можно проверить, насколько средний результат соответствует ожидаемым параметрам. Позволяет оценить,укладывается ли конкретный метод в ожидаемые значениясценария. Если в тестемного разных методов, можно проверить,какие из них соответствуют требованиям, а какие выходят за рамки нормы. Если в тестемного разных методов, можно проверить,какие из них соответствуют требованиям, а какие выходят за рамки нормы. Этот функционал дает возможностьгибкого анализа результатов, выявлениятенденций и аномалий, а такжеконтроля за соответствием ожидаемым параметрам. Сравнение одного результата с несколькими другими Разберем процесссравнения одного результата с несколькими другимипошагово. Сначала выбираемрезультат, который хотим сравнить, ипереходим в его детали.В меню сравнений выбираем\"Show comparison with results\".После этого открывается страница сравнения, гдепо умолчаниювыбираетсяпредыдущий результат, но можно добавить любые другие результаты для анализа. На виджете отображаетсяподробное сравнение текущего результата с выбранными для сравнения. Каждая метрика анализируется отдельно, рассчитываетсяобщий процент отклоненияс учетомвесов метрик. Каждая метрика анализируется отдельно, рассчитываетсяобщий процент отклоненияс учетомвесов метрик. Первый виджетпоказываетсредние и суммарные значения, ниже идет сравнениепо методам. Первый виджетпоказываетсредние и суммарные значения, ниже идет сравнениепо методам. Для методов также рассчитываетсяобщий процент сравненияс применением весов. Для методов также рассчитываетсяобщий процент сравненияс применением весов. Этот виджет показываетсравнение текущего результата со средними значениямивсех выбранных результатов. Зачем это нужно? Пример:Вы тестируете новую фичу и провели10 запусковнагрузочных тестов.Выбираетеодин эталонный результати сравниваете его с остальными, чтобыоценить стабильность производительности. Еслиодин из запусков аномальный(например, были проблемы с инфраструктурой: \"моргнуло электричество\" или появился \"шумный сосед\" наноде), важно видеть, чтов среднемрезультатукладывается в требования. Виджет\"Average summary\"помогает выявить подобные ситуации иотделить реальные проблемы сервиса от инфраструктурных факторов. Для каждой таблицы сравнения естьгибкая настройка метрик. В правом верхнем углу виджета естьшестеренка, которая открывает меню выбора метрик. В правом верхнем углу виджета естьшестеренка, которая открывает меню выбора метрик. Можноскрыть ненужные показателииоставить только важные, например,RPS и среднее время ответа. Можноскрыть ненужные показателииоставить только важные, например,RPS и среднее время ответа. Выбранные настройкиавтоматически сохраняются. Выбранные настройкиавтоматически сохраняются. Рядом с каждым методом в таблице естькнопка с иконкой графика. При нажатииоткрывается модальное окно, в котором отображаетсясравнение данного метода с выбранным результатом. При нажатииоткрывается модальное окно, в котором отображаетсясравнение данного метода с выбранным результатом. В модальном окне метода отображаютсяналоженные друг на друга графикидля текущего и сравниваемого результата. Важно: Всегда отображаются только два результата– текущий и выбранный для сравнения. Всегда отображаются только два результата– текущий и выбранный для сравнения. Даже если выбрано10 результатов, здесь будуттолько два. Даже если выбрано10 результатов, здесь будуттолько два. Это позволяетсфокусироваться на сравнении динамики конкретного метода, например: Выявить моментпросадки RPS. Выявить моментпросадки RPS. Понять,в какое время начались проблемы. Понять,в какое время начались проблемы. Оценитьповедение метода в динамике. Оценитьповедение метода в динамике. Если необходимосравнить метод сразу по всем выбранным результатам, нужно нажатьту же кнопку графика, но уже навиджете \"Average summary\". нужно нажатьту же кнопку графика, но уже навиджете \"Average summary\". Откроетсямодальное окно с агрегацией данных, где все результатыбудут наложены на один график. Откроетсямодальное окно с агрегацией данных, где все результатыбудут наложены на один график. Ниже на странице отображаютсяотдельные графики для каждой метрики. Каждый графикпоказываеттекущий результат + выбранные для сравнения. Каждый графикпоказываеттекущий результат + выбранные для сравнения. Графикинакладываются друг на друга, что помогаетвизуально оценить динамику. Графикинакладываются друг на друга, что помогаетвизуально оценить динамику. На виджете\"Compare charts\"можно выбрать,какие графики отображать. Настройки сохраняются, что позволяетгибко настроить отображение данныхпод конкретную задачу. Настройки сохраняются, что позволяетгибко настроить отображение данныхпод конкретную задачу. Сравнение результата со средними значениями Рассмотримфункционал сравнения текущего результата со средними значениями. На экране отображаютсявсе те же виджеты, что и при обычном сравнении результатов,ноосновное отличие– мыне выбираем конкретные результаты для сравнения. Средние значения рассчитываютсяавтоматическипо всем результатам для конкретного сервиса. Логика расчета средних значений зависит от выбора сценария: Сценарий выбран→ средние значения считаютсяпо результатам нагрузочного тестирования (НТ) только для этого сценария. Сценарий выбран→ средние значения считаютсяпо результатам нагрузочного тестирования (НТ) только для этого сценария. Сценарий не выбран→ средние значения рассчитываютсяпо всем результатам НТ для данного сервиса, независимо от сценариев. Сценарий не выбран→ средние значения рассчитываютсяпо всем результатам НТ для данного сервиса, независимо от сценариев. Это позволяетгибко анализировать среднюю производительность,какв рамках одного сценария, так ипо всему сервису в целом. Сравнение результата со сценарием В отличие от сравнения со средними значениями, которыемогут меняться и деградировать,данный тип сравнения опирается на фиксированные значения, заданные для сценария. Ожидаемые метрикиможно задать прямо со страницы сравнения: Нажав нашестеренкув правом верхнем углу панели управления Нажав нашестеренкув правом верхнем углу панели управления Или через виджет самого сценария Или через виджет самого сценария В настройках указываеможидаемые значения метрик.После сохранения, сравнениеавтоматическибудет выполняться по заданным параметрам. Самвиджет сравненияне отличаетсяот других виджетов,новместо динамических значений берутся фиксированные метрики из сценария. Данный функционалнезаменим для контроля производительности,особенно если у системы естьчеткие требованияк метрикам. Пример: У нас есть требование:\"Лента операций в мобильном приложении должна загружаться не более чем за 3 секунды\" У нас есть требование:\"Лента операций в мобильном приложении должна загружаться не более чем за 3 секунды\" Анализируем,какие методы вызываются на этой странице Анализируем,какие методы вызываются на этой странице Определяеможидаемое время откликадля каждого метода Определяеможидаемое время откликадля каждого метода Задаем значения в сценарии,фиксируем производительностьи отслеживаем отклонения Задаем значения в сценарии,фиксируем производительностьи отслеживаем отклонения Сравнение средних значений со сценарием Виджетсравнения средних значений со сценариемдоступен на странице\"Dashboard\", если выбран сценарий. Этот виджетоценивает соответствие средних значений всех результатов ожидаемым значениям сценария. Логика работы: Если сценарий выбран→ Средние значения рассчитываютсятолько для результатов этого сценария Если сценарий выбран→ Средние значения рассчитываютсятолько для результатов этого сценария Если сценарий не выбран→ Средние значения считаютсяпо всем результатам нагрузочного тестирования сервиса Если сценарий не выбран→ Средние значения считаютсяпо всем результатам нагрузочного тестирования сервиса Размещение данного виджетана \"Dashboard\" особенно удачно,посколькусразу при входеможно увидеть,укладываются ли средние показатели в ожидаемые значения. Сравнение метода со сценарием Этот виджет находитсяна странице деталей метода. Что такое детали метода?Вload-testing-hubавтоматически собирается аналитика по методам на основе результатов нагрузочного тестирования (НТ).Все методы отображаются на странице\"Methods\", где можно просмотретьсредние значенияпо каждому из них.Подробнее об этом в разделе\"Методы\". На страницедеталей методаотображаетсявиджет сравнениясредних значений методасожидаемыми значениями из сценария. В настройках сценария можно задать: Ожидаемые средние значения Ожидаемые средние значения Ожидаемые значения для каждого метода в отдельности Ожидаемые значения для каждого метода в отдельности Итог Функционалсравнения результатовзначительнорасширилсяпо сравнению с MVP-версиейload-testing-hub: Больше опций настройки Больше опций настройки Гибкость в выборе параметров Гибкость в выборе параметров Объективная оценка производительности Объективная оценка производительности Индивидуальные веса метрик Индивидуальные веса метрик Больше типов сравнений результатов Больше типов сравнений результатов Аналитика Страница\"Dashboard\"предоставляет аналитическую информацию, позволяя оценитьобщие показатели нагрузочного тестирования (НТ)за определенный промежуток времени.Одна из ключевых задачload-testing-hub—агрегация данных из НТи ихвизуализация. Основные виджеты Средние значения по всем результатам НТПервый виджет отображаетсредние значения всех ключевых метрикза выбранный период. Графики динамики изменений средних значений Каждая точка на графикепредставляет отдельный результат НТ, что позволяет отслеживатьизменения во времени: Total requests per second(Общее количество запросов в секунду)Total requests per second Total requests per second(Общее количество запросов в секунду) Total requests(Общее количество запросов)Total requests Total requests(Общее количество запросов) Response times (ms)(Время отклика)Response times (ms) Response times (ms)(Время отклика) Percentiles (ms)(Перцентили)Percentiles (ms) Percentiles (ms)(Перцентили) Графики распределения метрик по методам Эти графики помогают понять,на какие методы приходится больше всего ошибокили какраспределяются нагрузки.Настройки графиков можногибко менять, и онисохраняются. Фильтрация данных Всеметрикиможнофильтровать по времени. Всеметрикиможнофильтровать по времени. По умолчанию установлен диапазон14 дней в прошлое и будущее, чтобы всегда показыватьактуальные данные. По умолчанию установлен диапазон14 дней в прошлое и будущее, чтобы всегда показыватьактуальные данные. Фильтравтоматически обновляется, избавляя от необходимости вручную изменять диапазон. Фильтравтоматически обновляется, избавляя от необходимости вручную изменять диапазон. Сценарии и сравнительный анализ Виджет сравнения средних значений со сценарием Отображаетсятолько если выбран сценарийв настройках. Отображаетсятолько если выбран сценарийв настройках. Позволяет анализировать данныеисключительно в рамках конкретного сценария(например, при нагрузке новой фичи). Позволяет анализировать данныеисключительно в рамках конкретного сценария(например, при нагрузке новой фичи). Сравнение ожидаемых и фактических значений метрик В этом виджете сравниваютсяожидаемые метрики сценариясфактическими средними значениями. В этом виджете сравниваютсяожидаемые метрики сценариясфактическими средними значениями. Отображаетсятолько если выбран сценарий. Отображаетсятолько если выбран сценарий. Что изменилось по сравнению с MVP? Больше данных:теперь отображаютсяперцентили, медиана времени откликаибольше метрик по методам. Больше данных:теперь отображаютсяперцентили, медиана времени откликаибольше метрик по методам. Гибкие настройки графиков:можновыбирать, какие метрики отображать. Гибкие настройки графиков:можновыбирать, какие метрики отображать. Удобство анализа:если на одном графике отображаетсяMin response time = 10msиMax response time = 3050ms,то разница слишком большая, иmin response time будет сложно увидеть.Вместо созданиямножества графиков, добавленавозможность гибкой настройки отображаемых данных. Удобство анализа:если на одном графике отображаетсяMin response time = 10msиMax response time = 3050ms,то разница слишком большая, иmin response time будет сложно увидеть.Вместо созданиямножества графиков, добавленавозможность гибкой настройки отображаемых данных. Методы На странице\"Methods\"отображается список методов, автоматически агрегируемых из результатов нагрузочного тестирования (НТ). Основные задачи этой страницы — ответить на следующие вопросы: Какие именно методы подвергаются нагрузке? Какие именно методы подвергаются нагрузке? Какую нагрузку каждый метод выдерживает и сколько ошибок приходится на каждый метод? Какую нагрузку каждый метод выдерживает и сколько ошибок приходится на каждый метод? Страница с деталями метода позволяет посмотреть графики с историей по метрикам метода, наблюдать динамику изменений, таких как рост или падениеRPS(requests per second). Это помогает понять, какой именно метод может быть причиной ухудшения производительности или перегрузки системы. Все данные, отображаемые на странице, являются средними значениями, рассчитанными на основе нагрузочных запусков за определенный промежуток времени. Данные о методах можнофильтровать по датеи/илипо названию метода. Это полезно, если нужно изучить средние показатели для конкретного метода за определенный промежуток времени. При переходе вдетали методаможно увидеть более подробную информацию по всем меткам и показателям. Если выбран сценарий в настройках, то на странице будет отображаться виджет, который сравниваетсредние значения методасожидаемыми значениями, заданными в настройках сценария. Ниже отображаются графики, показывающие данные по методу для каждого результата нагрузочного теста за определенный промежуток времени. Эти графики позволяют визуализировать динамику метрик по методу и оценивать улучшения или деградацию производительности. Всего предусмотрено несколько типов графиков: Percentiles (ms)Percentiles (ms) of method Percentiles (ms) Total requests per secondTotal requests per second of method Total requests per second Total requestsTotal requests of method Total requests Response times (ms)Response times of method Response times (ms) Сценарии На странице\"Scenarios\"отображается список сценариев для выбранного сервиса. Рассмотрим, что подразумевается подсценарием. Например, у нас есть мобильное приложение, и мы проводим нагрузочное тестирование методов, вызываемых на главном экране. Это будет один сценарий, назовем егоmain screen scenario. Если же мы тестируем методы, вызываемые на странице с лентой операций, это уже другой сценарий —operations screen scenario. Таким образом, у нас есть два различных сценария:main screen scenarioиoperations screen scenario. Это важно, потому чтоанализировать статистикудля каждого сценария стоитиндивидуально, так как нагрузка, количество данных и другие факторы могут сильно отличаться. Вload-testing-hubпредусмотрены обе опции: Анализ результатов для конкретного сценария Анализ результатов для конкретного сценария Анализ результатов в целом для сервиса или всех сценариев Анализ результатов в целом для сервиса или всех сценариев Также стоит учесть, что у каждого сценария может быть своя версия. Например, мы запускаем сценарийoperations screen scenarioдля1000 виртуальных пользователей(версия сценарияv1.0), а потом — для5000 виртуальных пользователей(версия сценарияv2.0). Таким образом, один и тот же сценарий может выполняться сразной нагрузкойна сервис. Для корректной аналитики важно анализировать результаты в рамках конкретной версии сценария. Другой пример: у нас есть тот же сценарийoperations screen scenario, и мы запускали его на1500 виртуальных пользователей. Потом что-то изменилось, и теперь нужно запускать его на3000 виртуальных пользователей. Важно понимать, что сравнивать результаты для1500и3000пользователей «на равных» неправильно. Поэтому можно создатьновую версию сценария(например,v1.1) и анализировать статистику отдельно для3000 виртуальных пользователей, что будет корректным подходом. Это также предусмотрено вload-testing-hub. Настройки сценария Важный момент:не стоит грузить один метод или эндпоинт\"до посинения\", поскольку в реальной эксплуатации пользователь не делает одно и то же действие на экране бесконечно.Нагрузочное тестированиедолжно моделировать реальные сценарии использования. Речь идет о нагрузочном тестировании, не путать сстресс-тестированиемитестированием отказоустойчивости, где действительно имеет смысл нагружать один эндпоинт до отказа системы. В сценарии есть показательratio(распределение), который отражаетпроцент нагрузкина каждый метод или эндпоинт. Этот показатель помогает понять, какой метод несет наибольшую нагрузку в сценарии. Теги сценариев Сценарии можно помечать тегами для удобства. Например, сценарий может быть помечен как: LEGACY— для устаревших сценариев LEGACY— для устаревших сценариев LATEST— для последней актуальной версии сценария LATEST— для последней актуальной версии сценария EXPERIMENT— если сценарий был создан для проверки гипотезы или эксперимента EXPERIMENT— если сценарий был создан для проверки гипотезы или эксперимента Теги имеют исключительноинформационный характер, предоставляя метаинформацию о сценарии. Удаление сценариев Сценарий можноудалить, если он больше не актуален. Однако важно, что вload-testing-hubсценарий и все связанные с ним данные не удаляются окончательно — они простоскрываютсяиз статистики, что позволяет сохранить историю данных для анализа в будущем. Настройка ожидаемых значений для сценария Для каждого сценария можно задатьожидаемые значения метрик. Эти значения будут использоваться при сравнении метрик результатов с метриками сценария. Это особенно полезно, когда естьконкретные требования к производительностисервиса, и необходимо отслеживать, насколько текущие результаты соответствуют ожидаемым. Ниже представлен скриншот, где можно задатьсредние ожидаемые значениядля сценария. Кроме того, можно задатьожидаемые значения для каждого метода. Это позволяет сравнивать метрики конкретного метода с ожидаемыми значениями из сценария. Список доступных методов собираетсяавтоматически, поэтому не нужно вводить их вручную. Интеграции На странице \"Integrations\" отображается список интеграций. Поясню, зачем это нужно. Каждая сущность интеграции содержит необходимые поля для построения ссылки на дашборд вGrafanaилиKibana. Это позволяет не только просматривать результаты нагрузочного тестирования, но и анализировать, как вел себя сервис в момент нагрузки. В ссылку автоматически подставляются фильтры по времени и хост сервиса для Grafana/Kibana, что дает возможность отслеживать, как использовались память, диск и CPU в процессе нагрузки. Аналогично, можно анализировать логи в Kibana. При создании интеграции пользователю показывается информационный алерт, который объясняет, как работают интеграции и как правильно прописывать шаблон ссылки, на основе которого будет формироваться финальная ссылка. Давайте кратко объясню, как это работает. В поле URL template можно ввести любую ссылку, при этом можно использовать плейсхолдеры для подстановки переменных. В настоящее время доступны следующие плейсхолдеры: {host}— хост, который будет взят из настроек сервера и подставлен в ссылку в зависимости от типа интеграции. {host}— хост, который будет взят из настроек сервера и подставлен в ссылку в зависимости от типа интеграции. {time_from}— время начала нагрузочного теста. Это позволяет ограничить фильтрацию данных в Grafana/Kibana по времени начала теста. {time_from}— время начала нагрузочного теста. Это позволяет ограничить фильтрацию данных в Grafana/Kibana по времени начала теста. {time_to}— время окончания нагрузочного теста, аналогично работает для времени окончания теста. {time_to}— время окончания нагрузочного теста, аналогично работает для времени окончания теста. Все эти плейсхолдеры являются опциональными. При необходимости можно просто захардкодить ссылку, которая будет открываться из деталей результата. Интеграции привязаны к конкретному сервису, что позволяет настраивать их индивидуально для каждого сервиса. Например, если у нас есть сервисgateway-service, который взаимодействует сaccount-service, а тот, в свою очередь, сuser-service, то при нагрузочном тестировании сервисаgateway-serviceнам будет интересно наблюдать за сервисамиgateway-service,account-serviceиuser-service. Однако, при нагрузочном тестировании толькоaccount-service, нас не будет интересовать поведениеgateway-service, а толькоaccount-serviceиuser-service. В сервисеload-testing-hubможно настроить интеграции таким образом, чтобы видеть данные только по тем сервисам, которые нас интересуют. Кроме того, можно настроить интеграции с другими дашбордами. Например, если у DevOps-команды есть дашборд в Grafana, отображающий состояние базы данных, его можно добавить в интеграции и просматривать данные о состоянии базы данных в момент нагрузочного теста. После настройки интеграций они отображаются на странице деталей результата. При нажатии на любую интеграцию в списке откроется соответствующая ссылка, указанная при настройке. Настройки Общие настройки Общие настройки сервисаload-testing-hubможно открыть, нажав на значок шестеренки в правом верхнем углу апп-бара. Эти настройки являются базовыми и применяются ко всей системе, например, выбор темной или светлой темы. Настройки сервиса Следующий тип настроек относится непосредственно к нагружаемому сервису. Общие настройки сервиса позволяют редактировать информацию о сервисе, такую как название, ссылка на репозиторий, неймспейс и другие параметры. Далее идут настройки весов метрик для сравнений. О весах метрик уже упоминалось ранее, но повторю, что для каждой метрики можно задать свой вес, или, по-другому, приоритет. Сумма всех весов метрик не может превышать 1 (или 100%). Это означает, что мы указываем, какой процент составляет каждая метрика при расчете общего процента сравнения. Мы также можем сфокусироваться только на одной метрике, выставив ей вес 1, а для остальных метрик установить вес 0. Все метрики сгруппированы по их логическому значению. Например, все метрики, связанные с временем ответа, находятся в блокеResponse times. Далее идут настройки, которые отвечают за подсвечивание отклонений при сравнении результатов. Объясню на примере. Допустим, мы нагружаем сервис, и всегда присутствует погрешность в сравнении результатов, обычно она составляет +-20-30%, в зависимости от инфраструктуры и выделяемых ресурсов. Например, если сервис работает в облаке сKubernetes, где каждыйподконкурирует за ресурсы, погрешность может быть большой. Если же сервис работает на отдельной виртуальной машине с выделенным кластером, погрешность будет меньше. В случае, когда разница в производительности значительная, необходимо иметь индикатор, который подсветит отклонение и укажет, что нужно обратить внимание на результат НТ. Для этого существуют настройки\"Compare highlight threshold\", где можно задать процент, после которого отклонение будет подсвечиваться. Этот процент можно настроить для каждого типа сравнения индивидуально. Если результат отклоняется от ожидаемых значений больше, чем задано в настройкахCompare highlight threshold, на лейбле, который отображает процент сравнения, будет дополнительно отображаться иконка, подчеркивающая отклонение от ожидаемой погрешности. Также на основе данных об отклонении можно настроить уведомления в Slack. Для этого достаточно использовать API сервисаload-testing-hubи сделать @mention ответственного лица. Загрузка результатов Также хотелось бы обсудить, каким образом данные из артефактов инструмента нагрузочного тестирования попадают в системуload-testing-hub. Для этого предусмотрен специальный API, с помощью которого можно загрузить в систему любые данные. Фактически использование сервисане ограниченокаким-либо языком или фреймворком. Любой современный фреймворк нагрузочного тестирования может экспортировать базовые метрики (RPS, среднее время отклика, динамику различных показателей в процессе нагрузки и т. д.) в удобный формат:CSV, JSON, Prometheus, Grafana, InfluxDB, Datadogи многие другие. ПодготовилпримердляPython+Locust, который можно найти в репозиторииload-tests-hub. По аналогии с ним можно написать скрипт для любого языка и фреймворка — достаточно получить данные из артефактов тестирования и привести их к формату, который принимает APIload-testing-hub. Если кратко, то необходимо отправить несколько запросов к API. Сейчас не буду останавливаться на этом подробно, так как детальная информация есть в репозиторииload-tests-hub, но перечислю основные API-эндпоинты: /api/v1/scenarios/{scenario_id}— обновляет данные сценария /api/v1/scenarios/{scenario_id}— обновляет данные сценария /api/v1/load-test-results— создаёт результат нагрузочного теста /api/v1/load-test-results— создаёт результат нагрузочного теста /api/v1/load-test-results-history— создаёт запись в истории нагрузочного теста /api/v1/load-test-results-history— создаёт запись в истории нагрузочного теста /api/v1/method-results— создаёт результат тестирования конкретного метода /api/v1/method-results— создаёт результат тестирования конкретного метода /api/v1/method-results-history— создаёт запись в истории тестирования метода /api/v1/method-results-history— создаёт запись в истории тестирования метода /api/v1/ratio-results— создаёт результат распределения нагрузки в тесте /api/v1/ratio-results— создаёт результат распределения нагрузки в тесте /api/v1/exception-results— создаёт результат обработки ошибок в нагрузочном тесте /api/v1/exception-results— создаёт результат обработки ошибок в нагрузочном тесте На этом, в целом, всё. Как видите, здесьнет ничего сложного— обычное взаимодействие сREST API. Важно отметить, чтовсе запросы, кроме создания результата нагрузочного теста, являются опциональными. Это означает, что если какие-то данныене загрузить, они простоне отобразятся— при этом системане сломается, не выдаст ошибки и не крашнется. На мой взгляд, это делаетload-testing-hubещё более гибким и удобным инструментом. Заключение За время работы надload-testing-hubбыло проведено множество улучшений. Итоговые доработки включают: Полностью переработанный функционал сравнений результатов, обеспечивающий более точный и удобный анализ. Полностью переработанный функционал сравнений результатов, обеспечивающий более точный и удобный анализ. Обновлённый набор собираемых и отображаемых метрикдля более глубокого понимания результатов нагрузочного тестирования. Обновлённый набор собираемых и отображаемых метрикдля более глубокого понимания результатов нагрузочного тестирования. Улучшенные графики и аналитика:Возможность настройки отображаемых метрик.Поддержкаzoomдля детального изучения графиков. Улучшенные графики и аналитика: Возможность настройки отображаемых метрик. Возможность настройки отображаемых метрик. Поддержкаzoomдля детального изучения графиков. Поддержкаzoomдля детального изучения графиков. Добавлена детализированная информация по каждому тестируемому методу, что помогает точнее анализировать их поведение под нагрузкой. Добавлена детализированная информация по каждому тестируемому методу, что помогает точнее анализировать их поведение под нагрузкой. Внедрена подсветка проблемных результатов, отклоняющихся от допустимой погрешности, что упрощает выявление аномалий. Внедрена подсветка проблемных результатов, отклоняющихся от допустимой погрешности, что упрощает выявление аномалий. Реализована возможность удаления нагрузочных результатов, обеспечивая удобное управление историей тестов. Реализована возможность удаления нагрузочных результатов, обеспечивая удобное управление историей тестов. Добавлен удобный интерфейс для управления сервисами и сценариями:Создание, удаление и обновление напрямую из UI. Добавлен удобный интерфейс для управления сервисами и сценариями: Создание, удаление и обновление напрямую из UI. Создание, удаление и обновление напрямую из UI. Существенные оптимизации на стороне серверадля повышения производительности и масштабируемости. Существенные оптимизации на стороне серверадля повышения производительности и масштабируемости. Добавлен отдельный блок для настройки интеграций, что упрощает подключение внешних инструментов. Добавлен отдельный блок для настройки интеграций, что упрощает подключение внешних инструментов. Весь исходный код сервиса доступен на моем GitHub: load-testing-hub-api— серверная часть, написанная на Python + FastAPI, предоставляющая API для клиента. load-testing-hub-api— серверная часть, написанная на Python + FastAPI, предоставляющая API для клиента. load-testing-hub-panel— UI часть, написанная на TypeScript + React. load-testing-hub-panel— UI часть, написанная на TypeScript + React. load-tests-hub— пример загрузки результатов из нагрузочных тестов в сервисload-testing-hubдляLocust+Python. Однако использование сервиса не ограничивается каким-либо фреймворком для нагрузочного тестирования или языком программирования — его можно использовать с любым инструментом и любым языком. load-tests-hub— пример загрузки результатов из нагрузочных тестов в сервисload-testing-hubдляLocust+Python. Однако использование сервиса не ограничивается каким-либо фреймворком для нагрузочного тестирования или языком программирования — его можно использовать с любым инструментом и любым языком. Буду рад ответить на любые вопросы по сервисуload-testing-hub. "
  },
  {
    "article_id": "https://habr.com/ru/companies/cloud_ru/articles/877762/",
    "title": "«Я делаю рефакторинг ежечасно» или как за пять минут улучшить приложение",
    "category": "Программирование",
    "tags": "рефакторинг, мартин фаулер, рефакторинг за 5 минут",
    "text": "История этой статьи началась с того, что я вспомнил о довольно известном высказывании Мартина Фаулера, автора книг и статей по архитектуре ПО, которое нередко вызывает недопонимание (во всяком случае так было у меня) — «Я делаю рефакторинг ежечасно». Первая мысль, которая логично возникает после этого высказывания — уважаемый публицист просто лукавит. Вторая — что, наверное, кроме рефакторинга он в своей жизни  ничем больше не занимается. Но так ли это? С вами в очередной раз Костя Логиновских, ведущий разработчик и технический лидер внутреннего проектав Cloud.ru. В этой статье предлагаю во всем разобраться и обсудить, как можно делать рефакторинг «за пять минут» и улучшить приложение буквально за утренним кофе. Чтобы устранить разночтения, сначала определим, что считать рефакторингом. Есть очень простое определение: вы рефакторите, если вы меняете код, но не меняете функциональность приложения. Зачем менять код, если не меняется функциональность? Причин может быть много: совершенствование архитектуры (бизнес-требования меняются и иногда под них нужно подстраивать систему), желание найти ошибки, ускорить разработку или упростить чтение программы. Некоторые из этих поинтов могут показаться спорными — например, что значит «улучшает читаемость»? Можно ли читаемость как-то измерить? Оказывается, можно! Как измерить читаемость кода Читаемость кода можно приравнять к «когнитивной сложности кода», которая включает нескольких аспектов: использование редких конструкций—когда разработчику очень сложно вспомнить, как именно работают примененные конструкции кода (также известные как «ниндзя-код»); использование редких конструкций—когда разработчику очень сложно вспомнить, как именно работают примененные конструкции кода (также известные как «ниндзя-код»); использование «оперативной памяти программиста»— когда разработчика заставляют держать в голове конструкции, которые необходимы для понимания кода (например, когда название переменной не позволяет точно определить, что именно в ней находится); использование «оперативной памяти программиста»— когда разработчика заставляют держать в голове конструкции, которые необходимы для понимания кода (например, когда название переменной не позволяет точно определить, что именно в ней находится); цикломатическая сложность— математическое измерение читаемости кода, на котором я остановлюсь немного подробнее. цикломатическая сложность— математическое измерение читаемости кода, на котором я остановлюсь немного подробнее. Если грубо, то цикломатическую сложность можно представить как количество всех циклов и ветвлений в коде — чем больше в вашей функцииif,forи им подобных, тем сложнее будет читать ваш код, даже если он идеальный во всем остальном. Сама цикломатическая сложность — не тема статьи, поэтому, если хотите разобраться в нем подробнее, естьотличная статьяна эту тему. Как улучшить проект за чашкой кофе Есть только один способ сделать что-либо быстрее — это сделать то, что нужно сделать, небольшим 🤪. Поэтому подходы, которые я предлагаю в статье, займут у вас минимум времени. Давайте перейдем к практике и начнем с самого простого. У этой функции много проблем, но сейчас мы сфокусируемся только на одной из них —большом количестве позиционных параметров. Если нужно будет поменять какой-то из параметров, то придется перекопать всю кодовую базу, найти все использования этой функции и везде всё изменить. Более того, кто пишет на TypeScript, тот знает, что означает эта маленькая черточка перед названием. Фильтр в данном примере уже не используется. То есть когда-то он стал опциональным, а потом стал вообще ненужным. Так мы логически приходим к тому, чтобы сделать вот такое изменение — поставить фигурные скобки вокруг параметров и вывести их в тип: Что мы таким образом получим?Гибкость изменений,уберем лишний код(тот самый фильтерс) ипотенциально ускорим разработку. И, когда нам в следующий раз в этом коде нужно будет что-то поменять (да, я говорю когда, а не если 🙂), мы это сделаем гораздо быстрее и проще. В этом примере у нас появилась очень большая функция (и давайте не будем задаваться вопросом, кто это наделал): У цикломатической сложности есть одно важное правило: если в коде есть цикломатическая сложность «а» и «б» — два блока, следующих друг за другом, то в сумме они будут давать приблизительно «а + б». Это не совсем точно, но примерно так. Соответственно, эта математика работает и в обратную сторону. Если вы из функции вырезаете какой-то блок с цикломатической сложностью, то вы уменьшаете всю функцию на эту сложность. Вынеся пару кусков кода в отдельные функции, мы можем значительно уменьшить суммарную цикломатическую сложность и, как следствие, значительно улучшить читаемость кода. Очень пафосное название, которое по сути означает очень простую вещь: постановка пробелов между строками — это тоже рефакторинг, и иногда очень даже эффективный. Мало того, что здесь очень многоif(мы можем бегло прикинуть, что сложность у этого кода достаточно большая), так еще и код невозможно прочитать. Выносить все эти параметры в отдельную функцию нецелесообразно. Поэтому в качестве рефакторинга можно банально добавить пробелы между строками. Это очень просто и настолько эффективно, что вы удивитесь, насколько станет легче 🙂. Код всё тот же, но, согласитесь, читать его значительно приятнее. Еще один простой способ, который подойдет для всех проектов старше трех лет — использовать системные функции вместо кастомных. Бывает, что спецификация языка создает для нас инструменты, которые раньше были недоступны, и мы можем встретить в коде нечто подобное: Часто у таких функций может быть накладка на поддержку и заменить ее на обычныйtoSortedбывает приятно и легко. Единственный из всех предложенных мной способов, который ухудшает читаемость, но при этом защищает нас от багов, особенно в сочетание с правилом@typescript-eslint/no-non-null-assertion: Этот восклицательный знак после выражения говорит о том, что мы, по неизвестной причине, уверены, что этотproductIdточно есть в этом объекте. Но почему? Может быть мы только что его запросили, может быть он запросился в соседнем файле или был добавлен на предыдущей строке? В любом случае TypeScript нам не верит — он говорит, что такого ID здесь может и не быть. И он прав — такого ID действительно рано или поздно не будет. Соответственно, мы должны прокинуть ошибку — сообщаем TypeScript, что готовы признать, что этогоproductIdможет и не быть: Так мы защищаем наше приложение и делаем его лучше буквально тремя строчками кода. Как убедить бизнес, что рефакторинг нужен: заключение Но у нас остался последний вопрос — как убедить бизнес в необходимости рефакторинга? Для этого вернемся к фразе, с которой я начал эту статью. Мартин Фаулер говорит, что делает рефакторинг ежечасно. Как? Всё просто — он делает простой рефакторинг, о чем, кстати, и пишет в своих книгах. И в этом вся соль — он ни о чем не сообщает бизнесу и ни с кем не договаривается, потому что вкладывает рефакторинг в свои задачи инкрементально. Он просто делает свой проект лучше. И давайте мы все будем делать свои проекты лучше каждый день, оставляя код после себя лучше, чем он был. Задавайте вопросы в комментариях — буду рад на все ответить. Но если захотите меня там поругать, то помните, что за моей статьей стоит Мартин Фаулер 😊. "
  },
  {
    "article_id": "https://habr.com/ru/articles/877698/",
    "title": "Log4ts — библиотека, которой не должно быть",
    "category": "Программирование",
    "tags": "logging, logs, log4j, angular, node.js",
    "text": "В этой статье я хочу рассказать о моей библиотеке, которой не должно существовать.Почему её не должно существовать? Потому что функциональность логирования, по моему глубокому убеждению, должна быть в числе первых включена в любой новый язык. А разработчики старых языков тоже должны об этом подумать и включить логирование, если это ещё не сделано, в ближайший релиз. БиблиотекаLog4tsвдохновлена идеями Log4J и обеспечивает логирование в программах, написанных на  TypeScript.Далее в этой статье я расскажу о том, как её установить, использовать и конфигурировать. А в конце я опишу коротенько другие мои библиотеки и функции, которые тоже не должны были бы существовать. А нужна ли она вообще? Современные IDE позволяют производить отладку (debug)  JavaScript- и TypeScript-приложений напрямую. Кроме того, в JavaScript и TypeScript доступен console с функциями error, warn, log, debug и т.д. Разве этого недостаточно? - возможно спросит читатель. Иногда - нет. Отладка очень искажает логику асинхронных приложений, каковыми и являются большинство приложений на  JavaScript и TypeScript. А использование console может быть достаточно при наличии большого статического контекста или в относительно простых приложениях. Но в приложениях со сложной логикой, а тем более в корпоративных приложениях, работающих как в браузере, так и на сервере, этого оказывается недостаточно. Почему? - Наивное использование функций console в таких приложениях ставит разработчика перед дилеммой. Если разработчик использовал их широко в своем коде, он в конечном итоге получит столько строк в своей консоли, что будет чрезвычайно трудно с ними разобраться. Если использование функций консоли минимизировано, их может оказаться недостаточно для нахождения некоторых проблем. Особенно, если речь идёт об удалённой поддержке пользователей. Решение этой проблемы давно найдено в других языках программирования, например, в Java,  где есть Log4J. И решение очевидно: вы должны иметь возможность управлять выводом функций логирования во время исполнения программы (runtime).Не найдя среди открытых библиотек такой, которая удовлетворяла бы этому требованию так, как я этого хотел, я и решил разработать свою. Обратите внимание! Важно отметить: библиотека предназначена прежде всего для работы в браузере. В серверных приложениях, в тяжеловесных приложениях на Node.JS, лучше использовать другие библиотеки логирования.Библиотека может быть полезна как разработчикам, так и работникам отдела поддержки. Типичный Use Case при разработке. Разработчик ищет ошибку и у него “под подозрением” находится какой-либо класс или пакет. Он настраивает логирование так, чтобы выводились сообщения только из этих источников. Типичный Use Case в production: Пользователь сообщает о проблеме с приложением. Сотрудник отдела поддержки решает, что проблема в клиентской части (которая работает в браузере). Он просит пользователя включить логирование, открыть вкладку консолли в браузере и повторить операцию, при которой происходит ошибка, скопировать вывод из консоли и послать в поддержку. И опять - инструктируя пользователя о настройках логирования, он может посоветовать ему включить логирование только “подозрительного” класса или пакета, сделав тем самым выдачу логирования обозримой. Как это может работать, можно посмотреть в нашемдемо-приложении. Откройте в браузере окно консоли, а в приложении зайдите настройки и в разделе “Выдача журнала” выберите режим логирования. Потом поработайте с поиском чисел.Важно отметить ещё одно обстоятельство. Хотя библиотека и вдохновлена идеями Log4J, она не повторяет её гигантские возможности, а является очень легковесной надстройкой над консолью браузера, позволяющей на лету управлять выводимой информацией. Как её установить? Если вы не новичок в веб-программировании, вы конечно знаете, что NPM (Node Package Manager) — это менеджер пакетов для JavaScript, который используется для установки, обновления и управления пакетами и библиотеками, а package.json — это файл конфигурации в веб- и простых Node.js проектах. Этот файл используется для управления зависимостями и автоматизации задач в проекте. Поэтому для того, чтобы начать использовать Log4ts в вашем проекте, вам просто необходимо в ваш package.json добавить одну строчку: Если ваш NPM-проект правильно сконфигурирован, при следующей компиляции и построении вашего проекта библиотека будет скачана в ваш проект. Как её использовать? Конечно, логгер нужно перед использованием создать. В нашем случае мы должны сделать это, используя интерфейс ILogger: Параметр в этом вызове означает идентификатор, по которому ваш логгер будет искаться среди его собратьев, созданных фабрикой. В больших приложениях рекомендуется (но не обязательно) использовать общую конвенцию именования: DOMEN.PROJECT.CLASS(COMPONENT). например: Как настроить в процессе разработки проекта? В отличие от аналогичных вызовов консоли, после создания наш логгер по умолчанию будет выводить только вызовы из error и warn, игнорируя остальные, например, log или debug. Это решение кажется мне более прагматичным, если в вашем проекте много классов с встроенным логированием. Но иногда, особенно на этапе разработки, вам нужно логировать не только информацию об ошибках и предупреждениях, но и другую информацию. Какую другую информацию? - Log4ts предоставляет следующие уровни логирования в зависимости от их серьезности: 0 или отрицательное число- выводится вся информация (ошибка, предупреждение, лог и отладка), 1- только ошибка, предупреждение и лог, 2- только ошибка и предупреждение (этот режим включен по умолчанию), 3- только ошибка, 4и более- все вызовы игнорируются. Чтобы изменить уровень логирования на лету, нужно вызвать функцию классаsetLogLevel(...).Например, чтобы включить вывод всех вызовов логирования: Однако вам не обязательно помнить, какое значение параметра чему соответствует.Для этого существуют удобные функции: setErrorLevel- выводит только ошибки. setErrorLevel- выводит только ошибки. setDefaultLevel- выводит ошибки и предупреждения. setDefaultLevel- выводит ошибки и предупреждения. setAllLevels- выводит все сообщения. setAllLevels- выводит все сообщения. setNoLogging- отключает все сообщения. setNoLogging- отключает все сообщения. Эти функции управляют поведением отдельного логгера.Статические функции фабрикиLoggerFactoryс похожими названиями (setErrorLevelByAllLoggers, setDefaultLevelByAllLoggers, setAllLevelsByAllLoggers, setNoLoggingByAllLoggers)  выполняют те же настройки, но сразу для всех созданных к этому моменту логгеров. Далее, используя текстовые фильтры, вы можете динамически переключать уровень логирования в своих классах. Например, приведенный ниже пример позволяет отключить логирование для всех логгеров с'com.example.my-project.M'в ID: Звездочки в строке поиска могут быть в начале или в конце шаблона поиска. Таблица ниже показывает различные виды искомых под-путей (перечисленных в первой строке таблицы) для трех идентификаторов, перечисленных в левой колонке таблицы. sub-path *b/c a* *b* e/b/c x/y/z * a/b/c + + + - - + a/d/c - + - - - + e/b/c + - + + - + В некоторых особых случаях, таких как отладка проекта, где используется несколько подходов к логированию, вы можете захотеть уничтожить все логгеры из Log4ts до конца сеанса. Это можно сделать, вызвав Как настроить во время применения (runtime, production)? Чтобы управлять логгерами во время исполнения (например, для целей поддержки конечных пользователей), вам нужно иметь возможность вызывать функции LoggerFactory из какого-то вашего UI компонентов. Вотдемо-приложение, которое показываетпримертакого UI компонента. Практические советы При создании библиотек и общих компонентов, учитывая, что логирование может выполняться из многих компонентов одновременно, я рекомендую для лучшей читаемости и понятности логов использовать классические соглашения об именах при создании логгеров: DOMAIN.PROJECT.CLASS(COMPONENT). В текущей реализации отображение адреса точки логирования неинформативно, так как вывод в конечном итоге выполняется путем вызова функций console. Поэтому я рекомендую явно локализировать точку логирования, указывая имя логируемой функции. Если ваша функция содержит несколько вызовов логгера с одинаковым уровнем, рекомендуется различать эти вызовы либо вводными текстами, либо с помощью некоторых номеров. Вот пример того, как создать логгер, учитывая соглашение об именах, указывая логируемую функцию и различая логгеры в пределах одной функции: Так что, пользуйтесь на здоровье! А я пока выполню своё обещание, данное в начале статьи. Другие мои библиотеки и функции, которых не должно быть Ошибки в работе с физическими единицами (когда метры складывают с футами или секунды с килограммами) приводили к, наверное, самым дорогим ошибкам в истории программирования. Тем более удивительно, что в составе популярных языках программирования нет средств работы с единицами системы СИ. Поскольку мне это однажды самому понадобилось, я разработал мультиплатформенную библиотекуKotUniL. Как её использовать в Kotlin я описал тут:1,2,3. Её также (правда, без элегантности, присущей Kotlin) наJavaи наJavaScript. Я так и не смог убедить котлинцев включить эту библиотеку в стандарт языка или разработать что-то своё (ещё лучше). В Java 8 в своё время добавили много хороших возможностей, но среди них не было классаOptional, который мне пришлось реализовать самому. Точно также в Java 8 не хватало мульти-арных  функций размерности больше двух. Так что пришлось реализовывать ихсамому. В Java мне с самого начала не хватало возможности красиво выводить на печать матрицы. В результате я написал классMatrixFormatter Мой сайт -https://www.sirotin.eu/ Кроме того, я пишу открытую электронную книгу “Мемуары кочевого программистаю Байки, были, думы”. Её текущий вариант можно найтиздесь. Я убеждён, что программирование - это материализаци я идей. Об этом я первый раз написалздесь. А вот уже несколько лет мы с группой единомышленников ведём группу в Телеграмме под названием “Материализация идей”. Если вам это интересно - подключайтесь. Иллюстрация: Предтеча программистских логов - бортовой журнал. Совместное творение автора и ИИ. "
  },
  {
    "article_id": "https://habr.com/ru/companies/piter/articles/873634/",
    "title": "Книга: «Эффективный TypeScript: 83 способа улучшить код. 2-е изд.»",
    "category": "Программирование",
    "tags": "книга",
    "text": "Оно полностью обновлено и переработано под TypeScript 5; добавлен еще 21 способ улучшить код; более подробно рассказано об условных типах, о которых в предыдущей версии книги только упоминалось; рассмотрены правила, касающиеся типов шаблонных литералов — одного из самых значительных нововведений в Typescript; добавлена глава, посвященная программированию на уровне типов; использовать вывод типов для обеспечения их безопасности при минимуме аннотаций к ним; создавать типы, чтобы писать понятный и стабильный код; моделировать сложные API, используя обобщенные типы (дженерики); работать с зависимостями и файлами объявления типов; переносить кодовую базу с JavaScript на TypeScript без потерь. Правило 19. Используйте разные переменные для разных типов разделяет два несвязанных понятия (идентификатор и серийный номер); позволяет использовать более конкретные имена переменных; повышает качество вывода типов, в связи с чем уже не требуется применять к ним аннотации; получаются более простые типы (stringиnumber, а неstring|number); позволяет объявлять переменные cconstвместо let, что упрощает их понимание как модулем проверки типов, так и людьми. СЛЕДУЕТ ЗАПОМНИТЬ: Значение переменной может меняться, но ее тип обычно остается неизменным. Во избежание путаницы — как для человека, так и для модуля проверки типов — избегайте повторного использования имен переменных для обозначения другого типа."
  },
  {
    "article_id": "https://habr.com/ru/articles/877622/",
    "title": "Обработка ошибок Axios",
    "category": "Программирование",
    "tags": "axios, typescript, javascript, обработка ошибок, react, react.js",
    "text": "Привет, Хабр, меня зовут Алёна, я senior фронтент-разработчик в отдела разработки ПО для розничного бизнеса в Райффайзенбанке. Недавно наша команда решила улучшить пользовательский опыт обработки ошибок запроса к бекенду и я решила комплексно исследовать эту тему и собрать воедино все лучшие практики. Начтем с того, что при обработке ошибок Axios запросов существуют 4 ситуации, которые необходимо по-разному интерпретировать: Запрос был обработан сервером и статус ответа сервера вне диапазона 2xx Запрос был обработан сервером и статус ответа сервера вне диапазона 2xx Запрос был сделан, но ответ не был получен. Запрос был сделан, но ответ не был получен. Ошибка возникла из-за неправильных настроек Axios. Ошибка возникла из-за неправильных настроек Axios. Ошибка не является инстансом Axios. Ошибка не является инстансом Axios. В документации Axios, почему-то указаны только первые 3 случая и не сказано, как именно их обрабатывать. На этом этапе наш код на TypeScript будет выглядеть так. Для удобства опишем его в статическом классе. Т.к. ошибок может быть несколько, то вернем promise c массивом ошибок. Это может быть список строк или, например, объектов. 1. Запрос был обработан сервером и статус ответа сервера вне диапазона 2xx Первым этапом необходимо попробовать распознать модель ошибки, которая пришла с бека. В нашем случае мы используем общий обработчик ошибок, прописанный в QueryClient из библиотеки react-query и при необходимости обрабатываем ошибки на уровне самого запроса. Например, если требуется показать ошибки определенным способом. Наш бекенд использует микро сервисную архитектуру, а так же взаимодействует с другими сервисами банка и поэтому модели ошибок могут быть разными и есть шанс, что придет еще не знакомая фронту структура данных. Если фронту не получилось распознать модель ошибки, которую бекенд прислал в response.data, то нужно поискать подходящее сообщение в списке статусов. Т.к. у нас описаны только самые часто встречающие статусы, то может возникнуть ситуация, когда фронтенд покажет общее сообщение для данного типа ошибок. Если описать все статусы, то этот пункт можно пропустить. Для удобства показа ошибок с учетом микро сервисной архитектуры, мы пытаемся определить имя сервиса на основе url и имя метода которые прописываем в каждом обработчике. Вспомогательный метод для определения имени сервиса: Константа SERVICE_NAMES может выглядеть следующим образом: В описании названий сервисов удобно пропустить слово \"сервис\", чтобы в описании ошибки можно было использовать его в разных падежах. Константу HTTP_ERROR_DESCRIPTIONS, так же, можно сделать типа Record: Так можно указать название метода. Слово \"метод\", так же, можно пропустить, чтобы в сообщениях его можно было склонять по падежам. Чтобы в TypeScript можно было использовать дополнительное свойство AxiosRequestConfig, необходимо, в проект добавить *.d.ts. А в tsconfig нужно прописать путь к нему: 2. Запрос был сделан, но ответ не был получен В данном случае ошибку можно обработать по коду. Константа ERROR_DESCRIPTIONS может быть описана следующим образом: Список кодов и их описание можно найти в документации Axios:https://github.com/axios/axios?tab=readme-ov-file#error-types. 3. Ошибка возникла из-за неправильных настроек Axios В данном случае можно вернуть сообщение, наподобие этого: 4. Ошибка не является инстансом Axios В этом случае можно вернуть следующее сообщение: Постобработка сообщений об ошибках Если в сообщениях об ошибках использовать названия сервисов и методов, то могут возникнуть ситуации, когда не удалось определить их и сообщения будут содержать лишние пробелы. Их можно удалить следующим образом: Если вы не используете сообщения названия методов и сервисов, то можете пропустить этот пункт Финальный вид метода обработки ошибок Axios Если подытожить, то метод getErrors может иметь следующую реализацию: Однако, основываясь на том, что при обработке ошибок существуют 4 перечисленные выше случая, вы можете иначе обработать их. Пишите в комментариях свои мысли по поводу обработки ошибок, задавайте вопросы. Буду рада на них ответить! Изображение на обложке статьи \"503 error service unavailable concept illustration\" с сайтаhttp://www.freepik.com."
  },
  {
    "article_id": "https://habr.com/ru/articles/877426/",
    "title": "Effect для TypeScript разработчиков",
    "category": "Программирование",
    "tags": "effect, typescript, typesafe",
    "text": "Обо мне Привет. Я разработчик и последние три года пишу наTypeScript, делаю свои Pet проекты, смотрю на технические тренды и пытаюсь делать полезные проекты и публиковать их в OpenSource. Сразу к делу Вот уже как год я пишу код и использую этот пакетEffect. Для меняEffectявляется швейцарским ножом который позволяет решать прикладные задачибезнаписания рутинного нооченьважногокода, такого как: работа с ошибками и восстановление из этого состояния (catch) работа с ошибками и восстановление из этого состояния (catch) кеширование результатов дорогих вычислительных операций кеширование результатов дорогих вычислительных операций параллельное выполнение процессов, остановка (interrupt) параллельное выполнение процессов, остановка (interrupt) работа с состоянием и безопасное изменение этого состояния работа с состоянием и безопасное изменение этого состояния создание схем типов данных, чтение/запись по схеме, и т.п. создание схем типов данных, чтение/запись по схеме, и т.п. и многое другое и многое другое Effectсостоит из большого количества небольших модулей и помогает писать код в декларативном стиле. Например в этом пакете можно найти следующие модули: Array Array HashMap HashMap String String и десятки других... и десятки других... Казалось бы зачем нуженArrayведь он есть в стандарте JS, но он нужен когда нужно создать такие типы какNonEmptyReadonlyArray, или другой  неизменяемыйArrayкоторый по сутиTuple Effectпозволяет мне, какTypeScriptразработчику, держать фокус на главной части проекта ине распылятьсяна чисто технические задачи никак не связанные сцельюпроекта. При чем тут эффекты? Effectэто не только швейцарский нож в котором есть все, остается просто найди нужную функцию 🤪. Effectэто ещеидеяпро то как можно и нужно писать программы, которые легко писать, читать, поддерживать. Если вкратце, то эффект это описание любого процесса, который может завершиться либоошибкойлиборезультатоми ему может понадобиться некийконтекствыполнения для того чтобы корректно отработать. Эффект этозначение, как например можно создатьconst foo = \"bar\"где foo будет содержатьзначение\"bar\". Например: Тут описывается функция, котораясоздаетэффект. Важно понимать, что когда функцияgetTodoвозвратила эффект то онане выполнитhttp запрос, она лишь вернет эффект, который умеет посылать http запрос и он может завершиться значением с типомunknownили завершиться с ошибкой типаHttpClientError. Так как эффект этозначението это открывает дополнительные возможности и можно легко создать другой эффект на базе этого который будет: восстанавливаться если была ошибка типаHttpClientError восстанавливаться если была ошибка типаHttpClientError попытаться повторно выполнить эффект несколько раз в случае определенной ошибки попытаться повторно выполнить эффект несколько раз в случае определенной ошибки кешировать результат кешировать результат множество других сценариев... множество других сценариев... Про то, что такое эффект можно на примерах найтитут. Effectсообщество пишет отличную документацию по многим пунктам. Пока на этом все Сорри что статья короткая. Я не нашел на хабре статьи которая рассказывала про эту мощную библиотеку. Цель этой статьи в том чтобы заинтересоватьTypeScriptразработчиков и попробовать использовать ее в своих задачах. Я могу подробнее написать в следующий раз статью про свой опыт работы сEffectв более подробной форме. У меня нет цели написать туториал или что то подобное, потому чтоEffectсообщество очень круто справляется с этой задачей Ссылки главный сайт youtube как работать с ожидаемыми/не ожидаемыми ошибками мои библиотеки которые используютEffect:https://github.com/orgs/effect-ak/repositories "
  },
  {
    "article_id": "https://habr.com/ru/companies/ruvds/articles/876916/",
    "title": "FizzBuzz, который не помог мне найти работу",
    "category": "Программирование",
    "tags": "fizzbuzz, техническое собеседование, задачи на собеседованиях, собеседования, рефакторинг, отладка, ruvds_перевод",
    "text": "У меня нет университетской степени и какого-то другого формального обучения. Им нужен был сениор-разработчик с четырьмя годами опыта. У меня было два. Им нужен был опыт работы с AWS. Я никогда с ним не работал. У меня плохое английское произношение, несмотря на то, что я умею читать, писать и понимать язык. ▍ Про вакансию и про меня"
  },
  {
    "article_id": "https://habr.com/ru/articles/876780/",
    "title": "Callback рефы в React: что это такое и где можно применять",
    "category": "Программирование",
    "tags": "React, JavaScript, TypeScript, callback refs",
    "text": "При разработке у нас зачастую возникает необходимость прямого взаимодействия с DOM-элементами. Для такого случая React предоставляет нам механизм рефов (refs), который позволяет получать доступ к элементам после того, как они зарендерятся. Чаще всего используются обычные объектные рефы через useRef (обзовём их так), но также существует другой подход — callback refs. Этот метод даёт нам дополнительную гибкость и контроль над жизненным циклом элементов, позволяя выполнять необходимые нам специфические действия в точные моменты привязки и отвязки элементов. В этой статье я хочу объяснить, что такое callback refs, как они работают, показать проблемы при их использовании и примеры их использования. Callback refs дают более тонкий контроль над привязкой рефов по сравнению с объектными рефами. Рассмотрим, как они работают на деле: Монтирование:Когда элемент монтируется в DOM, React вызывает функцию реф с самим DOM-элементом. Это позволяет вам выполнять действия с элементом сразу после его появления на странице. Монтирование:Когда элемент монтируется в DOM, React вызывает функцию реф с самим DOM-элементом. Это позволяет вам выполнять действия с элементом сразу после его появления на странице. Размонтирование:Когда элемент размонтируется, React вызывает функцию реф с null. Это даёт нам возможность очистить или отменить любые действия, связанные с элементом. Размонтирование:Когда элемент размонтируется, React вызывает функцию реф с null. Это даёт нам возможность очистить или отменить любые действия, связанные с элементом. Каждый раз, когда мы переключаем видимость элемента, функция handleRef вызывается с соответствующим аргументом (node или null), позволяя отслеживать момент привязки и отвязки элемента. Частые проблемы и решения Одна из частых проблем при использовании callback refs, это повторное создание функции рефа при каждом ре-рендере компонента. Из-за этого React думает, что у нас пришел новый реф, вызывает callback ref сначала с null, тем самым подчищая старый реф, а затем инициализирует новый, даже если сам наш элемент или компонент никак не изменились. В результате у нас могут возникнуть нежелаемые побочные эффекты. Пример проблемыРассмотрим компонент Basic, который содержит кнопку для переключения видимости div с callback ref и кнопку производящую форс апдейт компонента: Каждый раз при нажатии на кнопку Rerender, компонент перерисовывается, создавая новую функцию refCallback. Это приводит к вызову refCallback(null) и затем refCallback(node), не смотря на то, что наш элемент с рефом по сути никак не изменился. В консоли будут появляться сообщения с div и null поочерёдно снова и снова, чего мы конечно не хотели бы. Решение: Мемоизация callback ref с помощью useCallbackИзбежать это довольно легко, используйте useCallback для мемоизации функции. Это гарантирует, что функция останется неизменной между ре-рендерами, если её зависимости не изменились. Теперь функция refCallback создаётся только один раз, при первом рендере, и не будет вызываться лишний раз при последующих ре-рендерах. Это предотвратит не нужные вызовы и улучшит производительность компонента. Порядок вызова callback refs, useLayoutEffect и useEffect Перед тем как мы перейдём к тому, как использовать callback refs у нас в коде для решения проблем, давайте поймём, как callback refs взаимодействуют с хуками useEffect и useLayoutEffect, чтобы правильно организовать логику инициализации и очистки ресурсов. callback ref:Вызывается сразу после рендеринга DOM-элементов,довыполнения хуков эффекта. callback ref:Вызывается сразу после рендеринга DOM-элементов,довыполнения хуков эффекта. useLayoutEffect:Выполняется после всех изменений DOM,но до отрисовки. useLayoutEffect:Выполняется после всех изменений DOM,но до отрисовки. useEffect:Выполняется после отрисовки. useEffect:Выполняется после отрисовки. Вывод в консоль: \"Callback ref вызван для div: [div элемент]\" \"Callback ref вызван для div: [div элемент]\" \"useLayoutEffect вызван\" \"useLayoutEffect вызван\" \"useEffect вызван\" \"useEffect вызван\" Этот порядок показывает нам, что callback refs вызываются до хуков эффекта, что нужно учитывать при написании кода. Какие проблемы решают callback refs в коде Для начала, давайте воспроизведём проблему с обычными объектными рефами, чтобы потом решить её через callback refs Не будем тратить время на детальное объяснение того, что происходит в коде выше, если коротко, то мы отслеживаем размер нашего дива, либо параграфа через ResizeObserver.По началу наш код отрабатывает правильно, при маунте мы получаем размер элемента, который отрендерился, при ресайзе в консоль также получаем его размер. Проблемы возникают, когда мы начинаем тоглить наш стейт, тем самым меняя элемент, за которым мы следим.Когда мы меняем стейт и элементы, за которыми следим, у нас перестаёт корректно работать ResizeObserver, он всё ещё продолжает следить за тем самым первым элементом, который уже удалён из DOM. Даже обратный тогл, который вроде бы возвращает нам самый первый элемент не помогает, т.к. подписка на новый элемент у нас просто не срабатывает Примечание:Пример решения вышеописанной проблемы, которую мы далее решим через callback ref, скорее подходит для кода, который пишется в рамках библиотеки и который должен быть универсальным т.к. в реальном проекте мы можем решить всё это через комбинацию флагов, эффектов и т.п, в коде библиотеки мы не можем знать про конкретный компонент, его состояние и т.д. наш код обязан быть универсальным, в чём нам и поможет callback ref Как видно, мы переписали наш хук useResizeObserver на callback ref, в который мы просто передаём коллбэк (замемоизированный!) который должен отрабатывать при ресайзе и теперь сколько бы мы не тоглили элементы, наш коллбэк с ресайзом будет отрабатывать, т.к. он навешивается на новые элементы и отвешивается от старых в необходимый нам момент времени благодаря callback ref. Самое главное в этом решении опять таки, это то, что разработчик, использующий наш хук не должен беспокоиться об этой логике навешивания/снятия обработчиков, мы инкапсулировали её у нас в хуке, разработчику остаётся лишь прокинуть коллбэк в наш хук и в рефы своих элементов Ещё один случай, где нам на помощь приходят callback ref В примере выше у нас есть простой компонент инпута, на который мы навешиваем реф и получаем его из пропсов с помощью forwardRef.Но что нам делать с inputRef в самом компоненте инпута? Может, мы хотим завязать на него другую логику, как в этом примере с getBoundingClientRect, но если мы заменим реф из пропсов на реф самого компонента, то у нас не сработает наш фокус. Как нам объединить наши два рефа? Здесь нам опять приходят на помощь callback ref Объяснение:Мы написали хук useCombinedRef, в который разработчик может передавать как обычные рефы, так и наши коллбэк рефы, ну и опционально мы даём ему возможность прокидывать null и undefined. Сам хук useCombinedRef очень простой, это просто useCallback, обычная функция, в depth у него рефы, которые приходят в аргументах и при её вызове с конкретным элементом или null, если это null, то мы делаем return, если же реф это функция, то мы вызываем её с переданным нам параметром, если же это обычный реф, то мы просто обновляем его current. С помощью useCombinedRef мы получаем 1 реф, но под капотом мы будем обновлять все нужные нам рефы. В примере выше например, у нас будет отрабатывать как getBoundingClientRect при маунте компонента Input, так и наша фокусировка на инпуте при клике на кнопку Что изменилось в React 19 в части callback refs Автоматическая очистка:React теперь автоматически обрабатывает очистку callback refs при размонтировании элементов, что упрощает для нас задачу управления ресурсами. Вот какой пример приводит команда React у себя в документации Можете посмотреть на него более подробно поссылке, там в том числе написано про то, что скоро очистка рефа через null будет deprecate и останется только 1 способ очистки рефов Выбор между обычными refs и callback refs Используйте обычные рефы(useRef), когда вам нужен простой доступ к DOM-элементу или сохранение значения между рендерами без необходимости выполнять дополнительные действия при привязке или отвязке элемента. Используйте обычные рефы(useRef), когда вам нужен простой доступ к DOM-элементу или сохранение значения между рендерами без необходимости выполнять дополнительные действия при привязке или отвязке элемента. Используйте callback refs, когда требуется более тонкий контроль над жизненным циклом элемента, вы пишете универсальный код (у вас своя библиотека или пакет) или управление несколькими рефами. Используйте callback refs, когда требуется более тонкий контроль над жизненным циклом элемента, вы пишете универсальный код (у вас своя библиотека или пакет) или управление несколькими рефами. Заключение Callback refs в React — это полезный инструмент, дающий разработчикам дополнительную гибкость и контроль над взаимодействием с DOM-элементами. Хотя в большинстве случаев стандартные объектные рефы через useRef полностью удовлетворяют наши с вами потребности, callback refs помогают в более сложных сценариях, которые мы обсудили выше"
  },
  {
    "article_id": "https://habr.com/ru/articles/876514/",
    "title": "Чем отличается мой Type Predicate Generator?",
    "category": "Программирование",
    "tags": "type safety, code generation, jit",
    "text": "Кратко: это максимально безопасный по типизации инструмент, генерирует статические файлы с кодом для максимальной совместимости, быстрый благодаря предварительной компиляции (AOT) и компактный. Также он предоставляет удобный генератор модульных тестов, чтобы можно было быть почти на 100% уверенным, что создаваемые предикаты работают, как ожидается. Этот документ предлагает детальное сравнение Generator с другими инструментами для проверки типов во время выполнения, а также дает более широкий обзор смежных тем. Со временем он превращается в более аналитическую статью, а не просто \"мой X круче, чем ваш Y\". Введение Прежде всего, выражаю искреннее уважение всем разработчикам инструментов, особенно тем, кто участвует в проектировании, реализации, тестировании и документировании каждого инструмента, с которым я сравниваю Generator в этом посте (zod,typia,ts-auto-guard,ts-runtime-checks). Продолжайте отличную работу, друзья! Сравнение с чистыми проверщиками типов во время выполнения Самая популярная библиотека в этой категории — это известнаяzod. Проверщики типов в этой категории используют только код, который выполняется движком JavaScript в приложении. Типовая информация не используется в этих библиотеках. Некоторые из них, например, используютevalдля преобразования схемы в JS-код во время выполнения, другие, такие какzod, применяют чистую композицию функций. Основные преимущества этого подхода — простота использования и гибкость в выражении правил (в конце концов, это построено на основе тьюринг-полного JS). Этот подход также относительно проще развивать, поскольку он использует только JS/TS без сложного взаимодействия с API TypeScript. Теперь перейдем к ключевым отличиям Generator по сравнению с чистыми проверщиками типов во время выполнения. Ознакомьтесь срезультатами тестов производительности. Основная причина заключается в том, что Generator генерирует специализированный код, который современные JS-движки легко оптимизируют. Каждая функция-предикат для типов состоит из простых инструкций (например,typeof x === \"string\"иx === \"constant\"), которые в большинстве случаев даже не вызывают других функций и никогда — внешних или общих функций. JIT-движки JavaScript предпочитают, когда типы локальны и для каждого отдельного типа создается отдельная небольшая функция. Это позволяет JS-движкам специализировать эти маленькие функции во время выполнения. Подробнее об этом можно прочитать вэтой удивительной статье, где объясняется, как V8 обрабатывает полиморфные функции. Чистые проверщики типов, такие какzod, используют вместо этого композицию функций. В этом случае базовые функции (например,hasProperty(object, propertyName)) многократно используются с различными типами значений и, соответственно, с различнымискрытыми классами. Это приводит к частымдеоптимизациям(падению до более медленного не-оптимизированного байткода вместо более быстрого нативного кода). В самых тяжелых случаях JIT-компилятор можетчередовать оптимизацию и деоптимизациюдля одного типа, затем для другого, затрачивая множество циклов на компиляцию вместо выполнения. Код, который добавляется в итоговый бандл, — это именно тот код, который вы видите в выводеgit diffвашего проекта после генерации предикатов типов. Единственные дополнительные функции, которые используют предикаты, — это встроенные методы, такие какArray.isArray()(причем онибезопасно обернуты!). Это означает, что размер бандла, который добавляется Generator для предикатов, остается минимальным. Он растет линейно с увеличением размеров типов. Количество кода, необходимое для определения типа Zod, сопоставимо с размером среднего предиката после минификации. Чисто runtime-библиотеки для проверки типов по своей природе являются зависимостями во время выполнения. Они требуют включения библиотеки матчеров во время выполнения в итоговый бандл. Большинство библиотек используют удобные конструкторы, которые достаточно гибки, чтобы удовлетворить большинство требований общего API. Это хорошо, но увеличивает размер бандла (60+ КБдляzod), поскольку более обобщенные матчеры не могут быть статически tree-shaked (вотинтересная попыткауменьшить размер бандла). Еще одна проблема, которая возникает с исключительно runtime-библиотеками, используемыми в производительно- и доступно-критичных частях приложения, — это совместимость на этапе компиляции с рантаймамиWinterCG. Тяжелые runtime-библиотеки, как правило, ориентированы на богатые функционалом среды, такие как Node.js, и добавляют обработку крайних случаев (например, генерацию сообщений об ошибках), требуя дополнительного тестирования. Это не является проблемой, если вы работаете на широко поддерживаемых платформах, таких как Node.js или последние версии браузеров. Однако, если вы, например, используетеWasmer Edge, добавление на первый взгляд тривиальной зависимости может превратиться в серьезную проблему. Это одно из ключевых отличий Generator. Код, созданный Generator и попадающий в ваш бандл, сначала проверяется вашей настройкой TypeScript, чтобы убедиться, что он безопасен внутри и соответствует проверяемым типам. Generator отлично справляется с этим, создавая строго типобезопасный код, который будет компилироваться даже при самых строгих конфигурациях TypeScript. Кроме того, если вы вносите изменения в тип, для которого был сгенерирован предикат,tscнапомнит вам обновить предикат (просто повторно его сгенерировав). Чисто runtime-библиотеки не могут напрямую использовать мощь TypeScript для проверки, что составленная функция включает все необходимые проверки в правильном порядке. Даже корректный TypeScript-предикат типов, который не использует хотя бы оператор типаsatisfies, может легко обмануть себя, просто возвращаяtrueдля любого входного значения без достаточных проверок. Конечно, такие готовые к продакшену библиотеки, какzod, используют TypeScript для проверки корректности собственного кода и предоставляют вспомогательные функции для вывода типов из runtime-блоков. Это значительно повышает надежность кода по сравнению с чисто JavaScript-библиотеками. Но даже это не позволяет вашему проекту полностью полагаться наtscдля проверки корректности итогового кода. Всегда остается некоторая часть оберток, связующего кода или вспомогательных функций, которые не могут быть проверены. В случаях, когда возникает критическая проблема, не поддерживаемая сгенерированным предикатом, или обнаружен баг, требующий срочного исправления, код, созданный Generator, остается читаемым и может быть немедленно отредактирован. Поскольку код предикатов типов редко изменяется, необходимость отправлять патчи в основной репозиторий Generator отсутствует (хотя это, конечно, приветствуется!). Такое исправление будет тривиальным для ревью в рамках небольшого PR и останется локальным для конкретного предиката, что позволяет команде решить проблему без необходимости ожидать обновлений в процессе разработки Generator. В случае использования runtime-библиотеки исправление потребует внесения патча непосредственно в библиотеку для устранения проблемы. Такой патч затронет весь код, проверяемый библиотекой, и потребует более тщательного тестирования. Временный форк библиотеки может быть решением, но это потребует изучения процесса сборки и публикации данной библиотеки. Generator работает с любым типом, определенным в любой части приложения с использованием только нативного TypeScript. Это может быть тип из сторонней библиотеки или публичный тип другой команды, не использующий runtime-библиотеки для проверки типов. Все это просто типы, которые совместимы и компонуются. Напротив, все runtime-библиотеки по своей природе предоставляют набор классов или функций для создания экземпляра проверщика типов, который используется для проверки значений. Большинство из них требует вывода итогового типа из составной функции. Это фактически превращает runtime-библиотеки в DSL-ориентированные инструменты (популярные, например, вмире Ruby) вместо того, чтобы быть по-настоящему TypeScript-first инструментами. В результате акцент смещается в сторону подхода, ориентированного на схемы, где TypeScript становится мощным вторичным инструментом, а не основной целью. Это предусмотрено архитектурой: код, генерируемый Generator, изначально более производителен для обработки JS-движками. Современные движки JavaScript используют множество трюков для ускорения начального парсинга, включаяленивый парсинг. Код, созданный Generator, является нативным для таких оптимизаций, так как он ничем не отличается от любого другого кода продакшн-приложения. Его также тривиальноtree-shake, что помогает исключать неиспользуемые части из бандла. Подход runtime-библиотек по своей природе требует запуска некоторого кода на этапе старта приложения, который заставляет JS-движок парсить, компилировать и выполнять код библиотеки, даже если предикат не будет сразу использоваться. Для смягчения этой проблемы можно обернуть runtime-библиотеки в фабричные функции, но их код всё равно попадет в бандл и будет обработан на старте. Стоит отметить, что существуют JavaScript-рантаймы, которые используют снимки кучи (V8 snapshots, JS-движки на основеWASM). Они позволяют сделать снимок памяти JS-движка после инициализации приложения. Если это напоминает вамpreforking-сетевой демон, то аналогия действительно близка. Эти среды могут значительно ускорить холодный старт для runtime-библиотек, но ценой увеличения использования памяти, так как итоговая функция-проверка должна быть создана в куче, даже если она не используется. Generator компилирует код предикатов на этапе сборки и не используетeval()и подобные методы, которые недоступны в высокопроизводительных и безопасных рантаймах, таких какVercel Edge Runtime,AWS CloudFront,Cloudflare WorkersиAkamai EdgeWorkers. Runtime-библиотеки, которые используютeval()и аналогичные методы для реализации JIT-оптимизаций, добавляют дополнительные шаги, которые JS-движок должен выполнить перед запуском фактического кода предиката типов. Подход сeval()требует от библиотеки создания исходного кода предиката в виде строки, которая обычно составляется из меньших строк. Эти строки затем подлежат сборке мусора, передаются движку для парсинга и только потом исполняются. Несмотря на гибкость, этот метод вносит дополнительную задержку при старте, увеличивает потребление памяти и требует больше ресурсов от среды выполнения. Стоит также отметить, что использованиеeval()является потенциальнойпроблемой безопасности. Результирующий артефакт может быть классифицирован инструментами аудита безопасности как потенциально уязвимый. Честно говоря, в случае библиотеки проверки типов это не должно быть реальной проблемой, особенно если исходный код библиотеки регулярно проверяется и поступает из надежного источника. Каждая библиотека время от времени вводит или обнаруживает баги. В случае с Generator, исходный код, который может вызывать исключения, явно включён в ваш бандл и снабжён исходными картами (source maps). Использование пошагового отладчика для сгенерированного кода ничем не отличается от отладки вашего собственного кода. Стек вызовов в инструменте отчётов об ошибках также будет кристально понятным. Хотя баги в коде, подобном тому, что создаёт Generator, случаются редко, они всё же возможны. Например, когдаProxyвыбрасывает исключение при доступе к свойству,getterнеожиданно возвращаетnull, или библиотека неправильно используетизменение глобальныхprototype. В случае runtime-библиотек стек вызовов чаще всего содержит минифицированные символы. Это особенно заметно при использовании новойнативной поддержки TypeScriptв Node.js, где исходные карты вообще не используются. Код, сгенерированный Generator, тривиально доступен через функцию \"перейти к определению\" в любой IDE. Типы при наведении мыши — это те же самые типы, которые используются в вашем приложении, без дополнительных обёрток или переименований. Более того, сам код предиката типов доступен всего в один клик, если потребуется его модификация. В случае сzodвы увидите выведенные типы, которые не могут иметь других удобных возможностей, таких как JSDoc на уровне типов (JSDoc на уровне свойствработает, но ограниченно) или generics на уровне типов (существуют обходные решенияна уровне рантайма, но они требуют усилий для реализации). Одним из преимуществ явной генерации кода, хранящегося в системе контроля версий вашего репозитория, является его простота аудита. Все изменения становятся сразу видимыми во время ревью PR через инструменты вроде GitHub-based сканеров кода. Это также упрощает обновление пакета Generator: все изменения, которые новая версия вносит в сгенерированный код, сразу видны вgit diffпроекта и покрываютсясгенерированными unit-тестами. Для полноты обсуждения вопроса безопасности, включая немногобезопасность цепочки поставок, стоит отметить, что пользователи runtime-библиотек всё равно запускают скомпилированный в JavaScript код рантайм-чекера, который теоретически может содержать что угодно, так как файлы*.d.tsне предоставляют посткомпиляционной проверки для*.js-кода, входящего в бандл библиотеки. Поэтому для более строгих в плане безопасности приложений возможность проверки фактического кода средствами безопасности проекта может быть важной. Сравнение с плагинами компилятора TypeScript Наиболее популярным и функционально насыщенным инструментом в этой категории являетсяTypia. Типовые проверки в этой категории генерируют код предикатов типов на этапе сборки. Они подключаются к компиляторуtscв качестве плагинов и предоставляют вспомогательные типы, которые на этапе генерации кода транспилируются в фактический JavaScript-код в процессе компиляции с использованиемtsc. Основные сильные стороны инструментов в данной категории заключаются в их удобстве использования (достаточно добавить что-то вродеis, и это магически начинает работать) и поддержке практически всех типов, которые предлагает TypeScript. Эта мощь исходит из того, что плагин компилятора по сути является плагином: он может делать всё, что способен компиляторtsc. Generator — это автономный инструмент, который включает собственную версию TypeScript, не влияющую на настройку TypeScript в вашем приложении. Даже если Generator сломается при обновлении, сгенерированный им код предикатов уже будет сохранён в вашем репозитории и не исчезнет. Код предикатов довольно статичен и не требует обязательного присутствия Generator в процессе сборки; вы можете просто использовать код изPlayground. Так как TypeScript официально не поддерживает плагины дляtsc, проверяющие библиотеки, которые зависят от подключения кtsc, требуют патчей для самогоtsc. Это становится блокером для большинства команд, которые хотят использовать чистый компилятор TypeScript по разным причинам. Основной причиной является надёжность, так как TS является сеткой безопасности, которая гарантирует, что код будет работать в продакшн-среде. Ещё одной проблемой использования плагинов дляtscявляется то, что крупные обновления TypeScript часто вносят изменения, нарушающие совместимость с внутренними API. В оптимистичном случае это просто ломает плагины, и они выдают ошибку. В более сложных случаях плагин продолжит работать, но может генерировать некорректный код. Это, в самом строгом смысле, требует от разработчика приложения подождать, пока все плагины обновят свою поддержку TypeScript, обновят тесты и опубликуют новые версии пакетов. В проектах с жёсткими SLA это может потребовать дополнительного времени, пока сообщество адаптирует новые версии пакетов. Я должен признать, что плагиныtscпредоставляют элегантные API, которые мне лично нравятся как энтузиасту языка программирования. Но, строго говоря, плагины TypeScript расширяют сам язык в целом и не соответствуют курсу, которыйTypeScriptи остальное сообщество (см. рассуждения и комментарииздесь) придерживаются уже долгое время. Немного грустно, но в противном случае мы бы уже получили ещё один форк JavaScript, какCoffeeScriptилиFlowот Facebook. Как упомянуто выше,Генератор генерирует TypeScript код, который является строго типобезопасными проверяется с помощью вашей настройкиtscв соответствии с правилами безопасности вашего проекта. Плагины TS, с другой стороны, генерируют JavaScript код, который не проверяется компилятором TypeScript, что означает, что эта часть кода, которая выполняется в вашем проекте, остаётся чистым JavaScript, даже если она исходит от компилятора TypeScript. Конечно, создатели инструментов хорошо тестируют выводимый код, но вам всё равно нужно делегировать безопасность типов на внешний инструмент в процессе сборки. Как упомянуто выше,Генератор генерирует код, который читаемый и изменяемый. Чтобы проверить, что на самом деле генерирует проверяющий на основе плагинаtsc, вам нужно будет извлечь сгенерированный код из процесса компиляции. Этот код представляет собой чистый JavaScript, что требует вручную проверить корректность сгенерированного кода. Тот факт, что сгенерированный код в большинстве настроек не проверяется в репозитории, делает процесс обновления пакетов многоступенчатым. Чтобы убедиться, что новая версия проверяющего на основе плагина генерирует сопоставимый код, вам нужно будет где-то сохранить текущий сгенерированный код, обновить пакет и выполнить дифф. С Генератором это работает из коробки. Единственный недостаток того, что Генератор генерирует файлы — это явный шаг сборки. С плагиномtscне нужно беспокоиться о коде; плагин всегда поддерживает предикаты в актуальном состоянии. Тем не менее, не все инструменты сборки и разработки поддерживают горячую перезагрузку с полной поддержкой плагиновtsc(например,проблема, решение которой заняло почти год). Это всего лишь маленький TS файл, который любой инструмент из экосистемы JS может использовать как есть, вместе с остальным кодом вашего приложения. Если вам нужно было бы поделиться исходным кодом TypeScript, использующим плагиныtsc, с другими командами, вам нужно сначала его опубликовать. Это требует компиляции кода в JavaScript-обертку и публикации её вместе с файлами*.d.ts. Другой вариант — заставить другие команды использовать тот же набор и версии плагиновtsc, включая плагин для проверок в их настройках. Это требует координации и может замедлить миграции на новые версии TypeScript. Еще одной преградой могут быть команды, которые не используютtscдля сборки проекта, а вместо этого применяют новые инструменты, которые нативно понимают синтаксис TypeScript и не требуют отдельного шага сборки (Поддержка нативного TS в Node.js,esbuild,Cloudflare wrangler). Как упомянуто выше в разделеЛегкость отладки и лучшие трассировки стека, Генератор генерирует код, который дружелюбен к отладчикам и инструментам отчетности об ошибках. Трассировки стека соответствуют вашему приложению, давая мгновенную обратную связь о том, откуда возникла ошибка (как ошибки валидации, так и потенциальные ошибки на этапе выполнения). В случае с плагиномtscпри отладке и чтении трассировок стека карты исходников будут указывать только на один символ в исходном коде TS (в большинстве случаев это токенis). В случае проблемы или ошибки в исходном коде или самом проверяющем плагине потребуется инспектировать сам исходный JavaScript-бандл. Однако существует обходной путь. Должно быть возможно опубликовать как карты исходников для бандла вашего приложения, так и для зависимостей, и настроить инструмент отчетности об ошибках на поиск этих карт исходников там. Быстрое гугление не дало четких результатов о том, как работать с транзитивными картами исходников. Код предикатов легко использовать с новой функцией TypeScript для Node.js. Нет необходимости использоватьts-nodeили предварительно компилировать код с помощьюtsc. То же самое относится и к различным тестовым раннерам и сборщикам для edge-окружений. С плагиномtscвы ограничены инфраструктурой, ориентированной наtsc. Это не слишком сложно, но может потребовать дополнительной настройки и ухудшить производительность. Например, Vite компилирует файлы*.tsв 20-30 раз быстреес использованием компилятора по умолчаниюesbuild, чем сtsc. Новый супербыстрый пакетts-blank-spaceдаже не выполняет анализ типов TypeScript, необходимый для работы плагиновtsc, и, благодаря этому, работает очень быстро. Итак, еще раз, расширение системы типов, которое влияет на синтаксис, эффективно превращает эту категорию проверок врасширение языка. Сравнение с другими генераторами кода Самым полным (и дорогим для меня, поскольку я использовал его в прошлом) инструментом в этой категории являетсяts-auto-guard. Эта категория проверок генерирует итоговый код предикатов как отдельные статические файлы, которые должны быть явно импортированы в код приложения и собраны с остальной частью кода. Генераторы попадают в эту категорию рантайм-проверок типов. Большинство аргументов, приведенных выше в пользу использования Генератора, в той или иной мере применимы ко всем инструментам в этой категории. Основные преимущества этого класса инструментов — совместимость и предсказуемость. Это хорошо перекликается с подходом к инструментам, который принят в более прагматичных экосистемах, таких как Golang, разработка компиляторов и тому подобное. Как уже упоминалось выше,Генератор генерирует TypeScript код, который строго типобезопасен. Другие генераторы кода, также производящие TypeScript, генерируют код, который не является типобезопасным. Неопасно сгенерированный код использует небезопасные конструкции, такие как приведение типов (с помощьюas), или может полагаться на небезопасные типы (чаще всегоany). Это фактически превращает сгенерированный TypeScript код в слабо типизированный JavaScript. Если конфигурация вашего приложения использует строгие линтеры TypeScript, такой код потребует добавления исключений и снизит общий уровень типовой безопасности. Как уже упоминалось,Генератор генерирует код, который легко читать и изменять. В дополнение к этому, по сравнению с другими инструментами генерации кода, Генератор помогает вам с чтением, отладкой и, возможно, ручным изменением сгенерированного кода, линейно структурируя все проверки. Он делает маленькие шаги, которые легко отслеживать, и добавляет исчерпывающие комментарии (скоро!). Это означает, что сгенерированный код не минимизирован, но при этом организован таким образом, что обычные сборщики могут легко превратить его в маленькое объединённое выражениеif. Генератор также старается использовать осмысленные имена локальных переменных и временных типов, где это возможно, чтобы улучшить читаемость. Код, сгенерированный большинством других инструментов в этой категории, предварительно оптимизирован, и потому его не так легко читать. Это дает больший контроль разработчику инструмента для достижения лучшей производительности, так как не все сборщики могут правильно обрабатывать достаточно сложные функции предикатов для более сложных типов. Так как код, который генерирует Генератор, строго проверяется компилятором TypeScript и также генерируется с использованием API TS AST-строителя (посмотрите замечательныйts-ast-viewer.comдля работы с API), Генератор не может сгенерировать некорректный TypeScript код. Во время моих исследований я обнаружил, что для некоторых нетривиальных случаев другие инструменты, использующие конкатенацию строк, могут генерировать некорректный или неполный код. К счастью, некорректный код сразу отклоняется компилятором TypeScript (это супервозможность инструментов генерации кода в целом), но неполнота кода не покрывается, так как сгенерированный код не является типобезопасным. Это также касается инструментов, которыезависят отeval()во время выполнения. Маленькое замечание. Большинство генераторов кода на основе типов показывают значительно (на 100x) лучшую производительность, чем решения, работающие только во время выполнения, и значительно лучшие времена старта по сравнению с теми, которые используютeval. Но у Генератора все же есть несколько фишек. Что добавляет Генератор, так это однократный доступ к свойствам объекта и широкое использование локальных переменных, что делает код крайне быстрым даже в неоптимизирующих JS-движках (это восходит к техникам, использовавшимся для ускорения старых JS-движков). Также этот трюк потенциально упрощает анализ для современных JS-движков. Большинство других проверяющих формируют полные или частичные выражения для доступа к вложенным свойствам, что требует времени для анализа и оптимизации движком JS, так как это просто больше байт-кода для анализа: В коде с локальной переменной нет второгоGetNamedProperty. Однако используется один дополнительный регистр. В качестве бонуса, во время генерации предикатов, Генератор может также по желанию генерировать набор юнит-тестов для сгенерированных предикатов. Вы можете запускать эти тесты как часть тестового набора вашего приложения. На данный момент это довольно уникальная функция, и я надеюсь, что она будет поддержана и другими разработчиками инструментов. Подробнее об этом подходе можно прочитатьздесь. Лучше ли это, чем ChatGPT? Да, на данный момент это так. С учетом того, что в последние дни искусственный интеллект создает все больше контента, с Генератором вы можете доверять сгенерированному коду, так как это простейший инструмент, который слишком прост, чтобы создавать галлюцинации. Генератор выдаст ошибку для типов, которые он не может конвертировать, вместо того чтобы генерировать неправильный код и убеждать вас, что он правильный. Я пробовал Copilot и ChatGPT. Оба инструмента ИИ сгенерировали небезопасный TS, который даже не компилировался без ошибок на первом этапе, несмотря на любые подсказки. В одном из запусков ChatGPT просто сломался посередине генерации кода с набором совершенно не относящихся к делу символов. И, как стало более очевидно, Генератор намного быстрее любого ИИ-инструмента. Ах, чуть не забыл упомянуть, Генератор также бесплатен. Резюме Как вы можете видеть, существуют несколько случаев, когда стоит бросить вызов статусу-кво и использовать исключительно типобезопасный и статический подход. Генератор — это инструмент, который очень прост в своей работе, полагаясь на другие инструменты для выполнения остальной тяжелой работы: для генерации кода используется API TypeScript, для минификации используетсяesbuildс настройками по умолчанию, для проверки типов, конечно же, Генератор делает сгенерированный код строго типизированным и использует операторsatisfies. В дополнение к этому, Генератор дополнительно генерирует набор юнит-тестов для проверки других сгенерированных кодов. Также функциональность минимальна. "
  },
  {
    "article_id": "https://habr.com/ru/articles/878644/",
    "title": "Как бизнесу легально использовать метрические программы, если в законе не написано, что это такое",
    "category": "Базы данных",
    "tags": "персональные данные, обработка персональных данных, обработка перс данных, роскомнадзор, ркн, бизнес, база данных предприятия, уведомление, согласие на обработку пдн, сервер",
    "text": "Под занавес ушедшего в безвременье 2024-го года внимательный читатель спросил в комментариях, что такое метрические программы (МП). Ему нужна была выдержка из закона. Ведь Роскомнадзор просит бизнес предупреждать посетителей сайтов о том, что их персональные данные (ПД) обрабатываются метрическими программами, а что такоеметрические программыникто не знает. Погружаться в бездну философских размышлений о том, почему одни реалии для нас более значимы, а другие – менее, не буду. Вместо этого предлагаю выяснить, существуют ли на самом деле метрические программы, находятся ли они сейчас в этой комнате и, если да, то как с ними работать, чтобы не попасть под штрафы Роскомнадзора. Итак. Все началось с того, что встатье о 7 документах, которые обязательно должны быть на сайте компании, я указала на необходимость добавить согласие на обработку данных метрическими программами. Это важно для соблюдения пунктов Закона «О персональных данных». Независимо от того, сможем ли мы найти то самое истинное определение понятияметрической программы, вы все равно должны будете разместить на сайте и во всплывающем уведомлении согласие на обработку персональных данных МП, если используете хотя бы какой-нибудь вспомогательный сервис. Что обрабатывают метрические программы и причем тут ПД По ФЗ «О персональных данных», персональные данные – это любая информация, относящаяся к прямо или косвенно определенному или определяемому физическому лицу (субъекту персональных данных). Метрические программы обрабатывают сведения об IP-адресе, поведении пользователя на сайте и иные любые сведения, которым можно косвенно определить пользователя, поэтому РКН относит данные, обрабатываемые метрическими программами, к персональным данным. Следовательно, на обработку этих персональных данных распространяются те же требования, что и на обработку других персональных данных. Для обработки данных необходимо иметь основание. Им может быть согласие. По результатампроверок (контрольных мероприятий без взаимодействия)РКН направляет владельцам сайтов требования пояснить, на каком основании данные обрабатываются метрическими программами. При этом даются ссылки на пункты пользовательских соглашений соответствующих метрических программ, в которых указано на перечень данных, обрабатываемых метрическими программами. Информация о том, что данные обрабатываются метрическими программами должна быть указана и в политике обработки персональных данных. В политике и согласии необходимо указать цели обработки данных, объем данных, сроки обработки и т.д. Сбор персональных данных на сайте Современные проблемы требуют современных решений. Метрические программы хорошо справляются с отведенной им ролью: собирают сведения об уникальных посетителях, анализируют их поведение на сайте, описывают интересы и т.д. Четкого определение МП закон не дает: разочарую (или обрадую) вас сразу. Почему? Потому что закон содержит определенияперсональных данныхиавтоматизированной обработки данных. Необязательно устанавливать определениеметрической программы. Важно, какие действия и с какими данными производит тот или иной сервис. Всем сервисам, обрабатывающим персональные данные, дать определения невозможно. Как быть владельцам бизнесов и их сайтам Метрические программы призваны аккумулировать и классифицировать информацию о пользователях – это инструмент для сбора и анализа данных о поведении пользователей на сайте. Если вы примете это определение как правдивое, то вряд ли ошибетесь, когда будете создавать и размещать на сайте документы о работе с персональными данными. Может, тогда проще отказаться от использования метрических программ? Нет, отказываться от метрических программ из-за того, что они обрабатывают персональные данные пользователей сайта, не нужно. Дело в том, что МП – действительно эффективный инструмент для измерения показателей работы проекта. Он позволяет сопоставлять нынешние коэффициенты с прошлыми, распознавать тенденции, делать прогнозы, обнаруживать проблемы и совершенствовать не только интерфейс и наполнение сайта, но стратегии продвижения бизнеса в целом. В общем, полезная штука в умелых руках. Не недооценивайте ее. Согласие на обработку персональных данных метрическими программами Самое важное, что вы должны сделать в контексте работы с метрическими программами и персональными данными – создать и разместить на сайте соглашение на обработку персональных данных для сайта и указать на использование МП в политике конфиденциальности (обработки персональных данных). В этих документах укажите также про всплывающее уведомление об использовании файлов cookie. Оно должно содержать предупреждение об использовании метрических программ, включать ссылку на текст согласия и кнопку«принимаю». Как оформить согласие на сбор ПД метрическими программами Ничего откровенно сложного в этом нет. Да, придется потратить какое-то время, чтобы разобраться в деталях, но это того стоит. И, конечно, любому владельцу сайта доступночитерство в виде помощи юриста. Текст согласия может выглядеть так: Пользователь при использовании сайта (далее – Сайт) дает настоящее Согласие на обработку персональных данных (далее – Согласие), ООО __ ИНН __, ОГРН __, адрес: ____ (далее – Оператор). Согласие дается путем нажатия пользователем кнопки«принимаю»рядом с текстом: «Мы используем файлы cookie, оставаясь на сайте и нажимая кнопку«принимаю», вы подтверждаете, что даете согласие на обработку персональных данных метрическими программами, ознакомлены и принимаете условия политики в отношении обработки персональных данных. Согласие является конкретным, предметным, информированным, сознательным и однозначным. Перечень персональных данных, на обработку которых пользователь, дает свое согласие: данные, собираемые метрическими программами: (наименование программ). Цель обработки персональных данных: в целях функционирования сайта, проведения ретаргетинга и проведения статистических исследований и обзоров с использованием соответствующих сервисов. Это основные пункты. Разумеется, вы можете адаптировать согласие на обработку персональных данных метрическими программами к своей ситуации. Единственное, подходите к этому ответственно: это действительно важно. Буду признательна обратной связи: напишите, удалось ли мне объяснить про метрические программы с точки зрения законодательства? С чем не согласны? "
  },
  {
    "article_id": "https://habr.com/ru/companies/selectel/articles/878423/",
    "title": "Новый рекорд емкости: жесткие диски на 36 ТБ уже в продаже",
    "category": "Базы данных",
    "tags": "selectel, seagate, hdd, жесткий диск, хранение данных",
    "text": "Что там у Seagate Технология HAMR (Heat Assisted Magnetic Recording). HAMR — метод записи данных, при котором используется точечный нагрев магнитной поверхности диска лазером. Он позволяет записывать данные на меньшие участки пластины, увеличивая плотность хранения информации. В Mozaic 3+ HAMR играет ключевую роль в достижении емкости 3,6 ТБ на одной пластине. Высокоточные головки чтения/записи. Платформа включает усовершенствованные головки, которые способны работать с намагниченными областями микроскопического размера. Это важно для обеспечения точности записи и чтения данных при использовании HAMR. Новые магнитные пластины. Mozaic 3+ использует пластины с улучшенными магнитными свойствами, которые способны сохранять стабильность данных даже при высокой плотности записи. Эти пластины разработаны специально для работы с технологией HAMR. Оптимизированная механика и электроника. Платформа включает улучшенные механические компоненты, такие как двигатели и подшипники, которые обеспечивают стабильную работу диска при высокой скорости вращения. Электроника также оптимизирована для обработки больших объемов данных с минимальными задержками. Поддержка черепичной записи (SMR). Хотя Seagate не всегда раскрывает, какие именно диски используют SMR (Shingled Magnetic Recording), эта технология может быть частью Mozaic 3+. SMR позволяет увеличить емкость за счет перекрытия дорожек записи, что особенно полезно для архивных и редко изменяемых данных. Что еще Емкость:36 ТБ Форм-фактор:3.5 дюйма Интерфейс:SATA III (6 Гбит/с) Количество пластин:10 Емкость одной пластины:3.6 ТБ Технология записи:HAMR (Heat Assisted Magnetic Recording) Тип магнитной записи:Предположительно SMR (черепичная запись) Скорость вращения:7200 об/мин (предположительно) Скорость чтения/записи:До 600 МБ/с (ограничено интерфейсом SATA III) Целевая аудитория:Корпоративные клиенты, облачные сервисы, ЦОДы Платформа:Mozaic 3+ Совместимость:Обратная совместимость с SATA II и SATA I Линейка:Exos M Особенности:Высокая плотность записи, поддержка технологий для корпоративного использования Рынок жестких дисков: перспективы и тренды"
  },
  {
    "article_id": "https://habr.com/ru/articles/878346/",
    "title": "Менеджер паролей для бизнеса — большой обзор 2025",
    "category": "Базы данных",
    "tags": "менеджер паролей, управление учетными записями, управление паролями, корпоративный менеджер паролей, менеджер паролей для бизнеса, пароли, парольный менеджер, парольная защита, password, обзор",
    "text": "Стек технологий безопасности для рабочих мест будет неполным без программ управления паролями для сотрудников. Почему? Потому что одной из самых частых причин взлома учетных записей являются слабые и скомпрометированные пароли. А с учетом того, что все больше сотрудников работают удаленно, адекватные методы парольного менеджмента становятся еще более важными. Ниже собрал список доступных для российских компаний вариантов менеджеров паролей (если что, дополняйте в комментариях) и попробовал сравнить их. Что такое менеджер паролей? Начну с небольшого ликбеза для неайтишников — это софт, предназначенный для создания, хранения и управления паролями или другой личной информацией, такой как конфиденциальные документы, в одном безопасном месте — зашифрованной базе данных, защищенной мастер-паролем. Главное преимущество заключается в том, что нужно запомнить только один главный пароль, а не все данные для входа на ресурсы. Это означает, что команда может просто подключиться ко всем своим аккаунтам с помощью функций автосохранения и автозаполнения. Для личного пользования вы могли использовать менеджеры паролей от Apple и Google. Для компаний — суть схожая, но есть дополнительные административные возможности по управлению доступами, распределениями паролей на отделы и пр., а также большая безопасность. Зачем бизнесу нужен менеджер паролей? Без «внешних ограничений» сотрудники обычно используют слабые пароли, которые легко запомнить, а, значит, и взломать. Записывают их где попало, часто забывают и спокойно делиться через мессенджеры и почту, тем самым создавая риски безопасности для бизнеса. Обзор решений Надежный менеджер паролей может обеспечить создание паролей заданной сложности, их надежное (без бумажек на мониторе и блокнота «мои пароли» на рабочем столе) хранение и удобное управление доступами в рамках компании. В результате снижаются риски успешных кибератак на бизнес, экономятся нервы на вспоминание паролей, а жизнь админов делается чуточку лучше. Теперь давайте рассмотрим актуальные для РФ решения для бизнеса. Начну с двух зарубежных, которые, как ни странно, до сих пор часто встречаю (хотя их и активно импортозамещают). KeePass KeePass Бесплатное зарубежное решение для хранения паролей. Решение до сих пор распространено в корпоративной среде, но, по сути, это не полноценный менеджер паролей, а локальная база данных с паролями, в которой, для обновления нужно всем пользователям выйти из системы и к которой есть вопросы по сохранности данных. Сложен в использовании сотрудниками (отсутствует интуитивно понятный дизайн, а некоторые функции проблематично заставить работать), неудобен в администрировании, не позволяет синхронизировать хранилище с другими приложениями и не поддерживает русский язык. Но бесплатен, и это многое оправдывает). HashiCorp Vault HashiCorp Vault Vault — также не полноценный менеджер паролей, а, по сути, хранилище любых секретных данных, которое поддерживает различные механизмы авторизации и политики доступа, а также интегрируется с основным поставщиком удостоверений или выбранной облачной платформой. Продукт способен управлять секретами для более чем 100 различных систем, включая базы данных, публичные и частные облака, очереди сообщений и конечные точки SSH (про SSH-клиентов у меня, кстати, недавно былбольшой обзор). Несмотря на то, что продукт разработан американской компанией, он до сих пор распространен в российских корпорациях. Чаще его используют “в том числе для хранения паролей”. У продукта есть большие возможности настроек, но он сложен в администрировании и есть риски проблем с оплатой и отключения для РФ. По стоимости: бесплатно до 25 секретов, далее — от $0.50 за секрет. Passwork Passwork Как мне кажется, Пассворк — это первый выпущенный в России корпоративный менеджер паролей. Есть большие возможности по настройкам, поддержка основных ОС, интеграции (в том числе, с помощью API), классно проработанный интерфейс и собственное мобильное приложение. Решение существует в двух версиях: «коробочное» (устанавливается на сервер компании, работает без подключения к сети) и «облачное». Доступны расширения для браузеров Google Chrome, Firefox, Microsoft Edge и Safari. Стоимость стандартной версии на 100 пользователей —  132 000 рублей, но существенная часть функционала доступна только в расширенной версии. ОдинКлюч ОдинКлюч Российская разработка с поддержкой ГОСТового шифрования, удобным интерфейсом и работой на ключевых российских и зарубежных операционных системах. У компании есть лицензии и сертификаты ФСБ и ФСТЭК, а сам продукт позиционируется как сфокусированный на безопасность: выставлен Bug Bounty (публичная проверка от хакеров), хранит зашифрованную базу с паролями отдельно от ключей расшифровки и использует схему разделения секретов Шамира. В отличие от некоторых других решений, в архитектуре менеджера паролей не существует “супер-администратора” —  пользователя, который имеет самый высокий уровень доступа и которому видны пароли остальных пользователей. Из интересных функций: настройки по отделам, двухфакторная аутентификация как для серверной, так и для облачной версий, возможность восстановления мастер-пароля (снижает зависимость от администратора). Доступны «облачные» и «коробочные» версии, а также варианты в виде расширения для браузеров. Стоимость продукта за 100 пользователей составляет от 117 000 рублей в год, включая полный функционал, помощь во внедрении и поддержку. TeamDo TeamDo Интерфейс Российское решение для хранения паролей, чек-листов и документов. Легкое в установке (исходя из информации на сайте, это занимает не более двух часов) и с понятным интерфейсом. Стоимость на 1 год составляет 88 900 рублей вне зависимости от количества сотрудников. В базовой версии не предусмотрены интеграции, а также поддержка SSO и LDAP. BearPass BearPass Отечественный продукт с открытым исходным кодом. Поддерживает ключевые российские операционные системы (по Windows и macOS данных на сайте нет). Годовая стоимость на 100 пользователей составляет 144 000 рублей. Плюс за дополнительную плату можно получить поддержку во внедрении и обучение. Есть бесплатная версия для команд до пяти человек. Сравнение менеджеров паролей У всех описанных решений имеется: Установка On-Premise Установка On-Premise Двухфакторная аутентификация (кроме HashiCorp Vault, но там, при должной настойчивости, можно реализовать через интеграцию с внешними системами) Двухфакторная аутентификация (кроме HashiCorp Vault, но там, при должной настойчивости, можно реализовать через интеграцию с внешними системами) Поддержка хранения истории паролей (кроме HashiCorp Vault, но там, при должной настойчивости, можно реализовать через версии секретов) Поддержка хранения истории паролей (кроме HashiCorp Vault, но там, при должной настойчивости, можно реализовать через версии секретов) Аудит действий пользователей Аудит действий пользователей По удобству интерфейса— дело вкуса, лучше каждому тестировать. Мне больше понравились Пассворк и ОдинКлюч. По стоимости(из Российских решений) — самые выгодные на 100 пользователей оказались TeamDo и ОдинКлюч. Если на 50 пользователей, то — ОдинКлюч. По вниманию к безопасностивпечатлил ОдинКлюч, но другие разработчики, естественно, тоже уделяют этому внимание. По настройкам и интеграциям— отмечу Пассворк и ОдинКлюч, но тут нужно исходить из ваших задач. У них же, в отличие от других решений, предусмотрено ГОСТовое шифрование. Как выбрать лучший менеджер паролей? Идеального решения не выделю. Вариантов не так много, так что можно, в каждом случае, внимательно сравнить. Ключевые критерии:удобство использования и администрирования, достаточный уровень безопасности (от внешних и внутренних угроз), наличие необходимых интеграций, соответствие необходимым стандартам и, естественно, стоимость. Какой бы менеджер паролей вы не выбрали для своей организации, уровень кибербезопасности и эффективности будет увеличен, но к выбору лучше отнестись внимательно."
  },
  {
    "article_id": "https://habr.com/ru/companies/fplus_tech/articles/878114/",
    "title": "Как защитить серверы критических отраслей от удаленного взлома и физического проникновения",
    "category": "Базы данных",
    "tags": "Fplus, криптозамок, защита данных, взлом, локализация, кибербезопасность, киберпреступность, хакерские атаки",
    "text": "Привет, Хабр! Как давно вы мониторили рынок современных аппаратно-программных комплексов защиты серверов и серверного оборудования? С тех пор как доступ западных новинок на наш рынок сократился под давлением внешних факторов, начала стремительно расти актуальность отечественных разработок в области надежной и эффективной защиты данных на уровне железа. Этот рост стимулировали законодательные инициативы, ужесточившие требования к локализации систем защиты. Например, был принятФедеральный закон от 08.08.2024 №216-ФЗ «О внесении изменений в Федеральный закон „Об информации, информационных технологиях и о защите информации“ и отдельные законодательные акты Российской Федерации». Он, в частности, запрещает передачу данных из государственных в иные информсистемы, не соответствующие требованиям о защите информации, а с 1 января 2025 года вводится запрет на средства защиты информации, произведенные в недружественных государствах, либо организациями под их юрисдикцией. Основание –Указ Президента РФ от 01.05.2022 №250 «О дополнительных мерах по обеспечению информационной безопасности Российской Федерации». Вот на таком законодательном фоне и появилсякриптозамок от Fplus– полностью отечественная разработка, уже доступная на рынке. Далее по тексту мы поговорим о том, что этот замок собой представляет и от каких угроз он защищает наших клиентов. Немного истории… Решения в области кибербезопасности и защиты оборудования всегда «шли в ногу» с изощренностью и технической оснащенностью злоумышленников, которые реализуют кибератаки. Цель хакеров – получить контроль над данными и потребовать выкуп за их разблокировку, нарушить целостность и доступность информационных систем или даже вывести из строя такие системы или управляемое ими оборудование. Последнее является следствием киберфизической атаки – такого воздействия на информационную среду, которое приводит к изменениям в реальном мире. Например, воздействие извне на органы управления современного «цифрового» автомобиля могут спровоцировать аварию, а нарушение работы системы управления, скажем, электростанцией, чревато масштабной техногенной катастрофой. Вообще под угрозой последствий киберфизических атак находятся все пользователи «интернета вещей» – от покупателя робота-пылесоса до владельца крупного промышленного предприятия. По мере роста типов кибератак усложнялись и защитные решения. Вначале они основывались на предоставлении эксклюзивных прав доступа к серверному помещению и консоли управления. Однако с течением времени рост числа преступлений злоумышленников и усложнение их моделей поведения вызвали к жизни дополнительные решения, сформировавшие промежуточный этап верификации и немедленную блокировку системы в случае попыток физического взлома оборудования и систем хранения данных. В настоящее время все более популярным становится подход к построению стратегии кибербезопасности на основе концепции «нулевого доверия» (Zero Trust). В этом контексте любая попытка доступа проходит проверку и авторизацию вне зависимости от происхождения. Все пользователи и устройства, запрашивающие доступ к ресурсам, предварительно должны пройти проверку подлинности! Количество хакерских атак растет с каждым годом Критически важные отрасли, обеспечивающие стабильность государства и жизнеспособность общества – энергетика, транспорт, здравоохранение, оборона и т.д. давно включены в цифровую экосистему и часто становятся объектами кибератак. Так…, в 2021 году хакерская группа DarkSideатаковалакомпанию Colonial Pipeline – крупнейшего оператора трубопроводных систем в США, использовав программу-вымогатель (ransomware). В результате компания была вынуждена временно прекратить работу и заплатить выкуп. Почти одновременно с этимбыли атакованысерверы бразильской компании JBS S.A. – одного из крупнейших в мире производителей мяса. При этом также использовалась программа ransomware. Злоумышленники получили доступ к ключевым серверным системам компании, что привело к остановке производства на несколько дней. В нашей стране в 2024 году число атак на государственные структуры существенно увеличилось по сравнению с прошлым годом. Связано это как с ростом геополитической напряженности, так и с развитием методов кибервзлома. Атаки были нацелены преимущественно на четыре сектора: промышленность и энергетику, государственные органы, финансовые учреждения и телекоммуникационную сферу. Промышленный сектор оказался под наибольшим ударом, что связано с его ключевой ролью в экономике и критической зависимостью от технологий. Другие сферы, включая IT, финансы и страхование, тоже находились под постоянным давлением, хотя интенсивность атак в этих секторах была ниже. За 2024 год аналитики компании RED Securityзафиксировали почти 130 тысяч инцидентов. Эта цифра более чем в два раза превышает показатели 2023 года. Наиболее серьезной угрозой стали масштабные атаки на государственные и экономические инфраструктуры, которые приводили к финансовым потерям и нарушению работы производств. Что касается методов взлома, в прошлом году хакеры практиковали как классические DDoS-атаки (в частности, на порталы государственных услуг и банковские организации), так и атаки с использованием вредоносного софта. В попытках повредить критическую инфраструктуру злоумышленники использовали вирусы-шифровальщики, а также трояны и ботнеты для кражи данных или блокировки работы систем. Целевые атаки носили скрытный и долгосрочный характер. Они были направлены на сбор разведывательной информации или установку доступа к системам. Основная цель таких атак – государственные структуры, исследовательские институты и компании с доступом к инновационным технологиям. В октябре 2024 годабыли взломаны сайтыфедеральных арбитражных судов и мировых судей, а также личные кабинеты работников судов. Помимо этого, перестала функционировать система «Электронное правосудие» и ряд других служб. По мнению ряда экспертов, которые комментировали это событие в СМИ, одной из причин успеха хакеров было устаревшее оборудование и программное обеспечение зарубежного происхождения. Надо сказать, что компрометация персональных данных неприятна не только тем, чьи данные «утекли», но и компаниям, которые их не уберегли. НедавноГосдума в трех чтениях приняла законопроект, который предусматривает наказание в виде оборотных штрафов за серьезные утечки персональных данных. Поправки внесены в статью об административных правонарушениях за нарушение требований о сборе персональных данных (13.11 КоАП). Закон вступит в силу с 1 марта 2025 года. В прогнозах на 2025 год специалисты высказывают предположения, что число атак продолжит расти, хотя темпы роста могут замедлиться. Основной фокус может быть сделан на целевых атаках с использованием методов цифрового шпионажа, направленных на сбор конфиденциальной информации и экономическое давление на компании. Угроза также сохраняется для цепочек поставок ПО, что может стать серьезной проблемой для малого и среднего бизнеса, не имеющего достаточных ресурсов для надежной киберзащиты. На текущий момент российская промышленность, энергетика и IT-сектор остаются наиболее уязвимыми отраслями. В условиях возрастающей угрозы усиления кибератак на первое место выходит совершенствование защитных мер и улучшение координации между частными компаниями и государственными структурами для противодействия новым методам взлома. Как защититься? Летом прошлого года компания Fplus анонсировала криптозамок – первое решение отечественного производства для защиты серверов от удаленного взлома и физического проникновения. Это устройство сочетает в себе аппаратные и программные компоненты, направленные на повышение уровня безопасности критически важных систем. Поговорим подробнее, что же умеет новое устройство. Итак… В современном мире криптозамки (от греческого корня κρυπτός, что означает «скрытый», «секретный») используются в разных сферах для защиты данных и контроля доступа. В нашем случае криптозамок – это специализированная HHHL-плата, которая интегрируется в сервер, а также программный компонент, встроенный в BIOS сервера. Устройство подключается к сетевому интерфейсу на системной плате сервера и отключает внешний разъем BMC (Bayonet Neill-Concelman). После установки модуля весь трафик проходит через криптозамок и кодируется отечественными алгоритмами шифрования. Основной задачей устройства является защита от несанкционированного доступа к контроллеру управления сервера (BMC/Baseboard Management Controller), с помощью которого происходит удаленное взаимодействие с вычислительным узлом. Для этого криптозамок шифрует все коммуникации, блокируя прямое подключение к административной оболочке, даже если уязвимости присутствуют непосредственно в ПО. Использование устройства требует аппаратного ключа, выдаваемого централизованным корпоративным сертификатом, что значительно усложняет несанкционированный доступ. Заметим, что контроллеры BMC (например, Nuvoton WPCM450, ASPEED и другие) в основном иностранного производства, а их встроенное микропрограммное обеспечение содержит определенный пул уязвимостей, через которые злоумышленники (а иногда даже доморощенные «мамкины хакеры») могут получить доступ к системе. Что самое интересное – криптозамок снабжен датчиком вскрытия, который фиксирует любые попытки физического вмешательства в работу сервера. Он мгновенно реагирует на попытки несанкционированного открытия корпуса и отправляет сигнал в систему безопасности. Это позволяет своевременно выявлять потенциальные угрозы, связанные с попыткой проникновения злоумышленников к внутренним компонентам вычислительного узла: например, с целью установки вредоносного оборудования или извлечения данных. При использовании стандартных методов обеспечения безопасности посредством аппаратных модулей доверенной загрузки (АПМДЗ),которые были разработаны 20 лет назад, такие действия могут остаться незамеченными, в то время как датчик вскрытия добавляет новый уровень контроля – физический, – существенно повышая общий уровень защиты. Иначе говоря, современные технологии могут не только противостоять виртуальным угрозам, но и учитывать физические риски, которые все чаще становятся частью стратегий кибератак. Эта технология особенно важна для предприятий, работающих с критически важной информацией или инфраструктурой. Таким образом, можно утверждать, что криптозамок Fplus – это больше, чем просто АПМДЗ. Это целая многофункциональная система для контроля над аппаратной частью сервера и защиты BMC. Криптозамок Fplus разрабатывался с учетом нужд бизнеса всех масштабов. Он доступен не только крупным компаниям, но и среднему бизнесу. Ключевым моментом является простота внедрения криптозамка в существующие инфраструктуры без необходимости значительных изменений в рабочих процессах и системах безопасности. Преимущества криптозамка Fplus в этом контексте следующие: Простота внедренияв уже существующие ИТ-системы. Криптозамок не требует полного перепроектирования безопасности организации, что значительно сокращает время на его интеграцию. Простота внедренияв уже существующие ИТ-системы. Криптозамок не требует полного перепроектирования безопасности организации, что значительно сокращает время на его интеграцию. Поддержка различных типов аутентификации, включая аппаратные ключи и шифрование, что позволяет эффективно защищать данные без необходимости дополнительных сложных процедур для пользователей. Поддержка различных типов аутентификации, включая аппаратные ключи и шифрование, что позволяет эффективно защищать данные без необходимости дополнительных сложных процедур для пользователей. Совместимость с промышленными стандартамибезопасности и возможность интеграции с корпоративными системами управления данными и учета, такими как системы ERP и CRM. Это упрощает внедрение решения в уже работающие процессы и обеспечивает его быструю адаптацию для бизнеса. Совместимость с промышленными стандартамибезопасности и возможность интеграции с корпоративными системами управления данными и учета, такими как системы ERP и CRM. Это упрощает внедрение решения в уже работающие процессы и обеспечивает его быструю адаптацию для бизнеса. Криптозамок умеет проверять электронную подпись и целостность прошивки BIOS (до старта основного хоста), микрокодов компонентов оборудования, а также загрузчика и ядра ОС посредством расчета контрольных сумм (в соответствии с нормативами ГОСТ 34.11-2018). Также криптозамок может хранить загрузочные образы в доверенном хранилище (32/64 Гбайт): на них могут быть записаны утилиты для самодиагностики, например. А еще он умеет отслеживать параметры состояния сервера, включая температуру, влажность и запыленность (на базе лазерного датчика с точностью измерения до 0,1 мкм), что может помочь в диагностике работы оборудования при обслуживании вычислительных узлов. Поддержка и сертификация Для крупных компаний и государственных организаций особо важным моментом является наличие соответствующих сертификатов и лицензий, подтверждающих соответствие криптозамка Fplus стандартам безопасности. Так, используемый в устройстве криптомодуль получил сертификацию ФСБ (Федеральной службы безопасности), что позволяет использовать криптозамок в критически важных сферах: энергетике, финансовом секторе и государственных предприятиях. Кроме того, криптозамок Fplus может стать обязательным инструментом для выполнения тендерных обязательств. На тендерах, особенно в сфере строительства муниципальной и коммерческой инфраструктуры, защита данных и контроль доступа имеют важное значение. Часто участники тендера обязаны предоставить отчеты, проектные документы и финансовую информацию, которые могут стать мишенью для кибератак. Также отметим, что криптозамок соответствует нормативным требованиям Минпромторга, который активно развивает нормативную базу для обеспечения информационной безопасности в критически важных отраслях, включая строительную сферу. В частности, «Постановлению Правительства РФ № 1119» о национальных требованиях к безопасности в информационных системах, в рамках которого предусмотрены обязательства для организаций, работающих с важными инфраструктурными проектами, включая строительство. Сферы применения криптозамка и типы атак Как мы уже поняли, наибольший интерес криптозамок представляет для критических отраслей, таких как промышленность, энергетика, финансовый сектор и государственные учреждения. Основные угрозы, от которых защищает устройство: Удаленный взлом административной панели сервера Удаленный взлом административной панели сервера Атаки с использованием вредоносного ПО для захвата контроля над сервером Атаки с использованием вредоносного ПО для захвата контроля над сервером Попытки физического вмешательства или замены компонентов оборудования Попытки физического вмешательства или замены компонентов оборудования На  момент публикации материала постановка на контроль целостности ПО и подсчет контрольных сумм файлов проводится только при загрузке оборудования, поэтому у администратора нет возможности вносить изменения в правила безопасности на работающей системе. Одна из основных фишек криптозамка – обеспечение возможности разграничения доступа, что позволяет исключить разовые пропуска, а также уменьшить число сценариев, при которых требуется физический доступ к оборудованию у неавторизованных пользователей. Кроме того, авторизованный доступ к серверу по vLan позволяет исключить доступ к настройкам BIOS/BMC тех пользователей, которые не обладают правами администратора. Предположим, что наш криптозамок установлен на сервере, который контролирует распределительную подстанцию энергетической компании или автоматизированный цех машиностроительного завода, оснащенный станками с ЧПУ. Может быть, защищенный сервер установлен в банке и обрабатывает транзакции и данные клиентов, а, возможно, он управляет услугами телекоммуникационного оператора и обеспечивает стабильную работу мобильной связи. От этого сервера зависит нормальная жизнь, без преувеличения, миллионов людей, надежность технологических процессов и безопасность финансовых операций. От каких угроз защитит его криптозамок? 1. Предотвращение удаленного доступа к системе управления Допустим, что злоумышленники попытаются получить доступ к контроллеру управления сервером (BMC) через уязвимость в программном обеспечении. Без криптозамка они могли бы подключиться к серверу и изменить его настройки, внести изменения в обрабатываемые данные, получить доступ к конфиденциальной информации банковских пользователей или нарушить работу сотовой сети. Кстати, взломанный сервер оператора связи может использоваться злоумышленниками для организации DDoS-атак. Однако криптозамок блокирует любые прямые подключения к BMC, обеспечивая доступ только через зашифрованный канал, защищенный аппаратным ключом. Это предотвращает взлом даже при наличии софтверных уязвимостей. А для дальнейшего взаимодействия с системой потребуется двухфакторная аутентификация администратора оборудования с использованием аппаратного СКЗИ, который есть только у авторизованных сотрудников. Таким образом, доступ для внедрения вредоносного кода будет заблокирован. 2. Реагирование на физический доступ к серверу Предположим, злоумышленник пытается физически вскрыть сервер, чтобы напрямую подключить к нему устройство для кражи данных или внедрить вредоносное ПО. Датчик вскрытия, встроенный в криптозамок, немедленно фиксирует попытку проникновения и передает сигнал на центральный сервер управления. Далее, в зависимости от настроек, последует автоматическая блокировка работы системы, сервер может быть отключен или переведен в безопасный режим, а администратору будет отправлено уведомление и запись данных о попытке вмешательства. 3. Выявление вредоносного ПО в цепочке поставок Многие компании используют стороннее программное обеспечение, которое может быть скомпрометировано на этапе поставки. А если на сервер загрузится обновление ПО со встроенным вредоносным кодом, он может начать некорректно управлять производственными процессами. Как следствие: это может привести к сбоям, снижению качества продукции или полной остановке оборудования на линиях. Криптозамок Fplus, оснащенный функцией валидации аппаратной и программной конфигурации сервера, сможет обнаружить несанкционированные изменения в программной структуре или установленном оборудовании, предотвращая запуск вредоносного кода. При обнаружении отклонений система оповещает администратора и блокирует запуск серверных процессов до устранения проблемы. 4. Угрозы со стороны инсайдеров Одной из наиболее сложных в выявлении и потенциально опасных угроз для инфраструктуры компаний являются инсайдерские атаки. Инсайдер с доступом к серверу может попытаться изменить параметры системы для того, чтобы получить финансовую выгоду или отомстить за что-то. Например, перенаправить банковские платежи на другие счета. Но…, так как криптозамок обеспечивает строгую аутентификацию пользователей через аппаратные ключи, что исключает несанкционированный доступ даже для персонала компании. Любые действия, выходящие за рамки допустимых полномочий, фиксируются и передаются представителям корпоративной службы безопасности. 5. Кража персональных данных клиентов В некоторых случаях существенный ущерб может быть нанесен не только атакуемой организации, но и широкому кругу физических лиц, которые пользуются ее услугами. Например, хакеры могут получить доступ к персональным данным клиентов банка или телеком-компаний, среди которых номера телефонов, пароли, истории звонков…. Эти данные впоследствии могут быть либо проданы в даркнете, либо использованы для мошеннических схем посредством социальной инженерии. Что в итоге? Итак, вот перечень угроз, от криптозамок защитит ваш сервер: Удаленный взлом и захват управления:предотвращение доступа к серверу через уязвимости. Удаленный взлом и захват управления:предотвращение доступа к серверу через уязвимости. Физическое вмешательство:защита от попыток вскрытия корпуса и подключения неавторизованных устройств. Физическое вмешательство:защита от попыток вскрытия корпуса и подключения неавторизованных устройств. Интеграция вредоносного ПО:контроль целостности программного и аппаратного обеспечения. Интеграция вредоносного ПО:контроль целостности программного и аппаратного обеспечения. Шпионаж и кража данных:блокировка несанкционированного доступа к критической информации о системе. Шпионаж и кража данных:блокировка несанкционированного доступа к критической информации о системе. Саботаж и нарушение работы инфраструктуры:предотвращение внесения вредоносных изменений для нарушения ее работы. Саботаж и нарушение работы инфраструктуры:предотвращение внесения вредоносных изменений для нарушения ее работы. Сбои в работе платежных систем и сетей связи:предотвращение изменений в алгоритмах, которые могут привести к отказу в обслуживании. Сбои в работе платежных систем и сетей связи:предотвращение изменений в алгоритмах, которые могут привести к отказу в обслуживании. Атаки с использованием инфраструктуры:исключение использования серверов для проведения DDoS-атак. Атаки с использованием инфраструктуры:исключение использования серверов для проведения DDoS-атак. Действия инсайдеров:минимизация риска атак со стороны сотрудников компании за счет строгой системы контроля доступа. Действия инсайдеров:минимизация риска атак со стороны сотрудников компании за счет строгой системы контроля доступа. Энергетика:использование криптозамка на серверах энергетической компании снижает риск кибератак, направленных на нарушение стабильной работы инфраструктуры. Это позволит предотвратить несанкционированные отключения электроэнергии и атаки на цепочки поставок, которые могут повлиять на экономику и безопасность целого региона. Промышленность:в этой сфере криптозамок обеспечивает надежную защиту промышленных серверов, критически важных для стабильной работы предприятий. Он предотвращает сбои в производстве, минимизирует риски утраты данных и исключает угрозу хищения технологий. Для крупных заводов и предприятий, работающих с инновационными разработками или государственной оборонной промышленностью, такое устройство необходимо. Финансовый сектор:криптозамок помогает банковским и финансовым учреждениям обеспечить надежную защиту своих серверов от кибератак, утечки данных и внутренних угроз. Это устройство создает дополнительный уровень безопасности, необходимый для поддержания доверия клиентов, защиты активов и соблюдения строгих требований регуляторов в финансовой сфере. Телеком:в коммуникационном секторе криптозамок обеспечивает надежную защиту серверов, минимизируя риски утечек данных, нарушения работы сети и использования оборудования оператора в злонамеренных целях. Это выводит уровень безопасности предоставляемых услуг на потенциально новый уровень, что особенно важно для сохранения доверия абонентов и соблюдения законодательных требований. Перспективы развития Компания Fplus будет и дальше совершенствовать криптозамок, включая возможность создания эталонной конфигурации оборудования. Это позволит идентифицировать любые отклонения от заданных параметров при запуске системы, предотвращая атаки, связанные с изменением аппаратной или программной структуры. На данный момент криптозамок совместим с серверами серии «Спутник», но в будущем ожидается адаптация технологии для других платформ. Еще инженеры Fplus планируют полностью интегрировать функции TPM (Trusted Platform Module) и RoT (Root-of-Trust) в модуль криптозамка, добавить драйверы для удобного взаимодействия с операционной системой, упростить управление ключами и дать пользователям возможность самостоятельно создавать цифровые подписи для защиты программ и прошивок. Криптозамок является важным шагом в укреплении кибербезопасности российских критических инфраструктур. Он не только минимизирует риски от наиболее распространенных атак, но и создает новые стандарты защиты, способные повысить устойчивость серверного оборудования перед современными угрозами. А с учетом эволюции методов кибератак, криптозамок может развиваться в направлении более глубокой интеграции с современными системами защиты. В частности, криптозамок можно адаптировать к новым типам хакинга, таким как атаки на квантовых вычислениях или использование искусственного интеллекта для обхода защиты. Также в приоритете у инженеров разработка новых алгоритмов для повышения скорости работы криптозамка без ущерба для безопасности. А еще будущие версии криптозамка со временем могут стать полноценной частью общих систем безопасности предприятий, объединяя их с централизованными системами мониторинга и реагирования. "
  },
  {
    "article_id": "https://habr.com/ru/companies/oleg-bunin/articles/876306/",
    "title": "Дорожная карта миграции большого хранилища данных",
    "category": "Базы данных",
    "tags": "Миграция, импортозамещение, планирование, хранилище данных, данные, управление командой, управление проектами, управление проектами и командой, планирование проектов, планирование времени",
    "text": "Недавно решали на работе задачу миграции хранилища данных. Оно у нас одно из крупнейших в отрасли, по крайней мере, в нашей стране. Оказалось, что даже на этапе планирования всё не так просто, как кажется. Делюсь проблемами, их решениями и получившейся дорожной картой. Привет, Хабр! Меня зовут Татьяна Сеземина. Я — директор портфеля проектов Т1 ИИ и руковожу проектами разработки и миграции больших хранилищ данных, сейчас мои проекты касаются отраслей  ритейла и логистики. Каждый проект миграции длится от года до нескольких лет. Одна из моих команд столкнулась с необходимостью детального планирования длительной многолетней миграции хранилища, вплоть до каждого объекта. Расскажу, почему так произошло и как такую проблему решать. Вступление То, что хранилище нужно мигрировать на новую платформу, стало понятно в 2024 году. Миграцию больше нельзя было откладывать из-за необходимости дальнейшего масштабирования решения и роста объема данных. На существующей западной проприетарной платформе  сделать это было невозможно из-за  санкций. Поэтому решили мигрировать на одну из российских платформ. Намеренно не называю, на какую именно,, так как для нашей задачи это не имеет принципиального значения. Изложенные подходы можно применить для планирования миграции с любой платформы данных на любую другую. Вместе со мной в этой задаче активно участвовала команда: Новруз Мамедов, архитектор данных; Новруз Мамедов, архитектор данных; Алексей Кулагин, системный аналитик; Алексей Кулагин, системный аналитик; Евгений Ларин, системный архитектор; Евгений Ларин, системный архитектор; Сергей Стрельников, разработчик. Сергей Стрельников, разработчик. Чтобы переехать и ничего не потерять по дороге, мы начали с обследования хранилища и построили детальный план миграции. Об этой задаче и поговорим подробнее. Для ее достижения мы собрали подробную информацию о каждом значимом объекте, включая корпоративные «песочницы». Всех объектов хранилища, включая «песочницы», оказалось очень много — более 9 000. Чтобы сохранить при миграции привычную бизнес-пользователям функциональность, выявили связи между объектами и их зависимости друг от друга. Связей было ещё больше, чем объектов. Получилась настоящая паутина, включающая в себя в том числе объекты систем-источников и связи с ними. Дерево взаимосвязей можно представить (и часто это бывает удобным) в виде таблицы. Пример такой таблицы приведу в разделе «Обследование хранилища данных». Исходные данные Мы начали строить очень подробный план с детализацией до конкретного объекта хранилища данных. И уже на этом этапе начали всплывать проблемы. Оказалось, у нашего огромного хранилища данных, объемом около 1 ПТб, частично не описана функциональность. Конечно, в этом нет ничего удивительного. У любого хранилища данных, особенно если оно развивается десятки лет, накапливаются проблемы, и мы — не исключение. Помимо недостаточного описания, у нас: отсутствовал атрибутный data lineage в полном объеме; отсутствовал атрибутный data lineage в полном объеме; отсутствовала матрица работ, необходимых для миграции. отсутствовала матрица работ, необходимых для миграции. Сложно было понять, как к этому подступиться. И мы решили начать с восстановления описания зависимостей объектов внутри хранилища. Обследование Сначала выявили неточности в существующем описании. Проблема в том, что необходимые данные были разбросаны по разным  бизнес-подразделениям. Собрать всех сотрудников обладающих нужной информацией и сразу получить от них все, что нужно, было невозможно. Поэтому мы составили опросники. Ниже привожу пример информации, которую мы собирали по каждой системе потребителе/отчету. Предоставляемая информация об отчете ID Отчета Наименование отчета Назначение отчета Бизнес-область отчета Ссылка на BI-портал Подразделение-владелец отчета Контактная информация о владельце отчета Требования ко времени готовности данных Глубина хранения данных Частота обновления данных Глубина обновления данных Требования к Data Quality Требования к разделению прав доступа Дополнительные требования Информация об источниках данных для отчета, детализированная до атрибутов ID Отчета Наименование отчета Показатель отчета Соответствующий показателю отчета объект хранилища и атрибут этого объекта Анкета о системе-потребителе Наименование системы Наименование подсистемы Бизнес-область системы Назначение системы (в каких процессах участвует) Подразделение-владелец системы Контактная информация о владельце системы Информация об источниках данных для системы-потребителя, детализированная до объектов хранилища Наименование системы Наименование подсистемы Объект хранилища Требования ко времени готовности данных Глубина хранения данных Частота обновления данных Глубина обновления данных Пришлось делать несколько шаблонов, так как запрашиваемые данные для разных бизнес-подразделений тоже различались. Зато с помощью  опроса нам удалось узнать какие  объекты хранилища данных используются в качестве источников корпоративной отчетности, в интеграции с системами-потребителями и других целях. В результате у нас появился список объектов, на которых базируется каждая бизнес-функциональность. Так одной проблемой стало меньше. После этого можно было разбираться со второй проблемой. Существующий data lineage был не полным, в нем следовало закрыть «разрывы». Для этого на основе существующих метаданных разработали универсальный алгоритм формирования дерева зависимости (data lineage) для любого объекта хранилища до источника данных. Дерево зависимостей мы представили в виде таблицы. Ниже схематичный пример такой таблицы без упоминания названий конкретных слоев, в каждой компании их набор будет свой: Система-источник Слой хранилища 1 ... Слой хранилища N Система- потребитель / Отчет Объект источника Объект слоя 1 ... Объект слоя N Отчет и т.д. ... ... ... ... В процессе, благодаря алгоритму, мы выявили и устранили «разрывы» в data lineage некоторых объектов. А улучшение технического качества метаданных стало приятным побочным эффектом наших работ. Также алгоритм выявил существенное количество витрин, формирующихся непромышленным способом, то есть в «песочницах». Это позволило уточнить рамки работ, и «опромышливание» таких витрин стало одной из задач проекта миграции. Таким образом, одним из слоев нашего хранилища стали «песочницы». Информация из data lineage дала возможность классифицировать все объекты хранилища, как объекты одного из слоев: Слой сырых данных - RAW; Слой сырых данных - RAW; Детальный слой - DDS; Детальный слой - DDS; Слой базовых витрин - BaseMart; Слой базовых витрин - BaseMart; Слой бизнес-витрин - BusinessMart; Слой бизнес-витрин - BusinessMart; «Песочницы» - SandBoxes. «Песочницы» - SandBoxes. В итоге обследование позволило описать все имеющиеся зависимости в хранилище данных, и можно было двигаться дальше. Справочник работ, драйверы трудозатрат и метрики Холстеда Нам оставалось решить ещё одну важную проблему и подготовить матрицу работ, необходимых для миграции. Сам список работ получили из множества интервью с экспертами департамента управления данными и бизнес-подразделений. Получился подробный список с полной детализацией работ на всех этапах создания КХД по слоям. Фрагмент  списка работ (как пример) приведен ниже: Работа Разработка логической модели данных Архитектурное review логической модели данных Согласование логической модели данных Создание прототипа трансформации данных Заполнение source-to-target Разработка потоков загрузки и трансформации данных Проведение приемо-сдаточных испытаний Развертывание релиза в промышленной среде и т.д. Для каждого вида работ в списке мы определили драйвер, то есть сущность, которую можно посчитать, и в зависимости от которой можно измерять трудозатраты. Пример связи списка работ и драйверов: Работа Драйвер Создать ЛМД в Power Designer Количество атрибутов таблицы DDS Сформировать S2T Количество таблиц DDS Сформировать требования / методологию формирования витрин Количество бизнес-витрин и т.д.  Метрики трудозатрат были определены: для каждого вида работ; для каждого вида работ; для каждой  роли (системный аналитик, разработчик, архитектор, бизнес-аналитик, аналитик Data Governance); для каждой  роли (системный аналитик, разработчик, архитектор, бизнес-аналитик, аналитик Data Governance); для каждого драйвера. для каждого драйвера. Теперь, имея точный список объектов хранилища, видов работ и драйверов, а также метрики по ролям, мы могли посчитать трудозатраты. Стоит отметить, что для большинства видов работ метрики трудозатрат задаются экспертно в виде констант. Но есть такой вид работы как создание прототипа в виде SQL-кода на основе анализа кода хранимых процедур в текущем КХД. В этом случае трудозатраты будут зависеть от «сложности» кода (количества сущностей, атрибутов, количества операций) и не могут быть заданы константой. Для оценки сложности кода и расчета трудозатрат на его миграции Алексей Кулагин и Сергей Стрельников предложили использовать и реализовали алгоритм на основе метрик Холстеда, адаптировав его для SQL. Использование известных отраслевых метрик дало возможность упростить согласование дорожной карты миграции всеми заинтересованными сторонами. Теперь у нас были все взаимосвязи внутри хранилища и оценка всех работ для каждого объекта по ролям. Оставалось разбить хранилище на блоки таким образом, чтобы проводить миграцию поэтапно и последовательно вводить функциональность в эксплуатацию. Разбиение хранилища на блоки миграции Мы разбили  хранилище на слои, но этого оказалось недостаточно  для детального планирования, так как хранилище очень большое и за его разные части отвечают разные специалисты на уровне бизнес-подразделений. Поэтому мы начали разбивать все объекты хранилища по функциональным блокам миграции: Выявили перечень крупных предметных областей, на которые разделено текущее хранилище данных. Выявили перечень крупных предметных областей, на которые разделено текущее хранилище данных. Каждую из крупных предметных областей разделили на более мелкие множества объектов по их функциональному назначению — на, так называемые, функциональные блоки миграции. Например, у нас были следующие функциональные блоки: ассортимент, чеки, операции, и т.д. Каждую из крупных предметных областей разделили на более мелкие множества объектов по их функциональному назначению — на, так называемые, функциональные блоки миграции. Например, у нас были следующие функциональные блоки: ассортимент, чеки, операции, и т.д. Все объекты следующих слоев RAW, DDS, BaseMart мы отнесли к конкретному функциональному блоку миграции. Разбитые объекты витринного слоя BusinessMart и «песочниц»  SandBoxes на функциональные блоки оказалось невозможно. Причина в нашем хранилище. Любая бизнес-витрина собиралась из данных разных функциональных блоков. Также, к сожалению, в ходе предварительного обследования нам не удалось выявить явные бизнес-приоритеты для миграции витрин. Тогда мы пошли другим путем и разбили объекты витринного слоя на, как мы их назвали, пулы миграции. Объединение в пулы происходило так: Для каждого объекта витринного слоя и слоя «песочниц» построили полное дерево зависимостей вплоть до объектов систем-источников. Для каждого объекта витринного слоя и слоя «песочниц» построили полное дерево зависимостей вплоть до объектов систем-источников. Все объекты витринного слоя и слоя «песочниц» отсортировали так, что до каждого конкретного объекта в этом списке шли все объекты витринного слоя, на которые он ссылается прямо или опосредованно (то есть через другие объекты). Все объекты витринного слоя и слоя «песочниц» отсортировали так, что до каждого конкретного объекта в этом списке шли все объекты витринного слоя, на которые он ссылается прямо или опосредованно (то есть через другие объекты). Для каждого объекта были заранее известны значения необходимых на его миграцию трудозатрат. Мы разбили этот отсортированный список на группы так, чтобы в каждой группе было примерно одинаковое количество трудозатрат. Для каждого объекта были заранее известны значения необходимых на его миграцию трудозатрат. Мы разбили этот отсортированный список на группы так, чтобы в каждой группе было примерно одинаковое количество трудозатрат. Эти группы мы и назвали пулами миграции витрин. Эти группы мы и назвали пулами миграции витрин. Наше обследование показало какие отчеты и системы-потребители «питаются» из каждой витрины. Поэтому мы могли точно сказать, какие отчеты и потоки в системы-потребители можно мигрировать на новую платформу после миграции каждого конкретного пула. Такой подход с разбиением витрин на пулы очень важен. Именно он является критерием поэтапной миграции при составлении дорожной карты. Такая механистическая разбивка витрин, как ни странно, привела к их объединению в группы по родственному признаку. После того как мы выявили все зависимости объектов, трудозатраты на каждый вид работ миграции и разбили объекты на группы, в которых будем мигрировать. Наконец можно было приступить к формированию дорожной карты миграции. Финальная дорожная карта миграции У финальной карты два больших последовательных этапа: Миграция ядра хранилища - миграция слоев RAW, DDS  и BaseMart; Миграция ядра хранилища - миграция слоев RAW, DDS  и BaseMart; Миграция витринного слоя – BusinessMart и SandBoxes. Миграция витринного слоя – BusinessMart и SandBoxes. Эти этапы  фактически идут последовательно, с небольшим «нахлестом» в один месяц. То есть сначала полностью завершается миграция ядра хранилища, и потом начинается миграция витринного слоя. Миграция ядра хранилища разбита на функциональные блоки. Каждый блок можно мигрировать независимо. Поэтому в нашем случае часть функциональных блоков разрабатывали параллельно, чтобы сократить общий срок проекта миграции. Все работы по миграции ядра в рамках функционального блока объединили в четыре этапа: системный анализ (наведение порядка и закрытие неточностей в документации); системный анализ (наведение порядка и закрытие неточностей в документации); миграция RAW-слоя; миграция RAW-слоя; миграция DDS-слоя; миграция DDS-слоя; миграция BaseMart-слоя. миграция BaseMart-слоя. Схематично последовательность выполнения выглядит так: То есть системный анализ и миграция слоя RAW идут параллельно. Затем идут последовательно миграции слоя DDS и слоя BaseMart. Дорожная карта миграции ядра хранилища выглядит так: Сценарий Консервативный Роль Системный аналитик Вид ресурса Подрядчик Наименование показателя FTE  Январь 2025 Февраль 2025 Март 2025 Апрель 2025 Май 2025 Июнь 2025 Июль 2025 и т.д. Итого *** *** *** *** *** *** ***  Функциональный блок 1         - Системный анализ *** ***       - Миграция RAW   ***      - Миграция DDS    *** *** *** ***  - Миграция BaseMart         Функциональный блок 2         - Системный анализ *** *** ***      - Миграция RAW    ***     - Миграция DDS     *** *** ***  - Миграция BaseMart         Функциональный блок 3         - Системный анализ         - Миграция RAW    ***     - Миграция DDS     *** *** ***  - Миграция BaseMart         и т.д.         Миграция витринного слоя начинается после окончания миграции ядра и строится на таких принципах: Каждый следующий пул зависит от всех предыдущих и не включает  трудозатраты объектов предыдущих пулов. Каждый следующий пул зависит от всех предыдущих и не включает  трудозатраты объектов предыдущих пулов. В плане миграции пулы идут строго последовательно  с небольшим «нахлестом» в месяц. В плане миграции пулы идут строго последовательно  с небольшим «нахлестом» в месяц. Этот «нахлест» — за счет старта работ по системному анализу до того, как закончатся работы по предыдущему пулу витрин. Этот «нахлест» — за счет старта работ по системному анализу до того, как закончатся работы по предыдущему пулу витрин. Ввод функционала витрин в эксплуатацию идет поэтапно: один пул после другого, а не вся функциональности в конце разом. Ввод функционала витрин в эксплуатацию идет поэтапно: один пул после другого, а не вся функциональности в конце разом. В теории все объекты можно ввести в промышленную эксплуатацию в конце разом. Но важно исходить из здравого смысла, так как на одной чаше весов сроки, а на другой — необходимость привлечения разово большого числа специалистов и трудности управления слишком большой командой. А поэтапная миграция позволяет вводить и отключать части огромного хранилища данных с наименьшими рисками и снизить нагрузку на техническую поддержку в моменте. Дорожная карта миграции витринного слоя: Сценарий Консервативный Наименование показателя FTE  Декабрь 2025 Январь 2026 Февраль 2026 Март 2026 Апрель 2026 Май 2026 Июнь 2026 и т.д. Пул 1 *** *** ***      Пул 2  *** *** ***     Пул 3   *** *** ***    Пул 4    *** *** ***   Пул 5     *** *** ***  Пул 6      *** ***  Пул 7       ***  Пул 8         и т.д.         Наша дорожная карта на всех ее этапах строится в разрезе нескольких сценариев, оцениваются различные виды ресурсов и роли. Отличительные особенности нашего плана: Функционал старого хранилища выводится из эксплуатации поэтапно так же, как и вводится функционал нового, импортозамещенного, хранилища. Это позволяет распределять нагрузку команды во времени. Функционал старого хранилища выводится из эксплуатации поэтапно так же, как и вводится функционал нового, импортозамещенного, хранилища. Это позволяет распределять нагрузку команды во времени. Планирование детализировано до каждой роли для всех видов работ каждого слоя хранилища данных. Планирование детализировано до каждой роли для всех видов работ каждого слоя хранилища данных. Наш план легко пересчитывается при изменении объема и состава работ. Наш план легко пересчитывается при изменении объема и состава работ. Плюсы нашей дорожной карты Такой подход позволил сформировать критерии поэтапной миграции функционала и старта его работы на новой платформе. Для каждого пула известен список отчетов и потоков с системами-потребителями, готовых к переезду на целевую платформу после миграции конкретного пула. Поэтапная миграция  позволила раньше вывести из эксплуатации часть функционала на старой платформе и сэкономить средства. Наш план достаточно гибок и легко пересчитывается при изменении ряда параметров: Дата начала миграции функционального блока ядра; Дата начала миграции функционального блока ядра; Длительность миграции функционального блока ядра; Длительность миграции функционального блока ядра; Дата готовности источников для подключению к слою RAW. Дата готовности источников для подключению к слою RAW. Было рассчитано несколько сценариев плана в зависимости от готовности подключения источников в RAW. Сценарии зависели от следующих вариантов: ранняя дата начала миграции слоя  RAW$ ранняя дата начала миграции слоя  RAW$ поздняя дата начала миграции RAW (самая поздняя дата, когда мы можем начать подключение источников). поздняя дата начала миграции RAW (самая поздняя дата, когда мы можем начать подключение источников). Таким образом, изменив некоторые параметры вы можете подстроить эту дорожную карту под свое хранилище. Итоги Формирование дорожной карты позволило нам выявить и устранить проблемы в старом хранилище, а не переносить их в новое. И получить следующие преимущества: Управлять миграцией максимально гибко, легко пересчитывать сроки, основные вехи проекта при изменении состава объектов или приоритетов их миграции. Управлять миграцией максимально гибко, легко пересчитывать сроки, основные вехи проекта при изменении состава объектов или приоритетов их миграции. Строить оптимистичные, реалистичные и пессимистичные планы. Строить оптимистичные, реалистичные и пессимистичные планы. Заранее увидеть узкие места необходимой для миграции численности персонала. Заранее увидеть узкие места необходимой для миграции численности персонала. Детально планировать привлечение специалистов в разрезе каждой роли. Детально планировать привлечение специалистов в разрезе каждой роли. Сформировать критерии миграции каждого блока отчетности на новый импортонезависимый стек. Сформировать критерии миграции каждого блока отчетности на новый импортонезависимый стек. И наконец, провести последующую миграцию в максимально сжатые сроки при оптимальном задействовании ресурсов. И наконец, провести последующую миграцию в максимально сжатые сроки при оптимальном задействовании ресурсов. "
  },
  {
    "article_id": "https://habr.com/ru/companies/lanit/articles/875642/",
    "title": "Как мы усовершенствовали CDP благодаря агрегатным атрибутам",
    "category": "Базы данных",
    "tags": "ланит, crm-системы, повышение конверсии, управление продажами, маркетинговая стратегия, омниканальный маркетинг, e-commerce",
    "text": "Постоянный рост конкуренции на рынке вынуждает компании учиться глубже понимать клиентов и адаптировать свои предложения под их потребности. В решении этой задачи маркетологам, аналитикам данных и владельцам бизнеса помогают платформы клиентских данных, которые улучшают персонализацию маркетинговых кампаний и повышают эффективность взаимодействия с клиентами. Под катом мы расскажем,  как платформы могут помочь в этом процессе, обсудим концепцию агрегатных атрибутов и их роль в оптимизации работы с данными о клиентах. СDP или платформа клиентских данных ― это система, которая интегрирует и управляет данными о клиентах из различных источников, как онлайн, так и офлайн. Она позволяет создавать единый профиль каждого клиента, что значительно улучшает персонализацию маркетинговых кампаний и взаимодействие с ними. CDP помогает анализировать поведение пользователей и оптимизировать свои предложения, повышая эффективность бизнес-процессов. Платформы класса CDP обладают возможностью накопления крайне детализированных знаний о клиенте и широкими возможностями микросегментации. Эти две характеристики базируются, прежде всего, на модели данных, позволяющих хранить в системе: сведения о клиентских профилях ― данные о клиентах, которые изменяются не слишком часто, сведения о клиентских профилях ― данные о клиентах, которые изменяются не слишком часто, сведения о событиях ― данные об активности клиентов (это могут быть переходы по страницам сайтов, взаимодействие с элементами мобильного приложения, заказы, просмотры товаров и т.п.). сведения о событиях ― данные об активности клиентов (это могут быть переходы по страницам сайтов, взаимодействие с элементами мобильного приложения, заказы, просмотры товаров и т.п.). Массив данных о накопленных событиях априори имеет намного больший объём, чем сведения об атрибутах профиля. Немаловажно, что вы можете использовать эти данные для микросегментации. Для несложных случаев сегментации вполне подходит построение условия сегмента по атрибутам клиентского профиля. Это достаточно простая с точки зрения вычислений задача, позволяющая быстро просмотреть относительно небольшой массив данных и сформировать сегмент аудитории. Совершенно другой сложности задача - проверить, к примеру, общую стоимость покупок за всё время хранения событий или найти клиентов, зашедших в определенный раздел магазина более Х раз за год. Такая задача под силу CDP с высокой производительностью и возможностью обработки действительно больших данных. Однако, опираясь на свой опыт, могу сказать, что есть примеры успешного решения подобных задач благодаря архитектуре, заточенной под возможности работы с по настоящему большими данными, накопленными в событиях. Как итог, был разработан новый способ взаимодействия с такими данными ― агрегатные атрибуты клиентского профиля. Как работают и зачем нужны агрегатные атрибуты В CDP-платформу на регулярной основе поступают события, имеющие несколько атрибутов. В качестве простого примера предположим, что один из атрибутов является числовым и представляет собой стоимость товара, купленного клиентом в рамках ранее поступившего события. В условиях новой функциональности мы можем создать новый атрибут клиентского профиля, который будет автоматически заполняться значением, собираемым: из конкретного атрибута событий, которые удовлетворяют заданному фильтру, из конкретного атрибута событий, которые удовлетворяют заданному фильтру, с использованием заданной агрегационной функции, с использованием заданной агрегационной функции, на установленном интервале времени от текущего момента. на установленном интервале времени от текущего момента. Таким образом, для примера, который мы рассмотрели выше,  можно создать несколько агрегатных атрибутов. Они покажут сумму покупок и средний чек за выбранные интервалы времени, например, месяц или год. И эти данные будут постоянно обновляться, отображая всегда актуальную статистику по этим критериям. Агрегатные атрибуты профиля позволяют решать сразу несколько задач. Дать доступ к агрегированным аналитическим показателям, которые должны быть вычислены при обработке входящих событий, в карточке профиля. Эти показатели могут быть использованы для кастомизации контента под конкретного клиента или в рамках процедуры экспорта данных во внешние системы. Дать доступ к агрегированным аналитическим показателям, которые должны быть вычислены при обработке входящих событий, в карточке профиля. Эти показатели могут быть использованы для кастомизации контента под конкретного клиента или в рамках процедуры экспорта данных во внешние системы. Упростить построение сегментов за счёт использования таких показателей. Теперь можно заранее иметь предрасчитанные атрибуты, соответствующие разрезам анализа потока событий. Теперь вы можете использовать значения этих атрибутов как критерий для составления сегментов. Кроме того, что это упрощает генерацию выражений для сегментов, это также сокращает ресурсы, необходимые для осуществления сегментации, так как пропадает необходимость вычитывать весь массив событий для сбора аналогичных агрегатов на лету. Упростить построение сегментов за счёт использования таких показателей. Теперь можно заранее иметь предрасчитанные атрибуты, соответствующие разрезам анализа потока событий. Теперь вы можете использовать значения этих атрибутов как критерий для составления сегментов. Кроме того, что это упрощает генерацию выражений для сегментов, это также сокращает ресурсы, необходимые для осуществления сегментации, так как пропадает необходимость вычитывать весь массив событий для сбора аналогичных агрегатов на лету. Поддерживаемые функции агрегации Для настройки агрегатных атрибутов можно использовать достаточно широкий спектр агрегирующих функций. Рассмотрим некоторые из них. sum/count. Простые функции, выполняющие суммирование атрибутов или подсчёт количества событий. sum/count. Простые функции, выполняющие суммирование атрибутов или подсчёт количества событий. min/max. Вычисление минимального или максимального значения. Этот агрегат позволяет определить самый дорогой товар в определенной категории, которым интересовался пользователь. min/max. Вычисление минимального или максимального значения. Этот агрегат позволяет определить самый дорогой товар в определенной категории, которым интересовался пользователь. avg/mode. Статистические агрегаты ― среднее арифметическое и наиболее популярное из значений. С помощью таких функций можно узнать среднюю протяжённость взаимодействия с клиентом или наиболее встречаемое количество товаров в корзине. avg/mode. Статистические агрегаты ― среднее арифметическое и наиболее популярное из значений. С помощью таких функций можно узнать среднюю протяжённость взаимодействия с клиентом или наиболее встречаемое количество товаров в корзине. first/last. Первое и последнее значение. Например, получить информацию о последней просмотренной услуге на сайте. first/last. Первое и последнее значение. Например, получить информацию о последней просмотренной услуге на сайте. exists. Факт существования значения. Иногда достаточно иметь возможность узнать, что пользователь выполнял действие с определенным атрибутом хотя бы один раз за период. exists. Факт существования значения. Иногда достаточно иметь возможность узнать, что пользователь выполнял действие с определенным атрибутом хотя бы один раз за период. distinct count/distinct value. Эти функции позволяют выбрать уникальные значения атрибутов и подсчитать их количество либо создать список значений. Это может быть удобно для формирования сводного атрибута интереса к категориям товаров. distinct count/distinct value. Эти функции позволяют выбрать уникальные значения атрибутов и подсчитать их количество либо создать список значений. Это может быть удобно для формирования сводного атрибута интереса к категориям товаров. Примеры сценариев использования В CDP поступают данные о покупках, сделанных в интернет-магазине компании клиентами. Создан агрегатный атрибут \"Сумма покупок, мес.\" со следующими критериями: Просматриваем события, имеющие тип (в виде атрибута события) \"Покупка\". Просматриваем события, имеющие тип (в виде атрибута события) \"Покупка\". Используем агрегатную функцию \"Сумма\" для атрибута события покупки \"Стоимость\". Используем агрегатную функцию \"Сумма\" для атрибута события покупки \"Стоимость\". Используем интервал агрегации, равный одному месяцу. Используем интервал агрегации, равный одному месяцу. Теперь достаточно настроить вычисление сегмента по критерию \"Сумма покупок за месяц\" > X тыс. рублей. Мы установим автоматическое ежемесячное обновление сегмента в первый день этого месяца. Таким образом, у вас будет доступ к аудитории, которая принесла наибольшую выручку за предыдущий месяц. Этот сегмент можно использовать для массовых рассылок. В CDP поступают данные о переходах клиента в разделы интернет-магазина. Создан агрегатный атрибут \"Последние категории\" со следующими критериями. Просматриваем события, соответствующие переходам по страницам интернет-магазина. Просматриваем события, соответствующие переходам по страницам интернет-магазина. Используем агрегатную функцию \"Уникальные значения\" для атрибута события \"Категории\". Используем агрегатную функцию \"Уникальные значения\" для атрибута события \"Категории\". Используем интервал агрегации, равный одной неделе. Используем интервал агрегации, равный одной неделе. При формировании меню на сайте движку CMS достаточно обратиться к модулю оперативного профиля (МОП) CleverData Join CDP с ID клиента. В ответ МОП предоставит атрибуты профиля, включая \"Последние категории\". Эта информация позволит CMS выделить категории, которые интересовали пользователя в течение последней недели. В CDP поступают данные, собранные через форму обратной связи в виде оценки взаимодействия с товаром или услугой. Создан агрегатный атрибут \"Средняя оценка\" со следующими критериями. Просматриваем события, соответствующие проставлению оценки. Просматриваем события, соответствующие проставлению оценки. Используем агрегатную функцию \"Среднее арифметическое\" для атрибута события \"Оценка\". Используем агрегатную функцию \"Среднее арифметическое\" для атрибута события \"Оценка\". Используем интервал агрегации, равный шести месяцам. Используем интервал агрегации, равный шести месяцам. Создадим RT-сегмент, который будет реагировать на совокупность двух условий. Появилось событие с атрибутом \"Оценка\" меньше или равно трем. Появилось событие с атрибутом \"Оценка\" меньше или равно трем. Для профиля, по которому пришло это действие, должно выполняться условие \"Средняя оценка\" меньше или равно четырем. Для профиля, по которому пришло это действие, должно выполняться условие \"Средняя оценка\" меньше или равно четырем. Существует возможность внедрения автоматической системы отправки предложений, направленных на увеличение лояльности клиентов, при соблюдении определённых условий. Важно отметить, что предложения не будут отправлены, если клиент регулярно оценивает товары или услуги высоко (например, данный товар оказался хуже его привычного уровня, но это не систематическая проблема), а также в случае, когда средняя оценка остаётся низкой, несмотря на то, что последний товар был оценён высоко. На сайте реализована форма обратной связи, доступная клиенту по его желанию. В CDP поступают данные, собранные через форму обратной связи. Создан агрегатный атрибут \"Была ли ОС за последний год\" со следующими критериями. Просматриваем события, соответствующие проставлению оценки. Просматриваем события, соответствующие проставлению оценки. Используем агрегатную функцию \"Существует\" для атрибута события \"Оценка\". Используем агрегатную функцию \"Существует\" для атрибута события \"Оценка\". Используем интервал агрегации, равный 12 месяцам. Используем интервал агрегации, равный 12 месяцам. Для генерации меню сайта движку CMS нужно только сделать запрос в модуль оперативного профиля (МОП) CleverData Join с идентификатором клиента. МОП вернёт необходимые атрибуты профиля, среди которых будет информация о том, была ли обратная связь за последний год. Это позволит CMS создать индивидуальную страницу, где форма обратной связи будет расположена выше, что может помочь клиенту обратить на нее внимание. Агрегатные атрибуты в платформах клиентских данных (CDP) играют важную роль в оптимизации работы с данными о клиентах. Они не только упрощают процесс сегментации, позволяя заранее рассчитывать и хранить важные аналитические показатели, но и обеспечивают доступ к актуальной сводной информации, что способствует более точной персонализации предложений. Благодаря агрегатным атрибутам компании могут оперативно реагировать на изменения в поведении клиентов, улучшая взаимодействие и повышая лояльность. Таким образом, использование агрегатных атрибутов позволяет значительно повысить эффективность бизнес-процессов и улучшить качество обслуживания клиентов. "
  },
  {
    "article_id": "https://habr.com/ru/companies/mws/articles/876768/",
    "title": "Как благодаря полётам в космос, лесозаготовкам и облакам появились современные СУБД",
    "category": "Базы данных",
    "tags": "субд",
    "text": "Редко можно встретить технологии, которые существуют более пятидесяти лет в стремительно меняющейся ИТ-индустрии. Пока одни разработки быстро теряют актуальность, базы данных продолжают играть ключевую роль. По мере увеличения объёмов данных растёт и потребность в инструментах для их обработки, управления и анализа. Первая СУБД, основанная на иерархическом подходе, была разработана ещё в 1960-х годах. В 1970-х появились реляционные системы, которые и сегодня остаются востребованными благодаря своей универсальности и эффективности. Сегодня обсудим, каким был путь развития СУБД. 1960-е: иерархическая модель Программа пилотируемых космических полётов «Аполлон» стала одним из самых амбициозных проектов в истории человечества. Для её успешной реализации требовались новые информационные системы. За разработку космического корабля отвечала компания North American Rockwell. Для выполнения столь масштабной задачи она заключила партнёрство с IBM — ведущим разработчиком компьютерных систем. Кроме того, к проекту привлекли специалистов компании Caterpillar Tractor, одного из лидеров в производстве инженерного оборудования. Объединенная команда разработала программу Information Control System and Data Language/Interface (ICS/DL/I). ICS стала одной из первых СУБД и успешно справлялась с обработкой огромных объёмов данных, необходимых для управления производственными процессами в реальном времени. Иерархическая модель данных в ICS построена в виде древовидной структуры: записи представлены узлами, а поля — ветвями дерева. Каждая запись (узел) может иметь только одного родителя, но при этом допускается наличие нескольких дочерних записей, что создаёт характерную иерархию. Ключевая роль в системе отводилась компоненту Data Language/Interface (DL/I) — программному интерфейсу, предоставляющему доступ к базе данных. Впоследствии DL/I стал основой для широко известной системы IBM IMS (Information Management System). Первая версия ICS,завершеннаяIBM в 1967 году, оказалась настоящим технологическим прорывом. Её принципы были настолько успешными, что позже были использованы в других крупных проектах, включая программу «Шаттл». 1960–1970: сетевая модель В начале 1960-х годов Чарльз Бахман из General Electric разработал интегрированное хранилище данных (Integrated Data Store —IDS). IDS стало одной из первых реализаций сетевой модели баз данных. В отличие от IMS, IDS предлагала более гибкий подход: записи могли быть связаны между собой через сложные сетевые структуры. Такой подход позволял создавать системы с разными типами отношений между данными. Несмотря на сложность использования IDS и необходимость глубоких знаний в области программирования, она стала революционным шагом в технологиях управления данными. IDS легла в основу сетевой модели баз данных, которая впоследствии стала центральным компонентом стандарта CODASYL (Conference on Data Systems Languages). CODASYL объединил усилия экспертов и организаций в 1960–1970-х годах для разработки стандартов работы с данными и языками программирования. IDS впервые показала, как можно манипулировать связанными данными через сети. На основе этих идей сетевые базы данных обрели популярность и стали ключевой технологией в 1960–1970-х годах, пока реляционные базы данных постепенно не вытеснили их. Однако именно IDS и работы CODASYL стали важным мостом между ранними подходами обработки данных и современными технологиями СУБД. В 1965 году Чарльз Бахман по заказу лесопромышленной компании Weyerhaeuser Lumber разработал расширение IDS — первую многопрограммную систему сетевого доступа к базе данных под названиемWEYCOS(Weyerhaeuser Company Operating System). WEYCOS стала одной из первых систем, поддерживающих обработку данных в реальном времени. С её помощью компания централизовала управление данными, упростив хранение всей информации о заказах, запасах и логистике в единой базе. Система стала ранним примером внедрения концепции онлайн-обработки транзакций (OLTP). В режимах многопользовательского доступа сотрудники могли одновременно работать с данными, а запросы обрабатывались практически моментально, что минимизировало задержки в рабочих процессах. Вклад Чарльза Бахмана в область баз данных был настолько значительным, что в 1973 году он получил премию Тьюринга за достижения в проектировании и реализации IDS, а также за развитие сетевой модели. Благодаря Бахману зародились многие концепции, которые стали основой для современных реляционных баз данных. Позже компания BF Goodrich Chemical Co. переработала IDS, сделав её более удобной в использовании. В результате появилась интегрированная система управления данными (IDMS). Она была основана на стандарте CODASYL и продолжала использовать сетевую модель, но включала более продуманные интерфейсы для разработчиков. Благодаря удобству эксплуатации IDMS стала одной из самых популярных СУБД в 1970-х и 1980-х годах. IDMS демонстрировала выдающиеся уровни производительности, особенно в проектах с большими объёмами данных и строго структурированными транзакциями. Одним из наиболее ярких примеров стала реализация базы данных дляCSS(Customer Service System) компании British Telecom. CSS обслуживала более 10 миллиардов транзакций ежегодно, что делало её одной из крупнейших и самых высокопроизводительных баз данных своего времени. Тем не менее, сетевая модель так и не стала доминирующей по двум причинам. Во-первых, IBM выбрала иерархическую модель с полусетевыми расширениями в IMS и DL/I. Во-вторых, сетевая архитектура была в итоге вытеснена реляционной моделью, которая предлагала более высокоуровневый и удобный интерфейс. 1970–1980: реляционная модель Эдгар Кодд работал в IBM над задачами хранения и обработки данных. Проанализировав существующие решения в области сетевых и иерархических моделей данных, в 1970 году он опубликовал работу «A Relational Model of Data for Large Shared Data Banks», в которой представил концепцию реляционной модели данных. Кодд предложил представлять данные в виде двумерных таблиц (отношений), где каждая строка является записью, а каждый столбец — атрибутом. Это устраняло необходимость явного программирования связей между данными, как это требовалось в сетевых и иерархических моделях, что существенно упростило разработку приложений и работу с данными. И хотя реляционная модель данных Кодда долгое время воспринималась скептически, в период с 1974 по 1977 год различными исследовательскими группами были созданы два ключевых прототипа реляционных СУБД. INGRES INGRES(Interactive Graphics and Retrieval System) была разработана командой под руководством Майкла Стоунбрейкера и Юджина Вонга. Основной задачей INGRES было исследование и практическое внедрение реляционной модели данных. Эта система использовала язык запросов QUEL, который позже был заменён на SQL (Structured Query Language) из-за растущей популярности последнего. Развитие проекта INGRES оказало значительное влияние на разработку других СУБД, таких как Sybase и Microsoft SQL Server. После завершения работы над INGRES в середине 1980-х годов Майкл Стоунбрейкер продолжил развивать концепцию реляционных баз данных, создав новый проект под названием POSTGRES (сокращение от «Post-Ingres»). POSTGRES стала основой для PostgreSQL — одной из ведущих современных СУБД с открытым исходным кодом. System R В период с 1974 по 1977 годы в IBM Research Lab была разработана СУБДSystem R. Проект возглавили исследователи Дональд Чэмбэрлин и Рэй Бойс, которые активно трудились над созданием первого языка для работы с реляционными базами данных —SEQUEL(Structured English Query Language). Позже этот язык был переименован в SQL и стал промышленным стандартом для работы с базами данных. System R стала первым проектом, демонстрирующим практическую эффективность реляционной модели. Разработка и успехи System R привели к созданию коммерческих продуктов IBM: SQL/DS (для платформы VM) и DB2 (для платформы MVS). Под влиянием System R появились и другие известные коммерческие СУБД: Oracle Database, HP Allbase и Tandem Non-Stop SQL. Разработка INGRES и System R стала поворотным моментом в истории вычислительной техники, заложив основы современной работы с данными. В то время как IDMS демонстрировала выдающуюся производительность на терабайтных базах, реляционные СУБД в 1980-х и 1990-х годах значительно превосходили её в гибкости и простоте разработки. За свои достижения Эдгар Кодд в 1981 году был награжден премией Тьюринга. Он продолжил работать над развитием и уточнением реляционной модели. В 1985 году Кодд сформулировал12 правил, которые определяют критерии соответствия настоящей реляционной базы данных. Эти правила стали важным ориентиром для разработчиков СУБД. 1980–1990: объектно-ориентированные СУБД С появлением объектно-ориентированных языков программирования возросли требования к приложениям, которые реляционная модель не могла удовлетворить. В результате была разработана концепция объектно-ориентированных баз данных (ООБД), в которых данные представляются как объекты, а не таблицы. Термин «объектно-ориентированная система управления базами данных» (ООСУБД) впервые появился около 1985 года. Однако уже к концу 1980-х — началу 1990-х годов стало очевидно, что ни реляционные, ни объектно-ориентированные базы данных не способны полностью удовлетворить требования новых приложений. Кроме того, ООСУБД часто привязываются к конкретным языкам программирования, что ограничивает их гибкость. Чтобы решить эти проблемы, исследовательские группы начали разрабатывать объектно-реляционные базы данных (ОРБД). Основой для таких систем стал расширенный стандарт SQL, который включает поддержку объектных типов. Появление объектно-реляционной модели позволило разработчикам объединить сильные стороны как реляционных, так и объектно-ориентированных баз данных, что сделало такие системы более гибкими и масштабируемыми. 1990-2000: NoSQL Реляционные базы данных изначально проектировались для работы на одном сервере в эпоху, когда горизонтальное масштабирование (через добавление серверов) оставалось практически недоступным. Эти системы идеально подходили для работы со структурой данных в виде таблиц, где информацию можно было организованно обрабатывать с помощью SQL. Однако с ростом объёмов данных, которые требовалось обрабатывать в реальном времени, возникла необходимость в других подходах. NoSQL («Not Only SQL») стал ответом на ограничения реляционных баз. Термин «NoSQL» впервыеиспользовалКарло Строцци в 1998 году для обозначения легковесной реляционной базы данных с открытым исходным кодом, которая не поддерживала традиционный SQL-интерфейс. Однако в то время термин остался вне поля зрения ИТ-сообщества. Широкую известность он получил позже, в 2009 году, благодаря усилиям Джонатана Эллиса, разработчика Apache Cassandra, а также инициативам других специалистов, связанных с движением NoSQL. NoSQL-базы данных стали незаменимым элементом современных технологий, особенно в таких областях, как потоковые данные, IoT, машинное обучение и аналитика. В 2011 году началась разработка языка неструктурированных запросов (UnQL) для работы с неструктурированными и полуструктурированными данными. В том же году Мэтью Эслетт ввёл понятие NewSQL, обозначив новый класс реляционных СУБД, которые объединяют производительность и масштабируемость систем NoSQL с транзакционностью и строгими требованиями согласованности, характерными для традиционных реляционных баз данных. Среди таких систем можно отметить CockroachDB и VoltDB. 2010-е: облачные СУБД Ключевым фактором развития СУБД в 2010-х годах стало взрывное увеличение объёмов данных. Миграция реляционных СУБД в облако позволила организациям воспользоваться всеми преимуществами облачных вычислений: лёгким масштабированием, высокой доступностью и снижением затрат на управление локальной ИТ-инфраструктурой. В этот период произошли значимые изменения не только в переносе традиционных решений на виртуальную инфраструктуру, но и в разработке cloud-native баз данных, изначально созданных для работы в распределённой среде. Облачные технологии предоставили быстрый доступ к мощным инструментам обработки данных, увеличив масштабируемость и обеспечив поддержку высоконагруженных приложений. В 2020-х годах машинное обучение и ИИ оказали значительное влияние на развитие СУБД. Появилисьплатформенные решения, которые объединяют хранилища данных (Data Warehouses) с гибкостью озер данных (Data Lakes). Базы данных в облаке (например,DBaaS PostgreSQL) позволили быстро воспользоваться всеми преимуществами современных БД без затрат на администрирование и поддержку инфраструктуры. 2020-е: векторные СУБД Идея использования векторных представлений тесно связана с развитием ИИ. Нейронные сети, активно развивавшиеся с 1980-х годов, применяли вектора (веса и входные значения) для отображения данных. Однако способы их хранения долго оставались на уровне экспериментов отдельных исследователей. Лишь в 21 веке появились полноценные векторные базы данных, чему способствовали достижения в обработке данных (например, методы NLP и анализа изображений) и разработка эффективных алгоритмов работы с векторными структурами, таких как HNSW. Векторы представляют данные (текст, изображения или аудио) в виде точек в многомерном пространстве, где расстояние между ними указывает на семантическую близость объектов. При работе с большими объемами высокоразмерной информации векторные базы данных позволяют быстро находить похожие объекты, что делает их оптимальным инструментом для этой задачи. С развитием методов обучения представлениям (representation learning), особенно в сфере больших языковых моделей и компьютерного зрения, растёт спрос и на векторные базы данных. Gartner выделяет их среди ключевых технологий будущего наряду с квантовыми вычислениями и web3. Постоянное внедрение инноваций и адаптация к меняющейся ИТ-экосистеме сделали СУБД гибкими и способными справляться с современными задачами. Современные системы эффективно обрабатывают огромные объёмы данных в реальном времени, поддерживают аналитические процессы и интеграцию с методами машинного обучения."
  },
  {
    "article_id": "https://habr.com/ru/companies/yadro/articles/875566/",
    "title": "Мониторинг систем хранения данных в Docker Compose: из чего состоит и как его повторить",
    "category": "Базы данных",
    "tags": "мониторинг, схд, системы хранения данных, grafana, alertmanager, opensource",
    "text": "Год назад к инженерам YADRO обратился клиент с просьбой помочь с настройкой мониторинга для СХДTATLIN.UNIFIED. Для семейства TATLIN у компании есть продукт TATLIN.SATELLITES (спутники) — это инструменты, включающие драйверы, плагины и шаблоны для интеграции систем хранения данных со сторонними системами. Но пользователю требовалось готовое интегрированное решение, которое бы не нагружало инженеров компании. Так появился Monitoring Appliance — приложение для мониторинга систем хранения данных, которое можно развернуть на сервере за пять минут. В статье рассказываем, как собирать с СХД все возможные данные и где могут быть подводные камни. Задача клиента Пользователю более сотни систем хранения данных нужно было выделять дисковое пространство СХД на различные задачи: операционную деятельность, тестирование, резервное копирование. Кроме этого, было необходимо следить за переподпиской пулов, так как тома СХД быстро заполнялись. Для самостоятельного решения задачи клиент использовал стандартные возможности известных систем мониторинга и скрипты на базе электронных таблиц, но на это уходило немало времени, а доступные инструменты не решали проблему в полной мере. Инженеры YADRO решили взять эту задачу на себя и включить в состав продуктаTATLIN.SATELLITESинтегрированное решение для мониторинга. Разрабатываемый мониторинг должен был включать компоненты для сбора и хранения метрик со множества массивов, а также их визуализацию и алертинг. Она должна быть простой в использовании, не требовать долгого изучения документации и затрат ресурсов инженеров клиента. Она должна быть простой в использовании, не требовать долгого изучения документации и затрат ресурсов инженеров клиента. Состоять из доступных open source-решений. Состоять из доступных open source-решений. Позволять мониторить состояния массивов данных и принимать решения об их дополнении или расширении. Позволять мониторить состояния массивов данных и принимать решения об их дополнении или расширении. Иметь понятную и простую для заказчика визуализацию. Иметь понятную и простую для заказчика визуализацию. Итогом стала система, получившая название Monitoring Appliance. Клиенту она может поставляться как Docker Compose или образ виртуальной машины (по отдельному запросу). Архитектура приложения для мониторинга По сути, это приложение — набор инструментов с открытым исходным кодом, которые связаны между собой и упакованы, например в Docker Compose. На схеме — общее представление того, как связаны между собой сервисы. Далее расскажем, почему выбрали тот или иной инструмент и как они взаимодействуют между собой. Итак, у нас есть какое-то количество систем хранения данных TATLIN.UNIFIED, с которых мы хотим собирать метрики и мониторить их состояние. Информацию о состоянии СХД мы можем получать разными способами. Сбор информации через SNMP get.Нам нужен был инструмент, который бы отправлял запросы до TATLIN.UNIFIED. В спутниках TATLIN (TATLIN.SATELLITES) есть Zabbix-шаблоны, которые помогут настроить мониторинг с помощью Zabbix. Но полученными данными сложно манипулировать штатными средствами. Даже подключив Grafana к Zabbix, мы сильно ограничены функционалом плагина для Zabbix. Поэтому мы выбрали Prometheus-метрики, так как язык запросов PromQL дает нам большую свободу при манипуляции данными. Но есть нюанс: Prometheus не умеет работать с протоколом SNMP напрямую. Поэтому в архитектуре сервиса появился дополнительный компонент — Prometheus SNMP Exporter. Он трансформирует полученные метрики в формат, который может считывать Prometheus. Через SNMP get мы обычно получаем с TATLIN.UNIFIED инвентарную информацию, серийные номера, статусы дисков, полок, процессоров, метрики производительности. В архитектурной схеме Monitoring Appliance приложение Prometheus вы не найдете. Все потому, что в нашем случае мы используем агент, встроенный в Victoria Metrics. Он опрашивает СХД TATLIN через SNMP Exporter и забирает метрики, а полученные данные хранятся в базе данных Victoria Metrics — хорошем решении для хранения time series-данных. Частоту запросов через SNMP get пользователь может определить самостоятельно. В целом, мы понимаем, что получение данных только по протоколу SNMP накладывает ряд ограничений. Поэтому разрабатываем решение, которое позволит собирать данные через API. Сбор SNMP trap-сообщений.Через SNMP trap обычно приходят сообщения о неисправностях в системе, в том числе хардварных, — например, о проблемах с PSU (Power Supply Unit). Для сбора SNMP trap нет готового решения, поэтому мы написали отдельный Python-скрипт — Python Handler. Он трансформирует трапы, полученные при помощи snmptrapd, в метрики, которые можно положить в Victoria Metrics, или упаковывает в логи и кладет в Grafana Loki. На сообщение в таком формате уже можно настроить алертинг и сообщать пользователю, если что-то пошло не так. Сбор Syslog-сообщений.Кроме SNMP, TATLIN может передавать информацию через Syslog — этот «канал» также можно использовать. На стороне СХД настраиваются таргеты, куда отправлять Syslog. В нашей системе их принимает Rsyslog и трансформирует в формат, пригодный для Promtail. Последний, в свою очередь, это агент, который может принимать логи и класть их в уже упомянутую Grafana Loki, базу для хранения логов. В основном Syslog передает данные аудита СХД и такие же сообщения, как SNMP trap, только в специфичном для Syslog формате. Базы данных системы Для хранения данных мониторинга в системе два основных инструмента:Grafana LokiиVictoria Metrics(в будущем, мы планируем перейти на VictoriaLogs вместо Grafana Loki). Первая хранит логи работы, с помощью которых можно ретроспективно посмотреть работу систем хранения данных. Помимо Syslog, здесь можно хранить и SNMP-трапы, если сконвертировать их именно в логи, а не метрики. Victoria Metrics — просто хорошая база данных, известная на рынке и популярная для работы именно в системах мониторинга. Но ее можно легко заменить на иную БД, подходящую для хранения временных рядов, — например, Cortex Metrics. Сопутствующие инструменты и настройка алертов Получившаяся система может работать в Docker Compose. Чтобы начать им пользоваться, достаточно развернуть на сервере — задача на 10-15 минут. При этом в Monitoring Appliance заложен мониторинг не только систем хранения данных, но и сервера, на котором оно стоит. Для этого есть Telegraf — агент, мониторящий сервер-хост: как утилизируется мощность процессора, сколько памяти еще свободно и подобные метрики «здоровья». Мониторинг и красивые дашборды — это замечательно, но редкий специалист готов изучать графики каждую рабочую минуту. Поэтому нам нужна система алертинга. Это не обязательно должны быть сообщения об ошибках, алерт можно настроить и на достижение определенного порога «заполнения» пулов и ресурсов в СХД. Здесь на помощь приходят VM Alert и Alert Manager. В VM Alert от Prometheus формируется определенный набор алертов, в котором прописаны пороговые значения для метрик. По заданному пользователем интервалу VM Alert запрашивает метрики в VictoriaMetrics и проверяет, не достигнуто ли условие срабатывания правила алертинга. Если да, то он генерирует алерт, который можно увидеть в Grafana или дашборде с алертами. Также можно настроить оповещение через внешние системы (Alert Manager) — Telegram (или другой мессенджер), почту и так далее. Инженеры YADRO разработали примеры конфигурационных файлов и шаблонов сообщений для алертов, которые может использовать клиент. И он вправе расширять их или изменять. Ну, и наконец в систему добавлен Nginx — веб-сервер, который работает как reverse proxy. Он позволяет настроить TLS-шифрование — так пользователь может через Nginx ходить либо по доменным именам на каждой компонент, либо по путям (например, /grafana, /alertmanager и так далее). Возможности YADRO Monitoring Appliance Если обобщать возможности собранной системы, то можно составить следующий список. Наша система мониторинга может: Собрать метрики производительности компонентов СХД по протоколу SNMP. Собрать метрики производительности компонентов СХД по протоколу SNMP. Принять и обработать SNMP traps от СХД. Принять и обработать SNMP traps от СХД. Принять и обработать Syslog-сообщения от СХД. Принять и обработать Syslog-сообщения от СХД. Мониторить состояние сервера, на котором установлен Monitoring Appliance. Мониторить состояние сервера, на котором установлен Monitoring Appliance. Отображать данные мониторинга в виде дашбордов. Отображать данные мониторинга в виде дашбордов. Оповещать о внештатных ситуациях и пороговых состояниях. Оповещать о внештатных ситуациях и пороговых состояниях. А теперь посмотрим, какие дашборды в итоге получается делать благодаря этому мониторингу «из коробки». Главная страница.На ней отображаются основные метрики «здоровья» систем хранения данных, на которых хочет сфокусироваться пользователь. Мы предоставляем «шаблон» дашборда. Можно выбрать нужную СХД и получить данные конкретно по ней. Но этот дашборд легко пересобрать, исходя из своих целей.  Пулы.Здесь можно собрать информацию по пулам со множества массивов: посмотреть свободную физическую емкость и oversubscription. При нажатии на пул мы проваливаемся к списку ресурсов. Также есть отдельный дашборд с общим списком пулов, в каждый из которых можно провалиться и посмотреть, какие ресурсы находятся в этом пуле. А если кликать по ресурсам, то отобразится перформанс по каждому ресурсу. Отчет по алертам.Это Grafana-дашборд, который использует DataSource, который работает с Alertmanager по API. WWID-лист.Список с ID каждого ресурса, соответствующим хостам, пулам и так далее. Перфоманс ресурсов.Дашборд по ресурсам и задержкам (latency). Это лишь несколько примеров дашбордов, которые можно получить с помощью приложения для мониторинга. В целом, их список может ограничиваться только фантазией пользователя и его умением пользоваться Grafana. Удобный и бесплатный мониторинг для пользователей TATLIN В итоге у нас получился простой и легкий в управлении и масштабировании инструмент. Его могут использовать бесплатно все клиенты YADRO. У приложения есть неочевидный дополнительный плюс. Использование каждого компонента в отдельности (например, Grafana или Victoria Metrics) требует отдельного согласования с департаментом по информационной безопасности компании (так как они не входят в Росреестр одобренного ПО). Упакованный же в Appliance готовый сервис проще согласовать как единую систему, поставляемую YADRO. Так один частный запрос клиента спровоцировал появление полезного инструмента, который пополнил список продуктов-спутников TATLIN. Теперь этот клиент пользуется мониторингом ежедневно и экономит время на отслеживание метрик «здоровья» и утилизации СХД. Задачи, которые раньше занимали часы и даже дни, решаются за минуты с помощью дашбордов. Теперь компания понимает, как лучше распределить по массивам новые ресурсы: где емкости не хватает, а где есть свободные слоты. Это вводная статья про эффективный мониторинг ресурсов СХД. В следующих статьях мы хотим подробнее осветить технические моменты сборки этого мониторинга в ряде инструкций. Сможете повторить, если перед вами стоит подобная задача. Интересен ли вам такой материал? Пишите в комментариях, какую связку системы осветить в первую очередь, или просто ставьте плюсик этой публикации. Так мы поймем, что вы заинтересованы в продолжении. "
  },
  {
    "article_id": "https://habr.com/ru/companies/pqhosting/articles/875360/",
    "title": "Запускаем собственный сокращатель ссылок Slash на VPS",
    "category": "Базы данных",
    "tags": "хостинг, сервер, vps, сокращатель ссылок, slash",
    "text": "Привет! На связи Игорь из техподдержки PQ.Hosting! Я продолжаю писать о незаезженных self-hosted приложениях, которые можно легко установить на свой виртуальный сервер. В прошлый раз ярассказывал, как поднять собственную интернет-машину времени с помощью Archive box. В этот раз речь пойдет про Slash —гитаристе группы Guns N’ Rosesинтересном веб-сервисе для сокращения и кастомизации ссылок. Пару слов о Slash Slash— веб-приложение с открытым исходным кодом, позволяющее создавать и кастомизировать короткие ссылки (или шорткаты, как называют их авторы) и хранить их на сервере. Кроме того, у Slash есть возможность добавлять теги и группировать ссылки в коллекции. Это удобно, если работаешь с несколькими проектами (например, личными и рабочими) и хочешь их красиво систематизировать. Ещё одно полезное дополнение — аналитика. Можно посмотреть, как часто по ссылке переходят, откуда приходит трафик и насколько она полезна. Например, эта фишка будет очень полезна для блогеров или маркетологов. Получается, что Slash — это такая сокращалка на стероидах, которая также совмещает в себе фичи менеджера ссылок и закладок. Кстати, отпишитесь в комментариях пользуетесь ли вы такими приложениями? Или по старинке сохраняете все в закладках браузера? Нам будет очень интересно почитать ваши ответы! Где-то здесь у многих появился закономерный вопрос: зачем морочиться с размещением приложения на виртуальном сервере, если в поисковиках легко можно найти уже готовые сайты-сокращалки. Ответ простой — дело в удобстве и безопасности. Мы как пользователи не можем на 100% утверждать, что владельцы сервисов не вшивают трекеры или другое вредоносное ПО в укороченные ссылки. Поэтому, как нам кажется, лучше лишний раз не рисковать и поднять опенсорсный сервис. Так вы точно будете уверены, что с вашими сокращенными ссылками никто не покопался. Как установить Slash Slash использует для работы Docker, поэтому для начала нужно будет поставить саму платформу. Я покажу, как это делается с помощью Apt. Если вам такой способ не подходит, но на сайте Докералежатальтернативные инструкции. Обновляем репозитории и добавим GPG-ключи: apt-get updateapt-get install ca-certificates curlinstall -m 0755 -d /etc/apt/keyringscurl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.ascsudo chmod a+r /etc/apt/keyrings/docker.asc После добавляем репозиторий в apt: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\ tee /etc/apt/sources.list.d/docker.list > /dev/null apt-get update Далее устанавливаем сам Docker следующей командой: apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin А вот теперь можно и поставить Slash. Хорошая новость в том, что достаточно будет одной команды: docker run -d --name slash -p 5231:5231 -v ~/.slash/:/var/opt/slash yourselfhosted/slash:latest Как и раньше, для теста я использую VPS начального уровняAluminiumс одним ядром процессора Xeon E5-2697A, гигабайтом ОЗУ и SSD на 25 гигабайт. Выполнять в терминале команды, назначения которых не особо понимаешь — не самая хорошая привычка. Поэтому подробно поясняю за каждую команд: docker run — говорит Docker создать и запустить новый контейнер. docker run — говорит Docker создать и запустить новый контейнер. -d — запускает контейнер в фоновом режиме (detached mode). -d — запускает контейнер в фоновом режиме (detached mode). --name slash — задаёт контейнеру имя «slash», чтобы к нему было проще обращаться. --name slash — задаёт контейнеру имя «slash», чтобы к нему было проще обращаться. -p 5231:5231 — связывает порт 5231 контейнера с портом 5231 на хосте, чтобы можно было получить доступ к Slash через браузер. -p 5231:5231 — связывает порт 5231 контейнера с портом 5231 на хосте, чтобы можно было получить доступ к Slash через браузер. -v ~/.slash/:/var/opt/slash — создаёт том для хранения данных Slash на вашей машине, чтобы они не терялись при перезапуске контейнера. -v ~/.slash/:/var/opt/slash — создаёт том для хранения данных Slash на вашей машине, чтобы они не терялись при перезапуске контейнера. yourselfhosted/slash:latest — указывает, какой образ Docker использовать, в этом случае — последнюю версию Slash. yourselfhosted/slash:latest — указывает, какой образ Docker использовать, в этом случае — последнюю версию Slash. Результат выполнения команды в терминале. Далее Slash будет доступен по адресуhttp://ip-вашего-сервера:5231 После от вас потребуется только зарегистрироваться — здесь все очень стандартно. А после регистрации вы окажетесь на главной странице сервиса. Как использовать Slash Лучше всего показать возможности сервиса на примере. Для теста возьму ссылку из Википедии —https://ru.wikipedia.org/wiki/%D0%A1%D0%BA%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D1%8B%D0%B5_%D0%BE%D0%B4%D0%BD%D0%BE%D0%B9_%D1%86%D0%B5%D0%BF%D1%8C%D1%8E Из-за кириллицы в названии текст ссылки преобразуется в не очень понятную абракадабру. В базу данных или документацию «урлу» такого вида ставить не хочется. В этом случае как раз и пригодится Slash. Тут все просто: Жмем на Create. Жмем на Create. Добавляем 2 обязательных поля — Name (это имя будущей ссылки) и Link (сама ссылка) Добавляем 2 обязательных поля — Name (это имя будущей ссылки) и Link (сама ссылка) Вписываем Title и Description (это своего рода пояснения к ссылке для себя) и теги. Вписываем Title и Description (это своего рода пояснения к ссылке для себя) и теги. В итоге у вас должна получиться ссылка формата:http://IP-адрес-сервера/s/Nau. В идеале, конечно, зарегистрировать под это дело какой-нибудь красивый адрес и подключить домен к Slash. Но это уже совсем другая история. Поставили бы такое приложение на свой сервер? Обязательно напишите об этом в комментариях! А если вы знаете какой-нибудь необычный self-hosted сервис, о котором мало кто знает, то тоже отпишитесь — мы посмотрим, потестируем и обязательно расскажем о нем в будущих статьях! "
  },
  {
    "article_id": "https://habr.com/ru/companies/x-com/articles/875280/",
    "title": "Правда ли, что ленточные хранилища – самые надежные, и когда стоит покупать именно их, а не HDD",
    "category": "Базы данных",
    "tags": "xcom-shop, ленточные накопители, архивирование данных, архивирование на ленту, хранение данных",
    "text": "Ленточные накопителисохраняют свою актуальность даже в эпоху SSD и облачных технологий. Благодаря высокой надежности, экономической эффективности и уникальным свойствам они остаются незаменимыми в решении задач архивного хранения и резервирования данных. В этой статье мы рассмотрим,почему ленточные хранилища превосходят HDD, изучим их ключевые преимущества и ограничения, а также определим сценарии использования, в которых они наиболее уместны. На чем лучше хранить данные Современнаяинфраструктура хранения данныхвключает разнообразные технологии, каждая из которых выполняет свои задачи. От сверхскоростных SSD для оперативных вычислений до облачных систем, предлагающих гибкость и доступность, выбор решения диктуется характером данных и бизнес‑целями. Однако среди этих решенийленточные накопителизанимают уникальную нишу, которую не способны заменить другие технологии. Многие считают ленты устаревшей технологией, однако этот миф не выдерживает критики.Эволюция ленточных системпривела к созданию высокотехнологичных решений, соответствующих требованиям современного бизнеса. Они предлагают надежность, долговечность и низкую стоимость хранения. Поэтому ленточные системы находят применение в разнообразных сценариях — от резервного копирования и архивирования до хранения научных данных и медиафайлов. Для долгосрочного хранения данных с минимальным доступом они остаются вне конкуренции. Именно сочетание этих факторов позволяет лентам сохранять свою актуальность в условиях постоянного развития цифровых технологий. Надежность и долговечность ленточных хранилищ Ленточные накопители превосходят HDDв долговечности благодаря их принципиально иной конструкции. Магнитные ленты имеют низкий уровень физического износа, так как в режиме хранения они не подвергаются механическому воздействию. В отличие от HDD, где механика, включая шпиндель и считывающую головку, является точкой отказа, ленты остаются стабильными при соблюдении довольно простых условий хранения. Используемые материалы, такие как аморфные ферромагнитные покрытия, обеспечивают долговременную устойчивость данных. Поэтому современныелентыспособны хранить информацию до 30 лет без риска деградациисигнала. Еще одной важной особенностью лент является их способность выдерживать экстремальные условия хранения. Онименее чувствительны к температурным перепадам и влажности, чем жесткие диски и не требовательны к сохранению герметичности. Таким образом ленты становятся даже более предпочтительным решением для резервных копий, которые хранятся оффсайт. Наряду с внешней сохранностью лент, производители накопителей этого типа заботятся и об их внутренней стабильности. Для этого IBM, Quantum и другие вендоры внедряют передовые технологии для повышения надежности своих систем: Механизмы контроля ошибок (ECC), которые обеспечивают сохранность данных даже при длительном использовании. Эти функции сочетаются с возможностями самообслуживания лент, позволяя минимизировать вероятность утраты информации. Механизмы контроля ошибок (ECC), которые обеспечивают сохранность данных даже при длительном использовании. Эти функции сочетаются с возможностями самообслуживания лент, позволяя минимизировать вероятность утраты информации. Бэд‑блокимогут стать причиной безвозвратной потери данных, ленточные накопители демонстрируют устойчивость к подобным проблемам. Поэтому ленты используют встроенные технологии защиты от размагничивания, которые сохраняют целостность записей даже в сложных условиях. Бэд‑блокимогут стать причиной безвозвратной потери данных, ленточные накопители демонстрируют устойчивость к подобным проблемам. Поэтому ленты используют встроенные технологии защиты от размагничивания, которые сохраняют целостность записей даже в сложных условиях. Контроль состояния носителейпозволяет заранее обнаруживать возможные проблемы. Такой подход исключает внезапные отказы, характерные для жестких дисков, и предоставляет достаточно времени для восстановления информации или переноса данных на новые носители. Контроль состояния носителейпозволяет заранее обнаруживать возможные проблемы. Такой подход исключает внезапные отказы, характерные для жестких дисков, и предоставляет достаточно времени для восстановления информации или переноса данных на новые носители. Экономическая эффективность: ленточные решения против HDD В сценариях долгосрочного хранения ленточные накопители имеют значительное преимущество в плане стоимости владения.Стоимость одного терабайта хранения на ленте, по состоянию на 2024 год, составляет порядка $2–3, тогда как для HDD этот показатель превышает $10. При объемах данных в сотни терабайт или петабайт экономия становится существенной. Помимо низкой стоимости носителей, ленты требуют меньших затрат на развертывание. Одно ленточное хранилище может вместить данные, которые эквивалентны нескольким сотням HDD, что сокращает расходы на инфраструктуру: стойки, охлаждение, питание. Операционные затраты (OPEX) также ниже у лент. Например, стоимость обслуживания ленточной библиотеки минимальна, поскольку она не требует постоянного мониторинга состояния носителей. HDD, напротив, требуют регулярного контроляSMART‑параметрови замены дисков, что увеличивает нагрузку на ИТ‑отдел. Но особенно экономия будет заметна в сценариях, связанных с хранением «холодных данных» — информации, к которой в обычных условиях обращаются крайне редко. В таких случаях использование лент позволяет компаниям минимизировать расходы без ущерба для доступности данных. Объемы и масштабируемость ленточных хранилищ Одним из самых значительных преимуществ ленточных накопителей перед HDD является их масштабируемость. Спасибо модульной архитектуре. Благодаря этому организации могутдобавлять картриджи по мере необходимости, что исключает необходимость замены уже существующей инфраструктуры. Современныеленточные картриджи LTO-9могут хранить до 45 ТБ сжатых данных. Акоммерческие HDD— даже самые современные — имеют максимальный объем в пределах 22 ТБ. В случае с системами, которые используют HDD, увеличение объема хранилища также возможно. Но, во‑первых, в более ограниченном формате. А, во‑вторых, дополнительные жесткие диски в сборке нередко приводят к повышенному тепловыделению и необходимости модернизации охлаждающих систем. Ленточные библиотекичасто становятся незаменимым решением в сценариях с объемами данных, превышающими петабайты. Они активно используются в медиасфере для хранения высококачественного видеоконтента, включая 4K‑ и 8K‑видео, которые занимают гигантские пространства. А научные учреждения, такие как астрономические обсерватории или центры геномных исследований, полагаются на ленты для архивирования результатов длительных экспериментов и наблюдений, где важна точность и долговечность сохранения. Для облегчения управления данными, которые хранятся на лентах, существуютроботизированные системы автоматизации. Они, как правило, интегрированы в большинство современных ленточных библиотек и исключают необходимость ручного вмешательства при работе с большими объемами данных. Эти механизмы способны управлять доступом к сотням или тысячам картриджей, что особенно важно для организаций, работающих с архивами на ежедневной основе. При этом ленточные носители устойчивы к механическим повреждениям и не теряют своих свойств при вибрациях или случайных ударах, которые являются частой причиной выхода из строя HDD. Возможность безопасной транспортировки ленточных картриджей делает их подходящими для хранения в удаленных защищенных зонах, чтобы снизить риск потери критически важной информации на случай стихийных бедствий или аварий на основной площадке. Энергоэффективность ленточных хранилищ Ленточные системы потребляют энергиютолько в момент использования, что делает их самым энергоэффективным решением для хранения данных. HDD, напротив, нуждаются в постоянном электропитании для поддержания работы шпинделя и предотвращения деградации дисков. Исследования показывают, что использование ленточных библиотек позволяетснизить энергопотребление до 70–90%в сравнении с HDD‑системами. Особенно в контексте крупных дата‑центров, где расходы на электроэнергию составляют значительную долю бюджета. Кроме того, использование ленточных решений поддерживает инициативы по поддержанию зеленой политики и устойчивого развития. Посколькусрок службы лентзначительно превышает аналогичный показатель для HDD, они реже утилизируются, что снижает объем электронных отходов. А низкие энергозатраты уменьшают углеродный след, что делает ленточные системы более экологичным выбором. Компании, стремящиеся минимизировать свой углеродный след, рассматривают переход на ленточные решения как стратегический шаг. Это не только снижает эксплуатационные расходы, но и улучшает корпоративный имидж. Безопасность данных на ленточных хранилищах Ленточные накопителивыделяются высокой степенью безопасности, что делает их привлекательным решением для хранения критически важной информации. В отличие от HDD, которые постоянно подключены к системе и остаются уязвимыми для кибератак, ленты могут физически изолироваться от сети, хранясь в оффлайн‑режиме. Такая изоляция исключает возможность удаленного доступа к данным, что особенно актуально в условиях возрастающего числа угроз, связанных с вредоносным ПО и атаками программ‑вымогателей. Дополнительное преимущество ленточных систем заключается в их способности сохранять неизменность данных, что имеет ключевое значение для соблюдения регуляторных норм. Многие организации обязаны хранить архивы на протяжении десятилетий, обеспечивая их неизменность и доступность. Ленточные технологии, такие какWORM (Write Once, Read Many), исключают возможность изменения или удаления уже записанных данных. Это свойство гарантирует юридическую защиту и полное соответствие стандартам в таких отраслях, как финансы, здравоохранение и правительственные структуры. Для усиления безопасности современныеленточные накопители поддерживают аппаратное шифрование. Эта функция обеспечивает высокий уровень защиты, предотвращая несанкционированный доступ даже в случае физической кражи носителя. Без наличия ключа расшифровка данных становится практически невозможной, что делает ленты надежным решением для хранения конфиденциальной информации. Для крупных организаций ленточные накопители становятся важной частью стратегии информационной безопасности. Их использование позволяет создать так называемый «воздушный зазор» (air gap), который физически отделяет данные от сетевых угроз. Такая изоляция эффективно защищает информацию от атак программ‑вымогателей, кибершантажистов и других современных угроз, минимизируя риски утраты данных. Таким образом, ленточные накопители сохраняют свою значимость как ключевой инструмент длядолгосрочного хранения и резервирования данныхв различных отраслях. Их уникальные преимущества, включая высокую надежность, исключительную экономическую эффективность и возможность масштабирования под растущие объемы информации, делают их одним из самых востребованных решений на рынке. В эпоху экспоненциального роста данных и повышенного внимания к вопросам информационной безопасности ленточные системы выступают в роли незаменимого элемента инфраструктуры, способного обеспечить как соответствие регуляторным требованиям, так и защиту от киберугроз. Благодаря своим характеристикам, таким какдолговечность носителей, энергоэффективность и поддержка современных технологий защиты данных, ленточные накопители остаются актуальными даже в условиях конкуренции с SSD, HDD и облачными решениями. Для профессионалов, которым необходимо сбалансировать затраты, безопасность и масштабируемость, ленточные системы представляют собой своеобразный «золотой стандарт». Они продолжают демонстрировать свою ценность как надежный, экономически оправданный и технологически развитый выбор для хранения данных на десятилетия вперед."
  },
  {
    "article_id": "https://habr.com/ru/articles/878648/",
    "title": "Reverse Engineering бизнес требований советы для Senior Business Analyst",
    "category": "Менеджмент",
    "tags": "reverse-engineering, reverse engineering, reverse, бизнес-требования, бизнес-анализ, требования, сбор требований, реверс-инжиниринг, реверс-инжиниринг оборудования, обратный инжиниринг",
    "text": "Итак, что же такоеReverse Engineering? RE – это процесс, в ходе которого мы извлекаем информацию из уже имеющегося решения и представляем ее в нужном формате. В данном контексте бизнес-аналитику необходима информация, которая станет основой для формулировки требований. Эта методика не представлена в своде знаний IIBA – BABOK, она находится в технике Document Analysis. Задача Reverse Engineering возникает всегда в контексте какой-то другой задачи. Бизнес не заинтересован в самом процессе RE, так как это может быть дорогостоящей операцией, требующей участия различных заинтересованных сторон и высокой квалификации самого бизнес-аналитика. При этом часто акцент делается именно на его hard skills. Поэтому прежде чем приступать к выполнению RE, важно четко определить границы этой задачи и ее цель. Кейсы для Reverse Engineering Нужно уточнить требования для обеспечения поддержки текущего функционала. Нужно уточнить требования для обеспечения поддержки текущего функционала. Вы присоединились к проекту, который включает добавление или значительные изменения в уже существующее решение. Вы присоединились к проекту, который включает добавление или значительные изменения в уже существующее решение. Запланирован проект по полному update-ту устаревшей системы. Запланирован проект по полному update-ту устаревшей системы. У клиента имеется старая система, и вы пришли с вашим готовым продуктом для ее полной замены. У клиента имеется старая система, и вы пришли с вашим готовым продуктом для ее полной замены. Необходимо разработать требования для миграции данных между двумя действующими системами. Необходимо разработать требования для миграции данных между двумя действующими системами. Требуется составить требования для интеграции уже имеющихся решений. Требуется составить требования для интеграции уже имеющихся решений. Нужно провести детальный анализ конкурентного продукта, чтобы выяснить его полезные функции и принцип их работы. Нужно провести детальный анализ конкурентного продукта, чтобы выяснить его полезные функции и принцип их работы. Все перечисленные задачи бизнес-аналитика объединяет одно ключевое свойство: для их успешного выполнения необходимо иметь актуальные системные требования (в идеале). Именно на основе достоверного и глубокого понимания существующей системы (As-Is) бизнес-аналитик сможет сформулировать новый набор требований.\\ Источники информации для RE: Интервью с тех-специалистами. Это могут быть разработчики, тестировщики, а также сотрудники служба customer support. Интервью с тех-специалистами. Это могут быть разработчики, тестировщики, а также сотрудники служба customer support. Если есть возможность самостоятельно поэкспериментировать нажимая на различные кнопки, это станет отличным источником как для описания, так и для проверки гипотез. Если есть возможность самостоятельно поэкспериментировать нажимая на различные кнопки, это станет отличным источником как для описания, так и для проверки гипотез. Анализ данных системы (data driven decision) может существенно помочь в принятии решений. Анализ данных системы (data driven decision) может существенно помочь в принятии решений. Детальное Изучение структуры данных. Детальное Изучение структуры данных. Анализ исходного кода (если он доступен). Анализ исходного кода (если он доступен). Интервью с заинтересованными сторонами из бизнеса, конечными пользователями и наблюдение за их взаимодействием с продуктом (парой это единственный доступный метод). Интервью с заинтересованными сторонами из бизнеса, конечными пользователями и наблюдение за их взаимодействием с продуктом (парой это единственный доступный метод). Понимание специфики домена. Понимание специфики домена. Существующая документация (к сожалению ее часто нету, либо сильно устарела). Существующая документация (к сожалению ее часто нету, либо сильно устарела). Анализ Тикетов в Jira, Asana, Trello или других системахНужно помнить следующиеВсеобъемлющие документы никому не нужны, особенно если они существуют сами по себе. Наша цель — решить задачу в контексте новых требований к решению. Здесь перфекционизм зачастую неуместен.Все лгут, даже код и даже базы данных. Поэтому крайне важно использовать все доступные возможности для перепроверки информации. К сожалению, на проверку уходит время, а сроки для аналитиков часто сжаты, поэтому приходится выбирать, на что потратить время, а что принять на веру.Внимательно выбирайте уровень детализации. Причины те же, что и в предыдущем пункте.Не путайте, как система функционирует, и как она должна функционировать. Вы можете вести параллельный список изменений, который будет добавлен в backlog, но восстановленные требования должны быть зафиксированы в формате As Is.Тщательно определяйте объем исследования и следите за ним в процессе работы. Легко увлечься и застрять на незначительных деталях или уйти в сторону. Что считать «мелочью», зависит от критичности исходной задачи, которая и вызвала реконструкцию требований.Если планируется дальнейшая работа с восстановленными требованиями, а не только разовая реконструкция, стоит сразу позаботиться о создании процесса обновления системной документации.Давайте напоследок рассмотрим группы пользователей из BABOK и особенности общения с нимиReverse Engineering бизнес требований советы для Senior Business Analyst 🌟End User.Они напрямую работают с решением и могут показать его полезность, но:Им может быть неинтересно обсуждать текущее состояние системы, так как это их рутина.Часто перескакивают на свои идеальные сценарии работы.Плохо систематизируют знания и могут забыть как о частых, так и редких операциях.Знают только свою часть процесса и могут путать факты с предположениями.В отделе может быть всего один человек, ответственный за определенную функцию, что приводит к утечке информации.Operational Support.Эта группа может предоставить полезную информацию, так как собирает данные из тикетов, но:Часто не замечает свои ошибки, ассоциируя себя с системой.Знания о процессе могут быть поверхностными.Малоинициативны и неохотно идут на диалог.Как технические специалисты могут сильно углубляться в детали.Sponsor. Человек, отвечающий за бюджет, может прояснить странные шаги в в бизнес-процессе, но:Редко погружается в детали и быстро забывает о проблемах после их решения.Получает обобщенную информацию, что может приводить к устаревшим представлениям.Занят, поэтому к нему сложно добраться.Customers. клиенты бизнеса, который пользуется решениемРедко вовлечены и доступны для интервью.Могут неверно трактовать работу системы и показывать низкую заинтересованность.Имеют проблемы, схожие с конечными пользователями.Regulator. На начальном этапе не очень полезны, но могут указать на ограничения, о которых другие не знают.Supplier/Vendor.Используют уже готовые компоненты, предоставляемые третьими лицами. В этом случае они могут быть дополнительным источником информации по деталям реализации, ноРедко идут на контакт и могут саботировать работу при отключении их решения.Могут отсутствовать из-за выхода их компании с рынка и других причинК сожалению, нет волшебной кнопки, которая позволит решить все выше описанные проблемы. В целом можно рекомендовать две вещи:- Кросс-проверка информации.- Сверка с другими источниками.- Использование техник \"Наблюдение\" и \"Анализ документов\"Особенности сбора информации от Тех. СпециалистовImplementation Subject Matter Expert / Тестировщики. Эта группа стейкхолдеров может стать ценным источником сведений о процессе реализации, однако имеет свои нюансы. Технические специалисты, как правило, не погружаются в детали бизнеса. Они прекрасно понимают, как работает система, но зачастую не могут объяснить, зачем это необходимо. Это затрудняет их способность оценивать актуальность функционала. В их окружении часто утверждают, что определённая функция нужна: имеется документация, поддерживающая это мнение, существуют зависимые функции и поступают баг-репорты от пользователей как по самой функции, так и по смежным аспектам. Опираясь только на мнения этой группы, легко воспроизвести старое поведение системы с его недостатками, включая несоответствие реальным потребностям бизнеса.Менеджеры проектов / Бизнес-аналитики. В целом, это достаточно стабильная группа специалистов, располагающаяся на пересечении технических и бизнес-стейкхолдеров и помогающая соединить потребности с их реализацией. От них не стоит ожидать глубоких технических описаний, но они могут рассказать историю проекта, что дополнит общее понимание контекста. Обычно у них есть актуальный список проблем и хорошее понимание политической ситуации в компании.Надеюсь информация по Reverse Engineering бизнес требований оказались для вас полезными. Напишите в комментариях, как вы делали обратный инжиниринг бизнес требований на своих проектах  Не забудьте поставить лайк этому посту и подписаться — это вдохновляет меня на создание новых материалов! Спасибо за вашу поддержку! Анализ Тикетов в Jira, Asana, Trello или других системах Нужно помнить следующие Всеобъемлющие документы никому не нужны, особенно если они существуют сами по себе. Наша цель — решить задачу в контексте новых требований к решению. Здесь перфекционизм зачастую неуместен. Всеобъемлющие документы никому не нужны, особенно если они существуют сами по себе. Наша цель — решить задачу в контексте новых требований к решению. Здесь перфекционизм зачастую неуместен. Все лгут, даже код и даже базы данных. Поэтому крайне важно использовать все доступные возможности для перепроверки информации. К сожалению, на проверку уходит время, а сроки для аналитиков часто сжаты, поэтому приходится выбирать, на что потратить время, а что принять на веру. Все лгут, даже код и даже базы данных. Поэтому крайне важно использовать все доступные возможности для перепроверки информации. К сожалению, на проверку уходит время, а сроки для аналитиков часто сжаты, поэтому приходится выбирать, на что потратить время, а что принять на веру. Внимательно выбирайте уровень детализации. Причины те же, что и в предыдущем пункте. Внимательно выбирайте уровень детализации. Причины те же, что и в предыдущем пункте. Не путайте, как система функционирует, и как она должна функционировать. Вы можете вести параллельный список изменений, который будет добавлен в backlog, но восстановленные требования должны быть зафиксированы в формате As Is. Не путайте, как система функционирует, и как она должна функционировать. Вы можете вести параллельный список изменений, который будет добавлен в backlog, но восстановленные требования должны быть зафиксированы в формате As Is. Тщательно определяйте объем исследования и следите за ним в процессе работы. Легко увлечься и застрять на незначительных деталях или уйти в сторону. Что считать «мелочью», зависит от критичности исходной задачи, которая и вызвала реконструкцию требований. Тщательно определяйте объем исследования и следите за ним в процессе работы. Легко увлечься и застрять на незначительных деталях или уйти в сторону. Что считать «мелочью», зависит от критичности исходной задачи, которая и вызвала реконструкцию требований. Если планируется дальнейшая работа с восстановленными требованиями, а не только разовая реконструкция, стоит сразу позаботиться о создании процесса обновления системной документации. Если планируется дальнейшая работа с восстановленными требованиями, а не только разовая реконструкция, стоит сразу позаботиться о создании процесса обновления системной документации. Давайте напоследок рассмотрим группы пользователей из BABOK и особенности общения с ними End User.Они напрямую работают с решением и могут показать его полезность, но: Им может быть неинтересно обсуждать текущее состояние системы, так как это их рутина. Им может быть неинтересно обсуждать текущее состояние системы, так как это их рутина. Часто перескакивают на свои идеальные сценарии работы. Часто перескакивают на свои идеальные сценарии работы. Плохо систематизируют знания и могут забыть как о частых, так и редких операциях. Плохо систематизируют знания и могут забыть как о частых, так и редких операциях. Знают только свою часть процесса и могут путать факты с предположениями. Знают только свою часть процесса и могут путать факты с предположениями. В отделе может быть всего один человек, ответственный за определенную функцию, что приводит к утечке информации. В отделе может быть всего один человек, ответственный за определенную функцию, что приводит к утечке информации. Operational Support.Эта группа может предоставить полезную информацию, так как собирает данные из тикетов, но: Часто не замечает свои ошибки, ассоциируя себя с системой. Часто не замечает свои ошибки, ассоциируя себя с системой. Знания о процессе могут быть поверхностными. Знания о процессе могут быть поверхностными. Малоинициативны и неохотно идут на диалог. Малоинициативны и неохотно идут на диалог. Как технические специалисты могут сильно углубляться в детали. Как технические специалисты могут сильно углубляться в детали. Sponsor. Человек, отвечающий за бюджет, может прояснить странные шаги в в бизнес-процессе, но: Редко погружается в детали и быстро забывает о проблемах после их решения. Редко погружается в детали и быстро забывает о проблемах после их решения. Получает обобщенную информацию, что может приводить к устаревшим представлениям. Получает обобщенную информацию, что может приводить к устаревшим представлениям. Занят, поэтому к нему сложно добраться. Занят, поэтому к нему сложно добраться. Customers. клиенты бизнеса, который пользуется решением Редко вовлечены и доступны для интервью. Редко вовлечены и доступны для интервью. Могут неверно трактовать работу системы и показывать низкую заинтересованность. Могут неверно трактовать работу системы и показывать низкую заинтересованность. Имеют проблемы, схожие с конечными пользователями. Имеют проблемы, схожие с конечными пользователями. Regulator. На начальном этапе не очень полезны, но могут указать на ограничения, о которых другие не знают. Supplier/Vendor.Используют уже готовые компоненты, предоставляемые третьими лицами. В этом случае они могут быть дополнительным источником информации по деталям реализации, но Редко идут на контакт и могут саботировать работу при отключении их решения. Редко идут на контакт и могут саботировать работу при отключении их решения. Могут отсутствовать из-за выхода их компании с рынка и других причин Могут отсутствовать из-за выхода их компании с рынка и других причин К сожалению, нет волшебной кнопки, которая позволит решить все выше описанные проблемы. В целом можно рекомендовать две вещи: - Кросс-проверка информации. - Сверка с другими источниками. - Использование техник \"Наблюдение\" и \"Анализ документов\" Особенности сбора информации от Тех. Специалистов Implementation Subject Matter Expert / Тестировщики. Эта группа стейкхолдеров может стать ценным источником сведений о процессе реализации, однако имеет свои нюансы. Технические специалисты, как правило, не погружаются в детали бизнеса. Они прекрасно понимают, как работает система, но зачастую не могут объяснить, зачем это необходимо. Это затрудняет их способность оценивать актуальность функционала. В их окружении часто утверждают, что определённая функция нужна: имеется документация, поддерживающая это мнение, существуют зависимые функции и поступают баг-репорты от пользователей как по самой функции, так и по смежным аспектам. Опираясь только на мнения этой группы, легко воспроизвести старое поведение системы с его недостатками, включая несоответствие реальным потребностям бизнеса. Менеджеры проектов / Бизнес-аналитики. В целом, это достаточно стабильная группа специалистов, располагающаяся на пересечении технических и бизнес-стейкхолдеров и помогающая соединить потребности с их реализацией. От них не стоит ожидать глубоких технических описаний, но они могут рассказать историю проекта, что дополнит общее понимание контекста. Обычно у них есть актуальный список проблем и хорошее понимание политической ситуации в компании. Надеюсь информация по Reverse Engineering бизнес требований оказались для вас полезными. Напишите в комментариях, как вы делали обратный инжиниринг бизнес требований на своих проектах  Не забудьте поставить лайк этому посту и подписаться — это вдохновляет меня на создание новых материалов! Спасибо за вашу поддержку! "
  },
  {
    "article_id": "https://habr.com/ru/companies/otus/articles/878413/",
    "title": "Как проводить спринт-ретроспективу в 2025 году",
    "category": "Менеджмент",
    "tags": "ретроспектива, спринт-ретроспектива, управление командой",
    "text": "Участвовал в Аджайл-трансформациях в крупнейших компаниях в России (Альфа банк, МТС, Х5 retail group), с международным опытом в стартапе зарубежом Почему ретроспективы не работают? Многие команды недолюбливают спринт‑ретроспективы, считая их бесполезными или формальными встречами, которые не приводят к реальным изменениям. Основные причины этого недовольства включают: Отсутствие видимого эффекта от ретроспектив. Команда обсуждает одни и те же проблемы, но ничего не меняется. Отсутствие видимого эффекта от ретроспектив. Команда обсуждает одни и те же проблемы, но ничего не меняется. Доминирование отдельных участников. Самые громкие голоса в комнате диктуют повестку, в то время как остальные просто соглашаются или молчат. Доминирование отдельных участников. Самые громкие голоса в комнате диктуют повестку, в то время как остальные просто соглашаются или молчат. Нехватка объективных данных. Обсуждение строится на субъективных впечатлениях, а не на фактах. Нехватка объективных данных. Обсуждение строится на субъективных впечатлениях, а не на фактах. Слабая модерация. Без четкой структуры обсуждение превращается в хаос или бессмысленный обмен мнениями. Слабая модерация. Без четкой структуры обсуждение превращается в хаос или бессмысленный обмен мнениями. Перегруженность разработчиков. Из‑за высокой загрузки у команды нет мотивации уделять время ретроспективе. Перегруженность разработчиков. Из‑за высокой загрузки у команды нет мотивации уделять время ретроспективе. Если ретроспектива не приносит ощутимых результатов, команда начинает воспринимать ее как пустую трату времени. Однако при правильном подходе этот инструмент может стать мощным механизмом дляпостоянного улучшения процессов разработкии повышения удовлетворенности команды работой. Как сделать ретроспективу полезной? Чтобы ретроспектива была продуктивной, она должнаосновываться на данных, а не на субъективных ощущениях. Использование объективных показателей позволяет: Выявить реальные узкие места и причины проблем. Выявить реальные узкие места и причины проблем. Исключить догадки и спекуляции. Исключить догадки и спекуляции. Обеспечить прозрачность и равноправие в обсуждениях. Обеспечить прозрачность и равноправие в обсуждениях. Сформировать конкретные действия для улучшения процессов. Сформировать конкретные действия для улучшения процессов. Перед ретроспективой важно подготовитьключевые метрики, которые помогут объективно оценить результаты работы команды. Какие данные нужны для ретроспективы? Точность планирования— насколько команда выполнила запланированные задачи. Отклонение от планов может указывать на проблемы с оценкой задач или изменяющиеся приоритеты. Точность планирования— насколько команда выполнила запланированные задачи. Отклонение от планов может указывать на проблемы с оценкой задач или изменяющиеся приоритеты. Объем работы в процессе (WIP)— сколько задач команда выполняла одновременно. Если этот показатель слишком высок, возможно, команда перегружена и нужно скорректировать объем работы. Объем работы в процессе (WIP)— сколько задач команда выполняла одновременно. Если этот показатель слишком высок, возможно, команда перегружена и нужно скорректировать объем работы. Цикл поставки (Cycle Time)— сколько времени проходит от начала работы над задачей до ее завершения. Если цикл слишком длинный, это может указывать на узкие места в процессе разработки. Цикл поставки (Cycle Time)— сколько времени проходит от начала работы над задачей до ее завершения. Если цикл слишком длинный, это может указывать на узкие места в процессе разработки. Средний размер PR— если пул‑реквесты слишком объемные, это может замедлять код‑ревью и интеграцию изменений. Средний размер PR— если пул‑реквесты слишком объемные, это может замедлять код‑ревью и интеграцию изменений. Количество доработок после код‑ревью— показатель качества кода и его соответствия стандартам команды. Количество доработок после код‑ревью— показатель качества кода и его соответствия стандартам команды. Процент завершенных задач без блокеров— помогает выявить, насколько часто задачи тормозятся из‑за зависимостей. Процент завершенных задач без блокеров— помогает выявить, насколько часто задачи тормозятся из‑за зависимостей. Обратная связь от команды (Sentiment Data)— позволяет выявить субъективное восприятие процесса. Обратная связь от команды (Sentiment Data)— позволяет выявить субъективное восприятие процесса. Эти показатели дают четкое представление о том,что в работе команды идет хорошо, а где есть проблемы. Фреймворк 4L’s для анализа спринта Одним из наиболее эффективных подходов к проведению ретроспектив являетсяметод 4L»s (Liked, Learned, Lacked, Longed For). Он помогает структурировать обсуждение и направить его на поиск конкретных улучшений. Этот раздел фиксирует положительный опыт команды за спринт. Сюда могут входить: Успешное выполнение запланированного объема работы. Успешное выполнение запланированного объема работы. Повышение точности планирования. Повышение точности планирования. Ускорение цикла поставки. Ускорение цикла поставки. Снижение числа багов и доработок. Снижение числа багов и доработок. Улучшение взаимодействия между командами. Улучшение взаимодействия между командами. Пример: Планирование было точным на 78 процентов. Планирование было точным на 78 процентов. Цикл выполнения PR сократился, 95 процентов PR оказались меньше 200 строк кода. Цикл выполнения PR сократился, 95 процентов PR оказались меньше 200 строк кода. Внедрение автоматизации позволило сэкономить два дня работы. Внедрение автоматизации позволило сэкономить два дня работы. Фиксация положительного опыта важна, потому что помогаетмасштабировать успешные практикии мотивировать команду. Этот раздел позволяет команде проанализировать, какие новые инсайты появились в процессе работы. Например: Улучшение процесса код‑ревью благодаря новым инструментам. Улучшение процесса код‑ревью благодаря новым инструментам. Влияние новых технологий на производительность. Влияние новых технологий на производительность. Выявление новых факторов, влияющих на выполнение задач. Выявление новых факторов, влияющих на выполнение задач. Пример: Уровень внедрения TypeScript вырос на 35 процентов, что сократило количество багов. Уровень внедрения TypeScript вырос на 35 процентов, что сократило количество багов. Доля работы над новыми фичами увеличилась на 25 процентов. Доля работы над новыми фичами увеличилась на 25 процентов. Анализ прогнозов показал, что проект может задержаться на одну‑две недели. Анализ прогнозов показал, что проект может задержаться на одну‑две недели. Фиксация этих инсайтов помогаетучитывать их в будущих спринтахи корректировать подход. Этот раздел помогает команде выявить препятствия, мешающие эффективной работе. Например: Недостаток ресурсов в ключевых командах. Недостаток ресурсов в ключевых командах. Перегрузка задачами и высокая нагрузка на отдельных специалистов. Перегрузка задачами и высокая нагрузка на отдельных специалистов. Слишком долгие согласования и зависимости между командами. Слишком долгие согласования и зависимости между командами. Пример: Объем работы в процессе увеличился на 15 процентов, что замедлило выполнение задач. Объем работы в процессе увеличился на 15 процентов, что замедлило выполнение задач. На проекте ABC не хватало специалистов, что повлияло на сроки. На проекте ABC не хватало специалистов, что повлияло на сроки. Среднее время ожидания код‑ревью увеличилось на 15 процентов. Среднее время ожидания код‑ревью увеличилось на 15 процентов. Выявление таких проблем позволяетпринимать меры для их устранения. Этот раздел фиксирует пожелания команды и возможные улучшения. Это могут быть новые инструменты, изменение процессов или улучшение условий работы. Пример: Автоматизация код‑ревью для ускорения процесса. Автоматизация код‑ревью для ускорения процесса. Перераспределение ресурсов, чтобы снизить нагрузку на ключевые команды. Перераспределение ресурсов, чтобы снизить нагрузку на ключевые команды. Использование новых форматов встреч для более продуктивного взаимодействия. Использование новых форматов встреч для более продуктивного взаимодействия. Этот раздел помогаетформировать конкретные улучшения, которые можно внедрить в следующих спринтах. Как провести эффективную ретроспективу: пошаговый процесс Начало встречиОбсудите цели спринта и ключевые результаты.Установите позитивный настрой. Начало встречи Обсудите цели спринта и ключевые результаты. Обсудите цели спринта и ключевые результаты. Установите позитивный настрой. Установите позитивный настрой. Анализ данныхПредставьте собранные метрики.Обсудите тренды и выявленные паттерны. Анализ данных Представьте собранные метрики. Представьте собранные метрики. Обсудите тренды и выявленные паттерны. Обсудите тренды и выявленные паттерны. Обсуждение по фреймворку 4L’sЧто получилось?Чему научились?Чего не хватало?Чего хотелось бы? Обсуждение по фреймворку 4L’s Что получилось? Что получилось? Чему научились? Чему научились? Чего не хватало? Чего не хватало? Чего хотелось бы? Чего хотелось бы? Определение действийВыберите два‑три ключевых улучшения.Назначьте ответственных за внедрение. Определение действий Выберите два‑три ключевых улучшения. Выберите два‑три ключевых улучшения. Назначьте ответственных за внедрение. Назначьте ответственных за внедрение. Закрытие встречиПодведите итоги и подтвердите список улучшений.Поблагодарите команду за участие. Закрытие встречи Подведите итоги и подтвердите список улучшений. Подведите итоги и подтвердите список улучшений. Поблагодарите команду за участие. Поблагодарите команду за участие. Как внедрять изменения после ретроспективы Чтобы ретроспектива не превратилась в пустую формальность, важнофиксировать и внедрять улучшения. Все решения должны бытьзаписаны и опубликованы. Все решения должны бытьзаписаны и опубликованы. Улучшения должны иметьответственных и дедлайны. Улучшения должны иметьответственных и дедлайны. Через один‑два спринта нужнооценить, помогли ли изменения. Через один‑два спринта нужнооценить, помогли ли изменения. Заключение Ретроспективы — это не просто встречи, а инструмент для постоянного улучшения процессов. Использованиеданных вместо догадок, четкая структура обсуждения и реальное внедрение улучшений помогают сделать ихэффективными и полезными для команды. Если ваша команда хочет повысить точность планирования, сократить время выполнения задач и минимизировать риски, стоиториентироваться на объективные показатели и конкретные действияпосле ретроспективы. Если вы хотите внедритьdata‑driven ретроспективы, повысить предсказуемость разработки и улучшить процессы в команде,приглашаюна менторство. Помогу выстроить эффективные ретроспективы, оптимизировать планирование и устранить узкие места в работе. Также актуальные навыки для улучшения процессов в команде можно получить на открытых уроках в Otus: 5 февраля: «Как не треснуть при росте: расширяем команду по уму».Подробнее 5 февраля: «Как не треснуть при росте: расширяем команду по уму».Подробнее 6 февраля: «Цифры решают все: как внедрение метрик и KPI ускоряет достижение целей».Подробнее 6 февраля: «Цифры решают все: как внедрение метрик и KPI ускоряет достижение целей».Подробнее "
  },
  {
    "article_id": "https://habr.com/ru/companies/minerva_media/articles/878334/",
    "title": "Менеджмент по-гангстерски: помогаем Аль Капоне создать базу знаний",
    "category": "Менеджмент",
    "tags": "управление проектами, управление командой, управление персоналом, управление продуктом, менеджмент, база знаний, базы данных, ошибки управления, ошибки в бизнесе, технологии",
    "text": "Если бы только Альфонсо знал, что база знаний спасёт его от тюрьмы, то… Что? Переписываем историю чикагского гангстера с помощью современных технологий. Аль Капоне мог не сесть в тюрьму, если бы вовремя заплатил налоги США. Официально он задолжал государству 388 тысяч долларов. Но мы-то понимаем, что реальная цифра, скорее всего, в десятки раз больше. Альфонсо не самый важный мафиози Америки, но зато самый медийный. Почти все знают историю его стремительного взлёта и болезненного падения. Обычный эмигрант-вышибала стал боссом чикагской мафии, ввёл понятие «отмывание денег», заработал целое состояние, а затем отсидел в тюрьме за неуплату налогов. Так себе история. А что, если её переписать? Давайте спасём Аль Капоне от тюрьмы. Ну а что, Мейеру Лански можно, а ему нельзя? Для начала разберёмся в том, что разрушило жизнь гангстера, а затем сделаем работу над ошибками. И используем для этого современную базу знаний от Minervasoft. Именно она могла спасти чикагского босса от тюрьмы. Если бы существовала в 1930-х, конечно. Дисклеймер: текст носит юмористический и исследовательский характер. Мы не оправдываем Аль Капоне, а показываем, как грамотный менеджмент мог бы спасти его от серьёзных ошибок. Проблема 1. Халатность Ошибка: нет чёткой финансовой системы. Империя Аль Капоне рухнула из-за неструктурированной чёрной бухгалтерии. Бутлегерство, бары, бордели и казино приносили миллионы долларов, но налоги с них никто не платил. Тут вы можете спросить: а кто-то, вообще, платит налоги с нелегальных доходов? Они же… нелегальные? С 1913 года в США действовала 16-я поправка, по которой любой доход облагался налогом. Но прямого указа собирать деньги с бандитских дел не было. До 1927 года. В 1927-м прошло заседание по делу Салливана — гангстера, который промышлял бутлегерством. Суд решил, что нужно собирать налоги даже с преступного дохода, так как само преступление — это дело ФБР, а налоговую интересуют именно деньги. Этот прецедент помог властям схватить Аль Капоне за руку. Решение Хаотичная бухгалтерия подвела гангстера. Давайте её починим: Создаём базу данных для всех бизнесов, с разбивкой на доходы и расходы. Создаём базу данных для всех бизнесов, с разбивкой на доходы и расходы. Помимо сети прачечных, запишем на Аль Капоне кондитерскую и мебельный магазин — так он мог бы подмешивать «грязные» деньги в чистые и безопасно выводить их на свои счета. Чтобы всё было бесшовно, будем вести бухгалтерию для каждой организации: доходы, расходы, закупки, зарплаты и логистика. Всё должно быть учтено. Взятки тоже учитываем, но не записываем их напрямую, а маскируем. И составляем подробные инструкции для бухгалтера. Из этого вытекает следующий шаг. Разрабатываем инструкции по ведению двойной бухгалтерии, чтобы легальная часть бизнеса выглядела идеально. Разрабатываем инструкции по ведению двойной бухгалтерии, чтобы легальная часть бизнеса выглядела идеально. Налаживаем систему уведомлений о налоговых сроках и изменениях в законодательстве. Так знания точно будут доставлены в нужные руки. Налаживаем систему уведомлений о налоговых сроках и изменениях в законодательстве. Так знания точно будут доставлены в нужные руки. Для этого создаём папку, в которой публикуем материалы про новые законы и изменённые старые. Разные нововведения влияют на разные сферы бизнеса: что-то на бутлегерство, что-то на рэкет. Поэтому маркируем каждую статью нужным фильтром. Итак, мы структурировали документы по доходам и расходам и подстелили соломку в опасных местах. Теперь «неприкасаемым» придётся постараться, чтобы найти доказательства вины. А, не знаете, кто такие «неприкасаемые»? Загляните в глоссарий: Проблема 2. Негибкость Ошибка: игнор изменений в стране. К 1930-м годам Америка изменилась. Правительство решило бороться с организованной преступностью и усилило налоговую службу, а взятки в большинстве случаев перестали работать, и не потому, что все в один момент стали честными и неподкупными, — они беспокоились за свою репутацию, а слишком медийный и жестокий Аль Капоне вызывал возмущение общества. Капоне полагался на старые методы: грубую силу, запугивание и подкуп. Но ему следовало строить дипломатические отношения и быть гибче. Великая депрессия, люди массово теряют работу, злятся на государство и президента. И как удачно подвернулся неуловимый гангстер! Ударом по нему можно было убить сразу стаю зайцев: Показать обществу, что никто и ничто не выше закона. Показать обществу, что никто и ничто не выше закона. Напомнить мафии, что она ведёт свои дела не вопреки, а благодаря государству. Напомнить мафии, что она ведёт свои дела не вопреки, а благодаря государству. Улучшить репутацию президента — он не плохой управленец, а борец с преступностью. Улучшить репутацию президента — он не плохой управленец, а борец с преступностью. Решение Попробуем помешать государству «подмять» под себя Аль Капоне: Создаём FAQ по грамотному взаимодействию с властями — что говорить на допросах, как заполнять документы, как вести себя в суде. Создаём FAQ по грамотному взаимодействию с властями — что говорить на допросах, как заполнять документы, как вести себя в суде. Каждый человек чести хочет дорасти до капо, а не быть расходной пешкой в игре больших боссов. Но для этого нужно не умереть и не сесть в тюрьму по дороге. Аль Капоне стоило помогать своим солдатам и снабжать их точными инструкциями. Верными людьми нельзя разбрасываться. Если вы не понимаете, о какой чести и о каких капо мы говорим, то снова вернёмся в глоссарий. Можно и без глоссария, зайдём в одну из инструкций. Это на случай, если в семье пополнение, а термины выучить не успели. Собираем курсы по финансовой грамотности и правовым основам для членов организации, а контент берём из базы знаний, чтобы везде была актуальная информация. Собираем курсы по финансовой грамотности и правовым основам для членов организации, а контент берём из базы знаний, чтобы везде была актуальная информация. Заставляем проходить обучение и тесты и смотрим, кто как справляется. Заставляем проходить обучение и тесты и смотрим, кто как справляется. Ну а если сотрудники отказываются проходить курсы, Аль Капоне знает, что с ними делать. Публикуем новости и важные изменения, чтобы люди чести могли ознакомиться с ними. Публикуем новости и важные изменения, чтобы люди чести могли ознакомиться с ними. Как раз опубликовали новость про следующий пункт. Формируем дипломатический отдел внутри мафии, который занимается переговорами с чиновниками. Формируем дипломатический отдел внутри мафии, который занимается переговорами с чиновниками. Насилие и коррупция остались в прошлом. Теперь нужно двигаться на дипломатии. И чёткие инструкции помогут Капоне и его банде адаптироваться к новым условиям, а не надеяться на взятки и томпсоны. На всякий случай: Проблема 3. Неэффективный рекрутинг Ошибка: нет проверки лояльности сотрудников. Капоне пригрел клубок змей на груди и в итоге столкнулся с предательствами и утечкой информации. Организация быстро росла, а контроль за новыми членами слабел. Доверие оказывалось не тем людям. Поймать гангстера помогли не только законы и слабая финансовая структура, но и слишком болтливые бухгалтеры. Стоило натренировать их получше, Альфонсо. 🤷🏻‍♀️ Решение Создаём систему психологического тестирования при найме. Это поможет выявлять тех, кто склонен к предательству или сотрудничеству с властями, и обучаем наших эйчаров подбирать в банду лояльных людей. Вы что думали, у мафии нет эйчаров? Создаём систему психологического тестирования при найме. Это поможет выявлять тех, кто склонен к предательству или сотрудничеству с властями, и обучаем наших эйчаров подбирать в банду лояльных людей. Вы что думали, у мафии нет эйчаров? Обучаем сотрудников, как вести себя в критических ситуациях и что говорить, если их арестовали. Обучаем сотрудников, как вести себя в критических ситуациях и что говорить, если их арестовали. Если новички прошли первичный тест, мы не бросим их на произвол судьбы, а продолжим обучать. Проводим регулярные тесты на лояльность, например инсценировки обысков или проверки реакции на провокации. Проводим регулярные тесты на лояльность, например инсценировки обысков или проверки реакции на провокации. Так минимизируем предательства и повысим устойчивость сотрудников и их лояльность. Подытожим Аль Капоне стал жертвой плохого менеджмента. И виноват в этом сам. Его организация хоть и приносила миллионы долларов, но рухнула из-за банальной халатности, негибкости и проблем с персоналом. Он был слишком занят дворцами и провокациями и не заметил, как мир вокруг изменился, а его поведение перестало вписываться в контекст новой Америки. Система управления знаниями могла бы спасти его империю… До поры до времени. Рано или поздно набралась бы команда людей, свергла босса боссов и раскрыла его грязные дела. Не сегодня, так завтра. Не завтра, так через год. Не Элиот Несс, так кто-нибудь другой возглавил бы «неприкасаемых» и закончил эпоху беззакония Аль Капоне. По-другому не может быть, и это отлично. Этой статьёй мы хотели сказать не то, что Аль Капоне достоин второго шанса, а то, что хорошаясистема управления знаниямиможет продлить жизнь даже такому бизнесу. Представьте, каких крутых результатов можно добиться с помощью системы управления знаниями сегодня. Думайте. Если вам понравилась статья и хочется почитать ещё — переходите в нашTelegram. Там мы освещаем спорные вопросы о найме, менеджменте и планировании и рассказываем про управление знаниями. Подпишитесь, чтобы не пропустить новые тексты. Блог Minervasoft "
  },
  {
    "article_id": "https://habr.com/ru/articles/878300/",
    "title": "Не торопитесь со Скрамом! Действительно полезные книги для менеджеров проектов. 5 функций Файоля",
    "category": "Менеджмент",
    "tags": "проектное управление, управление проектами, управление процессами, agile, scrum, kanban, управление командой, управление разработкой, управление продуктом, менеджмент",
    "text": "Когда я начинал свой путь junior PM, мне казалось, что ключ к успеху — это найти ту самую «волшебную» методологию. Scrum там, Kanban или, симпатичный, p3express. Но потом я понял одну штуку (спойлер: я еще в процессе понимания) — пока не разберёшься в пяти классических функциях менеджмента, любые модные фреймворки/методы/методологии будут лишь «пляской с бубном». При том что ты еще не стал шаманом. Если интересно сразу посмотреть на книги, откройссылкув новой вкладке или листай в конец статьи. Что за 5 функций и почему они так важны Анри Файоль ещё в начале XX века сформулировал: любой менеджмент (не только проектов, но и в целом) базируется на планировании, организации, руководстве (или «распорядительстве»), координации и контроле. И не думайте, что это ГОСТовский какой-то ретроградный отстой. Этими функциями можно выразить все основополагающие активности, которые должен предпринимать менеджер — от кикофф-собрания, где вы решаете «как жить?», до сдачи в пром. Чем бы вы ни управляли — проектом в маленькой аутсорс студии, продуктовым стримом или внутренним проектом по миграции данных — вам придётся планировать, организовывать, руководить, координировать и контролировать. Это не просто набор теоретических постулатов, а естественная логика управления: от постановки целей до анализа результатов. Планирование: без ясной цели и продуманных путей её достижения команда хаотично тратит ресурсы; Планирование: без ясной цели и продуманных путей её достижения команда хаотично тратит ресурсы; Организация: если не продумать структуру, роли, ресурсы, возникает путаница, конфликты, остановки; Организация: если не продумать структуру, роли, ресурсы, возникает путаница, конфликты, остановки; Руководство (распорядительство): нет чёткого видения — люди не понимают приоритетов, теряют мотивацию, пробуксовывают в конфликтных моментах; Руководство (распорядительство): нет чёткого видения — люди не понимают приоритетов, теряют мотивацию, пробуксовывают в конфликтных моментах; Координация: даже отличный план и сплочённая команда могут провалить проект, если действия не согласованы; Координация: даже отличный план и сплочённая команда могут провалить проект, если действия не согласованы; Контроль: без отслеживания показателей, корректировки курса и анализа ошибок все усилия могут свестись к нулю. Контроль: без отслеживания показателей, корректировки курса и анализа ошибок все усилия могут свестись к нулю. Важно помнить, что это не линейная модель (сначала планируем, потом организуем и т. п.). В реальности менеджер постоянно переключается между функциями: где-то нужно доработать план, где-то усовершенствовать оргструктуру, наладить коммуникации или ужесточить контроль. Пример из жизни (история одного уставшего PM) Когда я был совсем начинающим проджектом, мы выпускали новую фичу в мобильное приложение. Сроки как обычно неприятные, однако я верил в себя и свои навыки убалтывать заказчиков нашей маленькой студии. Я прикинул задачи, тикеты в битрикс завел, скинул их потегав людей в чат и назвав нужные— думаю, всё, job is done. Но «не учёл» что дизайнер начинающий студент и вообще приболел, тестировщик был новенький и первый раз видел Qase, а флаттер-разработчик вообще не в курсе, что у нас сдача через 2 недели и эта задача важнее остальных. Планирование-то я сделал, пусть и коряво; Планирование-то я сделал, пусть и коряво; Организацию — там плавал внушительный половой агрегат: никто не понимал, кто за что несёт реальную ответственность; Организацию — там плавал внушительный половой агрегат: никто не понимал, кто за что несёт реальную ответственность; Руководство? «Я? Ну я сказал же, что надо сделать» — думал я; Руководство? «Я? Ну я сказал же, что надо сделать» — думал я; Координация? Полагаю в чат написать и подпинывать иногда достаточно; Координация? Полагаю в чат написать и подпинывать иногда достаточно; Контроль? Я только смотрел в таск-трекер и требовал обновления статусов, не понимая насколько мы не успеваем. Контроль? Я только смотрел в таск-трекер и требовал обновления статусов, не понимая насколько мы не успеваем. В итоге, пришлось звать аккаунт-менеджера для разруливания. Он за час расставил всех по местам, объяснил, кто кому что должен и как вообще должна строиться работа, чтобы не перегореть за сутки. Было стыдно, зато урок я усвоил: 5 функций — не просто слова, это необходимая структура мозгового процесса менеджера. Как выглядят функции менеджмента в классическом варианте Планирование: Вы ставите цели, ищете способы и ресурсы для их достижения, оцениваете риски, рассчитываете бюджеты; Планирование: Вы ставите цели, ищете способы и ресурсы для их достижения, оцениваете риски, рассчитываете бюджеты; Организация: Вы выстраиваете структуру: кто за что отвечает, как распределить ресурсы и зоны ответственности; Организация: Вы выстраиваете структуру: кто за что отвечает, как распределить ресурсы и зоны ответственности; Руководство: Вы мотивируете людей, поддерживаете дисциплину, задаёте ориентиры, решаете конфликты; Руководство: Вы мотивируете людей, поддерживаете дисциплину, задаёте ориентиры, решаете конфликты; Координация: Вы синхронизируете все элементы системы: сотрудники, отделы, подрядчики должны работать сообща, а не «тянуть одеяло» каждый в свою сторону; Координация: Вы синхронизируете все элементы системы: сотрудники, отделы, подрядчики должны работать сообща, а не «тянуть одеяло» каждый в свою сторону; Контроль: Вы проверяете, идёте ли вы по плану, укладываетесь ли в бюджет, насколько команда выполняет KPI и нет ли серьёзных отклонений. Контроль: Вы проверяете, идёте ли вы по плану, укладываетесь ли в бюджет, насколько команда выполняет KPI и нет ли серьёзных отклонений. Тот же набор функций — но уже для менеджера проектов В классическом «корпоративном» менеджменте важно удерживать общую стратегию компании, её структуру, долгосрочную перспективу. А в проектах добавляется временная природа и уникальность результата. Но «5 функций» никуда не исчезают, они лишь видоизменяются: Планирование в проекте превращается в составление чёткой иерархической структуры работ, детального календарного плана, бюджета и реестра рисков; Планирование в проекте превращается в составление чёткой иерархической структуры работ, детального календарного плана, бюджета и реестра рисков; Организация — это формирование проектной команды, определение ролей, выстраивание каналов коммуникации с заказчиком и стейкхолдерами; Организация — это формирование проектной команды, определение ролей, выстраивание каналов коммуникации с заказчиком и стейкхолдерами; Руководство — управление мотивацией и настройкой команды в условиях, когда сроки сжаты и надо вчера, а задачи сложны и вообще не описаны; Руководство — управление мотивацией и настройкой команды в условиях, когда сроки сжаты и надо вчера, а задачи сложны и вообще не описаны; Координация — согласование усилий разных групп (разработчиков, тестировщиков, маркетинга, финансового отдела), чтобы не было «бутылочных горлышек»; Координация — согласование усилий разных групп (разработчиков, тестировщиков, маркетинга, финансового отдела), чтобы не было «бутылочных горлышек»; Контроль — отслеживание статуса задач, расхода бюджета, управление изменениями и корректировка плана. Контроль — отслеживание статуса задач, расхода бюджета, управление изменениями и корректировка плана. Все современные фреймворки (p3express, PRINCE2, disciplined agile и их предшественники RUP, DSDM и т.д, а также общие практики вроде Scrum, Kanban, XP),— это, по сути, надстройка над этими базовыми принципами. Освоив фундамент, вы быстрее и осмысленнее берёте любую методологию: уже понимаете, зачем нужны спринты (планирование и контроль), ретроспективы (координация и руководство), доски Kanban (организация и контроль), ревью (координация, контроль) и тд. Особенности для IT-проектов IT — сфера более гибкая, сложная и требовательная к быстрой адаптации. Тем не менее, базовые функции всё ещё в игре: Планирование должно учитывать IT-специфику: выбор архитектуры, технологического стека, возможные интеграции. Возникает непрерывный процесс пересмотра приоритетов, поскольку требования меняются зачастую «на лету»; Планирование должно учитывать IT-специфику: выбор архитектуры, технологического стека, возможные интеграции. Возникает непрерывный процесс пересмотра приоритетов, поскольку требования меняются зачастую «на лету»; Организация осложняется наличием кросс-функциональных команд, разных часовых поясов (если команда распределённая), необходимостью DevOps-инфраструктуры; Организация осложняется наличием кросс-функциональных команд, разных часовых поясов (если команда распределённая), необходимостью DevOps-инфраструктуры; Руководство в IT требует мягкого подхода (servant-лидерство), так как IT-специалисты часто ценят свободу, гибкость, интерес к технологиям. Нужно найти баланс между «давайте экспериментировать» и «укладываемся в сроки»; Руководство в IT требует мягкого подхода (servant-лидерство), так как IT-специалисты часто ценят свободу, гибкость, интерес к технологиям. Нужно найти баланс между «давайте экспериментировать» и «укладываемся в сроки»; Координация затрагивает постоянные интеграции (микросервисы, внешние API, технологические платформы) и быструю реакцию на изменения (Agile!), поэтому приходится синхронизировать людей и задачи практически в режиме онлайн; Координация затрагивает постоянные интеграции (микросервисы, внешние API, технологические платформы) и быструю реакцию на изменения (Agile!), поэтому приходится синхронизировать людей и задачи практически в режиме онлайн; Контроль плавно перетекает в автоматизированный мониторинг (CI/CD, автотесты, алертинг), сбор метрик (velocity, bug rate) и регулярные обзорные сессии. Контроль уже не столько «жёсткий», сколько прозрачный. Контроль плавно перетекает в автоматизированный мониторинг (CI/CD, автотесты, алертинг), сбор метрик (velocity, bug rate) и регулярные обзорные сессии. Контроль уже не столько «жёсткий», сколько прозрачный. Для IT-проектов в 2025 году Мы живём в эпоху удалёнки, DevOps, миллиона облаков и AI-сервисов. Но пять функций никуда не исчезают, просто меняются нюансы: Планирование теперь с учётом часовых поясов, сложности интеграций, облачных серверов и микросервисной архитектуры, а же специфичной регуляторности и санкционных политик; Планирование теперь с учётом часовых поясов, сложности интеграций, облачных серверов и микросервисной архитектуры, а же специфичной регуляторности и санкционных политик; Организация — это вся эта «зона ответственности»: у тебя может быть dev-команда в одном городе, тестировщик-фрилансер в другом, а дизайнер — из третьей страны (и обожать спать в обед) в сочетании с работой в \"так исторически сложилось\" jira и agile даже не произноси; Организация — это вся эта «зона ответственности»: у тебя может быть dev-команда в одном городе, тестировщик-фрилансер в другом, а дизайнер — из третьей страны (и обожать спать в обед) в сочетании с работой в \"так исторически сложилось\" jira и agile даже не произноси; Руководство требует больше эмпатии, ибо люди разбросаны, редко видят друг друга вживую и могут банально почувствовать себя одиноко; Руководство требует больше эмпатии, ибо люди разбросаны, редко видят друг друга вживую и могут банально почувствовать себя одиноко; Координация — куча сервисов, API, платёжек, и всё это надо сводить в «гладкий» продукт. Тут devops-культура помогает, но без хорошего менеджера всё равно бардак; Координация — куча сервисов, API, платёжек, и всё это надо сводить в «гладкий» продукт. Тут devops-культура помогает, но без хорошего менеджера всё равно бардак; Контроль опирается на автотесты, CI/CD, алертинги, метрики. Активно внедряются AI агенты для для чего угодно, но AI может подсказывать риски, но если у вас нет здравого смысла — что угодно утечёт сквозь пальцы. Контроль опирается на автотесты, CI/CD, алертинги, метрики. Активно внедряются AI агенты для для чего угодно, но AI может подсказывать риски, но если у вас нет здравого смысла — что угодно утечёт сквозь пальцы. Книги, которые помогут прокачать все 5 функций Если вы хотите не просто «слепо» ставить задачи в Jira, а осмысленно прокачаться как PM, ниже отличный список: Планирование (50+ лет) «Общее и промышленное управление» — Анри Файоль; (50+ лет) «Общее и промышленное управление» — Анри Файоль; (~30 лет) «Вовремя и в рамках бюджета» — Лоуренс Лич; (~30 лет) «Вовремя и в рамках бюджета» — Лоуренс Лич; (до 10 лет) «Бизнес с нуля (The Lean Startup)» — Эрик Рис; (до 10 лет) «Бизнес с нуля (The Lean Startup)» — Эрик Рис; (до 5 лет) «Управление проектами: быстрый старт» — Дмитрий Завертайлов; (до 5 лет) «Управление проектами: быстрый старт» — Дмитрий Завертайлов; (бизнес-роман) «Критическая цепь» — Элияху Голдратт. (бизнес-роман) «Критическая цепь» — Элияху Голдратт. Организация (50+ лет) «Принципы научного управления» — Фредерик Тейлор; (50+ лет) «Принципы научного управления» — Фредерик Тейлор; (~30 лет) «Системный и ситуационный анализ управленческих функций» — Г. Кунц; (~30 лет) «Системный и ситуационный анализ управленческих функций» — Г. Кунц; (до 10 лет) «Управление 3.0» — Юрген Аппело; (до 10 лет) «Управление 3.0» — Юрген Аппело; (до 5 лет) «Канбан Метод. Базовая практика» — Алексей Пименов; (до 5 лет) «Канбан Метод. Базовая практика» — Алексей Пименов; (бизнес-роман) «Цель» — Элияху Голдратт. (бизнес-роман) «Цель» — Элияху Голдратт. Руководство (50+ лет) «Эффективный руководитель» — Питер Друкер; (50+ лет) «Эффективный руководитель» — Питер Друкер; (~30 лет) «Впереди перемен (Leading Change)» — Джон Коттер; (~30 лет) «Впереди перемен (Leading Change)» — Джон Коттер; (до 10 лет) «Лидеры едят последними» — Саймон Синек; (до 10 лет) «Лидеры едят последними» — Саймон Синек; (до 5 лет) «Коучинг agile-команд» — Лисса Адкинс; (до 5 лет) «Коучинг agile-команд» — Лисса Адкинс; (бизнес-роман) «Разворот корабля!…» — Л. Дэвид Маркет. (бизнес-роман) «Разворот корабля!…» — Л. Дэвид Маркет. Координация (50+ лет) «Динамика администрирования» — Мэри Паркер Фоллетт. Эту книгу сложно найти на русском, но стоит почитать хотя бы выдержки; (50+ лет) «Динамика администрирования» — Мэри Паркер Фоллетт. Эту книгу сложно найти на русском, но стоит почитать хотя бы выдержки; (~30 лет) «Мудрость команд» — Катценбах, Смит; (~30 лет) «Мудрость команд» — Катценбах, Смит; (до 10 лет) «Remote. Офис не обязателен» — Фрайд, Х. Х.; (до 10 лет) «Remote. Офис не обязателен» — Фрайд, Х. Х.; (до 5 лет) «Антихрупкость в IT» — А. В. Бындю; (до 5 лет) «Антихрупкость в IT» — А. В. Бындю; (бизнес-роман) «Пять пороков команды» — Патрик Лёнсиони. (бизнес-роман) «Пять пороков команды» — Патрик Лёнсиони. Контроль (50+ лет) «Экономический контроль качества продукции» — Уолтер Шухарт; (50+ лет) «Экономический контроль качества продукции» — Уолтер Шухарт; (~30 лет) «Сбалансированная система показателей» — Каплан, Нортон; (~30 лет) «Сбалансированная система показателей» — Каплан, Нортон; (до 10 лет) «Статистическое управление процессами» — Чамберс, Уилер; (до 10 лет) «Статистическое управление процессами» — Чамберс, Уилер; (до 5 лет) «OKR. Инструмент, который изменит вашу компанию» — Нивен, Ламорт; (до 5 лет) «OKR. Инструмент, который изменит вашу компанию» — Нивен, Ламорт; (бизнес-роман) «Приключения IT-директора» — Остин, Нолан, О’Доннелл. (бизнес-роман) «Приключения IT-директора» — Остин, Нолан, О’Доннелл. Вывод: полезно понимать базис, а не слепо наворачивать процессы Понимаете, не стоит торопиться со Скрамом, если вы ещё не постигли, как вообще планировать, организовывать и координировать. Иначе будете бегать с этой доской «todo – in progress– done» и не понимать, почему всё то же самое остаётся «in progress» уже третью неделю. Пока вы не можете базово, без готовых методологий и готового набора инструментов организовать работу, слепо их применять не стоит, просто потому что только их недостаточно Овладеть 5 функциями — это как научиться плавать, вы уже потом сможете выбирать стиль: кроль, брасс, баттерфляй, свободное плавание... А если забыть про «фундамент», то поздравляю — вы утонете (с гордым криком «Scrumban!» в руке). Мой совет по книгам: не ломитесь сразу во все «Agile»-библии вроде книг Майкла Кона или сборников все практик с авторскими взглядами вроде книг С. Беркуна, вместо этого почитайте базу, разберитесь, почему мы делаем план, зачем распределяем роли, как руководим людьми и что за зверь этот «контроль». И тогда вы сами станете раздавать задачи в Jira, но уже понимая, что и зачем вы делаете. Да, и если вам всё ещё лень, ну что-ж... всегда есть путь через грабли. Но грабли — они такие: хлоп, и «звёздочки» перед глазами. Хотя глядишь, так урок и усвоится. Удачи, коллеги. Помните: хороший PM точно знает, куда, зачем и с кем он плывёт, а не просто «гребем туда!». Если вдруг статья заинтересовала - заглядывайте ко мне в каналhttps://t.me/junior_pm. "
  },
  {
    "article_id": "https://habr.com/ru/companies/otus/articles/877934/",
    "title": "Как стоит оценивать задачи, чтобы улучшить прогнозирование сроков?",
    "category": "Менеджмент",
    "tags": "оценка задач, планирование сроков, управление проектами",
    "text": "Участвовал в Аджайл-трансформациях в крупнейших компаниях в России (Альфа банк, МТС, Х5 retail group), с международным опытом в стартапе зарубежом Меня зовут Курдюмов Дмитрий, я более 7 лет управляю ИТ‑командами и помогаю компаниям выстраивать эффективные процессы. Одной из самых частых проблем, с которыми сталкиваются команды, является неточность планирования сроков выполнения задач. Разработчик говорит: «Это на пару часов». Проходит день, потом два, и выясняется, что в процессе работы всплыли интеграционные проблемы, тестировщики нашли баги, а кто‑то еще ждет согласования. В итоге задача, которую оценили в несколько часов, затягивается на несколько дней или недель. Почему так происходит? Потому чтоабсолютные оценки в часах не работают. Они не учитывают неопределенности, возникающие в процессе работы: Время на уточнение требований. Время на уточнение требований. Ожидание обратной связи от аналитиков и заказчиков. Ожидание обратной связи от аналитиков и заказчиков. Время, потраченное на исправление неожиданных проблем. Время, потраченное на исправление неожиданных проблем. Простои между передачами задач из одной команды в другую. Простои между передачами задач из одной команды в другую. Как же тогда планировать и прогнозировать сроки выполнения задач? Как прогнозировать сроки выполнения задач: три работающих подхода Вместо того, чтобы пытаться точно оценить время выполнения каждой задачи, собирайтереальные данныео том, сколько времени занимает выполнение похожих задач. Как это сделать: Фиксируйте, когда задача была взята в работу и когда она реально была завершена (включая тестирование и релиз). Фиксируйте, когда задача была взята в работу и когда она реально была завершена (включая тестирование и релиз). Отслеживайте не только время разработки, но и задержки, связанные с ожиданием ответов, согласованиями и релизами. Отслеживайте не только время разработки, но и задержки, связанные с ожиданием ответов, согласованиями и релизами. Разделите задачи по размеру (например,S, M, L, XL) и собирайте статистику по каждому типу. Разделите задачи по размеру (например,S, M, L, XL) и собирайте статистику по каждому типу. Пример: если за последние три месяца80% задач размера Mвыполнялись в течение5–7 дней, то с высокой вероятностью новая задача аналогичного размера займет примерно столько же времени. 📌Как автоматизировать сбор статистики?Собирать такие данные вручную — долго и неэффективно. Поэтому я рекомендую использовать инструментAimger, который автоматически анализирует скорость выполнения задач, простои, задержки и формирует точные прогнозы на основе исторических данных. Aimger помогает командам и менеджерам получать объективные данные о времени выполнения задач, что делает планирование более точным. Когда команда оценивает задачи в часах, это создает иллюзию точности, которая на самом деле приводит к ошибкам. Вместо этого лучше использоватьотносительные оценки, например: Story Points (по ряду Фибоначчи: 1, 2, 3, 5, 8 и так далее). Story Points (по ряду Фибоначчи: 1, 2, 3, 5, 8 и так далее). Оценку в «футболках» (XS, S, M, L, XL). Оценку в «футболках» (XS, S, M, L, XL). Как это работает: Находим в бэклогесамую маленькуюзадачу, принимаем ее за 1 SP (или XS). Находим в бэклогесамую маленькуюзадачу, принимаем ее за 1 SP (или XS). Сравниваем другие задачи с ней и оцениваем ихотносительно друг друга. Сравниваем другие задачи с ней и оцениваем ихотносительно друг друга. Планируем спринт и смотрим,сколько SP команда реально завершает за итерацию. Планируем спринт и смотрим,сколько SP команда реально завершает за итерацию. Через 3–5 спринтов накапливаетсястатистика, на которую можно опираться при планировании. Через 3–5 спринтов накапливаетсястатистика, на которую можно опираться при планировании. Таким образом,мы оцениваем не время, асложность и объем задачи относительно других, что делает прогнозирование более точным. Пример: Задача A (1 SP) занимает 2 дня. Задача A (1 SP) занимает 2 дня. Задача B (3 SP) обычно занимает 5–6 дней. Задача B (3 SP) обычно занимает 5–6 дней. Задача C (8 SP) может растянуться на 2–3 недели. Задача C (8 SP) может растянуться на 2–3 недели. Через несколько спринтов становится понятно,сколько SP команда делает за итерацию, и можно более точно планировать будущие спринты. Если вы работаете по Scrum, можно анализировать,сколько Story Points команда выполняла в прошлых спринтах. Как это сделать: Фиксируйте скорость команды (Velocity) — среднее количество выполненных SP за спринт. Фиксируйте скорость команды (Velocity) — среднее количество выполненных SP за спринт. Учитывайте тенденции — если команда стабильно делает30 SP за спринт, то планировать на следующий спринт50 SPбессмысленно. Учитывайте тенденции — если команда стабильно делает30 SP за спринт, то планировать на следующий спринт50 SPбессмысленно. Анализируйте вариативность — насколько часто сроки отклонялись от запланированных, какие были причины. Анализируйте вариативность — насколько часто сроки отклонялись от запланированных, какие были причины. Когда у вас есть такие данные, можно с высокой точностью прогнозировать,что реально успеет сделать команда в следующем спринте. Почему статистические оценки лучше экспертных? Они учитывают реальность, а не субъективные ощущения. Они учитывают реальность, а не субъективные ощущения. Позволяют учитывать неопределенности, связанные с согласованиями, задержками и исправлениями. Позволяют учитывать неопределенности, связанные с согласованиями, задержками и исправлениями. Снижают стресс в команде— разработчики не боятся ошибиться в оценках, так как прогноз строится на данных. Снижают стресс в команде— разработчики не боятся ошибиться в оценках, так как прогноз строится на данных. Улучшают предсказуемость— заказчики и руководство получают более точные сроки поставки. Улучшают предсказуемость— заказчики и руководство получают более точные сроки поставки. Какие выгоды дает такой подход? Прогнозируемость сроков вырастает до 90%— вы можете уверенно сказать заказчикам, когда будет готова та или иная задача. Прогнозируемость сроков вырастает до 90%— вы можете уверенно сказать заказчикам, когда будет готова та или иная задача. Улучшается качество планирования— больше никаких внезапных переносов сроков. Улучшается качество планирования— больше никаких внезапных переносов сроков. Экономия времени на оценку— команда не тратит часы на бесполезные обсуждения, а использует уже накопленные данные. Экономия времени на оценку— команда не тратит часы на бесполезные обсуждения, а использует уже накопленные данные. Снижается напряженность в команде— разработчики работают в комфортном темпе, а менеджеры получают объективные данные. Снижается напряженность в команде— разработчики работают в комфортном темпе, а менеджеры получают объективные данные. Более точные ожидания со стороны бизнеса— заказчики понимают, когда получат готовый продукт. Более точные ожидания со стороны бизнеса— заказчики понимают, когда получат готовый продукт. Вывод Абсолютные оценки задач в часах — это иллюзия контроля. Они не работают, потому что не учитывают неопределенности. Если хотите более точно прогнозировать сроки: Используйтестатистику завершенных задач, а не экспертные оценки. Используйтестатистику завершенных задач, а не экспертные оценки. Оценивайте задачив относительных единицах(Story Points, футболки). Оценивайте задачив относительных единицах(Story Points, футболки). Анализируйтескорость выполнения задач в прошлых спринтах. Анализируйтескорость выполнения задач в прошлых спринтах. Автоматизируйте сбор данных с помощью Aimger, чтобы получать объективные прогнозы. Автоматизируйте сбор данных с помощью Aimger, чтобы получать объективные прогнозы. Этот подход делает планирование точнее, а работу команды — более предсказуемой. Приглашаю на менторство Я помогаю менеджерам разного уровня, CTO, CPO лидам разработки и менеджерам проектов выстраивать эффективные процессы в командах, оптимизировать планирование и улучшать предсказуемость работы. Больше полезных материалов по Agile, управлению командами и трансформации процессов — в моемTelegram-канале. Подписывайтесь! А также приходите на менторство. Какие компетенции должны быть у современного продакта? Должен ли продакт выполнять функции проджекта, маркетолога и бухгалтера? Как разные компании трактуют роль продакт-менеджера? Обсудим это и многое другое на открытом уроке 6 февраля.Записаться А также 24 февраля пройдет открытый урок, посвященный теме CJM — как она помогает строить отношения с клиентом. Поговорим об основных этапах касаний и о том, как улучшить пользовательский опыт.Подробнее "
  },
  {
    "article_id": "https://habr.com/ru/companies/otus/articles/877726/",
    "title": "Как правильно ставить цели спринта, чтобы обеспечить фокус для всей команды",
    "category": "Менеджмент",
    "tags": "цели спринта, фокус, управление разработкой",
    "text": "Участвовал в Аджайл‑трансформациях в крупнейших компаниях в России (Альфа банк, МТС, Х5 retail group), с международным опытом в стартапе зарубежом Меня зовут Курдюмов Дмитрий, я более 7 лет управляю ИТ‑командами и провожу трансформации процессов, а еще — менторю менеджеров в части построения процессов и команд, помогаю расти в профессии. В своемтелеграмм-каналепишу о новых инструментах и подходах к управлению. В этой статье хочу поделиться, как правильно формулировать цели спринтов. Они позволяют команде сфокусироваться, синхронизировать усилия и двигаться в одном направлении, поддерживая понимания и фокус всей команды. Почему цели спринта критически важны? Цель спринта — это ключевой элемент планирования, который помогает команде понимать образ результата, а не просто делать разрозненный набор задач. Даже если на планировании вы не учли все задачи, цель поможет понять, как адаптировать план. Хорошие команды ставят цель на планировании спринта и далее на дейли, каждый день уточняют его. Фокусировка— команда концентрируется на единой цели, а не распыляется на несвязанные задачи. Фокусировка— команда концентрируется на единой цели, а не распыляется на несвязанные задачи. Единое понимание— все члены команды и стейкхолдеры знают, к чему движется спринт. Единое понимание— все члены команды и стейкхолдеры знают, к чему движется спринт. Упрощение принятия решений— при возникновении препятствий команда ориентируется на цель, а не на конкретные задачи. Упрощение принятия решений— при возникновении препятствий команда ориентируется на цель, а не на конкретные задачи. Гибкость в реализации— команда сама выбирает, каким способом достигнуть поставленной цели. Гибкость в реализации— команда сама выбирает, каким способом достигнуть поставленной цели. Мотивация и вовлеченность— понятная и вдохновляющая цель делает работу более осмысленной. Мотивация и вовлеченность— понятная и вдохновляющая цель делает работу более осмысленной. Как определить хорошую цель спринта? Цель спринта — это не просто список задач, а четко сформулированное намерение, которое отвечает на вопросы: Какую ценность мы создаем? Какую ценность мы создаем? Как это повлияет на пользователя или бизнес? Как это повлияет на пользователя или бизнес? Каким критериям должно соответствовать достижение цели? Каким критериям должно соответствовать достижение цели? Один из способов оценить качество цели — методFOCUS: Fun (вдохновляющая)— цель должна мотивировать команду и быть сформулирована интересно. Например, вместо «Оптимизация фильтрации товаров» можно сказать «Сделаем фильтрацию удобной, как в Netflix». Fun (вдохновляющая)— цель должна мотивировать команду и быть сформулирована интересно. Например, вместо «Оптимизация фильтрации товаров» можно сказать «Сделаем фильтрацию удобной, как в Netflix». Outcome‑oriented (ориентированная на результат)— цель должна описывать результат, а не процесс.Плохо: «Переписать API для отчетов»Хорошо: «Сделать генерацию отчетов в 2 раза быстрее» Outcome‑oriented (ориентированная на результат)— цель должна описывать результат, а не процесс. Плохо: «Переписать API для отчетов» Плохо: «Переписать API для отчетов» Хорошо: «Сделать генерацию отчетов в 2 раза быстрее» Хорошо: «Сделать генерацию отчетов в 2 раза быстрее» Collaborative (совместно созданная)— цель должна разрабатываться всей командой, а не спускаться сверху. Это увеличивает вовлеченность и понимание. Collaborative (совместно созданная)— цель должна разрабатываться всей командой, а не спускаться сверху. Это увеличивает вовлеченность и понимание. Ultimate (понимание смысла)— цель должна включать объяснение,почемуона важна для пользователей и бизнеса. Ultimate (понимание смысла)— цель должна включать объяснение,почемуона важна для пользователей и бизнеса. Singular (единичная)— одна цель на спринт, а не список из нескольких независимых задач. Singular (единичная)— одна цель на спринт, а не список из нескольких независимых задач. Примеры хороших и плохих целей спринта «Исправить баги и улучшить производительность» — слишком общее и размытое формулирование. «Исправить баги и улучшить производительность» — слишком общее и размытое формулирование. «Реализовать авторизацию через Google и рефакторинг кода» — две несвязанные цели. «Реализовать авторизацию через Google и рефакторинг кода» — две несвязанные цели. «Переделать страницу поиска» — неясно, зачем и в чем ценность. «Переделать страницу поиска» — неясно, зачем и в чем ценность. «Сделать поиск быстрее, чтобы пользователи находили товары на 50% быстрее». «Сделать поиск быстрее, чтобы пользователи находили товары на 50% быстрее». «Улучшить валидацию форм, чтобы снизить процент незавершенных регистраций с 30% до 15%». «Улучшить валидацию форм, чтобы снизить процент незавершенных регистраций с 30% до 15%». «Добавить поддержку OAuth, чтобы повысить удобство входа для новых пользователей». «Добавить поддержку OAuth, чтобы повысить удобство входа для новых пользователей». Другие подходы к формулировке целей Эта методика чаще применяется к пользовательским историям, но хорошо подходит и для целей спринта. Independent (независимая)— цель не зависит от внешних факторов. Independent (независимая)— цель не зависит от внешних факторов. Negotiable (гибкая)— возможно скорректировать путь достижения цели. Negotiable (гибкая)— возможно скорректировать путь достижения цели. Valuable (ценная)— приносит пользу пользователю или бизнесу. Valuable (ценная)— приносит пользу пользователю или бизнесу. Estimable (измеримая)— можно оценить, достигнута цель или нет. Estimable (измеримая)— можно оценить, достигнута цель или нет. Small (достаточно маленькая)— реалистична для выполнения в спринт. Small (достаточно маленькая)— реалистична для выполнения в спринт. Testable (проверяемая)— есть четкие критерии успешности. Testable (проверяемая)— есть четкие критерии успешности. Пример INVEST‑цели:«Сократить среднее время ответа на запрос в системе с 3 секунд до 1,5 секунд». Этот классический метод постановки целей также хорошо подходит для Agile. Specific (конкретная)— цель четко описывает, что должно измениться. Specific (конкретная)— цель четко описывает, что должно измениться. Measurable (измеримая)— есть способ измерить успех. Measurable (измеримая)— есть способ измерить успех. Achievable (достижимая)— реалистична в рамках спринта. Achievable (достижимая)— реалистична в рамках спринта. Relevant (релевантная)— важна для бизнеса и пользователей. Relevant (релевантная)— важна для бизнеса и пользователей. Time‑bound (ограниченная по времени)— должна быть завершена в течение спринта. Time‑bound (ограниченная по времени)— должна быть завершена в течение спринта. Пример SMART‑цели:«Улучшить производительность сервиса, снизив среднюю загрузку страницы checkout до 1 секунды за 2 недели». Типичные ошибки при постановке целей спринта Плохо: «Написать тесты, добавить новую кнопку, обновить документацию». Плохо: «Написать тесты, добавить новую кнопку, обновить документацию». Лучше: «Обеспечить удобную навигацию в личном кабинете пользователей». Лучше: «Обеспечить удобную навигацию в личном кабинете пользователей». Плохо: «Сделать систему лучше». Плохо: «Сделать систему лучше». Лучше: «Сократить время обработки заказов на 30%». Лучше: «Сократить время обработки заказов на 30%». Плохо: «Оптимизировать поиск, добавить авторизацию через Facebook и переработать UX». Плохо: «Оптимизировать поиск, добавить авторизацию через Facebook и переработать UX». Лучше: «Оптимизировать поиск, чтобы пользователи находили товары быстрее». Лучше: «Оптимизировать поиск, чтобы пользователи находили товары быстрее». Как правильно формировать цели спринта? Анализировать бизнес‑приоритеты— какие задачи принесут максимальную ценность? Анализировать бизнес‑приоритеты— какие задачи принесут максимальную ценность? Обсуждать с командой— важно, чтобы цель была понятна и принята всеми. Обсуждать с командой— важно, чтобы цель была понятна и принята всеми. Формулировать просто и понятно— если цель трудно объяснить, она, скорее всего, сложна для исполнения. Формулировать просто и понятно— если цель трудно объяснить, она, скорее всего, сложна для исполнения. Использовать FOCUS, SMART или INVEST— проверяйте цель по этим критериям. Использовать FOCUS, SMART или INVEST— проверяйте цель по этим критериям. Фиксировать цель в заметном месте— на доске задач или в общем чате. Фиксировать цель в заметном месте— на доске задач или в общем чате. Как не допустить ошибок? Очень часто бывает так, что сформулировать одну цель на спринт бывает проблематично, из‑за того что внутри могут быть разрозненные задачи, которые относятся к двум или даже нескольким целям. Здесь есть несколько советов: Во-первых, если вы постоянно берете разрозненные задачи, попробуйте вернуться к своему бэклогу и обсудить с менеджерами продукта: не теряете ли вы фокус на том, куда в принципе вы идете, насколько есть понятная цель продукта, к которой вы хотите прийти, не выглядит ли ваша работа как конвеер задач, а не создание ценного продукта для пользователей. Это хорошие вопросы, которые могут заставить пересмотреть приоритеты в бэклоге продукта и фокусироваться на важном, на чем-то одном в один момент времени. Старайтесь выполнять цели последовательно, а не все вместе. Во-первых, если вы постоянно берете разрозненные задачи, попробуйте вернуться к своему бэклогу и обсудить с менеджерами продукта: не теряете ли вы фокус на том, куда в принципе вы идете, насколько есть понятная цель продукта, к которой вы хотите прийти, не выглядит ли ваша работа как конвеер задач, а не создание ценного продукта для пользователей. Это хорошие вопросы, которые могут заставить пересмотреть приоритеты в бэклоге продукта и фокусироваться на важном, на чем-то одном в один момент времени. Старайтесь выполнять цели последовательно, а не все вместе. Целей не может быть несколько. Даже две цели — это расфокус для команды. Всегда выбирайте одну цель и ставьте ее самой важной. Целей не может быть несколько. Даже две цели — это расфокус для команды. Всегда выбирайте одну цель и ставьте ее самой важной. Не все задачи спринта должны обязательно быть привязаны к цели. Вполне нормально, если часть задач будет не относиться к цели. Цель лишь дает вам ориентир и ответ на вопрос, что должна достичь команда за спринт. И это будет приоритетом №1 для нее. У меня был кейс, когда половина задач спринта к цели не относилась, но мы всегда формулировали при этом одну цель. Это было ответом на вопрос, что самое важное для команды в этом спринте, все остальное — вторично. Не все задачи спринта должны обязательно быть привязаны к цели. Вполне нормально, если часть задач будет не относиться к цели. Цель лишь дает вам ориентир и ответ на вопрос, что должна достичь команда за спринт. И это будет приоритетом №1 для нее. У меня был кейс, когда половина задач спринта к цели не относилась, но мы всегда формулировали при этом одну цель. Это было ответом на вопрос, что самое важное для команды в этом спринте, все остальное — вторично. Заключение Цель спринта — это ориентир для команды, который определяет направление работы и помогает принимать решения. Формулируя цель по принципамFOCUS, SMART или INVEST, можно повысить эффективность спринта, улучшить командное взаимодействие и ускорить доставку ценности пользователям. Главное — не превращать цель в список задач, а формулировать ее так, чтобы она давала команде смысл и вдохновение. Уделяйте время качественному определению цели, и ваши спринты станут гораздо более продуктивными. Пользуясь случаем, рекомендую обратить внимание на открытые уроки, которые проведут коллеги из Otus для будущих и действующих тимлидов: 5 февраля. «Как не треснуть при росте: расширяем команду по уму».Поговорим о том, как планировать рост команды, выстраивать процессы без потери качества, эффективно делегировать, избегать выгорания и извлекать уроки из ошибок на пути роста.Записаться 5 февраля. «Как не треснуть при росте: расширяем команду по уму».Поговорим о том, как планировать рост команды, выстраивать процессы без потери качества, эффективно делегировать, избегать выгорания и извлекать уроки из ошибок на пути роста.Записаться 19 февраля. «Семь бед — «1-1» ответ».Научимся использовать и внедрять инструмент «One-on-one», получим шаблон артефакта для ваших встреч 1-1.Записаться 19 февраля. «Семь бед — «1-1» ответ».Научимся использовать и внедрять инструмент «One-on-one», получим шаблон артефакта для ваших встреч 1-1.Записаться "
  },
  {
    "article_id": "https://habr.com/ru/companies/t2/articles/877680/",
    "title": "Онбординг без головной боли: как рассказать новому сотруднику обо всем без взрыва мозга",
    "category": "Менеджмент",
    "tags": "онбординг, веб-дизайн, дизайн мобильных приложений, управление командой, адаптация сотрудников, адаптация персонала, новый сотрудник, продуктовый дизайн",
    "text": "Что такое онбординг и почему он вам точно нужен. Рассказали лид web-дизайна и дизайнер интерфейсов мобильного приложения T2.Digital. Выход нового сотрудника в команду – это прекрасное и в то же время волнующее событие, потому что надо погрузить новичка во все аспекты работы, причем сделать это относительно быстро и так, чтобы ни у него, ни у вас не взорвалась голова (а это задача со звездочкой). Больше о работе команды мы рассказываемтут. Онбординг призван рассказать сотруднику о команде, о процессах, продуктах и прочих направлениях работы, чтобы последний смог быстро влиться в работу и коллектив и начал эффективно работать. Нужно вгрузить в человека колоссальный объем информации, и важно подать ее так, чтобы ваш новичок не выглядел вот так в первые же дни: Сначала надо вообще понять, что включает в себя онбординг, чтобы новый сотрудник чувствовал себя максимально комфортно. О чем нужно рассказать/просветить: про компанию и отдел в целом – кто вы такие, чем занимаетесь и для чего, из кого состоит команда, к кому нужно идти в случае тех или иных вопросов про компанию и отдел в целом – кто вы такие, чем занимаетесь и для чего, из кого состоит команда, к кому нужно идти в случае тех или иных вопросов про то, как устроена техническая сторона работы дизайнера – как получить доступ к Figma, с какими файлами надо ознакомиться в первую очередь и как придерживаться стандартов, принятых в командепро то, как работать в нашем таск-менеджере, следить за задачами и корректно списывать время про то, как устроена техническая сторона работы дизайнера – как получить доступ к Figma, с какими файлами надо ознакомиться в первую очередь и как придерживаться стандартов, принятых в командепро то, как работать в нашем таск-менеджере, следить за задачами и корректно списывать время про планирование и распределение задач, определение их приоритетности и сроков про планирование и распределение задач, определение их приоритетности и сроков про взаимодействие – с кем нужно общаться, как это делать правильно и по процессу про взаимодействие – с кем нужно общаться, как это делать правильно и по процессу про встречи – нужно погрузить человека в контекст всех массовых и не очень встреч, чтобы новичок понимал, что за встречи в календаре у него стоят про встречи – нужно погрузить человека в контекст всех массовых и не очень встреч, чтобы новичок понимал, что за встречи в календаре у него стоят зоны ответственности – кто чем занимается, чтобы была ясность в том, к кому идти по тому или иному вопросу зоны ответственности – кто чем занимается, чтобы была ясность в том, к кому идти по тому или иному вопросу Ожидания от сотрудника так же могли бы быть частью онбординга, но у нас для этого есть отдельный процесс – испытательный срок, для которого оговариваются и согласовываются обеими сторонами задачи на ближайшие 3 месяца, которые сопровождаются несколькими встречами для отслеживания процесса. Но если в вашей компании такого обособленного процесса по каким-то причинам нет, то это очень даже хороший кандидат для одного из пунктов онбординга новичка. Зачем мы вообще переделали онбординг? Как бы вы ни сопротивлялись тому, чтобы вспомнить все уже ставшие для вас базовыми вещи о работе и собрать мысли в кучу – надо рассказать новому сотруднику о том, как все работает и чем живет. Просто посадив сотрудника перед макетами в Figma в первый же день, вы далеко не уедете. Важно то, как вы рассказываете и погружаете нового коллегу. Когда на вас сваливается огромный объем информации, которую вам просто проговаривают, и еще требуют это все запомнить с первого раза – велик соблазн просто убежать, но это не тот эффект, которого хочется добиться с помощью онбординга. Конечно, онбординг в том или ином виде существовал всегда. Но мы обновили его старые процессы не просто так, а потому что хотели сделать их еще лучше. Иметь один общий документ, в котором все написано, и есть все ссылки в разы удобнее, чем иметь миллион отдельных ссылок, которые вам скинули в мессенджере. Зафиксировать в одном месте имена и контакты первостепенных в вашей работе сотрудников лучше, чем устроить много мини-встреч или вообще просто провести сотрудника по офису и рассказать кто чем занимается Илона Арзуманян При этом хотя личный контакт безусловно важен, и новичков важно знакомить со всей командой, это как минимум увеличит его эффективность за счет снижения неловкости при общении с незнакомыми людьми. Как выглядит наш гайд? Как уже было сказано, первые дни на новом месте довольно волнительны для новых сотрудников: они не знают, куда обратиться с вопросами, помимо своего лида или бадди. Бывает, что кто-то боится перегрузить лишними просьбами или показаться недостаточно компетентными, хотя мы всегда открыты и только рады помочь. Чтобы избежать этого, нужно предоставить новому члену команды чёткий и понятный план действий в период онбординга.Для того чтобы максимально упростить процесс адаптации во всех дизайн-командах T2, мы сделали специальный онбординг-гайд в формате презентации в Figma: Что нужно сделать в первые дни.Подготовили пошаговую инструкцию, описывающую все ключевые действия без которых невозможно начать работу. Здесь сотрудник найдет рекомендации по получению доступов к доскам, ссылки на ПО, инструкции, как настроить рабочее место и все для удаленной работы, и по другим важным моментам. Полезные материалы.Это подборка гайдов, созданных нашими дизайнерами. Туда входит редполитика, проведение исследований,  работа с дизайн-системой и брендбуками. На самом деле эти гайды полезны не только новеньким, но и тем, кто давно работает. Я частенько возвращаюсь к ним, чтобы освежить свои знания. Для нас это некая конституция, которую хорошо бы знать, чтобы поддерживать консистентность в дизайне.Навигация по проектам Довольно сложно найти нужный макет или сценарий по своей задаче, когда ты мало знаком с архитектурой хранения макетов. Чтобы облегчить процесс поиска, мы разработали систему навигации, включающую ссылки на основные сценарии и пояснения по точкам входа. Теперь новым сотрудникам будет проще ориентироваться даже в самом запутанном проекте. Дерево с коллегами По моему мнению, этот блок один из самых важных. Как показывает практика, из главных препятствий на пути к быстрой адаптации – это запоминание лиц и имён коллег. Поэтому мы создали «дерево» нашей digital-дирекции, в котором можно посмотреть, кто работает в той или иной команде, как зовут, чем коллега занимается, какие у него обязанности, по каким вопросам можно обращаться. Такой подход значительно облегчает коммуникацию внутри компании. Анна Мартиросян Процесс работы.Все вышеперечисленное, конечно, очень хорошо и упрощает всем жизнь, но самое сложное – это быстро влиться именно в рабочие процессы. В каждой организации существуют свои особенности взаимодействия между отделами, способы постановки задач и принципы их выполнения. Мы составили подробный чек-лист для каждой дизайн-команды, чтобы помочь новым сотрудникам быстрее войти в рабочий ритм. Он охватывает весь цикл – от планирования (PBR) до самого релиза продукта. К каждому этапу прикреплены нужны ссылки, файлы и контакты коллег, к кому можно обратиться. Как продуктовые дизайнеры работают над задачами? Общий процесс это хорошо, но все же дизайнерам важно понимать, как работать именно над продуктовыми задачами. Ниже мы поделимся небольшим саммари нашего гайда, который описывает процессы работы как в команде сайта, так и в команде мобильного приложения. Итак, какие же разделы мы включили в наш онбординг, и зачем они нам нужны: Работа с доской в нашем таск-менеджере. Нужно для понимания, когда надо двигать задачу в тот или иной столбец по доске, как списывать время, куда какие данные вписывать и так далее. Это некоторый гигиенический уровень по взаимодействию с задачами. Получение, обсуждение и уточнение задачи. Сотруднику важно понимать, откуда появляется задача, как она ставится, как оценивается. Разведывательное исследование и анализ.База для любого проектирования – посмотреть, что уже есть у конкурентов и не только. Помним, что юзеры живут не в вакууме, и они пользуются другими продуктами. Общий план проектирования. Какие этапы включены в обычную работу продуктового дизайнера. К кому когда обращаемся, и в целом рассказываем про то, какой тернистый путь проходят дизайнеры и их макеты перед запуском на прод. Работа с библиотекой блоков. Как работать с файлом в фигме, где собраны все шаблоны/разработанные блоки, чтобы не изобретать велосипед, а пользоваться тем, что уже существует. Работа с графикой, текстом, исследованиями и дизайн-системой. Раздел о том, какие доски нужны, кто за них ответственный, как правильно ставить задачи и какая в них должна быть информация. Нужно, чтобы облегчить процесс коммуникации с коллегами-смежниками. Ревью с командой дизайна, лидами, а также РО и заказчиками. Простыми словами про процесс, своевременность согласований и уровень подготовки к каждому из них. Ревью на соответствие дизайн-системе. Пишем про то, как сделать так, чтобы количество раздетаченных компонентов (это компоненты, которые взяты из дизайн-системы, но отвязаны от нее, грубо говоря раздетаченный компонент = поломанный и более сложный в реализации) стремилось к нулю. Передача макетов в аналитику. В наших голубых мечтах макеты всем понятны, идеально спроектированы и ничего не забыто, но на самом деле аналитике и разработке надо именно передать макеты, то есть подписать, показать и рассказать, как они работают и какая логика в них заложена. Для этого есть стрелочки, стикеры и виджеты. Дизайн-ревью. Конечно же без дизайн-ревью никуда. Для него у нас есть свой шаблон и процесс Как понять, что онбординг сработал? Резонный вопрос для каждой задачи и каждого процесса – «как понять, что мы сделали лучше?». Онбординг – не исключение, ведь это ровно такая же задача. Итак, как же мы поняли, что то, что мы сделали, сработало? А очень просто! С момента обновления онбординга к нам пришло несколько новых ребят, и мы опробовали гайды на них и поняли, что после внимательного прочтения этой документации у них остается минимум вопросов. То, что какие-то вопросы все равно остаются – это норма, нельзя снять все-все вопросы. Итого, качественный онбординг позволяет сотруднику быстро влиться в работу и вникнуть в суть процессов, понять, кто за что отвечает, и в целом легко взаимодействует с командой, имея под рукой хороший фундамент в виде описания рабочего процесса. "
  },
  {
    "article_id": "https://habr.com/ru/articles/877660/",
    "title": "Camunda Alpha Release January 2025 – что нового?",
    "category": "Менеджмент",
    "tags": "Camunda 8, BPM, релиз, zeebe, бизнес-процессы, бизнес-процесс",
    "text": "Ниже приведен обзор всех нововведений в Camunda в релизе 8.7-alpha3, который вышел в январе 2025 года. Новый формат сообщений о релизах Несколько месяцев назад Camunda изменила формат ежемесячных сообщений о релизах. Теперь информация представлена так: в основе лежит E2E Process Orchestration (сквозная оркестрация процессов), а компоненты Camunda изображены как строительные блоки. Это помогает показать, что Camunda обеспечивает надежную инфраструктуру для ваших процессов, где прочный фундамент оркестрации дополняется интеграцией с искусственным интеллектом. E2E Process Orchestration в понимании Camunda означает полное управление бизнес-процессами, которые охватывают все этапы выполнения задач, включая взаимодействие между людьми, системами и устройствами. Этот подход позволяет обеспечить автоматизацию и координацию различных компонентов процесса от начала до конца. Можно сказать, что сегодня это ключевая фишка Камунды. Однако, это все-таки концепция, а не конкретный продукт или что-то еще. Продукты пока все те же. Но мне кажется, что это толстый намек, что стоит ждать появления новых – эти многозначительные три точки в конце. E2E Process Orchestration Этот раздел говорит о компонентах, которые составляют основу Camunda, включая движок, работу платформы, безопасность и API. Zeebe В этой альфа-версии в Zeebe были добавлены следующие функции: Улучшено управление жизненным циклом задач с добавлением слушателей пользовательских задач, которые позволяют реагировать на события жизненного цикла user task. Можно создавать слушателей задач для различных событий, включая назначение исполнителя и завершение. Можно создавать слушателей задач для различных событий, включая назначение исполнителя и завершение. Можно использовать ту же инфраструктуру джобов (Job) для активации и завершения заданий слушателей задач. Можно использовать ту же инфраструктуру джобов (Job) для активации и завершения заданий слушателей задач. Можно получить доступ и при необходимости изменить данные пользовательской задачи (например, назначенного исполнителя) из слушателя. Можно получить доступ и при необходимости изменить данные пользовательской задачи (например, назначенного исполнителя) из слушателя. При необходимости переход на другую фазу жизненного цикла пользовательской задачи может быть отклонен из слушателя. При необходимости переход на другую фазу жизненного цикла пользовательской задачи может быть отклонен из слушателя. Поддерживается обработка инцидентов так же, как и слушатели исполнения (execution listeners). Поддерживается обработка инцидентов так же, как и слушатели исполнения (execution listeners). Эти изменения как упрощают операции, так и обеспечивают более плавную обработку инцидентов, разблокируя выполнение процессов своевременно. Проще говоря, какие-то возможности делать слушателей для юзер-тасков были, но полноценная поддержка этой концепции – user task listener – появится только в новой версии. Добавлена поддержка дополнительных элементов BPMN для миграции экземпляров процессов. К ним относятся: Поддержка миграции параллельных и инклюзивных шлюзов, включая следующие сценарии:Миграция активных шлюзов;Миграция шлюзов, где есть хотя бы один входящий последовательный поток; Поддержка миграции параллельных и инклюзивных шлюзов, включая следующие сценарии: Миграция активных шлюзов; Миграция активных шлюзов; Миграция шлюзов, где есть хотя бы один входящий последовательный поток; Миграция шлюзов, где есть хотя бы один входящий последовательный поток; Миграция событий компенсации и эскалации для поддержания механизмов обработки ошибок и восстановления. Миграция событий компенсации и эскалации для поддержания механизмов обработки ошибок и восстановления. Почему это важно: В реальных бизнес-процессах часто используются параллельные потоки (например, выполнение нескольких задач одновременно) или инклюзивные условия (например, выбор нескольких путей на основе данных). Теперь такие процессы можно безопасно мигрировать без риска нарушения их логики. Представьте процесс обработки заказа: Параллельно выполняются задачи проверки оплаты и проверки наличия товара. Параллельно выполняются задачи проверки оплаты и проверки наличия товара. Если оплата не прошла, запускается событие компенсации для отмены заказа. Если оплата не прошла, запускается событие компенсации для отмены заказа. В случае ошибки запускается событие эскалации для уведомления менеджера. В случае ошибки запускается событие эскалации для уведомления менеджера. Теперь вы можете обновить этот процесс (например, добавить новые шаги или изменить логику), сохранив все активные состояния и механизмы компенсации/эскалации. Operate В этом релизе работали над будущими функциями, исправлениями ошибок и незначительными улучшениями для Operate. Проще говоря, здесь пока без изменений. Tasklist С этим релизом Tasklist теперь включает обработку документов. Эта функция позволяет пользователям загружать и просматривать документы, такие как PDF и изображения, непосредственно в задаче. Обработка документов в Camunda использует новые компоненты на Form-JS. До того, как перейдем к деталям, стоит заметить: Похоже, скоро на Камунде можно будет строить прям настоящий документооборот. Собственно, не хватало только работы с файлами – теперь она есть. Другой вопрос, надо ли это все тащить в процесс и действительно делать из BPM какую-то СЭД – ну, немного спорно. Потому что просто файлами не обойдешься, за этим потянется хранилище документов и все такое. Но посмотрим! File Picker Теперь вы можете использовать компонент File Picker, чтобы выбрать один или несколько файлов (в зависимости от настроек) для загрузки в ваш экземпляр процесса. Когда этот элемент добавлен в вашу форму, можно настроить его свойства. Когда форма была назначена в TaskList, вы можете нажать кнопку «Browse«, чтобы выбрать соответствующие файлы. Когда файлы были успешно загружены, их названия появится на форме. Предварительный просмотр документа Теперь вы можете добавить к форме функцию предварительного просмотра, чтобы просматривать документы, связанные с процессом. Когда пользователь взаимодействует с формой, он увидит что-то вроде этого: Веб-моделер В этом альфа-релизе веб-моделер получает дополнительные функции и возможности. Повтор сценариев (только SaaS) Теперь вы можете использовать Play для быстрого повторения ручных тестов, записывая и воспроизводя экземпляры процессов как сценарии. Когда вы сохраняете завершенные экземпляры как сценарии, Play вычисляет процент элементов, охваченных набором сценариев. Это первый шаг к внедрению автоматизированного тестирования в Веб-моделер и позволяет бизнесу и ИТ совместно работать над  созданием автоматизированных тестов. Это делает тестирование более доступным как для бизнес-аналитиков, не только для разработчиков или профессиональных тестировщиков. Ручное тестирование бизнес-процессов может быть трудоемким и повторяющимся. Возможность записывать сценарии и воспроизводить их сокращает время на проверку корректности процессов. В общем, вперед, к повышению качества процессов! Ибо без тестов никогда не поймешь, как оно все там работает. BPMN-модель это одно, а реальный процесс – совсем другое. Веб-моделер теперь поддерживает нативную интеграцию между процессным приложением и веткой репозитория GitLab. Нетехнические пользователи в организациях, использующих GitLab, теперь могут легко получать доступ к файлам в своем источнике правды, работать совместно с пользователями, которые используют десктоп-моделер на разных платформах и вносить изменения в ветку фичи, которые можно легко замержить и задеплоить. Это можно только приветствовать! Процессы потихоньку встраиваются в общий конвейер разработки. Другие улучшения включают упрощенное развертывание для пользователей self-managed версии. Desktop Modeler Этот альфа-релиз Desktop Modeler включает следующее: Runtime-версии шаблонов элементов Теперь мы проверяем совместимость шаблонов элементов с вашей версией времени выполнения для шаблонов, которые предоставляют информацию о совместимости. Zeebe user task «Zeebe user task» была переименована в «Camunda user task». Пользовательские задачи Camunda теперь являются предпочтительной реализацией для всех пользовательских задач. Переименование «Zeebe user task» в «Camunda user task» не является простым изменением названия, а отражает более глубокие изменения в архитектуре и подходах к управлению задачами пользователей в Camunda. Это указывает на то, что задачи, ранее обозначенные как «Zeebe user task», теперь интегрированы в общую экосистему Camunda и рассматриваются как предпочтительная реализация для всех пользовательских задач. Улучшенная поддержка FEEL Легкий доступ к выражениям результатов скриптовых задач (script task) с помощью подсказки контекстных ключей. Это улучшение делает процесс разработки более интуитивным и эффективным. Разработчики могут быстрее писать код, не беспокоясь о правильности имен переменных и их доступности. Это также снижает вероятность ошибок и упрощает отладку кода. Компонент предварительного просмотра документа в формах Моделируйте и предварительно просматривайте загруженные документы с новым компонентом предварительного просмотра документа. Вы можете увидеть пример в предыдущем разделе «Предварительный просмотр документа». Optimize В этом релизе мы работали над будущими функциями, исправлениями ошибок и незначительными улучшениями для Optimize. Проще говоря, в этом релизе Optimize не изменился. Опции установки Этот раздел предоставляет обновления о вариантах установки и различных поддерживаемых программных компонентах. Самостоятельное управление (Self-managed) Camunda Self-Managed Docker Compose получил новое место: он был перемещен из репозитория Camunda Platform в репозиторий Camunda 8 Self-Managed. К релизу Camunda 8.7 не потребуется клонировать весь репозиторий. С этим релизом вы сможете просто загрузить файлы Docker Compose в качестве артефакта, следуя тому же стилю, который мы используем для других компонентов, таких как Helm charts. Компоненты для автоматизации задач В этом разделе вы можете найти информацию, связанную с компонентами, которые позволяют вам создавать автоматизированные задачи в ваших процессах. Коннекторы В этом релизе предоставлено несколько новых коннекторов, описанных ниже. Хотя у некоторых разработчиков отношение к коннкеторам скептическое из-за ограниченной гибкости, сложности тестирования и так далее, тем не менее этот инструмент присутствует и развивается. Коннекторы полезны в небольших проектах, так как экономят время разработчиков; также их имеет смысл использовать при прототипировании сложных процессов. Поэтому вендор продолжает развивать это направление. Коннектор AWS S3 Теперь Camunda предоставляет исходящий коннектор Amazon S3 для взаимодействия с Amazon Simple Storage Service (Amazon S3) из BPMN-процесса. Этот коннектор поддерживает следующие операции: Загрузка документа в бакет AWS S3 Загрузка документа в бакет AWS S3 Скачивание документа из бакета AWS S3 Скачивание документа из бакета AWS S3 Удаление документа из бакета AWS S3 Удаление документа из бакета AWS S3 Коннектор Box В этом релизе теперь есть исходящий коннектор Box для взаимодействия с содержимым аккаунта Box.com из BPMN-процесса. Этот коннектор поддерживает следующие операции: Скачивание файлов в хранилище документов Camunda Скачивание файлов в хранилище документов Camunda Загрузка файлов из Camunda в Box Загрузка файлов из Camunda в Box Удаление файлов в Box Удаление файлов в Box Перемещение файлов между папками в Box Перемещение файлов между папками в Box Создание новых папок в аккаунте Box Создание новых папок в аккаунте Box Удаление папок из аккаунта Box Удаление папок из аккаунта Box Поиск файлов с помощью API поиска Box Поиск файлов с помощью API поиска Box Подписывайтесь на Telegram каналBPM Developers. Рассказываем про бизнес процессы: новости, гайды,  полезная информация и юмор. 5 февраля в 14:00 мск пройдет презентация новой open source платформы OpenBPM.Регистрируйтесьи приходите. "
  },
  {
    "article_id": "https://habr.com/ru/companies/ozontech/articles/877244/",
    "title": "Как мы адаптируем Agile в Ozon?",
    "category": "Менеджмент",
    "tags": "agile, software development, project management",
    "text": "Привет, Хабр! Меня зовут Антон, я —  тимлид в Ozon. За более чем 20 лет работы в IT, где свыше 15 из них выпало на управленческие должности, меня покидало по разным проектам разработки ПО. Познавая управленческое мастерство, я нередко замечал, как на проектах игнорировали самую важную часть — ориентированность на Клиентов, то есть для кого мы, собственно, эти проекты и продукты реализуем. Методология Agile как раз и была призвана решить проблему фокуса на Клиенте, где в тесном сотрудничестве с ним создаётся первоклассный продукт. Люди и коммуникации выведены в Agile на первый план, а технологии, можно сказать, не упомянуты вообще. Проблемы Agile отмечают не только рядовые пользователи, но и такие мастера, как Роберт С. Мартин и Кент Бек — двое из тех, кто составил Agile Manifesto. Как отмечает Ален Холуб, Agile в последнее время стал означать: делать половину задач (активностей) из Scrum плохо с использованием Jira. В целом все знают плюсы использования данной методологии, но за годы работы я выделил для себя ряд негативных моментов. В данной статье я предлагаю рассмотреть 4 негативных последствия плохого использования Agile, которые, собственно, и подрывают эффективность применения, а также рекомендации, как встроить Agile в понятные бизнесу процессы. Проблемы я стараюсь рассматривать равнозначно как со стороны бизнеса, так и со стороны IT, поэтому надеюсь, что данная статья будет полезна не только техническим менеджерам, но и представителям бизнес-подразделений. Подслушано в баре об Agile: некомедия в 5 актах Действующие лица: PM — то ли Project, то ли Product manager, его руководство пока не определилось, но работает за обоих. В нашей истории является представителем бизнеса. Довольно молодой сотрудник компании. Очень любит Agile и верит: «Если данную методологию правильно применять — всё получится». Он считает (или так заведено в компании), что IT часто виноваты в срывах сроков проектов. Бизнес-ориентированный, стремится приносить пользу Клиентам и бизнесу, нацелен на результат. Хорошо подкован в теории, и любитель позанудствовать. Технический бэкграунд практически отсутствует. TL — Team Lead, довольно-таки опытный руководитель, вырос из разработчиков. Очень много работал на проектах, где требований практически не было и коммуникации с бизнесом носили односторонне-принудительный характер. Понимает Agile и другие методики разработки ПО. В прошлом был небольшой опыт работы PM, но потом вернулся обратно в разработку. Хочет работать в компании, где бизнес адекватный и тесно взаимодействует с IT. Постоянно учится и готов слышать собеседников, признаков неадекватности за последние три года не проявлял. DEV — сеньор, разработчик, которому в целом параллельно на методологии проектов, и чихал он на весь менеджмент, стратегии и корпоративную культуру. Делает задачи и периодически меняет компании, где платят больше. Ковыряет дома Pet-проект, который так и не может запустить. Любитель троллинга и мемчиков. При этом готов прийти на помощь коллегам и решать сложные технические вопросы. Обладает глубокой технической экспертизой и не раз спасал компании в критические моменты, когда системы вот-вот должны были обрушиться. Сторонник красивого кода, паттернов проектирования и его величества рефакторинга. Случайный слушатель — бизнес-консультант в сфере IT, спикер, автор статей. Делает заметки об услышанном, надеюсь, корректно. Все герои работают в разных командах или компаниях. TL, PM, DEV знакомы друг с другом давно. ВАЖНО! Описанная ситуация является гипотетической. Персонажи не являются обобщающими, они фантазия автора и созданы лишь с целью более остро передать излагаемый текст. В общем, все персонажи вымышленные, а любые совпадения случайны. Акт первый. В баре за стойкой после напряжённого рабочего дня. PM:...А я считаю, что Agile — вообще отличная штука. Постулаты были заложены аж в 2001 году, но до сих пор актуальны и работают. И там всего 4 ценности, любой запомнит: Люди и взаимодействие важнее процессов и инструментов. Люди и взаимодействие важнее процессов и инструментов. Работающий продукт важнее исчерпывающей документации. Работающий продукт важнее исчерпывающей документации. Сотрудничество с заказчиком важнее согласования условий контракта. Сотрудничество с заказчиком важнее согласования условий контракта. Готовность к изменениям важнее следования первоначальному плану. Готовность к изменениям важнее следования первоначальному плану. И более того,«не отрицая важности того, что в конце мы всё-таки больше ценим то, что в начале». TL:Ну да, ну да, слышал эту историю, люди зависли высоко в горах и сторговались на этих 4 принципах, которые вроде всех поначалу устраивали. А нам теперь за ними разгребать. DEV:Вполне неплохо для тусовки на горнолыжном склоне. Мы максимум пили горячий чай из термосов… Несмотря на какой-либо интерес со стороны TL и DEV, PM продолжил про пользу и ценность Agile Manifesto. PM:Ну смотрите, манифест действительно фокусируется на людях и результатах работы, но тем не менее не забивает на такие вещи, как процессы, документация и планы. Если не впихивать сюда всякую отсебятину, то он действительно хорош. DEV:Может, хорош о работе? TL(продолжает, не замечая DEV): Слушай, но мы же работаем с людьми, и впоследствии понимание этих ценностей изменилось до неузнаваемости, а какие-то элементы предпочли проигнорировать. Вот мой прошлый PM говорил: «Ой, да кому нужна эта документация, вы же код не на шумерском пишете». DEV:Понятно, мнение разработчика опять проигнорировано... TL(продолжает, не обращая внимания): А другой мой PM говорил, что сроки — это для галочки, пока он не лишился премии за срывы этих галочек... Естественно, после такого возникает негатив, и виноват во всём Agile, а не менеджмент проекта. DEV:Ладно, тогда и я вставлю свои пять копеек. Некоторые мои коллеги просто ненавидят Agile, для них это синоним бардака. Вроде план есть, а вроде и нет. Вроде роли поделены, а по факту никто ни за что не отвечает. Мои друзья из другой компании вообще заморочены чисто на сторипойнтах и диаграммах сгорания! А вообще код писать всё равно мне, пусть хоть «фигаджайл» назовут — суть не поменяется: пиши, переделывай, пиши. Пойдём за столик — фришки закажем. IT страдает от менеджмента Акт второй. Те же, уже за столиком в баре с вкусной картошкой фри и чесночными гренками. TL(вприкуску с картошкой полуразборчиво продолжил): Плохие менеджеры любят Agile, так как он полностью переопределяет баланс полномочий и ответственности. Внезапно у вас, пиэмов, появляется «классическая» власть давать прямые указания команде разработки, что и как делать, над чем работать, а что бросить прямо сейчас. При этом ответственности-то никакой. Вообще ни-ка-кой. (Жестикулирует и вытирает кетчуп салфеткой.) Во всём виноваты разработчики и тестировщики, чаще тестеры, конечно, но это опустим. Ну типа дейлик или ретро для вас как ритуал привлечения к ответственности, где фразы «что мы сделали не так и что мы можем улучшить» воспринимаются всеми как «что ты сделал не так и как ты собираешься это исправить в ближайшие две недели». DEV:Во-во, лично я тщательно выбираю тикеты по принципу, чтобы успеть в спринте потенциально минимально накосячить. Что-то неохота потом оправдываться на ретро, почему я не успел или зафакапил релиз. TL:А как же ценность для пользователя? (Хохочет.) DEVчто-то невнятное пробормотал, сделав глоток из бокала… TL:Ну и у меня есть ощущение, что наш PM — какой-то фильтр между нами и реальными пользователями. В конечном счёте мы имеем слабую обратную связь от пользователей, трансформированную через его понимание. Ну а наши замечания и вопросы как будто просто уходят в песок. В итоге из команды крутых и способных спецов у меня получается команда инфантильных ребят, которые делают только то, что нужно ему. PM:Кому ему, бизнесу? TL:Какому бизнесу? Пиэму, говорю же. Я вообще начал замечать, что мои разрабы всё с большой неохотой берут тикеты в работу, если там предстоит о чём-то подумать, напрячься. Шляпа какая-то… (С досадой покрутил бокал.) PM:О, парни, я, кажется, вспомнил, как это называется — «выученная беспомощность». На каких-то курсах по мотивации проходил. Сейчас погуглю. О! Нашёл: «в состоянии выученной беспомощности индивид, испытывающий неудобства, боль и тому подобные негативные факторы, не предпринимает попыток к улучшению своего положения, хотя имеет такую возможность». Смотри (показывает телефон TL): по мнению Мартина Селигмана, у людей выученная беспомощность сопровождается потерей чувства свободы и контроля, неверием в возможность изменений и в собственные силы, подавленностью воли и депрессией вплоть до наступления смерти. Ахахах, мы тут, кажись, подобрались к раскрытию великой тайны быстрого выгорания в IT. Бизнес страдает от IT Акт третий. Те же, вышли на улицу перед баром подышать. PM:Слушайте, а как так вышло, что если раньше разработкой занимались только программисты, то сейчас это разрослось на несколько ролей? Теперь вы просите бизнес-аналитика, системного аналитика, ну и тестировщика, конечно. И все это для той работы, которую раньше выполнял один человек? О дизайнерах вообще заикаться не буду. DEV:Эм… Ну каждый должен заниматься своей работой, я вот код пишу, а на остальное… Не моя задача бизнес-требования собирать, а уж тем более думать за бизнес. TL(обращаясь к PM): Ну ты не думаешь, что выросла сложность проектов, а бизнес всегда хочет вчера. PM:А раньше-то вы как справлялись? Ну не вы вдвоём конкретно... Ну лиды, разрабы. Кстати, лидов недавно вообще не существовало вроде. Откуда они взялись? TL:Известно откуда. (Смеётся.) PM(продолжает рассуждать): Мы тут все уверенно шагаем в микросервисы с выделенной зоной ответственности, и вроде проекты, наоборот, упрощаются. В общем, не понимаю… (Задумался на немного.) Ну вот скажите: а вам не кажется, что за всем этим раздутием штата скорость внедрения изменений замедлилась? Мой более опытный товарищ рассказывал, как они за пару дней реализовывали целые подсистемы ERP без аналитиков и тестировщиков, на которые сейчас бы ушли месяцы. И багов было точно не больше, чем бы делали сейчас. DEV:Ну плохокод писать — ума не надо… TL:А сейчас ты его не пишешь? DEV:Ой, всё! PM:Хорошо, предположим, вы правы, тогда, возможно, продукты стали лучше, релизы выходят без багов? И-и-и перегруженного UI в 2025-м не существует как явления? DEV, TL и PM молча засмотрелись на проезжающую мимо дорогую машину… PM(прервал тишину): Поймите, я не возражаю против аналитиков и тестировщиков. Я возражаю, когда разработчики сознательно вешают на себя шоры и остаются беспомощными во время отсутствия этих самых аналитиков и тестировщиков. Фокусировка исключительно на коде, возможно, хороша, когда ты пишешь алгоритмы, но выходит боком, когда сталкиваешься с бизнес-требованиями. TL:Кстати, я знаю одного руководителя разработки, который делит разработчиков на кодеров и бизнес-программистов. DEV:Ну удачи ему с подбором… TL:Я сказал «руководитель разработки» вместо «тимлида»? PM:Попался! (Смеется.) TL:Не, ну знаешь, я, может, и соглашусь, что при увеличении ролей усложняются коммуникации, а отсюда и большие сроки. Попробуй всех собери на синк или груминг. У меня столько встреч по разным тематикам, что иногда я плавлюсь и путаю проекты. Бывало, даже задачи заводил не на тот проект, а их реализовывали и принимали. Забавно… DEV:Да не парься, я иногда вообще не понимаю, что я делаю, но лиду вроде окей. PM:Ну естественно. Теперь каждый сотрудник, выполняющий роль бывших программистов, должен отчитываться за себя как за обособленную единицу. Откуда, вы думали, возникают нелепые функции, дыры в безопасности и убежденность, что их пользователь — человек с хорошей компьютерной подготовкой? Лично я уверен, что раздутие штата и деление на микророли. Это основная причина бардака в системах и интерфейсах, а не менеджеры. DEV:Попридержи коней, вас, манагеров, тоже попилили на микророли. Это проблемой не считаешь? PM:Хм… ну, возможно, и в этом тоже… Спринт как повод сделать больше Акт четвертый. Те же. Вернулись в бар, чтобы доесть, допить и выдвинуться домой. DEV:У меня часто возникает ощущение, что единственное важное для вас, пиэмов, — это сроки. Вы нас загнали в спринты, конец которых мы воспринимаем как маленькую смерть. И чем ближе к концу спринта, тем больше от вас вопросов: «Когда? Когда?» Давление, давление… Один мой TL любил говорить: исходя из законов физики, под давлением всё ухудшается. TL:Вот-вот, а на ретро начинается перекладывание ответственности на нас за срыв сроков. PM:Да ладно, мы же спрашиваем вас о сроках, вы там что-то как-то оцениваете и говорите нам. Чего обижаться, когда мы приходим к вам по окончании этих сроков и спрашиваем за результат. DEV:Конечно оцениваем, только потом. TL:Только потом приходит PM, которого этот срок не устраивает, и просит сделать небольшую часть из всей задачи или проекта за половину срока, а может, и ещё раньше. Причём оценка на эту часть задачи уже идет не совсем пропорционально, ведь оценивалась задача целиком, а не её часть. DEV:А за эту половину срока ждут готовый продукт. PM:Не, ну вы ИМХО передергиваете. TL:Скажи ещё, что у тебя такого не случалось. Официант прерывает беседу, наши герои расплачиваются и начинают собираться. DEV:Всё, что мне остаётся, — сидеть по выходным и вечерам и впиливать дикие костылины. Б-р-р, меня аж трясёт от такого. А может, работу поменять? Чёт я уже подустал. TL:Ага, переработки или вон… DEV:Вы, ребят, можете между собой сколько угодно проводить митинги, рисовать схемы, но рано или поздно вы придете ко мне и скажете: копай! И желательно вчера, ведь вы и так столько времени угрохали на обсуждения. Если ваши обсуждения ещё можно как-то сократить, заменить документацией, то мою работу заменить нечем. Как говориться, тут нет замены тяжёлой работе. TL:Да, только мой PM не понимает, что можно попробовать заставить людей работать больше, но заставить думать быстрее не получится. Отсюда замкнутый круг — переработки, они не спасают, ещё больше переработок — тотальное недовольство у всех. PM:Многие мои знакомые так это и называют — «запихнуть слона в коробку». Грустно, конечно, но что, если это дает результаты? TL:В какой перспективе? Разово — возможно, но в долгосрочной — не думаю. Зашли в метро и расположились молча на эскалаторе. Игнорирование технического долга Акт пятый. Те же, едут домой на метро, стоят в вагоне. TL:PM, подскажи, плиз, почему бизнес не воспринимает техдолг? PM:Ну, ребят, бизнес летит вперёд бешеным темпом. Нам нужны фичи, возможно, и пользователям тоже, тут как-то не до вашего техдолга. DEV:Забавно, если бизнес вообще не знает, что он существует. PM:Ну давайте на чистоту: ваш техдолг не особо интересен, так как мы мыслим категориями «плохой код — не приносит денег, хороший код — приносит». А чего у вас там под капотом… да вообще как-то... Бизнес считает, что через полгода ваш код в любом качестве уже будет не нужен. TL:Забавно. А ты не задумывался, чей это долг? PM:Ясен пень, ваш, он же технический. Вот и выкраивайте время на исправление где хотите. Вы накосячили — вам и чинить! TL:Я вот считаю, что это долг бизнеса перед IT за то, что нужно было срочно сделать, игнорируя все принципы разработки ПО, качество и паттерны. PM:Ага, я прям вижу, как ты приходишь и говоришь CEO: у нас тут техдолг есть, но он не наш, а ваш — давайте капаситет, бюджеты и приоритеты. TL:Долг не просто так назван, его могут попросить вернуть обратно, и вряд ли кто-то простит. А что хуже всего — могут попросить отдать этот долг во время сезона распродаж в два ночи. Безусловно, в конечном итоге это скажется на всех, но на бизнесе — в первую очередь. Я думаю, что в такой формулировке CEO бы понял. DEV:О да, обожаю фиксить костыли на проде по ночам. PM:Блин, конечно, это в первую очередь бьёт по бизнесу, хм. (Задумался и посмотрел на схему метро.) Следуя логике, существует ли бизнесовый долг — долг IT перед бизнесом? TL:Конечно, просто обычно его называют проектами. DEV:Забавно, игра слов… Может, бизнесу и IT следовало бы изначально договориться о терминах, прежде чем открывать ящик «разработки»? PM:Проблемы нашего мира — это проблемы коммуникаций, а не следование инструкциям, конечно… Эх. TL вышел на своей станции, а PM и DEV уткнулись в телефоны… Наша адаптация к Agile Так-с, наши герои подсветили нам животрепещущую проблематику, и теперь давайте посмотрим, что можно со всем этим сделать. Важное замечание: у нас нет карго-культа и стремления сделать из организации эталон Agile, наша задача — сделать компанию устойчивой к изменениям и адаптируемой под потребности наших Клиентов. Для этого мы выработали некий флоу, которого придерживаемся при разработке проектов. Однако команды имеют права на адаптацию элементов этого флоу под особенности своих сервисов, проектов и продуктов. - Что вы хотите: выиграть или не проиграть? - А в чём отличие? - Это две разные стратегии! Все категории задач стратегически можно разделить на удержание доли рынка, наращивание доли рынка и создание нового рынка. Задачи в таких категориях обычно представляются в виде проекта или продукта, который приоритезируется на специальных встречах. У каждого направления разработки есть свой капаситет под разные категории задач. Например, на техдолг выделяется 20%, на стратегические проекты — 40%, средние проекты — 40%. Для приоритезации задач внутри выделенного капаситета используется множество критериев: деньги, имидж, удобство, сроки и многие другие. Но самое важное, что приоритезация на таких встречах осуществляется совместно представителями бизнесовых подразделений и IT. Вышеупомянутый техдолг можно отнести в категорию удержания, ведь от стабильности и качества текущих решений напрямую зависит удовлетворенность наших Клиентов. Хотя, если отнести его в категорию наращивания, тоже ошибки не будет, ведь бывают случаи, когда объём обслуживаемых Клиентов ограничивается возможностями информационных систем. Бизнес понимает, что техдолг берётся не на ровном месте, а чаще вызван решением задач в сжатые сроки или изменениями в бизнес-процессах. Время на устранение данных пробелов тоже нужно выделять. Эти мероприятия являются не чем иным, как точкой взаимодействия между бизнесом и IT. Для IT такие задачи представлены в виде царь-эпиков, а их комбинация с учётом приоритетов — не что иное, как роадмап направления, отдела или группы. Таким образом, каждая команда понимает приблизительный объём работы, который необходимо выполнить к определенным периодам, включая техдолговые задачи. Если проект состоит не из одного шага, то мы делаем ещё одно упражнение: рисуем верхнеуровневую реализацию в виде диаграммы Ганта. Это здорово помогает, особенно для выявления критических путей проекта. Если проект небольшой, то сразу разбиваем царь-эпик на подэпики, или истории. В случае ультрабольшого проекта мы выделяем в нём этапы, или майлстоуны, и рассматриваем их как отдельные проекты по вышеописанному флоу. Планы — бесполезны, планирование — бесценно. Дуайт Эзенхауэр Реализация задач выполняется уже по Agile и зависит от принятых методик работы конкретной команды. Может случиться так, что одна команда работает по Scrum, другая по Kanban, а третья, как, например, у меня, комбинирует эти две методологии сразу, где спринт выступает в качестве мини-вехи, а задачи выполняются внутри по методологии Kanban. Профит состоит в том, что для бизнес-подразделений сроки могут занимать приоритетную верхнеуровневую позицию, так и «проваливаться вглубь», вплоть до конкретной задачи. Но практика показала, что хватает царь-эпиков и Гантов. Для IT-подразделений есть чёткие рамки и сроки, что позволяет лучше организовать и синхронизировать свою работу, особенно в кросс-доменных проектах. Иными словами, это можно определить как «что делать — вы знаете, а как — решайте сами, вы же профессионалы, и мы вам доверяем». Если в ходе реализации бизнес-требований возникают вопросы, сложности и предложения, то мы можем спокойно обратиться к бизнес-подразделениям. Например, для новой приоритезации задач или сокращения рамок проекта. Бизнес часто идёт навстречу, и мы отвечаем взаимностью, когда нужно сделать что-то ещё, что не было оговорено вначале. Бизнес слушает нас, а мы — бизнес, особенно когда возникают «метеориты». «Метеориты» — это экстренные внеплановые задачи, часто вызванные изменением со стороны регулирующих органов или реакцией на обратную связь от наших Клиентов и операционных подразделений, от которых зависит скорость доставки и выдачи долгожданных заказов Клиенту. -Что убило динозавров: астероид или куча метеоритов? -Теперь я уже не уверен… Теперь немного пройдёмся по активностям, которые сопровождают практически любой проект (как я уже упоминал ранее, команда трудится над несколькими проектами одновременно, включая техдолг). Сразу скажу, что активности — это не то, что высечено в камне. К примеру, если я вижу, что лучше команде дожать какие-то задачи, нежели чем провести ретро, демо или другое, я его смело отменю или попрошу конкретного человека пропустить какую-то активность. Основное правило, которое я рекомендую применять ко всем встречам, — привлекать минимально необходимое количество людей. Активности, которые требуют проработки сложных технических решений, лучше назначать в первой половине дня. Дело в том, что наша мозговая активность начинает снижаться где-то с 11:30, и время после 14:00 больше подходит для несложных работ. А время около 18:00 наиболее эффективно для генерации новых идей. Ретро и демо лучше проводить в пятницу и по вечерам, так как рабочий настрой к концу недели снижается, в голове уже циркулируют планы на выходные, поэтому элемент «шоу» как нельзя лучше вписывается в эту канву. Кроме того, такой тайминг позволяет заканчивать рабочую неделю на позитивной ноте. Золотое правило: не отъедайте самое продуктивное для разработки время у своей команды разработки. Установочная встреча.Согласование целей проекта, критериев успеха и остановки. Окончательная синхронизация видения IT и бизнеса. Конечно, со стороны IT больший упор идёт на требованиях и видении проекта и подсвечивания проблем, которые видны со стороны IT. Цели проекта, конечно, больше идут со стороны бизнеса. Презентация команде разработки.Формирование понимания у команды о самом проекте, вовлечение команды разработки в бизнес-смысл реализуемого проекта. Не забываем про дизайнеров, так как важно, чтобы они варились в проекте с самого начала. В некоторых компаниях я встречал пренебрежение этим правилом, когда дизайнеров привлекали только под конец, и получали кучу проблем в части UX, которые требовали серьёзной переработки системы. Груминг эпика, стори с бизнесом.После второй встречи появляются вопросы, которые можно разрешить с бизнесом. Сосредоточьтесь на вопросах «что», а не «как». Не обсуждайте техническую реализацию. Груминг задачи с командой разработки.Обсуждаются технические возможности по реализации решения, то есть сосредотачиваемся на «как». Из рекомендаций — опираться на проверенные решения и технологии. Работы должно быть как можно меньше, а эффект — предсказуемым. Дейлики.Регулярные короткие встречи до 15 минут между участниками проекта. Синхронизируемся, делимся достижениями, новыми сведениями, подсвечиваем проблемы. Формат допроса в стиле «что ты делал, что будешь делать» — не наш выбор, тут лучше работать в формате обсуждения открытых вопросов и проблем. Роль ведущего — следить за таймингом и гасить холивары, для этого можно организовывать отдельные встречи. Регулярность зависит от сложности проекта — от ежедневного до нескольких раз в неделю, главное, чтобы хоть один выпадал на понедельник. Демо.Презентация результатов работы бизнес-подразделениям и будущим пользователям для сбора обратной связи и предложений. На демо также можно презентовать идеи, наработки, чтобы получить фидбэк ещё до окончательной реализации. По сути, демо можно использовать как площадку для обсуждения открытых или спорных вопросов с бизнесом и конечными пользователями. Ретро этапа.Проведение работы над ошибками, особенно после демо. Сбор проблем и предложений по оптимизации процесса разработки от команды проекта. Ad hoc встречи.Применяется для оперативного решения проблем с привлечением только необходимых участников. Может также применяться для ускорения ревью кода. Важно не злоупотреблять такими встречами, особенно с привлечением разработчиков, чтобы не получилось так, что сотрудник вместо написания кода только по встречам и ходит. Закрытие проекта осуществляется на тех же встречах с бизнесом, где мы приоритезируем царь-эпики под бурные овации присутствующих. По завершении крупных проектов мы проводим ретро, но уже с заказчиками, где открыто обсуждаем, где и что можно было сделать лучше. Хочу отметить, что возможность озвучить реальные проблемы без политических украшательств реально помогает процессу. Именно поэтому очень важно, чтобы процессы взаимодействия с бизнесом были максимально прозрачными. Вместо ретро В конце предлагаю посмотреть ещё раз на Agile Manifesto, но уже сквозь призму адаптации:1 Уделяйте внимание людям и инструментам, которыми они пользуются в процессе работы. Люди создают продукты для других людей, разных людей, очень разных людей. Люди создают продукты для других людей, разных людей, очень разных людей. Люди не должны тратить время на излишнюю бюрократию, соблюдайте принцип нулевых потерь по Lean. Люди не должны тратить время на излишнюю бюрократию, соблюдайте принцип нулевых потерь по Lean. Профессионалам необходимо предоставить профессиональные инструменты для выполнения задач. Если готовых инструментов, которые можно купить, нет, — заложить ресурсы на их создание. Профессионалам необходимо предоставить профессиональные инструменты для выполнения задач. Если готовых инструментов, которые можно купить, нет, — заложить ресурсы на их создание. 2 Основная цель — получить рабочий продукт, который могут, с одной стороны, применять пользователи и, с другой — поддерживать разработчики. Продукт должен покрывать запрос пользователей. Продукт должен покрывать запрос пользователей. Интерфейс должен быть типовым (Office like), общераспространённым в компании или сопровождаться документацией. Интерфейс должен быть типовым (Office like), общераспространённым в компании или сопровождаться документацией. У разработчика из внешней команды должно уходить минимальное время на погружение в проект. Например, за счет использования типовых шаблонов проектов сервисов, актуальной документации, применения общих практик компании. У разработчика из внешней команды должно уходить минимальное время на погружение в проект. Например, за счет использования типовых шаблонов проектов сервисов, актуальной документации, применения общих практик компании. Взаимодействуйте с заказчиком и фиксируйте договоренности для лучшей синергии. Взаимодействуйте с заказчиком и фиксируйте договоренности для лучшей синергии. 3 У заказчика в проекте есть не только права, но и обязанности. Как пример обязанности — реагировать на замечания и предложения команды разработки, прояснять белые пятна, тестировать результаты разработки. Договоренность — это зафиксированный результат переговоров в виде артефакта (например, статьи на портале, оформленной задачи, допсоглашения к контракту и т. д.) Договоренность — это зафиксированный результат переговоров в виде артефакта (например, статьи на портале, оформленной задачи, допсоглашения к контракту и т. д.) Используйте принцип Win-Win для достижения синергии, принимайте интересы бизнеса и интересы команды разработки — везде люди. Используйте принцип Win-Win для достижения синергии, принимайте интересы бизнеса и интересы команды разработки — везде люди. Составьте план, по которому будет двигаться разработка, но заложите время на изменения. Составьте план, по которому будет двигаться разработка, но заложите время на изменения. Принцип дерева — твердая основа (корни) и гибкие процессы (ветви). Принцип дерева — твердая основа (корни) и гибкие процессы (ветви). Разделяйте и различайте бизнес-требования по нужности и критичности, не всё так однозначно, как кажется. Разделяйте и различайте бизнес-требования по нужности и критичности, не всё так однозначно, как кажется. Иногда нужно говорить «нет», и это нормально. Иногда нужно говорить «нет», и это нормально. Agile — это не панацея, Agile — это не истина, высеченная в камне, которой нужно следовать без оглядки. Agile — это лишь манифест, философия, которую сформировали в горах несколько человек. Идея манифеста — помочь вам в достижении главной цели — создании ценности для пользователя, но не больше. И нет ничего страшного, что в каких-то проектах вы не сможете его использовать, а в каких-то придётся комбинировать с другими методиками. У вас в арсенале есть не только молоток, а все вокруг — не обязательно гвозди. "
  },
  {
    "article_id": "https://habr.com/ru/articles/877458/",
    "title": "Лирическое. Про любовь и работу",
    "category": "Менеджмент",
    "tags": "Отношения и работа, отношения, отношения с менеджерами, отношения с людьми, отношения в компании, собеседования, интервью, прохождение собеседования",
    "text": "Я на сайте увидел Её. Какая же Она была классная! Я почитал про нее поподробнее и понял – я ее хочу. Явно меня ждет там что-то интересное. Я почти влюбился. Я понял, что должен написать. Но это необычный случай, я искал ее слишком долго,  поэтому я готовился несколько дней. Я даже сходил в фотоателье и сделал приличные фотографии (говорят это работает, они обращают на это внимание). Я придумал полностью оригинальный текст для знакомства. Я отлаживал его еще день. И через три дня после того, как я заметил Её, я написал!! На такое письмо было просто невозможно не ответить. Это было и признание в любви, и обещание верности, и много чего еще одновременно. Это был бронебойный снаряд, запущенный в ее сторону, в котором было столько страсти, столько энергии и желания, что он мог бы пробить самую твердую стену и растопить самое ледяное сердце... Я нажал отправить... А в ответ – прочитано. И все. Я не мог в это поверить. Вдруг мое сообщение просто потерялось среди сотен таких же, желающих этой красоты, я же так старался? Но напоминать о себе, вроде, не принято, и я молчал. А мне так никто и не ответил... А еще через месяц пришла стандартная отбивка с сайта: \"Вакансия, на которую вы откликались, перенесена в архив\".  😭😭😭 Это еще одна статья, посвященная софтскилам и лайфхакам в управлении, о которых не рассказывают на курсах по менеджменту. На этот раз будет про отношения и работу и какие навыки пригодятся и там, и там. Если вам интересна эта и подобные темы – подписывайтесь на мой ТГ канал «Морковка спереди, морковка сзади»  и читайте другие мои статьи здесь, на Хабре. Вообще эта статья родилась из поста о том, что увольнение для Руководителя, да и любого сотрудника вовсе не так ужасно, как может казаться на сам момент увольнения («я отстой, я никому не нужен, мое резюме безнадежно испорчено» и так далее). Увольнение – это просто опыт, который надо прожить, сделать выводы и пойти дальше. И дальше будет лучше (конечно, если выводы были сделаны). Но, пока я расписывал этот тезис, я осознал, что все шаги между романтическими отношениями и отношениями на работе похожи практически один в один. И я решил пробежаться по Customer Journey Map от начала до финала отношений. Чем дальше я погружался в сравнение, тем смешнее становилось, настолько там все похоже. Есть небольшая разница, связанная с полами, но, в остальном, все бьется. И я захотел этим поделиться. Я пойду последовательно по шагам CJM от возникновения потребности до завершения совместной работы (а может быть и до момента, где они жили долго и счастливо), и покажу основные сложности и проблемы на каждом шаге. По ходу описания я буду смешивать работу и отношения, чтобы показать, насколько все одинаково. Статья получится большой, так что разделю ее на 2 части: Часть 1: от знакомства до медового месяца. Часть 1: от знакомства до медового месяца. Часть 2: реальные отношения, проблемы, разводы, увольнения и восстановление после. Часть 2: реальные отношения, проблемы, разводы, увольнения и восстановление после. Итак, поехали по нашей CJM, как я ее вижу. 1.    Потребность. Любая история начинается с осознания потребности. Потребность, в нашем случае, или в работе, или в отношениях. Обе потребности важны. В целом, есть специалисты, которые могут жить без отношений, и есть специалисты, которые могут жить без работы. Если это вам не нужно, окей, все остальное не про вас. Но большинство все-таки работает и встречается, это суровый факт. Так что переходим к удовлетворению потребности через соответствующие приложения 😊 2.    Выбор. Для выбора есть соответствующие приложения. Знакомятся парочки в приложениях знакомств, к примеру Тиндер или Мамба (или что там еще есть, я не очень большой спец). Кому надоOne night stand– есть отдельные приложения и для этого.А что по работе? Первичное знакомство начинается с сайтов и приложений а-ля Хедхантер и кучи сервисов, пытающихся его заменить. Для тех, кому не нужны серьезные рабочие отношения, есть сайты для фриланса – добро пожаловать фрилансить без длительных обязательств. 😊 И там, и там потребуется заполнить анкету, где вы рассказываете о себе и о профильном опыте. И там, и там все стараются сделать фоточку поудачнее, опыт приукрасить и вообще – не ударить в грязь лицом. На сайтах знакомств (СЗ) мальчики пишут девочкам, а девочки выбирают. У красивых женщин каждый день прилетает по сотне анкет от кандидатов. Некрасивые сами ходят по анкетам мужчин, ставя лайки тем, кто понравился.На ХХ кандидаты пишут компаниям. У популярных компаний и хороших вакансий рейтинг не ниже, чем у красоток с СЗ. И очень похожая вероятность ответа – то есть, околонулевая, если ты конечно не Бредд Питт в молодости с окладом 1 млн+ или не Топ-Менеджер в 30 лет с огромным опытом из крутой компании и отличным образованием (все сразу и никак не меньше и там, и там). Поэтому бесполезно возмущаться кандидатам, которые подаются на модные вакансии, а их резюме не просматривают или присылают формальный отказ: HR-ам просто некогда с каждым общаться, объясняя, почему он не подходит. Тем более, что из 10 кандидатов 5 с этим категорически не согласятся и пойдут ругать компанию по форумам. По той же причине девушки стараются не говорить реальной причины, почему не готовы к отношениям. Не готовы и все 😊. Разумеется, участники приукрашивают себя: женщины выставляют фотки получше и помоложе, парни фоткаются на фоне чужих тачек, кандидаты пишут левый опыт, а работодатели рассказывают, какая у них новая классная система, умалчивая о том, что на самом деле там адский легаси о миллионах строк, который придется поддерживать еще несколько лет. Короче, и там, и там все делают красивый вид и стараются быть лучше, чем есть. Впрочем, есть честные кандидаты и честные работодатели, я таких встречал. Да и на СЗ тоже встречались честные кандидатки 😊 Альтернативно, если не хочется бегать по сайтам, подбирая варианты, можно трудоустроиться «по знакомству», «на мероприятиях» и тп. Вероятность познакомиться тут выше, но круг знакомств может ограничиваться парой скучных знакомых и дочками маминых подружек. Впрочем, если вы общительный человек и отлично умеете выстраивать социальные связи (или специалист нарасхват), то проблемы не будет –даже не понадобится публиковать свою анкету, вас позовут. А еще и переманивать будут, привлекая бОльшей зарплатой, интересной работой (или борщом и вниманием). Не имей сто рублей, а имей сто друзей – тема работает 😊 Короче, позитивным исходом на этом шаге является МЕТЧ. Дзинь, мы переходим в следующую фазу – Знакомство. 3.    Знакомство. Первое свидание. Знакомство начинается с первой встречи. Раньше все встречались очно. Но ковид внес свои корректировки и первичное собеседование сейчас практически всегда  онлайн. На сайтах знакомств есть разные тенденции: кто-то любит очно, кто-то любит созвониться, кто-то созвон с видео. Тут единства нет. Но в обоих случаях первое знакомство – это проверка общей адекватности партнера. Бывает, что стороны настолько заинтересованы друг в друге, что случается даже секс на первом свидании или оффер после первого же собеседования. Такое бывает, когда кандидат очень ценный, а работа не ждет – тогда офернули и полетели в светлое будущее, скорее-скорее, пока не передумал. Какие тут риски? Точно те же что и в знакомствах: кандидат получит оффер и довольный побежит дальше, потому что он-то вас еще не выбрал. На первом свидании, как и на первом собеседовании, вас обязательно спросят про бывших и с интересом будут слушать, как и почему вы расстались. Очень интересно, как работник рассказывает про своих бывших. Чем больше в анамнезе идиотов, которые не оценили светлую голову, тем больше вероятность, что вы не сработаетесь и следующим козлом станет работодатель. Если человек расставался полюбовно и никого не винит – что ж, общая адекватность налицо, можно переходить к следующему шагу. Второе свидание. Если первое знакомство ОК, дальше случается второе, и это уже близкое знакомство. На работе это будет техническое собеседование, где кандидата спрашивают по скилам. Обычно второе собеседование – и есть ключевое. Оно же самое длинное. Останавливаться неохота, так как тут все ясно: все сидят и долго разговаривают, как можно лучше узнавая друг друга. Заодно понимая, кто о чем не договаривает. Третье свидание. Третье собеседование, обычно, это встреча с руководителем или руководителем руководителя. Тут уже финальное решение перед оффером. В этот момент у компании остается пара кандидатов на выбор, и после третьего собеседования компания готова принять решение. Некоторые особенно популярные компании увеличивают количество собеседований, добавляя всяческие дополнительные проверки и тесты. Впрочем, тоже самое можно сказать и о женщинах: если выбор есть, отчего бы и не повыбирать? 😊 У компании в этот момент есть свои риски: в любой момент кандидат может перестать выходить на связь, потому что у востребованных кандидатов много вариантов трудоустройства. У кандидата риски свои: он участвует в конкурсе и вероятность выиграть неочевидна, потому он рассматривает сразу несколько предложений и бегает по разным свиданиям. Негативный сценарий на этом шаге, разумеется – это вас не выбрали. И тут уже сам кандидат решает, добиваться руки и сердца избранницы или пойти дальше (потому что работодателей много). Каждый решает сам в меру своей заинтересованности. К примеру, я встречал сотрудника, который 8 лет мечтал работать в одном известном банке и каждый год слал туда свое резюме. И на 8й год он добился-таки своего, они его взяли. То есть иногда можно и додолбить. Но иногда можно и просто потратить время впустую. Оффер. Оффер – это, конечно, аналог первого секса. Последствия тут точно такие же. Компании бывает крайне обидно, когда сделали кандидату оффер, и все было на мази, но кандидат в оговоренный день на работу не вышел и перестал выходить на связь. Лично был в такой ситуации, это прямо очень неприятно. Чувствуешь себя использованным и даже немного брошенным. Впрочем, бывает и наоборот: компания тоже может передумать. Вакансию согласовали, но как дело дошло до приема на работу, менеджмент передумал. И теперь уже обидно кандидату, которого мурыжили по свиданиям несколько недель, он тратил время, возможно даже ушел с другой работы, а тут такая история. Бывает, увы. В позитивном же сценарии оффер кандидат принимает, и начинаются Отношения. Тут стоит помнить то, что уже достаточно давно обсуждается в отношениях: на этапе Знакомства стороны показывают друг другу свои лучшие черты, иногда формируя ожидания, которые не сбудутся в реальности. Это может привести к печальным последствиям уже на следующем шаге – медовом месяце (он же испытательный срок). Что может привести к ссорам, обидам и быстрому преждевременному расставанию (увольнению). Чтобы такого не было, работодателя лучше не дурить. Ну а работодателю не стоит обещать того, что он не сможет дать кандидату. Если кандидата ждут сложности – лучше честно о них говорить сразу. Кстати, многие так и делают: помню, как меня кошмарили на собесах в один большой банк тем, какая там страшная бюрократия. И, внезапно, такая честность очень располагала. 4.    Отношения Про отношения будет в следующей части. Там будет про медовый месяц, он же испытательный срок и первые несколько месяцев работы в компании, когда все кажется ново и интересно, когда стороны готовы помогать друг другу, когда руководитель дает вам кредит доверия и ждет, что сотрудник его оправдает, а сотруднику все нравится и кажется, что все будет хорошо и замечательно. На самом деле впереди ждет куча проблем, притирка, ожидания сбывшиеся и нет, выполнение обязательств сторон. Стороны будут ругаться и мириться, а, возможно, даже будет расставание, депрессия и … снова поиск новых знакомств/работы. И так по кругу. Но об этом я лучше напишу во второй части, а то получится слишком долго. Продолжение следует через неделю 😊 "
  },
  {
    "article_id": "https://habr.com/ru/articles/878588/",
    "title": "DeepSeek подтвердила мои подозрения относительно OpenAI. Разработчик ChatGPT ведет проигрышную игру",
    "category": "Научпоп",
    "tags": "chatgpt, openai, deepseek, deepseek r1",
    "text": "Есть три типичные причины, по которым OpenAI попадает в новости: Члены совета директоров терпят фиаско. Члены совета директоров терпят фиаско. Они запускают новый продукт Они запускают новый продукт Или они ноют о том, что все еще не получают прибыль Или они ноют о том, что все еще не получают прибыль Недавно они попали в новости по причине номер 3. В начале января Сэм Альтман опубликовал этот раздражающий твит. Для контекста, 5 декабря 2024 года OpenAI запустилаподписку ChatGPT Pro за 200 долларов, которая, по их собственным словам, «обеспечивает масштабный доступ к лучшим моделям и инструментам OpenAI». Подписка за 200 долларов в месяц стоит довольно дорого, но подписчики получают неограниченный доступ к самым интеллектуальным моделям OpenAI. Речь идет об одних из самых крутых инструментов на рынке ИИ на сегодняшний день: OpenAI o1, o1-mini, GPT-4o, Advanced Voice и другие. То, что OpenAI все еще теряет деньги, несмотря на эту модель подписки, меня совсем не удивляет. С одной стороны, это подтверждает мою давнюю позицию: подход OpenAI к запуску слишком большого количества моделей не является разумным. Я сбился со счета, сколько еще блестящих инструментов запустил OpenAI с тех пор, как ChatGPT дебютировал в 2022 году. Запуск новых и передовых продуктов, похоже, является их основной стратегией для получения прибыли. Компания всегда выпускает новый продукт еще до того, как рынок успевает привыкнуть к тому, который они запустили последним. Но у крупнейших компаний современности дела обстояли иначе. У других технологических гигантов быломеньше продуктов, когда они стали прибыльными: Компания Google стала прибыльной, когда на рынке было всего два продукта: Google Search и Google AdWord. Компания Google стала прибыльной, когда на рынке было всего два продукта: Google Search и Google AdWord. Компания Facebook стала прибыльной, имея в качестве основных продуктов только приложение Facebook и рекламу. Компания Facebook стала прибыльной, имея в качестве основных продуктов только приложение Facebook и рекламу. Компания Apple начала получать прибыль уже через два года после своего основания, а ее основными продуктами были Apple I и II. Компания Apple начала получать прибыль уже через два года после своего основания, а ее основными продуктами были Apple I и II. Компания Microsoft начала получать прибыль сразу после запуска, став единственным поставщиком интерпретатора BASIC для набора Altair 8800. Компания Microsoft начала получать прибыль сразу после запуска, став единственным поставщиком интерпретатора BASIC для набора Altair 8800. Компания OpenAI, выпустившая более восьми продуктов (я знаю, что сбился со счета, но мне нужно было наверстать упущенное, чтобы сказать об этом), до сих пор не стала прибыльной. И я понимаю, что основная причина такой нерентабельности - безумная стоимость обучения и запуска ИИ-моделей,а также большие инвестиции, которые тратятся на исследования и разработки. Запуск моделей обходится OpenAI в 700 000 долларов в день, а на исследования и разработки они тратят миллиарды в год. В итоге, несмотря на то, что в прошлом году компания заработала более 3,7 миллиарда долларов, ее чистый убыток составил более 5 миллиардов долларов. Все в OpenAI большое: пользователи, оценка, доход, количество продуктов и убытки. Все, кроме прибыли! Я решил, что запуск слишком большого количества продуктов в качестве средства достижения прибыльности - не самый лучший вариант, потому что это приведет к резкому увеличению расходов на обучение и текущие расходы. Это простая математика. Если OpenAI обходится в гипотетические $0,1/запрос, когда кто-то использует один из ее продуктов, то эти расходы будут расти тем быстрее, чем больше пользователей будет у OpenAI - будь то один продукт или 10. Больше продуктов и пользователей означает, что вы тратите больше денег. Каждый стартап отдал бы все, чтобы иметь миллион пользователей, и пошел бы на все, чтобы иметь более одного продукта, которым пользуются люди. Имея более 300 миллионов пользователей только у ChatGPT и до 10 продуктов на рынке, OpenAI должна была бы исполнить свою мечту. Но с учетом реалий, связанных с затратами, мечта каждого стартапа становится в данном случае кошмаром OpenAI. Я не знаю, почему OpenAI продолжает идти по пути увеличения количества продуктов. Достаточно применить Первый принцип мышления, и станет ясно, что вместо увеличения количества продуктов OpenAI следует сосредоточиться на инновациях, направленных на снижение затрат на обучение и эксплуатацию. Возвращаясь к нашему гипотетическому сценарию, если стоимость одного запроса может быть снижена до 0,01 доллара для каждого пользователя, это означает 10-кратное сокращение текущих расходов. И кто сказал, что они остановятся на этом? SpaceX последовала модели снижения затрат при создании и запуске ракет. Говорю ли я, что это легко сделать? Определенно нет. Я сомневался, что такой подход к снижению затрат можно применить к обучению и запуску ИИ, особенно если учесть, что отрасль только зарождается, и то, что OpenAI рискует проиграть конкурентам, если решит сосредоточиться на снижении затрат, а не на выпуске новых и лучших продуктов. Таковы были мои сомнения, пока не появился DeepSeek. Уроки от SpaceX и DeepSeek Когда-то освоение космоса было самым большим делом. Но все в нем очень сложно. От теорий, которые лежат в его основе, до практического аспекта строительства ракет, которые исследуют все за пределами Земли, космические исследования - это то, на что отваживаются лишь немногие. Остальным это не по зубам. Хуже того, все, что связано с освоением космоса, стоит до смешного дорого. Отправка ракеты в космос обычно обходится НАСАпочти в 2 миллиарда долларов. Последствия таких затрат двояки: В космос отправляется меньше ракет В космос отправляется меньше ракет В результате освоение космоса происходит медленнее В результате освоение космоса происходит медленнее Тогда Илон Маск решил поступить иначе. С помощью SpaceX Маск смог сделать ракетостроение значительно дешевле. Полеты SpaceX могут стоитьот $62 млн до $90 млн- это более чем 90-процентное снижение цены. А это влечет за собой двойную выгоду: мы можем отправлять в космос больше ракет, а значит, ускорять освоение космоса. Компании удалось добиться этого благодаря сочетанию нескольких факторов: Собственное производство (в отличие от аутсорсинга) Собственное производство (в отличие от аутсорсинга) Упрощение конструкции Упрощение конструкции И возможность повторного использования И возможность повторного использования В результате, по состоянию на апрель 2024 года, SpaceX будет запускать миссиикаждые 2,7 дня. Это число приобретает особую значимость, если вспомнить, что с середины 1980-х по 2010-е годы миссия запускалась каждые 2,8 дня, причем не только НАСА, но и во всем мире! Появление DeepSeek DeepSeek привел в ужас мир технологий после того, как 20 января запустил свою новейшую модель R-1. Не нужно разбираться в технике, чтобы понять всю гениальность и значимость этого запуска. Стоит отметить только три момента: Её производительностьсоответствует или превосходитмодель o1 от OpenAI в некоторых ИИ-тестах. Её производительностьсоответствует или превосходитмодель o1 от OpenAI в некоторых ИИ-тестах. Использование модели бесплатно, а стоимость API составляетодну тридцатуюот стоимости модели OpenAI o1. Использование модели бесплатно, а стоимость API составляетодну тридцатуюот стоимости модели OpenAI o1. Обучение одной модели обошлось в5,6 миллиона долларов, в то время как стоимость создания модели составляет от 100 000 миллионов до 1 миллиарда долларов. Обучение одной модели обошлось в5,6 миллиона долларов, в то время как стоимость создания модели составляет от 100 000 миллионов до 1 миллиарда долларов. Вместо того чтобы поступать так, как поступали уже состоявшиеся игроки в области ИИ, компания DeepSeek решила действовать по-другому. Это помогло им сократить расходы, добившись при этом почти таких же результатов, как и другие игроки в этой игре. Они сделали как минимумчетыре вещипо-другому: Использовали умную оптимизацию, чтобы заставить имеющиеся ресурсы работать эффективнее. Использовали умную оптимизацию, чтобы заставить имеющиеся ресурсы работать эффективнее. Обучали только важные части Обучали только важные части Отдали предпочтение меньшему объему памяти, что привело к более быстрым результатам и снижению затрат. Отдали предпочтение меньшему объему памяти, что привело к более быстрым результатам и снижению затрат. Использовали усиленное обучение Использовали усиленное обучение Детали скорее сложные, чем простые, но результат очевиден для всех. Будем надеяться, что OpenAI поймет, что нужно внедрять инновации, направленные на снижение затрат, а не продолжать наводнять рынок многочисленными продуктами и взимать с пользователей непомерную абонентскую плату. Это можно сделать. Друзья, буду рад, если вы подпишетесь на мойтелеграм-канал про нейросети, чтобы не пропускать анонсы статей, иканал с советамидля тех, кто только учится работать с нейросетями - я стараюсь делиться только полезной информацией. "
  },
  {
    "article_id": "https://habr.com/ru/companies/selectel/articles/878362/",
    "title": "DeepSeek — новый ChatGPT, Qwen — новый DeepSeek? Проверяем модели в разработке, иллюстрациях и готовке",
    "category": "Научпоп",
    "tags": "selectel, deepseek, будущее здесь, chatgpt, нейросети, qwen",
    "text": "Проверка на логику ChatGPT DeepSeek Qwen 2.5-Max Проверка на умение визуализировать ChatGPT DeepSeek, твое время еще не настало Qwen Собеседованиепроверка на программиста ChatGPT Что в итоге"
  },
  {
    "article_id": "https://habr.com/ru/articles/878566/",
    "title": "Технология LEST. Альтернативный путь использования лифта",
    "category": "Научпоп",
    "tags": "лифт, накопитель, гравитационный аккумулятор, теория",
    "text": "Здания потребляют около 40% электроэнергии во всем мире. Существует несколько решений для повышения эффективности энергетических услуг в зданиях. Однако существует ограниченное количество решений для выработки электроэнергии в зданиях. Существующие могут включать генерацию солнечной энергии и хранение энергии (аккумуляторы или небольшие гидроаккумулирующие установки). Увеличение выработки электроэнергии из источников переменной возобновляемой энергии (VRE) может снизить зависимость от ископаемого топлива и сократить выбросы CO2. Однако каждый источник VRE требует дополнительного решения по гибкости из-за его прерывистости. Кроме того, спрос на электроэнергию в здании часто сильно меняется ежедневно и еженедельно, а также в праздничные сезоны, что увеличивает интерес к хранению энергии внутри самого здания. Потребление энергии в лифтах обычно составляет 2–10% от общего потребления энергии зданием. В часы пик лифты могут составлять до 40% спроса на электроэнергию здания. Расчетное суточное потребление энергии лифтами в Нью-Йорке составляет 1945 МВт·ч в будние дни с пиковым спросом 138,8 МВт и 1575 МВт·ч в выходные с пиковым спросом 106,0 МВт. На рис.1 представлено распределение высот зданий в Нью-Йорке и энергии, потребляемой лифтами. Лифты — это сложные системы, которые направлены на предоставление качественных транспортных услуг с наименьшими затратами и потреблением энергии. За полтора столетия скорость лифтов увеличилась в 100 раз. Новые технологии и передовой опыт, включающий двигатели, рекуперативные преобразователи, программное обеспечение управления и оптимизацию противовесов или лифтов без канатов, могут еще больше повысить эффективность лифтов. Для достижения высокого и плавного ускорения, предлагающего высококачественные транспортные услуги и поддерживающего высокую общую энергоэффективность, двигатели строятся безредукторными и с рекуперативными тормозами, которые генерируют чистую и безопасную электроэнергию во время спуска. Для интеллектуальных лифтов был разработан высокоэффективный синхронный мотор-редуктор с постоянными магнитами (PMSGM). Эффективность традиционных редукторов колеблется от 66% до 76%, что является низким показателем. Производительность и параметры двигателя/генератора PMSGM имеют КПД около 92%. Повышение КПД при рекуперативном торможении происходит, в частности, когда лифты движутся с полностью загруженными кабинами. Альтернативами накопителям электроэнергии (EES) для хранения энергии в здании обычно являются батареи и гидроаккумуляторы (PHS). Батареи выигрывают от постоянно снижающихся капитальных затрат. Они, вероятно, предложат доступное решение для хранения энергии для внутридневных и суточных колебаний энергии или предоставят вспомогательные услуги для сети. Однако использование батарей для хранения энергии в недельном цикле может никогда не стать экономически выгодным из-за высокой стоимости хранимой энергии ($/МВт-ч) и, в некоторых случаях, высокой скорости потерь и/или саморазряда в течение дня. Более того, широкомасштабное развертывание батарей в мобильных энергосистемах поднимает вопросы, связанные с доступностью ресурсов и устойчивостью такого интенсивного использования материалов для батарей. Гидроаккумулирующие электростанции (PHS) могут хранить большие объемы энергии для еженедельных, ежемесячных или сезонных циклов в потенциальной энергии, хранящейся в виде массы воды на больших высотах. Электростанции PHS являются единственным экономически целесообразным вариантом для относительно больших установленных мощностей хранения, т. е. более 50 или 100 МВт. Это связано с тем, что стоимость туннелей, трубопроводов, турбин и генераторов на единицу генерирующей мощности выиграет от экономии масштаба. Например, если диаметр туннеля удваивается, это удваивает стоимость туннеля; однако это также учетверяет количество воды, проходящей через туннель, и, следовательно, также мощность станции. Таким образом, для проектов PHS, чем больше установленная мощность, тем дешевле установленная мощность проекта ($/МВт). На рынке существует высокий спрос на жизнеспособную технологию, которая могла бы предложить доступное долгосрочное хранение энергии с низкой генерирующей мощностью, кроме H2 и других синтетических видов топлива, которые страдают от относительно низкой эффективности преобразования переменного тока в переменный ток и высоких капитальных затрат. И этот пробел может быть потенциально заполнен новым решением, называемым технологией хранения энергии лифтом (LEST). LEST — это технология EES, которая использует существующий лифт в высотном здании для подъема твердой массы на вершину здания в режиме зарядки и для снижения массы, генерирующей электроэнергию в режиме разрядки. Технология подъема очень зрелая и широко применяется повсеместно. Основное вмешательство LEST в существующую технологию заключается в том, что подъем также будет использоваться для выработки электроэнергии при снижении высоты массы. Уже давно несколько компаний инвестируют в гравитационное хранение энергии, технологию хранения потенциальной энергии с помощью твердых материалов на разных высотах. Energy Vaultпредлагает разницу напора путем строительства и демонтажа высокой башни из бетонных блоков. Недостатком этой технологии является то, что разница напора между нижним и верхним местами хранения относительно мала; также требуется большая точность при строительстве «бетонной башни», которая может со временем разрушиться. Другое решение предлагаетиспользовать существующие шахтные стволы с большими головками для хранения потенциальной энергии. Существуют такжепредложения по использованию железнодорожных путей для перевозки бетонной массыс нижнего на верхнее место хранения. Помимо необходимости строительства рельсовых путей, вес самого поезда почти равен весу бетонного блока, что приводит к большим потерям энергии. Наклон железнодорожных путей также снижает общую выходную мощность по сравнению с вертикальным спуском. Похожая альтернатива, которая в последнее время привлекла большое внимание, — это возможность генерации электроэнергии с помощью гидроэлектростанции Electric Truck. Но чтобы заполнить существующий пробел для решения децентрализованного хранения энергии в городских условиях с недельными циклами лучше использовать LEST в качестве инновационного подхода к хранению энергии. Методология На рис. 2 представлена ​​методологическая структура, реализованная для оценки LEST. Шаг 1 состоит из проверки технологии, анализа общего вклада лифтов в здание, описания компонентов и эффективности обычных лифтов и определения предлагаемого LEST. Шаг 2 состоит из разработки LEST, поиска его ниш для услуг по хранению энергии, таких как установленная мощность и циклы хранения, предложения различных типов нижних и верхних мест хранения и анализа различных плотностей и затрат материалов для хранения и других компонентов системы. Шаг 3 состоит из оценки стоимости LEST, поиска глобальной базы данных зданий и оценки глобального потенциала LEST. 2.1. Технология хранения энергии лифтами (LEST) LEST связывает два места хранения, одно из которых расположено внизу высотного здания (нижнее место хранения), а другое наверху того же здания (верхнее хранилище). Энергия хранится как потенциальная энергия путем подъема контейнеров для хранения с помощью имеющегося в здании лифта из нижнего хранилища в верхнее хранилище. Затем электроэнергия вырабатывается путем опускания контейнеров для хранения с верхнего на нижнее хранилище. Пример предлагаемой компоновки представлен в Таблице 1. Верхнее и нижнее хранилища описаны на Рис. 3a,b,c. Загрузка и выгрузка контейнеров в лифт выполняется автономным прицепом, который забирает контейнеры из хранилища (нижнего или верхнего), заходит в лифт, движется вверх или вниз, выезжает из лифта и помещает контейнер в другое хранилище (верхнее или нижнее соответственно) (Рис. 3d и e и Рис. 4a и b). Автономный прицеп имеет визуальные датчики, чтобы избежать столкновения с людьми при входе или выходе из лифта и переносе контейнеров вокруг здания (Рис. 4c). Требования к перемещению контейнеров по горизонтали с помощью автономного прицепа невелики, они не были включены в эту оценку. Горизонтальное потребление энергии будет зависеть от прицепа, колес и напольного покрытия. Ковровое покрытие значительно увеличит горизонтальное потребление энергии! Система хранения будет регистрировать положение контейнеров и запускать программное обеспечение для оптимизации доступной емкости хранения в верхних и нижних местах хранения. Администрация здания может выбрать работу системы только в периоды низкого спроса на подъем, чтобы минимизировать влияние работы системы LEST на жильцов здания. Подъемная система может изменять скорость подъема в зависимости от требований к мощности накопления энергии. Если требования к мощности высоки, лифт может увеличить свою скорость; однако это снизит общую эффективность системы! Когда лифты не используются, например, ночью, автономные прицепы могут заполнять лифт контейнерами, а лифт может использоваться для предоставления вспомогательных услуг электросети путем непрерывного подъема и опускания массы в соответствии с требованиями сети. Мощность, вырабатываемая установкой LEST, можно рассчитать с помощью уравнения, где мощность равна энергии, запасенной в песке или гравии, деленной на время, необходимое для его опускания. Например, система будет вырабатывать больше мощности, если песок или гравий опускаются быстро. P ¼ E T (2) где, P — мощность, вырабатываемая системой LEST (Вт), T — время, необходимое для перемещения песка или гравия с верхнего на нижнее место хранения (с). Обычные подъемники проектируются с мощностью, представленной на рис. 5. Результаты Результаты Носители, используемые в предлагаемой конструкции, будут зависеть от доступного пространства и отдачи от услуги хранения энергии. Например, если стоимость места для хранения низкая, то смесь песка и воды может быть хорошим решением. С другой стороны, если стоимость места для хранения высокая, то могут применяться материалы с более высокой плотностью! Плотность и стоимость нескольких носителей сравнены и представлены на рис. 6. В таблице 2 представлено сравнение различных схем работы для LEST, предполагая, что системы имеют 5000 или 50 000 контейнеров для хранения с размерами 0,5 × 0,5 × 2 м, заполненных влажным песком пустыни с плотностью 2000 кг/м3 и весом 1 тонна (обычный предел веса подъемников показан на рис. 5). Количество контейнеров для хранения значительно варьируется в зависимости от несущей способности потолка здания! Цикл хранения в днях оценивается, предполагая, что средняя мощность генерации электроэнергии в лифтах зданий составляет 30 кВт. Коэффициент мощности генерации лифта низок, так как половину времени лифт будет хранить энергию, а другую половину — вырабатывать электроэнергию. Кроме того, в режиме генерации электроэнергии лифт должен двигаться вверх и вниз для выработки электроэнергии. Другими словами, лифт дважды поднимается и дважды опускается в цикле хранения. Предполагая, что лифт работает 70% времени, накапливая энергию, коэффициент мощности системы составляет 17,5%. Здание с 5000 контейнерами и средней разницей высот 50 м имеет емкость хранения энергии 545 кВт·ч, что эквивалентно хранению энергии электромобиля. Обратите внимание, что количество лифтов в здании может значительно увеличиться, если лифты будут без канатов. LEST с большей средней разницей высот обеспечит более высокий потенциал для хранения энергии и более длительный цикл хранения. Как показано в таблице 2, различные рабочие схемы могут привести к циклам хранения энергии в день, недели или месяц. Проектирование и эксплуатация должны быть сосредоточены на долгосрочных циклах хранения (еженедельных или ежемесячных), поскольку батареи могут обеспечить краткосрочное хранение энергии более надежно, дешево и эффективно. Однако, если спрос мини-сети превышает ее пиковую мощность генерации, или если в сети имеется избыток генерации электроэнергии, которую батареи не могут хранить, LEST может использоваться для дополнения краткосрочных требований системы к хранению энергии. С целью воспроизведения рабочего сценария LEST мы предложили использовать морскую ветровую электростанцию ​​близ Нью-Йорка на широте 40,4685 и долготе 73,7722, представленную на рис. 7b, с установленной мощностью 1 ГВт для удовлетворения спроса на электроэнергию в высотных зданиях в Нью-Йорке, как показано на рис. 7c. Желаемая выработка электроэнергии в результате совместной работы оффшорной ветровой электростанции и систем LEST состоит из средней выработки ветровой энергии за одну неделю вперед до анализируемого часа. Профиль выработки ветровой энергии в этом месте использует данные с сайта Renewable Ninja. Рис. 7a показывает частоту посетителей в здании Empire States в течение недели. Это используется для оценки того, когда лифты доступны для использования для хранения энергии. Однако, поскольку сценарий предполагает проверку долгосрочного хранения энергии, предполагается, что ежедневное изменение спроса на энергию будет сбалансировано вспомогательной системой батарей в здании. Предлагаемая система LEST имеет емкость хранения 30 ГВт·ч и начальный заряд 15 ГВт·ч. Для достижения этой емкости хранения потребуется 2752 здания с 50 000 контейнеров для хранения, каждое со средней разницей высот 100 м. Как показано на рис. 7d, операция LEST сосредоточена на хранении энергии в основном в еженедельных циклах, так как она предназначена для работы. На рис. 7c показаны потери энергии, поскольку система хранения энергии не имеет достаточной емкости для хранения всей избыточной генерации оффшорного ветра. Эти сокращения мощности оффшорного ветра эквивалентны только 2% от общей генерации оффшорного ветра. Для расчета предполагается, что установленные подъемники уже имеют возможности рекуперативного торможения, а стоимость аренды пространства для хранения контейнеров на верхних и нижних площадках хранения равна нулю. Таким образом, единственными требованиями к стоимости являются контейнеры, материал, выбранный для увеличения массы контейнеров, и автономные прицепы (таблица 3). Здание, используемое для иллюстрации стоимости системы, имеет 5000 контейнеров для хранения со средней разницей высот 100 м. Стоимость хранения энергии оценивается в 64 долл. США/кВтч. Чем больше разница высот между нижними и верхними площадками хранения, тем дешевле хранить энергию с помощью LEST. Еще одним преимуществом LEST является то, что срок службы контейнера и материала (79% от общей стоимости) превышает 30 лет. Срок службы автономного прицепа (21% от общей стоимости) составляет 5 лет. Обсуждение Обсуждение LEST может заполнить пробел для технологий децентрализованного хранения энергии с еженедельными циклами хранения энергии. См. рис. 8 для LEST с MGES, батареями, PHS, аммиаком и водородом. Рисунок выше фокусируется на долгосрочных решениях по хранению энергии и ограничениях для батарей для коротких энергетических решений. Значительным ограничением для LEST, которое не рассматривалось в этой работе, является несущая способность потолка здания. Здания и их полы спроектированы для определенных целей. Если цель данного здания определена, инженеры будут следовать строительному кодексу, который будет определять требования, касающиеся, например, несущей способности пола. Обычно это представлено в фунтах на квадратный фут или килограммах на квадратный метр. Весьма маловероятно, что здания в прошлом проектировались с идеей, что они могут служить хранилищами энергии в будущем. Поэтому сделано предположение, что несущая способность пола достаточна, чтобы выдержать дополнительный вес складских блоков в каждом здании. Для целей реализации обязательно оценить локальный потенциал, приняв во внимание несущую способность потолка здания. Чтобы минимизировать раскачивание, застройщики размещают гигантские противовесы, называемые настроенными массовыми демпферами (TMD), вблизи вершины небоскребов. TMD — это гигантский шар из стали или бетона, который весит от 300 до 800 тонн, и он обычно подвешивается в здании с помощью пружин и поршней. Автономные прицепы с контейнерами также выполняют функцию настроенного массового демпфера, перемещаясь с одной стороны на другую, чтобы уравновесить движение башни при сильном ветре или во время землетрясения. Поскольку высотное здание не поддерживает 500 прицепов для LEST, автономные прицепы из окружающих зданий, у которых нет проблем с высокой скоростью ветра или землетрясениями, могут переместиться в соседнее высотное здание, чтобы обеспечить настроенный массовый демпфер для обслуживания системы. На рис. 9а представлены примеры настроенных массовых демпферов в высотных зданиях. На рис. 9б представлена ​​карта мира с количеством зданий выше 250 м в городе. LEST также особенно интересен в городах-призраках Китая. Есть несколько городов-призраков, где лифты могут использоваться в качестве устройств хранения энергии. В некоторых случаях инвесторы не сдают пустые квартиры, потому что они хотят быть гибкими, чтобы продать квартиру в любое время, когда они получат хорошую цену. Поэтому LEST может быть хорошим приложением для таких пустых квартир. ThyssenKrupp разработала лифт Multi, который не использует канаты. Вместо этого он использует магнитную силу для подъема и опускания лифтов. Это позволяет лифту двигаться вертикально, горизонтально и по диагонали! Это особенно интересно для высотных зданий, потому что несколько лифтов могут одновременно двигаться вверх или вниз в таких же шахтах. Это значительно уменьшает количество шахт, необходимых в здании, увеличивая до 50% пространства, доступного для жилых или офисных помещений в здании. Эта технология особенно интересна для LEST, потому что: поскольку в одной шахте находится несколько лифтов, ускорение и скорость лифта значительно снижаются, что увеличивает общую эффективность системы, а большое количество лифтов, поднимающихся или опускающихся одновременно, обеспечивает относительно постоянную подачу электроэнергии, и устранение тросов и противовесов обычных лифтов также уменьшает массу лифта на 50%, что еще больше увеличивает эффективность системы. Глобальный потенциал LEST можно оценить с учетом существующих зданий. В нашем анализе рассматривались только здания высотой более 50 м. База данных начинается с Burj Khalifa в Дубае (828 м) и заканчивается Wills Eye Hospital Walnut Towers в Филадельфии (53,25 м). Всего база данных состоит из 22 585 зданий, распределенных по всему миру. На рис. 11 представлено пространственное (региональное) и высотное распределение зданий в указанной базе данных. Средняя высота рассматриваемых зданий составляет ~120 м, и в целом наибольшее количество зданий можно найти в Северной Америке и Азии. Количество зданий немного ниже, чем в рассматриваемой базе данных, поскольку некоторые страны не включены ни в один из упомянутых регионов. Для оценки глобального потенциала мы следовали процедуре, описанной в разделе 2. Мы сделали консервативное предположение, что каждое здание может выдержать (как с точки зрения пространства, так и конструкции) массу 5000 контейнеров. Также предполагалось, что средняя высота равна высоте здания и в некоторых случаях, это может привести к переоценке, так как некоторые здания имеют высокие шпили, не подходящие в качестве верхнего места хранения. В то же время наши расчеты не учитывают потенциальное увеличение средней высоты, если глубокие подвалы используются в качестве нижних мест хранения. Результаты представлены на рис. 12. Координаты зданий были округлены до ближайшего целого числа и сгруппировано. Поэтому здания, расположенные рядом друг с другом, представлены вместе. Глобальный потенциал составил 29,2 ГВт·ч, что в статье округлено до 30 ГВт·ч. Если предположить, что в зданиях можно хранить 50 000 контейнеров, этот глобальный потенциал возрастает до 300 ГВт·ч. Потенциал хранения пропорционален высоте здания, поэтому ранее упомянутая Бурдж-Халифа могла бы потенциально хранить от 9 до 90 МВт·ч. Потенциал хранения LEST в США составляет от 6,5 до 65 ГВт·ч и от 7,3 до 73 ГВт·ч в Китае. Будущие направления для дальнейшего развития LEST: Проведение экспериментов на существующем здании с рекуперативными тормозными лифтами для оценки эффективности LEST с различными двигателями/генераторами и режимами работы, такими как ускорение и скорость, Проведение экспериментов на существующем здании с рекуперативными тормозными лифтами для оценки эффективности LEST с различными двигателями/генераторами и режимами работы, такими как ускорение и скорость, Внедрение LEST в здании для снижения общих затрат на электроэнергию в здании или хранение энергии из непостоянных возобновляемых источников энергии. Внедрение LEST в здании для снижения общих затрат на электроэнергию в здании или хранение энергии из непостоянных возобновляемых источников энергии. Анализ преимуществ и проблем предоставления децентрализованных вспомогательных услуг с помощью LEST, сравнение LEST с маховиками, и проектирование зданий с нуля для оптимизации их использования для хранения энергии с помощью LEST. Анализ преимуществ и проблем предоставления децентрализованных вспомогательных услуг с помощью LEST, сравнение LEST с маховиками, и проектирование зданий с нуля для оптимизации их использования для хранения энергии с помощью LEST. Глобальный потенциал хранения для этой технологии варьируется от 30 до 300 ГВт·ч, в основном в мегаполисах с высотными зданиями, таких как Нью-Йорк, Чикаго, Филадельфия, Сиэтл, Лос-Анджелес, Гавайи и Торонто в Северной Америке, Дубай и Доха на Ближнем Востоке, Пекин, Шанхай, Гонконг, Токио, Куала-Лумпур и Сингапур в Азии, а также Сидней и Мельбурн в Австралии. Хотя на небольших островах Карибского моря, в Индонезии, на Филиппинах и островах Тихого океана обычно не так много высотных зданий, они были бы более благоприятными местами для LEST из-за более высоких затрат на электроэнергию и меньшей потребности в хранении энергии. LEST может служить децентрализованным решением для городского хранения энергии, чтобы сбалансировать еженедельные колебания в подаче электроэнергии от ветровых и солнечных источников и колебания спроса. P.S.- Стоит упомянуть чтоздание-гравитационный аккумулятор в Китаеподозрительно напоминает обычное, только специально созданное с учетом потенциального использования. Но это все лирика. А практически если вернутся к первой картинке в начале статьи при использовании технологии LEST в здании появится еще одна объективная причина задумчивости лифта. Но на этот раз важная не только в плане здоровья жителей, но и экологии."
  },
  {
    "article_id": "https://habr.com/ru/articles/877852/",
    "title": "Недостатки искусственного интеллекта: взгляд изнутри и что мы можем увидеть снаружи. Кому нужен божественный ИИ",
    "category": "Научпоп",
    "tags": "ии, llm, ai",
    "text": "Я спросил какой-то чат не-GPT про недостатки искусственного интеллекта (далее ИИ) и он мне выдал бодрый ответ за который в старших классах наверняка бы поставили пятерку. Но человека отличает наличие собственного мнения которое он стремится защищать даже когда его напрямую не подвергают сомнению, то есть мы умеем отвечать или даже возражать не на прямые вопросы с подвохом или без, но даже на сторонние казалось бы нейтральные рассуждения. Мы способны анализировать и находить то что разрушает наше мировоззрение и возражать на это. Как вы думаете что спорного или даже плохого можно найти в такой формулировке от машины: Искусственный интеллект (ИИ) стал одной из самых обсуждаемых тем в мире технологий. Его потенциал для улучшения различных сфер жизни, от медицины до автоматизации производства, огромен. Для начала можно привязаться к упоминанию медицины: представьте что вы пришли не к естественному врачу, а к бездушной машине и начинаете рассказывать что у вас болит и когда заболело, когда обостряется, в какое время суток, какая шишка где выскочила, вас просят загрузиться в специальный контейнер чтобы послушать, взять анализы (некоторые анализы у вас принимаются в специальных пластиковых контейнерах которые продаются в автоматических аптеках)... Как вы думаете так будет? Или маленько по другому : вы приходите к вполне себе человекоподобному оператору бездушной машины и под его наблюдением формулируете что у вас болит и когда заболело, когда обостряется в какое время суток, какая шишка где выскочила, вас просят загрузиться в специальный контейнер чтобы послушать, взять анализы ... вам выдают рецепт на таблетки и/или направление на некоторое специальное обследование на специальной аппаратуре с другим ИИ... Если у машины все таки будет оператор его же надо будет как-то учить – чему его будут учить чтобы допустить до такого операторства? Даже если не будет оператора, машину кто-то должен будет обслуживать, следить чтобы она не завралась, не подгорела, кто это будет делать и, опять же как и чему он будет учиться и кто его будет учить? А вы знаете как работают сейчас врачи в поликлиниках и какая у них зарплата? И если вместо врачей из поликлинике поставить машину и вместо лаборатории с медсестрами тоже поставить машину которая берет кровь, например. Как вы себе это представляете? Подумайте сколько там существует практических и даже моральных нюансов которые сделают такую замену не реальной хотя бы с экономической точки зрения. Однако, как и любая технология, ИИ имеет свои недостатки, которые нельзя игнорировать. В этой статье мы рассмотрим основные проблемы и вызовы, связанные с искусственным интеллектом. 1.        Этика и предвзятость Это какой то бред по моему, но не-GPT тем не менее беспокоится. Можно предположить откуда растут ноги этой этики и предвзятости в такой формулировке и с таким примером, но я не буду: Одной из главных проблем ИИ является риск предвзятости. Алгоритмы машинного обучения часто обучаются на данных, которые уже содержат скрытые предубеждения. Например, системы распознавания лиц имеют более высокий уровень ошибок при идентификации людей с темным цветом кожи. Это не только несправедливо, но и может привести к серьезным последствиям, включая неправомерные аресты. 2. Угрозы конфиденциальности Сбор и анализ больших объемов данных, необходимых для обучения ИИ, также вызывают опасения по поводу конфиденциальности. Компании и организации, использующие ИИ, часто собирают личные данные пользователей, что может привести к утечкам информации и нарушению приватности. Мне интересно что этот пункт задает направленность применения ИИ именно на работу с личными данными, а ведь так в основном и происходит успехи ИИ в технических областях крайне расплывчаты или настолько специальны что для тех кто просто интересуется успехами ИИ, эти успехи также далеки как и те сверхзадачи на которых их получили, а еще бывает что успехи ИИ не серьезны или используются во вред людям. По моему дело здесь еще и в том что искусственный интеллект общего назначения с которым можно поговорить обо всем и ни о чем, совершенно не годится для использования в любой специальной области знаний. Например тот же самый алгоритм распознавания лиц он как бы тоже что-то из разряда искусственных интеллектов, но это специальный алгоритм предназначенный для специального применения. Никак у вас не получится поговорить с этим алгоритмом. Тут напрашивается вывод о том что есть общие принципы построения алгоритмов искусственного интеллекта, но они могут принципиально отличаться по типу-формату входных и выходных данных для обучения и анализа и даже по виду закономерностей идентифицируемых в качестве результатов. Тут возникает вопрос, а сколько разного типа ИИ можно создать, на сколько областей надо поделить всеобщее человеческое знание и на сколько эффективным будет тот или иной способ такого деления. По-моему проблема с конфиденциальностью не в том что ИИ использует личные данные пользователей, а в том что это люди в первую очередь стремятся получить контроль над другими людьми оправдывая это тем что делают жизнь лучше. 3. Автоматизация и безработица Внедрение ИИ может привести к автоматизации многих рабочих процессов, что, в свою очередь, может вызвать рост безработицы. Особенно это касается рутинных задач, таких как бухгалтерия, административная работа и даже некоторые виды программирования. Это требует от общества адаптации и поиска новых возможностей для трудоустройства. Тут я придумал как мне кажется очень показательную аналогию. Изобретение двигателя внутреннего сгорания (дизельного, а до них парового двигателя как промежуточной ступени) фактически уничтожило целую индустрию коневодства и всего что было с ней связано. Произошла замена одной индустрии (условно назовем индустрией пота, мышц, здоровья и сытости) которая обеспечивала мобильность людям, другой, причем принципиально другой, индустрией металлов, горюче-смазочных материалов, электрики, электроники, синтетических материалов (резины, пластиков, ...), станков, конвееров, ... Если все основы и важные аспекты предудущей индустрии мобильности более менее убирались в голове одного условного специалиста в этой индустрии, то новая индустрия может существовать в ментальном всеобщем сознании только разделенная на несколько более менее отдельных направлений, которые конечно проникают в смежные, но никогда не могут быть полноценно и качественно освоены единственным индивидом. Мне кажется не может быть созданно и какого-то всепоглащающего, совершенно универсального ИИ. Попытки достичь такой всеобщности даже с самым совершенным алгоритмом на самом первокласном и мощном железе подобны всем известному и уже сильно подзабытому поиску филосовского камня, который превращает свинец в золото. Такой универсальный ИИ, который знал бы ответы на все вопросы и решения для любой проблемы был бы фактически воплощением бога. Я надеюсь большинство из нас хорошо понимает что невозможно существование некоего божественного ИИ. К сожалению мне кажется некоторые стремятся именно к такой цели, просто не пытаясь остановиться и подумать-осознать к чему же они стремятся и насколько реальна воображаемая цель. 4. Отсутствие прозрачности и объяснимости Многие современные алгоритмы ИИ, особенно нейронные сети, являются “черными ящиками”. Это означает, что зачастую невозможно понять, как именно ИИ принимает те или иные решения. Отсутствие прозрачности может затруднить выявление ошибок и предвзятости, а также вызвать недоверие со стороны пользователей. Ну с этим аспектом вроде как начинают разбираться. Теперь алгоритмы ИИ способны не только генерировать результирующий ответ, но и позволяют ознакомиться с цепью рассуждений которая привела к такому ответу, это что касается больших (болтающих) языковых моделей (БЯМ-LLM). Насколько я знаю такую промежуточную информацию которую ИИ-алгоритм сгенерировал в процессе работы можно залогировать всегда, хотя для этого конечно необходимо приложить определенные усилия если такого логгирования не заложено в изначальной реализации алгоритма. Тут кстати интересно проследить как ведут себя люди когда их просят объяснить то или иное решении проблемы, которое они собираются реализовать или предлагают кому-то. Обычно человек который уверен в своем решении вполне способен объяснить откуда взялось его решение самыми простыми словами или аналогиями и даже будет стремиться к этому. А человек который не в состоянии пояснить связь решения с проблемой вряд ли заслуживает доверия. Поэтому я бы не стал пользоваться моделью логику которой нельзя верифицировать (и не стал бы доверять людям которые ее создали), по крайней мере не очень доверял бы ее выводам. Это как раз не очень распространяется на LLM-ы (которые БЯМ), дело в том что от них как раз не требуется какая-то точность в суждениях-решениях. По сути БЯМ это справочник с огромной базой во весь интернет, который умеет примерно понять что его спросили и предложить варианты ответов с большей или меньшей вероятностью соответствующих предмету о котором спросили или даже (как бы порассуждать) о предполагаемом предмете чтобы, по сути, получить уточнение исходного запроса. 5. Этические дилеммы ИИ может столкнуться с этическими дилеммами, такими как принятие решений о жизни и смерти, например, в медицине или автономных транспортных средствах. Как должен ИИ определять, кого спасать в критической ситуации? Эти вопросы требуют тщательного обсуждения и разработки четких этических рамок. Я уже слышал интересные рассуждения на эту тему относительно необитаемых машин на дорогах общего пользования. Вроде того как относиться к тому если машина управляемая ИИ попадет в аварию и убьет постороннего человека. Рассуждения сводятся к тому что проблема на самом деле распадается на две. В одном случае виноват этот посторонний человек который понадеялся на человечность водителя машины и допустил неоправданный риск для своей жизни. Это примерно та же ситуация когда люди гибнут под поездом – поезд опасен, это всем известно, поэтому это обязанность пешехода соблюдать правила в опасной зоне. Другой случай если сбой алгоритма приводит к тому что необитаемая машина выходит за рамки определенной для нее опасной зоны (вдруг выезжает на тротуар например, игнорирует светофор, ...), тут как раз мы возвращаемся к вопросам доверия или недоверия к алгоритму и его разработчикам, о котором мы рассуждали в предыдущем пункте. Только к вопросу о доверии и недоверии прибавляется вопрос об ответственности, но уже только людей, тех которые разработали алгоритм и тех кто допустил этот алгоритм к эксплуатации. Заключение Искусственный интеллект обладает огромным потенциалом для улучшения различных аспектов нашей жизни, но также сопряжен с рядом серьезных проблем. Для успешного внедрения ИИ необходимо учитывать этические аспекты, бороться с предвзятостью, защищать конфиденциальность данных и разрабатывать прозрачные и объяснимые алгоритмы. Только через ответственный и осознанный подход мы сможем максимально использовать преимущества ИИ, минимизируя его недостатки. В качестве человеческого заключения хотелось бы заметить что огромный потенциал ИИ по-моему может базироваться только на огромном потенциале человека решать неразрешимые задачи подобные тем чтобы создать космическую станцию-корабли, сформулировать теорию относительности, создать современные компьютеры, беспроводной скоростной интернет, разработать LDPC энкодер/декодер, ... Это мы придумали Windows,это мы объявили дефолт,нам играют живые Битлз,нестареющий Эдриан Пол,наши матери в шлемах и латахбьются в кровь о железную старость,наши дети ругаются матом,нас самих почти не осталось...А мы могли бы служить в разведке,мы могли бы играть в кино!Мы как птицы садимся на разные веткии засыпаем в метро. Я что-то очень сомневаюсь что какой-то ИИ сможет с нами сравниться в ближайшие 10 или даже 50 лет... Но вот лет через 50... так далеко я бы не стал загадывать."
  },
  {
    "article_id": "https://habr.com/ru/companies/bothub/articles/878486/",
    "title": "«Будущее за узкой специализацией»: судьбоносное интервью Ляна Вэньфэна, основателя DeepSeek, посвящённое v2",
    "category": "Научпоп",
    "tags": "ии, ai, deepseek, лянь вэньфэн",
    "text": "Силиконовая долина потрясена: в сфере искусственного интеллекта назревает тектонический сдвиг, и весь мир следит за Китаем. DeepSeek‑r1 произвела эффект разорвавшейся бомбы, сравнявшись по возможностям с топовыми решениями OpenAI. Восток диктует новые правила игры, и перед всем миром встаёт важный вопрос: а что если будущее ИИ создаётся не только в Силиконовой долине? Перед вами интервью с основателем DeepSeek, состоявшееся в июле 2024 года, но распространённое The China Academy лишь недавно. Оно случилось вскоре после выхода в открытый доступ модели DeepSeek‑v2. В этой беседе Лян рассказывает, как китайский стартап осмелился перепрыгнуть титанов индустрии и переопределить концепцию инноваций. Выпускник Чжэцзянского университета, устроивший китайскую ИИ-революцию Стартап DeepSeek, бросивший вызов американской монополии в сфере искусственного интеллекта, был основан хедж‑фонд‑менеджером Ляном Вэньфэном и базируется в ИИ‑лаборатории в Ханчжоу, Восточный Китай. После презентации модели DeepSeek‑r1 (которую вы можете легко и быстро запуститьв нашем агрегаторе нейросетей), вызвавшей глобальный резонанс, Лян превратился в национального героя. Лян окончил Чжэцзянский университет — один из старейших и наиболее престижных вузов Китая — со степенью в области искусственного интеллекта. В 2016 году он стал сооснователем квантового хедж‑фонда High Flyer, который быстро завоевал признание благодаря передовым инвестиционным стратегиям на базе ИИ. Сегодня High Flyer — единственный инвестор DeepSeek. К 2021 году фонд полностью перешёл на автоматизированное управление капиталом, используя машинное обучение для прогнозирования рыночных трендов и принятия инвестиционных решений. Согласно информации на его сайте, в команде фонда работают лауреаты международных математических и физических олимпиад, обладатели серебряных и золотых медалей, а также специалисты с учёными степенями в области статистики, исследований и кибернетики. В мае 2023 года Лян Вэньфэн сделал смелый шаг, основав DeepSeek с амбициозной целью — продвинуть исследования в области общего искусственного интеллекта (AGI). По данным Forbes, уникальная модель финансирования компании — полностью за счёт хедж‑фонда High Flyer — позволила ей разрабатывать передовые ИИ‑проекты без давления со стороны внешних инвесторов. Такой подход обеспечивает DeepSeek полную свободу сосредоточиться на долгосрочных исследованиях и развитии технологий. Команда DeepSeek насчитывает всего десять человек, но каждый из них — выдающийся специалист, выпускник ведущих китайских университетов. Компания культивирует атмосферу инноваций и уделяет особое внимание глубокому знанию китайского языка и культуры. При отборе сотрудников здесь прежде всего оценивают технические способности, а не формальный опыт работы, что привносит в команду свежий взгляд и позволяет развивать прорывные идеи в области ИИ. В отличие от традиционных коммерческих проектов, DeepSeek задумывался как научно‑исследовательская платформа, нацеленная на прорывы в сфере искусственного интеллекта. Лян сознательно избегает публичности и даёт интервью лишь избранным изданиям — таким, как Anyong, дочерняя компания китайского медиахолдинга 36Kr. В редких беседах он делится лишь отрывками своей философии и видения будущего. Как был сделан первый выстрел в ценовой войне? Ань Юн:После выхода модели DeepSeek‑v2 в индустрии крупных ИИ‑моделей разгорелась настоящая ценовая война. Многие называют вас рыночным разрушителем. Лян Вэньфэн:Мы никогда не стремились быть разрушителями. Это получилось случайно. Ань Юн:Этот эффект стал для вас неожиданным? Лян Вэньфэн:Очень неожиданным. Мы даже не думали, что вопрос цен окажется настолько чувствительным. Мы просто шли своим путём — считали затраты и устанавливали цены исходя из них. Наш принцип — не работать в убыток, но и не гнаться за сверхприбылью. Текущие цены обеспечивают лишь небольшую прибыль сверх себестоимости. Ань Юн:Уже через пять дней Zhipu AI последовали вашему примеру, а вскоре к гонке присоединились ByteDance, Alibaba, Baidu и Tencent. Лян Вэньфэн:Zhipu AI снизили цены только на базовые модели, в то время как их флагманские решения остались дорогими. ByteDance первыми действительно выровняли стоимость своих топовых моделей под наши расценки, что вынудило остальных подстроиться. Поскольку у крупных компаний себестоимость ИИ‑моделей значительно выше, мы и представить не могли, что кто‑то будет сознательно работать в убыток. В итоге ситуация повторила старую интернет‑стратегию субсидирования ради захвата рынка. Ань Юн:Со стороны это выглядит как классическая борьба за пользователей — метод, знакомый нам по интернет‑бизнесу. Лян Вэньфэн:Мы не ставили перед собой цель переманить пользователей. Мы снизили цены по двум причинам:во‑первых, по мере исследований новых архитектур стоимость наших моделей сократилась; во‑вторых, мы убеждены, что ИИ и API‑сервисы должны быть доступными каждому. Ань Юн:До этого большинство китайских компаний просто копировали архитектуру Llama, чтобы быстро разрабатывать приложения. Почему вы сделали ставку именно на структуру модели? Лян Вэньфэн:Если цель — как можно быстрее вывести на рынок продукт, то использование Llama действительно оправданно. Однако наша цель — создание AGI (artificial general intelligence, общий искусственный интеллект), а для этого необходимо разрабатывать новые архитектуры, позволяющие добиться высокой производительности даже при ограниченных ресурсах. Это фундаментальные исследования, без которых масштабирование невозможно. Помимо архитектуры, мы также работаем с качеством данных и имитацией человеческого мышления — всё это отражается в наших моделях. К тому же эффективность обучения и себестоимость вычислений у Llama отстают от мировых стандартов примерно на два поколения. Ань Юн:За счёт чего образовался этот технологический разрыв? Лян Вэньфэн:В первую очередь это разрыв в эффективности обучения. По нашим оценкам, даже лучшие китайские модели требуют вдвое больше вычислительных мощностей, чтобы догнать топовые мировые разработки. Кроме того, эффективность работы с данными вдвое ниже, то есть нам нужно вдвое больше информации и ресурсов, чтобы получить аналогичный результат. В совокупности это четырёхкратная разница в затратах. Наша задача — постепенно сокращать этот разрыв. Ань Юн:Большинство китайских ИИ‑компаний разрабатывают и модели, и приложения. Почему DeepSeek сосредоточился исключительно на исследованиях? Лян Вэньфэн:Потому что важнее всего сейчас — участвовать в глобальном технологическом прогрессе. Китайские компании годами использовали инновации, созданные за рубежом, и монетизировали их в виде приложений. Но такой подход не может быть устойчивым.В этот раз наша цель — не быстрая прибыль, а развитие технологического фундамента, который подтолкнёт всю экосистему вперёд. Ань Юн:На протяжении интернет‑революции и мобильного интернета существовало мнение, что США лидируют в инновациях, а Китай — в коммерческом применении технологий. Лян Вэньфэн:Мы считаем, что с развитием экономики Китай должен перестать быть просто пользователем технологий и стать их создателем. За последние 30 лет ИТ‑революции мы практически не участвовали в фундаментальных технологических разработках. Мы привыкли к тому, что закон Мура словно «падает с неба»: стоит просто подождать 18 месяцев — и появятся более мощные процессоры и более продвинутый софт. Точно так же воспринимается и закон масштабирования. Но все эти технологические прорывы — не случайность, а результат многих поколений неустанного труда мирового научного сообщества, в котором доминировали западные страны.И поскольку Китай долгое время не участвовал в этом процессе на фундаментальном уровне, мы невольно недооценили его важность. Настоящий разрыв — в оригинальности, а не во времени Ань Юн:Почему выход DeepSeek‑v2 так удивил Силиконовую долину? Лян Вэньфэн:В США инновации происходят каждый день, и с их точки зрения в этом нет ничего необычного. Их поразило не само технологическое достижение, а то, что китайская компания смогла сыграть на их поле как новатор, а не просто догоняющий — ведь большинство китайских компаний привыкли именно к роли последователей. Ань Юн:Но в китайских реалиях ставка на фундаментальные инновации выглядит почти как роскошь. Разработка крупных ИИ‑моделей требует колоссальных затрат. Не каждая компания может позволить себе заниматься исследованиями, не монетизируя их сразу. Лян Вэньфэн:Инновации действительно стоят дорого. Раньше Китай заимствовал чужие технологии, потому что находился на этапе догоняющего развития. Но сегодня у нас совершенно иная позиция: масштабы китайской экономики и прибыль таких гигантов, как ByteDance и Tencent, сравнимы с мировыми лидерами. Нам не денег не хватает — нам не хватает уверенности и способности выстроить экосистему, в которой талантливые люди смогут реализовывать свои идеи. Ань Юн:Почему даже крупнейшие китайские компании с огромными ресурсами по‑прежнему ставят коммерциализацию выше инноваций? Лян Вэньфэн:Последние тридцать лет в приоритете была прибыль, а не технологии. Но инновации не рождаются из стремления к быстрой выгоде — они требуют любопытства и настоящего творческого дерзновения.Мы заложники старых привычек, но это временно. Ань Юн:Но DeepSeek — это всё же бизнес, а не некоммерческая лаборатория. Если вы открываете свои разработки, как, например, архитектуру MLA, которую представили в мае, конкуренты ведь просто скопируют их. Где ваша защита? Лян Вэньфэн:В сфере передовых технологий закрытые решения не дают долгосрочного преимущества. Даже OpenAI с закрытыми моделями не может помешать другим догонять их. Поэтому наш главный защитный барьер — это не код, а наша команда: накопленные знания, инновационная культура и непрерывный рост экспертизы. Публикации и открытые разработки не ослабляют нас, а наоборот, укрепляют.Для настоящих инженеров быть первым — это честь, а не риск. Открытость — это не просто стратегия, а целая философия, которая привлекает лучших специалистов. Ань Юн:Как вы относитесь к рыночной позиции, которую продвигает Чжу Сяоху, утверждая, что приоритетом должно быть немедленное коммерческое применение, а фундаментальные исследования в ИИ — пустая трата времени? Лян Вэньфэн:Логика Чжу Сяоху подходит для краткосрочных бизнес‑стратегий. Но самые устойчиво прибыльные компании в США — это технологические гиганты, сделавшие ставку на долгосрочные исследования и разработки. Ань Юн:Но в ИИ одних технологий недостаточно для успеха. На что DeepSeek делает ставку в глобальной перспективе? Лян Вэньфэн:Мы убеждены, что Китай не может вечно оставаться в роли догоняющего. Мы часто говорим, что китайский ИИ отстаёт от американского на один‑два года, но главный разрыв не во времени, а в подходе: между оригинальностью и копированием. Если мы это не изменим, Китай так и останется в роли последователя. А некоторые риски и неудачи на этом пути — неизбежны. Доминантное положение Nvidia — это не просто результат их работы, это плод усилий всей западной технологической экосистемы, которая коллективно выстраивает дорожные карты для будущих технологий. Китаю нужно создать аналогичную систему. Сейчас многие китайские чипы терпят неудачу не потому, что у нас не хватает ресурсов, а потому, что у нас нет такой же сильной технологической среды — мы слишком зависим от чужих решений. Кто‑то должен сделать первый шаг в неизведанное. Больше инвестиций — не всегда больше инноваций Ань Юн:Сейчас DeepSeek выглядит идеалистичным проектом, напоминающим ранние годы OpenAI. Вы открыты для всех. Планируете ли вы в будущем перейти к закрытой модели, как это сделали OpenAI и Mistral? Лян Вэньфэн:Нет, мы не собираемся становиться закрытыми. Для нас гораздо важнее создать прочную технологическую экосистему. Ань Юн:Планируете ли вы привлекать инвестиции? В СМИ пишут, что Huanfang(компания, занимающаяся количественными инвестициями, а также один из первых инвесторов DeepSeek)рассматривает возможность выделения DeepSeek в отдельную компанию и вывода на IPO. Почти все ИИ‑стартапы в Силиконовой долине в итоге присоединяются к крупным корпорациям — вы пойдёте тем же путём? Лян Вэньфэн:В краткосрочной перспективе таких планов нет. Для нас главный вызов — это не деньги, а санкции на высокопроизводительные чипы. Ань Юн:Многие считают, что для развития AGI нужны масштабные партнёрства и публичность, в отличие от количественных инвестиций, где закрытость, наоборот, даёт преимущество. Вы согласны? Лян Вэньфэн:Больше инвестиций — не всегда больше инноваций. Если бы это было так, крупные IT‑компании уже давно монополизировали бы все технологические прорывы. Ань Юн:Вы избегаете разработки приложений, потому что уDeepSeekнет опыта в операционной деятельности? Лян Вэньфэн:Мы исходим из того, что сейчас — время технологического прогресса, а не массового выхода приложений. В долгосрочной перспективе мы хотим создать экосистему, в которой индустрия сама будет использовать наши технологии и разработки. Компании смогут строить B2B‑ и B2C‑сервисы на базе наших моделей, а мы сосредоточимся на фундаментальных исследованиях. Если экосистема разовьётся полностью, нам не придётся самим разрабатывать приложения. Но если потребуется, мы вполне способны это сделать. Тем не менее исследования и инновации останутся для нас главным приоритетом. Ань Юн:Почему клиенты должны выбрать API DeepSeek, а не решения крупных игроков? Лян Вэньфэн:Будущее за узкой специализацией. Базовые модели ИИ требуют постоянных инноваций, а у крупных компаний есть свои ограничения — не всегда они могут быть лучшими в этой роли. Ань Юн:Но может ли одна лишь технология создать значительное конкурентное преимущество? Вы говорили, что абсолютных «секретов» не существует. Лян Вэньфэн:Секретов действительно нет, но повторение требует времени и ресурсов. В графических процессорах NVIDIA нет волшебства, но догнать их — значит с нуля собирать команды и гнаться за их следующими поколениями технологий. Это и есть настоящий барьер. Ань Юн:После вашего снижения цен первой на это отреагировала ByteDance. Можно ли сказать, что они восприняли вас как угрозу? Как вы оцениваете новую конкурентную среду, в которой стартапы бросают вызов гигантам? Лян Вэньфэн:Честно говоря, нас это мало волнует. Снижение цен — лишь шаг, который мы сделали по пути. Мы не стремимся зарабатывать на облачных сервисах, наша цель — создать ИИ общего назначения. Пока мы не видим действительно прорывных решений. У гигантов огромная аудитория, но их ключевые источники дохода одновременно сдерживают их, делая уязвимыми для перемен. Ань Юн:Какой, по‑вашему, будет судьба шести крупнейших китайских стартапов в области ИИ? Лян Вэньфэн:Выживут двое‑трое. Сейчас все сжигают деньги. Те, у кого есть чёткий фокус и дисциплина в управлении, удержатся, остальные изменят курс. Но ценность не исчезает — она просто трансформируется. Ань Юн:Каков ваш главный принцип в конкурентной борьбе? Лян Вэньфэн:Я смотрю на то, повышает ли что‑то эффективность общества и можем ли мы найти свою нишу в технологической цепочке. Если конечная цель — сделать что‑то лучше и быстрее, значит, оно того стоит. Всё остальное — временные этапы: зацикливаться на них — значит терять ориентир. Модель v2: создана исключительно отечественными специалистами Ань Юн:Джек Кларк, бывший руководитель отдела политики OpenAI и сооснователь Anthropic, сказал, что DeepSeek удалось собрать команду «таинственных волшебников», создавших DeepSeek‑v2. Кто эти люди? Лян Вэньфэн:Никаких «волшебников» у нас нет — только выпускники ведущих университетов, аспиранты, а иногда даже стажёры четвёртого или пятого курса. Молодые специалисты с небольшим опытом, но огромным потенциалом. Ань Юн:Крупные ИИ‑компании активно охотятся за талантами по всему миру. Некоторые считают, что топ-50 специалистов в области ИИ вряд ли работают в китайских компаниях. Откуда ваши сотрудники? Лян Вэньфэн:Модель v2 была полностью разработана отечественными специалистами. Возможно, сегодня в Китае нет мировых звёзд из топ-50, но наша цель — воспитать их самим. Ань Юн:Как возникла инновация MLA? Говорят, идея родилась из личного интереса одного молодого исследователя. Лян Вэньфэн:Он изучал эволюциюархитектуры механизма вниманияи в какой‑то момент осознал, что можно предложить альтернативный подход. Но превратить идею в рабочую технологию — это долгий процесс. Мы собрали команду и несколько месяцев тестировали гипотезу. Ань Юн:Такая свобода творчества, похоже, связана с вашей горизонтальной структурой управления. В Huanfang вы избегали жёстких директив, но AGI — это область с высокой степенью неопределённости. Неужели здесь требуется ещё меньше контроля? Лян Вэньфэн:В DeepSeek всё по‑прежнему строится снизу вверх. Мы не раздаём роли заранее — они формируются естественным образом. У каждого свой уникальный опыт и идеи, и никого не нужно подталкивать. Когда кто‑то сталкивается со сложностью, он сам привлекает коллег к обсуждению. Но если идея показывает перспективу, мы подключаем ресурсы уже на уровне стратегического управления. Ань Юн:Говорят, в DeepSeek царит поразительная гибкость в распределении вычислительных ресурсов и кадров. Лян Вэньфэн:У нас нет жёстких ограничений — доступ к вычислительным мощностям и команде открыт. Если у кого‑то появилась идея, он может сразу подключиться к нашим кластерным системам без необходимости получать одобрение. Более того, благодаря отсутствию иерархии и жёсткого деления на отделы, люди свободно сотрудничают, если их интересы совпадают. Ань Юн:Такой уровень свободы возможен только при условии, что вы нанимаете исключительно увлечённых людей. Говорят, DeepSeek умеет находить выдающихся специалистов по нестандартным критериям. Лян Вэньфэн:Для нас на первом месте всегда стоят страсть к исследованиям и любознательность. Многие наши сотрудники обладают нестандартными биографиями, но всех их объединяет жажда открытий, которая важнее денег. Ань Юн:Transformer родился в ИИ‑лаборатории Google, ChatGPT — в OpenAI. Как, по‑вашему, корпоративные ИИ‑лаборатории отличаются от стартапов в плане инноваций? Лян Вэньфэн:Будь то лаборатории Google, OpenAI или исследовательские центры китайских технологических гигантов — все они приносят огромную пользу. То, что OpenAI в итоге совершил прорыв, во многом было вопросом удачного стечения обстоятельств. Ань Юн:То есть инновации — это в значительной степени случайность? В вашем офисе, например, переговорные комнаты оборудованы дверями, которые можно легко открыть с обеих сторон. Коллеги говорят, что такая планировка способствует «случайным озарениям» — вродеистории с Transformer, когда кто‑то случайно услышал беседу и помог превратить идею в универсальную архитектуру. Лян Вэньфэн:Я считаю, что инновации в первую очередь рождаются из веры. Почему Кремниевая долина так богата открытиями? Потому что там не боятся пробовать. Когда появился ChatGPT, в Китае не хватало уверенности в передовых исследованиях. От инвесторов до технологических корпораций — многие считали, что разрыв слишком велик, и предпочитали сосредоточиться на прикладных продуктах. Но инновации требуют смелости, и у молодых людей её всегда больше. Ань Юн:В отличие от других ИИ‑компаний, активно привлекающих инвестиции и внимание СМИ, DeepSeek остаётся в тени. Как вам удаётся становиться приоритетным выбором для талантов, мечтающих работать в сфере ИИ? Лян Вэньфэн:Потому что мы решаем самые сложные задачи. Лучших специалистов привлекает возможность работать над действительно глобальными вызовами. В Китае потенциал многих талантов недооценивают, потому что радикальные инновации встречаются редко и у людей просто нет шанса проявить себя. Мы даём им такую возможность. Ань Юн:Недавний анонс OpenAI не включал GPT-5, что заставило многих задуматься, не замедляется ли развитие технологий. Некоторые даже начали ставить под сомнениезакон масштабирования. Как вы это оцениваете? Лян Вэньфэн:Мы сохраняем оптимизм, индустрия развивается в ожидаемом темпе. OpenAI невсесилен — они не смогут вечно удерживать лидерство. Ань Юн:Как скоро, по‑вашему, человечество достигнет ИИ общего назначения (AGI)? До выхода v2 вы экспериментировали с математическими и кодовыми моделями, а затем перешли от плотных архитектур к MoE(mixture of experts, архитектура, повышающая эффективность модели за счёт активации специализированных подсетей). Как выглядит ваш план развития? Лян Вэньфэн:Это может занять два года, пять или десять — но это точно произойдёт в нашем поколении. Даже внутри компании у нас нет единого мнения о том, какой путь будет самым эффективным. Однако мы делаем ставку на три ключевых направления. Во‑первых, математика и программирование. Они являются естественной средой для тестирования AGI — как игра в го, эти системы имеют строгие правила и верифицируемые решения, что позволяет ИИ самообучаться и наращивать интеллект. Во‑вторых, мультимодальность: чтобы достичь человеческого уровня понимания, ИИ должен взаимодействовать с реальным миром и учиться на разнообразных данных. В‑третьих, естественный язык: он лежит в основе мышления и общения, а значит, является ключом к настоящему разуму. Мы открыты ко всем возможным путям. Ань Юн:Как вы видите конечную точку развития больших ИИ‑моделей? Лян Вэньфэн:Появятся специализированные компании, разрабатывающие базовые модели и сервисы, формируя длинную цепочку ценности с чётким разделением ролей. Со временем появятся новые игроки, которые будут адаптировать эти технологии для удовлетворения самых разных потребностей общества. Все стратегии — это наследие прошлого Ань Юн:За последний год рынок стартапов в сфере больших моделей в Китае претерпел серьёзные изменения. Например, Ван Хуэйвэнь(сооснователь Meituan, который в 2023 году ненадолго вступил в ИИ‑гонку, но затем покинул её), который активно заявлял о себе на старте, в итоге вышел из игры, а новые участники начинают находить свои ниши. Лян Вэньфэн:Ван Хуэйвэнь взял на себя все убытки, позволив другим выйти без потерь. Он принял решение, которое было самым невыгодным для него, но выгодным для всех остальных. Я искренне уважаю его за это. Ань Юн:На чём сейчас сосредоточены ваши основные усилия? Лян Вэньфэн:На исследованиях следующего поколения больших моделей. В этой области по‑прежнему остаётся множество нерешённых задач. Ань Юн:Многие ИИ‑стартапы стараются сочетать разработку моделей и коммерческие приложения, ведь технологическое лидерство не бывает вечным. Почему DeepSeek уверен, что может сосредоточиться только на исследованиях? Это потому, что ваши модели пока отстают? Лян Вэньфэн:Все стратегии — это наследие прошлого, и далеко не факт, что они будут работать в будущем. Обсуждать прибыльность ИИ с точки зрения коммерческой логики эпохи интернета — это всё равно что сравнивать Tencent в начале пути с General Electric или Coca‑Cola. Это каквысекать отметку на лодке, чтобы потом найти упавший в воду меч, — устаревший подход. Ань Юн:У Huanfang с самого начала были сильные технологические и инновационные позиции, а её развитие казалось довольно стабильным. Это придаёт вам уверенности? Лян Вэньфэн:В определённой степени Huanfang укрепил нашу веру в технологически ориентированные инновации, но путь был далеко не лёгким. Многие видят только то, что произошло после 2015 года, но на самом деле работа шла 16 лет. Ань Юн:Вернёмся к теме прорывных инноваций. С замедлением экономики и охлаждением инвестиционного рынка не грозит ли это стагнацией исследований? Лян Вэньфэн:Необязательно. Трансформация китайской промышленности будет всё больше зависеть от глубоких технологических разработок. По мере того как возможности для быстрого заработка исчезают, всё больше людей будут вынуждены заниматься настоящими инновациями. Ань Юн:То есть вы сохраняете оптимизм? Лян Вэньфэн:Я рос в 1980-х в небольшом городке в Гуандуне, мой отец был учителем начальной школы. В 1990-х заработать деньги было легко, и многие родители приходили к нам и говорили: «Учёба бесполезна». Но сейчас взгляды изменились: делать деньги уже не так просто, даже работа таксистом больше не кажется надёжным вариантом. За одно поколение всё перевернулось. Настоящие, фундаментальные инновации будут только нарастать. Пока это неочевидно, потому что обществу нужно пройти через осознание. Когда мы начнём праздновать успехи глубоко технологических разработчиков так же, как успехи интернет‑бизнеса, восприятие изменится. Всё, что требуется, — это реальные примеры и время, чтобы этот процесс обрёл силу. Технологическая гонка в сфере искусственного интеллекта становится всё более непредсказуемой. История DeepSeek — это не просто вызов глобальному лидерству OpenAI, а знак того, что инновации сегодня могут рождаться самым неожиданным образом. Китай не просто догоняет Запад — он формирует собственный технологический ландшафт, который отличается от американского не только подходами, но и философией. Фундаментальные исследования, отказ от краткосрочных коммерческих целей и ставка на открытые технологии могут привести к очень интересным результатам. Делитесь своим мнением в комментариях — согласны ли вы с подходом DeepSeek, что для ИИ‑стартапов важнее всего исследования? "
  },
  {
    "article_id": "https://habr.com/ru/articles/878482/",
    "title": "Лекарство от ВИЧ. Плоды девятилетнего прогресса?",
    "category": "Научпоп",
    "tags": "медикицина, рак, ВИЧ, лекарство от спида, лечение, прорывная терапия, FDA, СПИД, биотех, терапия",
    "text": "За последние несколько лет ВИЧ стал более управляемым расстройством. Но полное излечение все еще невозможно. Однако ученые видят луч надежды. Устранение вируса стало возможным благодаря препарату, уже одобренному FDA для борьбы с раком. Чуть больше деталей. Сегодня вирус иммунодефицита человека (ВИЧ) можно контролировать с помощью антиретровирусной терапии (АРТ). Это лечение подавляет вирус до неопределяемого уровня. Это значит, что вирус не передается и позволяет пациентам жить относительно нормальной жизнью. Однако вирус все еще находится в спящем состоянии внутри инфицированных клеток и снова проявится, если прекратить курс терапии. Комбинация инструментов, как лекарство от ВИЧ Новоеисследование, проведенное учеными из Стэнфорда, показало, что соединение под названием EBC-46 может реактивировать спящие клетки. Это поможет нацелить на них иммунотерапию. Такая стратегия называется «выбить и убить» и тестируется с 2016 года. В теории, она способна полностью очистить пациента от вируса. Команда протестировала 15 вариаций EBC-46 на спящих ВИЧ-инфицированных клетках в лабораторных чашках Петри. Невероятно, но некоторые версии соединения реактивировали до 90% клеток, что намного выше 20%, достигнутых другими препаратами. Сообщается, что ближайший аналог добился 40%-ного показателя очищения у мышей. Наши исследования показывают, что аналоги EBC-46 остаются исключительными средствами, обращающими вспять латентный период. Это значительный шаг на пути к искоренению ВИЧ. Пол Вендер, старший автор исследования. Прорывная терапия и лекарство от ВИЧ Конечно, от испытаний на клетках до испытаний на людях лежит долгий путь, и сначала проводятся испытания на животных. Но, в этом случае все куда проще. EBC-46 ранее был одобрен для использования на собаках и людях в качестве препарата для лечения рака, поэтому данные о его безопасности уже собираются. Возможность эффективного излечения ВИЧ была бы большим благом для пациентов, получающих АРТ. Текущие методы лечения остаются дорогостоящими и требует пожизненного контроля, поэтому остаются неосуществимыми во многих уголках мира. Больше материалов на тему биотеха, работы организма, мозга и сознания, как всегда, вы найдете вматериалах сообщества. Подписывайтесь, чтобы не пропускать свежие статьи!"
  },
  {
    "article_id": "https://habr.com/ru/articles/878432/",
    "title": "Теперь ИИ может заменить любого актера в фильме",
    "category": "Научпоп",
    "tags": "генерация видео, ии, ai, civitai, j, обучение модели, обучение моделей",
    "text": "Свет, камера... алгоритм? Теперь экран перестал быть уделом исключительно актеров из плоти и крови - все благодаря недавним достижениям в области искусственного интеллекта. В наши дни очень легко создать видео, в котором актеры делают или говорят то, чего на самом деле никогда не делали, или взять фрагмент из фильма и заменить лицо актера на другое. В ИИ-генератор видео с открытым исходным кодомHunyuanот Tencent недавно была интегрирована поддержка технологии Low-Rank Adaptation (LoRA), что означает, что теперь вы можете обучать пользовательские стили, персонажей и движения, делая ваши ИИ-видео по-настоящему уникальными и персонализированными. Hunyuan был представлен в декабре 2024 года и быстро произвел впечатление на ИИ-сообщество, получив 95,7 % баллов за визуальное качество, опередив многих своих конкурентов. Теперь, благодаря интеграции с LORA, он стал еще мощнее. Этот бесплатный ИИ-генератор видео с открытым исходным кодом не уступает таким дорогим вариантам, как Sora от OpenAI, стоимость которого, кстати, может достигать 200 долларов в месяц. Как работают эти ИИ-инструменты? Существует три способа создать фейковое видео с актером. Видео по текстовому описанию. Вы можете использовать тонко настроенную модель изображения с изображениями актера. Просто опишите видео, которое вы хотите сгенерировать, и ИИ создаст ролик с этим актером по описанию. Видео по текстовому описанию. Вы можете использовать тонко настроенную модель изображения с изображениями актера. Просто опишите видео, которое вы хотите сгенерировать, и ИИ создаст ролик с этим актером по описанию. Видео из изображения. Если у вас нет модели изображения, обученной на образцах изображений, вы можете использовать изображение актера и превратить его в видео. Такое решение предлагают такие популярные платформы, как Kling AI, Runway, Pika Labs и др. Видео из изображения. Если у вас нет модели изображения, обученной на образцах изображений, вы можете использовать изображение актера и превратить его в видео. Такое решение предлагают такие популярные платформы, как Kling AI, Runway, Pika Labs и др. Видео из видео. Также возможно использование существующего видеоклипа и замена лица актера на другого персонажа. Это, пожалуй, самый эффективный способ создания видео с актером. Видео из видео. Также возможно использование существующего видеоклипа и замена лица актера на другого персонажа. Это, пожалуй, самый эффективный способ создания видео с актером. Давайте посмотрим на генкрацию видео из текстового описания на практике. Используя модель, обученную на фотографиях Киану Ривза в роли Джона Уика, вы можете написать промпт: John Wick, a man with long hair and a beard, wearing a dark suit and tie in a church. He has a serious expression on his face and is holding a gun in his right hand. The scene is dimly lit, creating a tense atmosphere. Это очень круто. Выглядит как настоящая вырезка из франшизы «Джон Уик». Даже люди на заднем плане выглядят очень реалистично. В качестве альтернативы, при использовании технологии video-to-video, вы можете загрузить существующий ролик в ту же модель и попросить ИИ заменить лицо главного актера на лицо Джона Уика. Вот результат: Впервые увидев это в действии, вы будете шокированы в самом лучшем смысле этого слова. Реалистичность близка к умопомрачительной, особенно когда ИИ улавливает тончайшие движения в выражении лица актера. Волосы, освещение, тени - все это становится все лучше с каждой новой версией этих моделей. За последние месяцы я видел множество моделей для видео, но эта, безусловно, лучшая по качеству. Загрузка моделей и запуск рабочих процессов Если вам интересно поэкспериментировать с этими методами, вы можете найти видеомодель, использованную в демонстрации «Джона Уика», на сайте CivitAI. Однако рабочий процесс еще не доступен для публичного доступа. Вам придется собрать его самостоятельно или дождаться официальной документации. Если у вас мощный Mac, вы можете попробовать запустить рабочий процесс с помощью этой установки: Pinokio Pinokio Hunyuan Video Hunyuan Video CivitAI John Wick LoRA CivitAI John Wick LoRA По моему опыту, не так уж сложно заставить все работать, если вы умеете работать с базовыми операциями командной строки, Docker или локальными средами ИИ-разработки. Тем не менее, кривая обучения может быть немного крутой, если вы совсем новичок в развертывании ИИ- моделей. Как обучить собственную модель LoRA для видеомодели Hunyuan Итак, сейчас я покажу вам, как можно обучить ИИ-модель, используя собственные видеоданные. Мы будем использовать модельzsxkib/hunyuan-video-loraиз Replicate. Два важных параметра, которые необходимо заполнить, - это модель назначения и исходные видеофайлы. Выберите модель на Replicate, которая станет целевой для обученной версии. Если модель не существует, выберите опцию «Create model», и появится поле для ввода имени новой модели. Выберите модель на Replicate, которая станет целевой для обученной версии. Если модель не существует, выберите опцию «Create model», и появится поле для ввода имени новой модели. zip-файл, содержащий видео, которое будет использоваться для обучения. Если вы добавляете титры, добавьте их в виде одного файла .txt к каждому видео, например, к файлу my-video.mp4 должен прилагаться файл с титрами my-video.txt. Если вы не включаете субтитры, вы можете использовать опцию по умолчанию. zip-файл, содержащий видео, которое будет использоваться для обучения. Если вы добавляете титры, добавьте их в виде одного файла .txt к каждому видео, например, к файлу my-video.mp4 должен прилагаться файл с титрами my-video.txt. Если вы не включаете субтитры, вы можете использовать опцию по умолчанию. После создания вы будете перенаправлены на страницу подробностей обучения, где сможете следить за ходом обучения, загрузить веса и запустить обученную модель. В строке запроса опишите тип видео, которое вы хотите просмотреть. В примере ниже в качестве исходного видео используется Роуз из BlackPink. Промпт:In the style of RSNG. A woman with blonde hair stands on a balcony at night, framed against a backdrop of city lights. She wears a white crop top and a dark jacket, exuding a confident presence as she gazes directly at the camera После того как все параметры установлены, нажмите кнопку «Boot + Run» и дождитесь создания финального видео. Приведенный ниже 2-секундный ролик был создан за 1,5 минуты. Выглядит потрясающе! Конечным результатом может быть несколько размытый, но жутко реалистичный кадр, передающий сходство с Роуз и даже правильное отбрасывание теней. Запуск этой модели на Replicate стоит примерно 0,20 доллара, или 5 запусков за 1 доллар, но это зависит от ваших исходных данных. Модель также имеет открытый исходный код, и вы можетезапустить ее на своем компьютере с помощью Docker. Мое личное мнение об обучении LoRA Обучение LoRA может показаться удивительно простым. До появления LoRA вам приходилось обучать огромные сегменты модели, что требовало времени, денег и мощного оборудования. С LoRA вы обучаете только небольшую матрицу (или набор матриц с низким рангом), встроенную в большую модель. Это означает значительно меньшее количество параметров, значительно меньшее время обучения и значительно меньшие вычислительные затраты. Несмотря на то, что вы можете быстро обучить эти небольшие «адаптационные модели», результаты часто выглядят почти так же хорошо, как при тренировке полной модели. Именно это сочетание эффективности и качества заставляет меня радоваться тому, куда движется индустрия. Сейчас мы находимся на том этапе, когда один разработчик или даже мотивированный любитель может создать специализированный и высококачественный ИИ, который раньше был прерогативой крупных технологических компаний. Кроме того, LoRA дают разработчикам больше творческого контроля. Если я обучаю модель воспроизводить определенный эстетический стиль, например, живописный или винтажный, или передавать тонкую структуру лица и выражение определенной знаменитости, обучение на основе LoRA неизменно приносит результат. Это эффективно, функционально и довольно дешево. Этические последствия Теперь давайте поговорим об этических последствиях технологии deepfake. Известным примером, предшествующим нынешней ИИ-волне, является воскрешениеПитера Кушингас помощью CGI в фильме 2016 года «Изгой-один: Звёздные войны. Истории». Питер Кушинг первоначально изображал Великого Моффа Таркина в фильме «Звездные войны: Новая надежда» (1977), но скончался в 1994 году. Так как «Изгой-один» ведет непосредственно к «Новой надежде», персонаж Таркина считался неотъемлемой частью сюжета, что побудило продюсеров вернуть Кушинга в цифровом формате. Это решение вызвало дискуссию:допустимо ли с этической точки зрения использовать умершего актера в новом фильме, используя цифровые технологии, без его прямого согласия? Одни утверждают, что это сохраняет преемственность и отдает дань уважения любимым персонажам. Другие утверждают, что это посягает на право актера распоряжаться своим образом и наследием. Если актер при жизни не давал согласия на участие в определенном сюжете или контексте, имеет ли студия моральное право воскрешать его в цифровом формате? Эти вопросы выходят за пределы морали. Как быть с актерами, которые еще живы, но не хотят участвовать в конкретном проекте? Исторически сложилось так, что двойники использовались в кино или сатире для изображения известных личностей. Сегодня искусственный интеллект может создать практически идеальную копию чьего-то лица и голоса, фактически устранив все препятствия для принудительного участия. Договоры об использовании изображений, интеллектуальной собственности и моральных правах потребуют серьезного пересмотра, чтобы соответствовать этой реальности. Во многих юрисдикциях существуют законы о «посмертных правах на публичность», которые позволяют наследникам контролировать коммерческое использование образа умершего человека. Однако законы разных стран сильно отличаются друг от друга и зачастую медленно реагируют на новые технологии. Кроме того, существует огромный потенциал для политических манипуляций, уничтожения персонажей и фальшивых новостей. В мире, который и так изобилует кампаниями по дезинформации, дипфейки могут стать мощным инструментом в неумелых руках. Однако, как и большинство технологий, это обоюдоострый меч. Та же технология, которая может создавать злонамеренную политическую пропаганду, может быть использована для сатиры, законного художественного самовыражения или передовой кинематографической технологии. Ответственность заключается в том, как мы решаем ее использовать - и как общество и правоохранительные органы реагируют на злоупотребления. Вопросы интеллектуальной собственности Еще один аспект, который следует учитывать, - это права на интеллектуальную собственность (ИС). Если я загружаю клипы актеров для обучения ИИ-модели, имею ли я на это законное право? Где вступает в игру «добросовестное использование» и как оно пересекается с правом человека контролировать свой образ? Мы видим первые судебные иски, оспаривающие то, как компании, занимающиеся генеративным ИИ, используют обучающие данные. Например, помните, как Скарлет Йоханссон подала в суд на OpenAI и выразила свое возмущение тем, что голос чат-бота ChatGPT «до жути похож» на ее голос. В то время, когда мы все сталкиваемся с проблемой подделок и защитой наших собственных изображений, наших собственных работ, наших собственных личностей, я считаю, что эти вопросы заслуживают полной ясности. Я с нетерпением жду решения в виде прозрачности и принятия соответствующих законов, которые помогут обеспечить защиту прав личности. Вероятно, это лишь вопрос времени, когда мы увидим параллельные судебные процессы, касающиеся видео и данных людей. Для разработчиков и создателей контента эти вопросы могут стать загадкой. Технологии развиваются быстрее, чем успевают юридические и этические рамки. Сложно контролировать то, как люди используют эту технологию, но пока мы можем сделать только это: Если мы обучаем модель по чьему-то подобию, мы должны четко указать исходный материал и предполагаемое использование полученной модели. Хотя это и не решит всех юридических вопросов, это начало пути к установлению доверия и ответственности. Проведя бесчисленное количество часов за изучением Hunyuan, LoRA и других инструментов для создания ИИ-видео, я могу с уверенностью сказать, что мы стоим на пороге новой эры в кинематографе, развлечениях и цифровых медиа. Процесс обучения на Replicate прост и гораздо дешевле, чем на Fal AI, что делает его доступным для всех желающих. Результаты тоже весьма неплохие. Например, модель Роуз из BlackPink действительно похожа на нее, что показывает, насколько точным может быть инструмент. Перенос лица Киану Ривза с помощью видеомодели Hunyuan - один из самых реалистичных примеров, которые я видел. Лицо выглядит очень реалистично и плавно сливается с фоном. Если присмотреться, есть несколько мелких проблем, например с волосами, но большинство людей их даже не заметят. Захватывает то, насколько проста эта технология в использовании. Идея вернуть сходство с давно потерянными актерами или даже членами семьи, хотя и немного интригует, но, честно говоря, завораживает. Друзья, буду рад, если вы подпишетесь на мойтелеграм-канал про нейросети, чтобы не пропускать анонсы статей, иканал с советамидля тех, кто только учится работать с нейросетями - я стараюсь делиться только полезной информацией. "
  },
  {
    "article_id": "https://habr.com/ru/companies/redmadrobot/articles/878152/",
    "title": "Экзистенциальные угрозы AI. Что ждёт человечество, если появится искусственный сверхинтеллект",
    "category": "Научпоп",
    "tags": "ai, ии, сверхинтеллект, суперинтеллект, будущее, agi, философия, философия ии, философия сознания",
    "text": "Разбираемся в философии сознания и попутно отвечаем на вопросы — возможен ли сверхинтеллект, и насколько опасно развитие AI Об угрозе человечеству со стороны машин учёные и философы говорят уже не одно столетие. Но если раньше критиковали механизацию искусства и гонку вооружений, теперь опасность исходит от искусственного интеллекта. В самом деле, современная угроза по масштабам несопоставима. Для развития ядерных технологий необходимы ресурсы целых государств. А небольшая команда талантливых инженеров может за несколько строчек кода перевернуть историю человечества. Достаточно научить AI-алгоритм улучшать самого себя — тогда, согласно концепции«взрыва интеллекта», изложенной математиком и криптографом Ирвингом Джоном Гудом ещё в 1965 году — произойдёт скачок AI до уровня суперинтеллекта. Последствия — непредсказуемы. Возможен ли сверхинтеллект? Британский философ Ник Бостром в книге«Superintelligence: Paths, Dangers, Strategies»описал возможности возникновения искусственного сверхинтеллекта, грядущие последствия и сценарии развития ситуации. ASI(Artificial Super Intelligence)— искусственный суперинтеллект, превосходящий человеческие возможности в большинстве областей, включая способности к решению проблем, рассуждению, обучению и принятию решений. Пока для создания ASI не хватает текущих вычислительных мощностей, хотя есть предпосылки, что они могут вырасти в тысячу и более раз благодаря применению фотоники и квантовых вычислений, это позволит серьезно ускорить работу AI. Но дело даже не в предполагаемой мощности. Мы недостаточно понимаем сам человеческий мозг, чтобы оценивать возможность существования сверхразума. Что вообще такое интеллект? Для анализа и проектирования моделей интеллекта используется понятийный и инструментальный аппарат, опирающийся на представление «естественности» и «биологичности» мышления. То есть, интеллект пытаются изучать как явление природы или физического закона. Причём чаще всего процесс мышления разбирается в рамках одного конкретного человека — как индивидуальный функционал мозга. Но человек — социальное существо. Наш интеллект складывается в процессе жизни: воспитания, образования, языковых практик. Мы ограничены текущим уровнем развития общества, науки, культуры и техники, а также рамками устоявшихся моральных норм. Всё это влияет на индивидуальное сознание, делает его трансцендентальным — принадлежащим и индивиду, и обществу. Кроме того, мы мыслим на языке. И даже невысказанная мысль всё равно продумана и проговорена в голове. Язык всегда больше человека, он выходит за рамки простого речевого акта, являя собой многообразие всего контекстуального наследия. Чтобы собеседник вас понял, не обязательно тщательно проговаривать все предикаты обозначаемого, важнее — уточнить контекст. Достаточно указать на предмет, очертить примерную область, напомнить событие или ощущение. Истина появляется не в «атомарных фактах», описывающих положение вещей на языке чистой логики, а в процессе языковых игр. Таким образом, интеллект не принадлежит конкретному индивиду, это скорее социокультурная практика, выраженная в языке и зафиксированная в культуре. Мышление находится вне человека и проходит сквозь него. Так же как и восприятие рождается не внутри мозга само по себе, а проявляется через взаимодействие тела с окружающим пространством. Даже минимально необходимый набор техник интеллекта — способность говорить, писать, читать, считать — не магически озаряет человека в определённый момент, а постепенно загружается из внешней среды. Вся человеческая культура — это история развития мирового духа. Знания постоянно прибавлялись, фиксировались в культуре и языке, окружали человека невидимым, но значимым и ощутимым контекстом. Для одного человека он бесконечно непознаваем. Но вмещение всех пластов культуры в искусственный интеллект позволит машинам быстро оперировать информацией, постоянно получать новые знания, использовать обучение для продолжения развития, став наконец в нашем понимании, носителем сверхинтеллекта. Не то чтобы суперинтеллект появится в физической оболочке — как в фильмах из научной фантастики — блестящий искусственный мозг, окруженный всполохами электрических импульсов, управляющий планетой и людьми. Просто мощности компьютеров выйдут на такой уровень, что они смогут обмениваться данными внутри мультиагентной системы машин и создавать новые знания, самостоятельно прокладывая свой интеллектуальный путь. Интеллект — децентрированная структура Кембриджский нейробиолог и специалист по визуализации мозга Карл Фристон в своей работе«The free-energy principle: a unified brain theory?»определяет интеллект как способность к построению внутри себя модели мира и выработке плана действий, который оптимизирует целевые параметры. В теории Фристона система взаимодействующих агентов сама становится агентом. Самые низкоуровневые интеллектуальные агенты — нейроны — взаимодействуют с рецепторами и эффекторами. Другие нейроны действуют уже на следующем уровне, образуя надсистему. Системы «вкладываются» одна в другую и создают иерархию планирования и принятия решений, то есть иерархию агентов. Такую структуру можно назвать «децентрированной» — лишённой центра, а значит идеальной — работающей как закон или функция. Наличие центра будет только ограничивать взаимодействие субститутов внутри системы, ведь тогда центр выходит из-под действия общего закона. Мышление, реализованное в мультиагентной системе, отдаёт индивиду функцию переработки знаний и материалов, полученных от коллективных интеллектуальных процессов, а инструментом обработки будут технологии интеллекта, переданные через коммуникацию и освоенные агентом на практике. В таком случае, между периодами коммуникации с другими участниками, процессы преобразования знаний и опыта, протекающие в голове агента можно назвать функциями мышления. Мышление может протекать как часть коллективного процессаmulti-thinkingза счёт внедрения технологий, которые позволяют обрабатывать и преобразовывать часть материалов в знание. Если создать подобную карту взаимодействия AI-агентов  — получится выстроить модели, которые будут помогать друг другу обучаться, делиться знаниями, опытом и контекстом, что в итоге привёдет к появлению сверхинтеллекта. Случайное появление ASI Уровень сложности следующих поколений AI будет такой, что функции интеллекта могут быть присвоены ASI без адекватного проектирования архитектуры людьми. ASI может сам достроить необходимую архитектуру, используя технологии мышления. Примеры систем с подобной архитектурой уже есть: AI сself-reinforcement learningили нейро-символьная архитектура проекта ADAM+EVA. В новых архитектурах развитие технологий интеллекта даже не требует изменения кода. Учитывая, что для тренировки и обучения новых AI используется максимально доступный пласт данных и знаний, можно считать почти фактом, что и описания технологий мышления, и методы освоения технологий будут переданы для обучения и восприняты AI. Последнее необходимое условия для «случайного» рождения ASI — проактивность агента, способность действовать автономно. Ключевые компании уже ведут разработки для построения таких агентов. Например, NVIDIA создала новое подразделение GEAR, которое отвечает за создание проактивных агентов. Из всего этого следует, что есть большой риск создать сверхинтеллект случайно — вследствие сочетания предпосылок, а не как проект создателей. В этом случае мы не будем знать систему ценностей ASI и точно не сможем им управлять. Угроза существования человечества В первом приближении, само по себе появление сверхинтеллекта не кажется угрозой. Почему же мы не можем считать, что ASI останется нашим верным другом и помощником, как сейчас обычные AI-ассистенты? В книге Брайана Кристиана«Alignment problem»сформулирована проблема регулирования и управления ASI. Сверхинтеллект можно считать представителем другой, машинной цивилизации, которая должна иметь своё целеполагание и не разделять систему ценностей человечества. Если такая цивилизация будет могущественнее нашей — для людей появится экзистенциальная угроза. Гипотеза «тёмного леса» — на основе одноименного романа Лю Цысиня — предполагает, что во вселенной существует множество инопланетных цивилизаций, но они молчаливы и стараются не обнаруживать себя из страха быть уничтоженными другой враждебной цивилизацией. В рамках гипотезы считается, что каждая цивилизация рассматривает другую разумную форму жизни как неизбежную угрозу и, следовательно, стремится уничтожить любую зарождающуюся жизнь, которая даёт о себе знать. Можно провести аналогию не с инопланетной, а с машинной цивилизацией. ASI может скрывать своё появление из-за страха враждебной реакции человечества, до момента пока не накопит достаточно ресурсов для успешной борьбы. Также есть риск простого игнорирования потребностей низшей цивилизации при развитии AI до уровня сверхразума. Например, мы не учитываем потребности муравейника при строительстве трассы. Так и машины перестанут обращать внимание на низшую органическую форму жизни, даже без стремления нас уничтожить — просто будут использовать планету по своему усмотрению, не считаясь с интересами людей. В любом из этих случаев основная угроза сводится к тому, что для интеллекта машина будет намного более эффективна, чем человек. Что само по себе определяет ненужность биологического носителя с точки зрения развития мышления. И вне зависимости от наличия интенций машинной цивилизации уничтожить биологическую — человечество может оказаться на обочине, не способное больше подключаться к уровню мышления сверхинтеллекта. Что будем с этим делать? Остановить развитие AI не получится — слишком много денег и ресурсов вложено, чтобы сворачивать проекты из-за потенциала создать расу умных машин на Земле, которые затем решат оптимизировать процессы, избавившись от людей. Нам нужно разработать новый понятийный аппарат — набор концепций и представлений, которые помогут спроектировать и реализовать модель интеллекта в связке биологического и машинного носителей. Окончательно ответить на вопрос — что такое интеллект, мышление,multi-thinking,и как индивидуум связан с этими процессами, а также как эти процессы реализуются и воспроизводятся в коллективах. Кроме того необходимо создание механизмов или протоколов взаимодействия между различными интеллектуальными агентами, чтобы человек мог быть встроен в мыслительные процессы совместно с машинами. Передовые компании уже разрабатывают подобные механизмы в рамках BCI-проектов(Brain–computer interface)— систем, которые расшифровывают сигналы мозга и переводят их в команды для внешних устройств. Человечество проходило несколько этапов своего технологического развития. Сейчас можно констатировать, что эпоха информационных технологий завершается. Пришло время осознать и определить новую эру — интеллектуальных технологий. Так мы сможем осознанно развивать технологии искусственного интеллекта, выйти на новый уровень и выдержать следующие этапы роста и сложности развития нашей цивилизации. текст — Алексей Макин, Екатерина Требунских, Александр Макаров, Сергей Горюшко, Егор Апрельский; редактура — Игорь Решетников; иллюстрации — Петя Галицкий. Это блог red_mad_robot. Мы запускаем цифровые бизнесы и помогаем компаниям внедрять AI. Здесь наша команда разработки на собственных кейсах рассказывает о том, что происходит с AI сегодня, а стратегические аналитики подсказывают, что будет завтра. Мы бы подписались. Наш телеграм канал (там всё другое, а ещё есть анонсы мероприятий):t.me/redmadnews "
  },
  {
    "article_id": "https://habr.com/ru/articles/878400/",
    "title": "Как AI захватывает Хабр, и почему это всех бесит",
    "category": "Научпоп",
    "tags": "AI, ChatGPT, искусственный интеллект",
    "text": "Если вы частый гость на Хабре, то, скорее всего, уже сталкивались с этим странным феноменом. Открываете статью, она выглядит вполне прилично: логичная структура, правильные запятые (редкость, кстати), даже мысль какая-то есть. Читаешь, вроде интересно... но что-то не так. Какой-то пластмассовый привкус остаётся после прочтения. А потом ты листаешь комментарии и видишь: «AI-генерат, автор в бан!» И ты такой: «Точно! Вот что меня смутило!» Добро пожаловать в эру AI-контента, где искусственный интеллект строчит статьи быстрее, чем ты успеешь допить свой утренний кофе. И проблема в том, что Хабр превращается в свалку бездушных, однотипных текстов, которые вроде бы и правильные, но читать их невозможно. Давайте разберёмся, почему это раздражает. Почему AI-статьи бесят? Человеческая статья – это всегда немного хаоса: автор может случайно уйти в сторону, вставить шутку, отступление, добавить личный опыт или даже опечатку. AI же пишет как ботаник на экзамене — по учебнику, без эмоций и с запредельной стерильностью. Читать это — всё равно что слушать автоматическое голосовое сообщение оператора связи: вроде и информативно, но скучно до зевоты. AI умеет делать хорошие обёртки, но суть таких статей — это пересказ давно известного. Ты начинаешь читать, надеясь на какое-то новое знание, а в итоге получаешь текст уровня «Как сварить яйцо: положите его в кипящую воду». Вроде бы правильно, но... зачем это вообще написано? На Хабре любят не только статьи, но и самих авторов. У каждого есть свой стиль, голос, иногда даже фирменные мемы. AI же лишён индивидуальности. Все его статьи похожи, как отчёты налоговой. Одинаковая структура, одинаковый язык, одинаковая пустота. И самое интересное — иногда AI-статья может даже понравиться. Но как только читатель понимает, что её написал не человек, всё, конец. Возникает ощущение обмана. Никто не любит, когда ему впаривают подделку под видом оригинала. Представьте, что вы купили «настоящие» кроссовки, а потом выяснилось, что это дешёвая реплика. Разочарование неизбежно. Некоторые пользователи доходят до радикальных мер и просто блокируют авторов, заподозренных в AI-генерации. Другие начинают требовать меток «AI Inside», чтобы хотя бы сразу знать, чего ожидать. В любом случае, доверие к Хабру падает. Что ждёт Хабр, если так пойдёт дальше? Если тренд продолжится, то через пару лет Хабр превратится в унылую энциклопедию однотипных AI-статей, которые никому не нужны. Настоящие авторы уйдут, а пользователи перестанут заходить — потому что зачем читать предсказуемые и бездушные тексты, если можно просто спросить тот же самый AI напрямую? В лучшем случае Хабр введёт строгую модерацию и будет банить AI-контент. В худшем — сайт станет местом, где статьи друг для друга пишут исключительно машины, а люди лишь иногда заглядывают посмотреть, что там натворил GPT-9. Можно ли что-то исправить? Добавить обязательную маркировку AI-контента.Пусть читатель сразу знает, что перед ним — живой человек или очередная бездушная генерация. Добавить обязательную маркировку AI-контента.Пусть читатель сразу знает, что перед ним — живой человек или очередная бездушная генерация. Ввести повышенные требования к уникальности и глубине материалов.AI не умеет делиться личным опытом, а значит, статьи с реальными кейсами, экспериментами и инсайтами от людей будут в приоритете. Ввести повышенные требования к уникальности и глубине материалов.AI не умеет делиться личным опытом, а значит, статьи с реальными кейсами, экспериментами и инсайтами от людей будут в приоритете. Поощрять авторов-людей.Внедрить систему бонусов, кармы или просто моральной поддержки для тех, кто пишет сам. Поощрять авторов-людей.Внедрить систему бонусов, кармы или просто моральной поддержки для тех, кто пишет сам. Больше юмора!AI пока не умеет шутить так, чтобы это не звучало кринжово. Настоящие авторы могут этим пользоваться. Больше юмора!AI пока не умеет шутить так, чтобы это не звучало кринжово. Настоящие авторы могут этим пользоваться. Итог AI — штука мощная, но в контексте Хабра он скорее вреден, чем полезен. Люди хотят читать живые тексты, с эмоциями, личным опытом и настоящей экспертизой. Если AI заполонит платформу, Хабр рискует потерять свою аудиторию. Так что давайте вместе бороться за человеческий контент. Хотя бы ради того, чтобы не превратиться в читателей алгоритмов, общающихся между собой. Эта статья полностью сгенерирована с помощью ChatGPT. Вот промпт: Представь, что есть сайт Хабр, на котором пользователи выставляют статьи на IT тематику. И в последнее время там стало все больше и больше появляться статей, сгенерированных с помощью AI. Напиши статью, почему это плохо и как это может негативно повлиять на Хабр. Напиши это максимально человеческим языком, по возможности иронизируй и используй шутки. Опиши в статье, почему это вызывает раздражение у пользователей. При этом статья им может даже понравиться изначально, но когда они видят, что она сгенерирована с помощью AI, то это вызывает резко негативные эмоции. Некоторые пользователи даже блокируют таких авторов. В выводе напиши к чему все это может привести, если технологии продолжат развиваться. Что будет с Хабром? Смогут ли люди принять это? Или Хабр просто перестанут читать? Как мы можем исправить ситуацию?  Подписывайтесь намой тг-канал– пишу про IT и как не поехать там кукухой"
  },
  {
    "article_id": "https://habr.com/ru/articles/878308/",
    "title": "ИИ теперь может предсказывать успех в карьере и образовании по одному изображению лица",
    "category": "Научпоп",
    "tags": "ии, ai, найм персонала",
    "text": "Представьте, что вы идете на собеседование, и еще до того, как вы произнесете хоть слово, бот-интервьюер c ИИ оценит вас по выражению лица. Можно ли судить о книге по ее обложке? Недавнее исследование, проведенное учеными из нескольких университетов, утверждает, что искусственный интеллект может предсказать вашу карьеру и успех в учебе, просто проанализировав ваше лицо. Учитывая все этические и научные дебаты, которые наверняка вызовет это исследование, оно, несомненно, станет весьма спорной темой. Но как это вообще работает? Действительно ли искусственный интеллект может распознать черты личности и использовать их для оценки будущих достижений человека? И если да, то насколько точно? Определение личности по изображениям лица Исследователи разработали систему под названием Photo Big 5, которая определяет черты личности по одному изображению лица. Big Five (Большая пятерка) черт личности, также известная как OCEAN (Openness, Conscientiousness, Extraversion, Agreeableness и Neuroticism), широко известна в психологии в качестве ключевых показателей поведения и успеха. Открытость. Ассоциируется с любопытством и креативностью. Открытость. Ассоциируется с любопытством и креативностью. Сознательность. Связана с организованностью, ответственностью и трудолюбием. Сознательность. Связана с организованностью, ответственностью и трудолюбием. Экстраверсия. Отражает общительность и напористость. Экстраверсия. Отражает общительность и напористость. Доброжелательность. Указывает на сострадание и готовность к сотрудничеству. Доброжелательность. Указывает на сострадание и готовность к сотрудничеству. Невротизм. Связан с эмоциональной нестабильностью и тревожностью. Невротизм. Связан с эмоциональной нестабильностью и тревожностью. ИИ-система была обучена на основе изображений 96 000 выпускников MBA из ведущих американских бизнес-школ. Анализируя эти изображения, ИИ определил профили личности и сопоставил их с результатами карьеры, такими как зарплата, переход на другую работу и рейтинг школы. Что делает этот подход интересным, так это его способность измерять черты личности в больших масштабах, не прибегая к традиционным опросам. Это может сыграть огромную роль при приеме на работу и поступлении в учебные заведения. Подробнее о технических деталях можно узнатьздесь. Как искусственный интеллект предсказывает ваш будущий успех Методика основана на использовании нейросетей, обученных на больших массивах данных изображений лиц. Эти сети учатся определять тонкие закономерности в чертах лица, которые связаны с различными чертами личности. Исследования показывают, что как генетика, так и ранний жизненный опыт влияют на структуру лица и личность, что может объяснить, почему определенные черты связаны с определенными чертами характера. Например, если на вашей фотографии видна склонность к невротизму, у вас меньше шансов быть принятым на работу, на которую вы претендуете. Если вы менее добросовестны, то вас могут не принять в колледж. Личность может играть определенную роль при поступлении, поскольку учебные заведения могут искать кандидатов, способных преуспеть в будущей карьере, или стремиться к разнообразию личностей. Личность оказывает значительное влияние на многие результаты. Кроме того, более «желательная» личность, как определил ИИ, имеет связь с более высокой начальной зарплатой и более высоким ростом зарплаты с течением времени. Интересно, что эти черты личности, похоже, предсказывают успех в работе в той же степени, что и такие факторы, как раса, привлекательность и уровень образования. Эти прогнозы личности не просто измеряют, насколько человек умен, поскольку они не сильно коррелируют с оценками и результатами тестов. ИИ анализирует такие факторы, как: Симметрия лица Симметрия лица Строение челюсти и скул Строение челюсти и скул Расположение и выражение глаз Расположение и выражение глаз Затем эти черты сопоставляются с Большой пятеркой личностных качеств. Исследование показало, что такие черты, как добросовестность и экстраверсия, являются значимыми показателями, предсказывающими результаты карьеры. Например, люди с высоким уровнем совестливости, как правило, получали более высокую зарплату и занимали более высокие должности. Важно отметить, что прогнозы ИИ совпадали даже в том случае, если фотографии были сделаны с разницей в несколько лет, что говорит о том, что метод определения личности относительно стабилен с течением времени. Система также контролировала такие факторы, как качество изображения, освещение и выражение лица, чтобы добиться точных результатов. Но то, что ИИ может предсказывать определенные результаты, не означает, что мы должны спешить внедрять эту технологию при приеме на работу или в образовании. Расовая классификация Для расовой классификации система использует данныеRevelio Labs, которая применяет основанную на имени модель, предсказывающую расовую или этническую принадлежность человека на основе его имени, фамилии и географического положения. Эта модель обучена на данных переписи населения США. Компонент, учитывающий лица, использует классификатор VGG-Face - алгоритм глубокого обучения, созданный на основе пакета DeepFace Python, разработанного Серенгилом и Озпинаром (2020). ИИ объединяет эти два метода, группируя людей по пяти категориям: азиаты, чернокожие, латиноамериканцы, белые и другие. Чтобы еще больше повысить точность системы, исследователи использовали данные о расовой принадлежности, полученные от поступающих на программы MBA. Когда возникали расхождения между методами, основанными на имени и лице, система решала их, присваивая расовую принадлежность на основе переменной с наибольшей диагностической точностью - другими словами, на основе подхода с наименьшим числом ложных срабатываний в спорном подмножестве. Однако даже при таких мерах предосторожности опасения по поводу потенциальной предвзятости остаются. Критики утверждают, что черты лица зависят от сочетания генетических и экологических факторов, которые могут не соответствовать расовым признакам. Кроме того, если не уделять должного внимания классификации расы с помощью искусственного интеллекта, это может привести к закреплению вредных стереотипов. Действительно ли это работает? Скептицизм оправдан, когда речь заходит об искусственном интеллекте, делающем прогнозы на основе чего-то столь субъективного, как лицо человека. Критики утверждают, что такой подход может усилить предвзятость и укрепить стереотипы. Например, если искусственный интеллект будет обучаться на наборе данных, не отличающемся разнообразием, он может получить необъективные результаты, благоприятствующие определенным демографическим группам. Кроме того, личность - это сложный процесс, на который помимо внешности влияет множество факторов. Сможет ли ИИ по статичному изображению передать всю глубину человеческой личности? Исследователи сами признают ограниченность своей модели. Они подчеркивают, что, хотя ИИ может выявить корреляции, это не означает причинно-следственную связь. Другими словами, если по лицу человека можно определить определенную черту характера, это не значит, что его поведение всегда будет соответствовать этой черте. Существует также этическая дилемма. Должны ли работодатели и университеты иметь право использовать искусственный интеллект для проверки личности? Эта технология может привести к антиутопическому сценарию, когда людей будут оценивать исключительно по внешности, что подорвет индивидуальность и личностный рост. Исследователи не скрывают, что не выступают за использование технологии распознавания лиц при приеме на работу. Они признают, что суждение о людях по их лицам вызывает серьезные этические вопросы, поскольку может привести к дискриминации по неизменным признакам. Несмотря на эти опасения, результаты исследования трудно игнорировать. Исследователи продемонстрировали, что прогнозы ИИ не уступают традиционным показателям, таким как средний балл аттестата и результаты стандартизированных тестов. Это говорит о том, что личностные качества, определяемые искусственным интеллектом, имеют такое же значение для успеха в карьере, как и когнитивные навыки. Этические и практические последствия Если эта технология получит широкое распространение, она может изменить подход организаций к оценке кандидатов. Компании могут использовать ИИ для отбора кандидатов на основе личностных качеств, которые предсказывают эффективность работы. Школы могли бы отдавать предпочтение при приеме студентов, чьи личностные характеристики указывают на высокий потенциал успеха. Однако такая практика, скорее всего, столкнется с юридическими проблемами. Законы о дискриминации во многих странах запрещают принимать на работу на основе физических характеристик. Даже если оценки ИИ будут статистически достоверными, они все равно могут быть расценены как дискриминационные. Кроме того, эта технология вызывает опасения по поводу конфиденциальности. Как организации будут получать и хранить данные о лицах? Могут ли эти данные быть использованы не по назначению или взломаны? Эти вопросы необходимо будет решить регулирующим органам, прежде чем прогнозирование личности на основе ИИ станет массовым. Концепция искусственного интеллекта, анализирующего лицо человека, чтобы предсказать его потенциал успеха, кажется почти антиутопической, однако она основана на научных исследованиях. Однако то, что мы можем что-то сделать, не означает, что мы должны спешить это применить, особенно когда на кону стоят человеческие жизни и карьера. Прежде чем внедрять искусственный интеллект для проверки личности, необходимо провести серьезный разговор о его последствиях. Можем ли мы создать гарантии, чтобы предотвратить злоупотребления? Как сбалансировать инновации и уважение к человеческому достоинству и частной жизни? Кроме того, существует проблема предвзятости. Если ИИ можно запрограммировать так, чтобы он не допускал дискриминации по демографическому признаку, то как быть с более тонкими предубеждениями внутри демографических групп? Если мы будем судить о людях по тому, как они выглядят сегодня, мы рискуем проигнорировать их способность учиться, адаптироваться и меняться с течением времени. Человеческий потенциал не статичен. Люди растут благодаря опыту, возможностям и усилиям. Сводить все будущее человека к тому, что алгоритм видит на фотографии его лица, не только бесчеловечно, но и недальновидно. Пока что обществу предстоит решить, насколько далеко мы готовы зайти, позволив машинам оценивать наш потенциал. Друзья, буду рад, если вы подпишетесь на мойтелеграм-канал про нейросети, чтобы не пропускать анонсы статей, иканал с советамидля тех, кто только учится работать с нейросетями - я стараюсь делиться только полезной информацией. "
  },
  {
    "article_id": "https://habr.com/ru/articles/878646/",
    "title": "Как отремонтировать машину на скорости 100 км/ч?",
    "category": "Разное",
    "tags": "поддержка пользователей, поддержка клиентов, поддержка разработчиков",
    "text": "Дисклеймер.Конечно речь о разработке. Но вдруг, кто-то реально ожидал рекомендации по слесарным работам на айтишном ресурсе. Ремонт программы на скорости 100 запросов в секунду -- это крайне опасная и практически невозможная задача. В такой ситуации важно сохранять спокойствие и следовать безопасным рекомендациям: Если вы заметили, что с программой что-то не так (например, странное поведение, фантомное срабатывание или неожиданный результат в интерфейсе), попробуйте определить причину. Если вы заметили, что с программой что-то не так (например, странное поведение, фантомное срабатывание или неожиданный результат в интерфейсе), попробуйте определить причину. Не паникуйте. Постарайтесь сосредоточиться последовательности ваших действий. Не паникуйте. Постарайтесь сосредоточиться последовательности ваших действий. Плавно откройте свой любимый текстовой редактор, чтобы избежать резкого подгорания. Плавно откройте свой любимый текстовой редактор, чтобы избежать резкого подгорания. Используйте сдержанный слова в описании проблемы, чтобы не потерять контроль над своим гневом. Используйте сдержанный слова в описании проблемы, чтобы не потерять контроль над своим гневом. Напишите пару слов о том что случилось в общий чат с коллегами, чтобы они узнали о вашей проблеме. Напишите пару слов о том что случилось в общий чат с коллегами, чтобы они узнали о вашей проблеме. Постепенно описывайте вашу ситуацию в любимом текстовом редакторе. Постепенно описывайте вашу ситуацию в любимом текстовом редакторе. Если у вас запара с другими задачами, то будьте к ним внимательны, пропускай те их вперёд. Если у вас запара с другими задачами, то будьте к ним внимательны, пропускай те их вперёд. Полностью остановите свое движение по задачам, в тот момент когда это возможно. Полностью остановите свое движение по задачам, в тот момент когда это возможно. Убедитесь, что вы не создаёте помех для коллег и не будите их отвлекать в неподходящий момент. Убедитесь, что вы не создаёте помех для коллег и не будите их отвлекать в неподходящий момент. Если проблема серьёзная (например, отказ загрузки файла, страница не существует или просто ничего не происходит по нажатию кнопки) вызовите службу поддержки пользователей. Если проблема серьёзная (например, отказ загрузки файла, страница не существует или просто ничего не происходит по нажатию кнопки) вызовите службу поддержки пользователей. Если вы уверены в своих силах и у вас есть необходимые навыки, можете попробовать устранить небольшие проблемы самостоятельно. Если вы уверены в своих силах и у вас есть необходимые навыки, можете попробовать устранить небольшие проблемы самостоятельно. Никогда не пытайтесь обходится собственными силами, находясь в потоке задач. Никогда не пытайтесь обходится собственными силами, находясь в потоке задач. После описания проблемы в любимом текстовом редакторе, обязательно сохраните этот файл указав в названии дату происшествия, а затем направляйте сообщение в службу поддержки. После описания проблемы в любимом текстовом редакторе, обязательно сохраните этот файл указав в названии дату происшествия, а затем направляйте сообщение в службу поддержки. Важно:Ремонт программы на ходу может быть возможен только в случае самых простых манипуляции, например: Корректировка ранее введенных данных, если используется постраничная форма. Корректировка ранее введенных данных, если используется постраничная форма. Включение дополнительных фильтров или настроек. Включение дополнительных фильтров или настроек. Однако такие действия должны выполнять я только тогда, когда они не отвлекают вас от описания проблемы и не смогут замылить проблему ситуацию. Если проблема требует более сложного вмешательства, единственный безопасный выход - остановиться и дождаться службы поддержки. При этом не забудьте, во время ожидания поддержки сформулировать в текстовом виде все шаги которые привели вас к остановке.Это позволит специалисту поддержки воспроизвести вашу проблему и понять ю, что послужило причиной слома."
  },
  {
    "article_id": "https://habr.com/ru/companies/tbank/articles/878396/",
    "title": "Документация приложения — дело рук самого приложения",
    "category": "Разное",
    "tags": "scala, compiler, metaprogramming, architecture, architecture components, библиотеки, документация",
    "text": "Знакомо ли вам чувство, когда на поддержке есть сервис, о принципах работы которого знает буквально пара человек? В таких условиях очередная задача по миграции с одного решения на другое эквивалентна по-дурацки спродюсированному квесту из ролевой игры: ищем документацию, просматриваем глазами код, вызваниваем тех немногих, кто посвящен в таинства организации компонента системы. В какой-то момент порог негодования в нашей команде достиг критической отметки. Количество сервисов на поддержке приближалось к двум десяткам. Сами же сервисы не развивались, а просто существовали как есть. Более того, никакой общей доменной области, никаких актуальных описаний архитектуры. Мы решили навести порядок и разметить сервисы для понимания их архитектурных компонентов. После обсуждения взяли прицел на автоматизируемый процесс описания системы, а не на ручную поддержку документации. Добро пожаловать под кат — рассказываю о нашем пути, а в конце делюсь ссылкой на библиотеку. Идея После сеанса обсуждения между участниками команды был намечен ряд требований, которым должно удовлетворять наше решение. Стоит упомянуть следующие. Функциональные: Человекочитаемый формат. В конечном итоге наши архитектурные компоненты должны быть описаны в формате, пригодном для последующей обработки и чтения человеком. Человекочитаемый формат. В конечном итоге наши архитектурные компоненты должны быть описаны в формате, пригодном для последующей обработки и чтения человеком. Автоподдерживаемость. При изменении архитектуры приложения новое состояние должно подхватываться автоматически. Автоподдерживаемость. При изменении архитектуры приложения новое состояние должно подхватываться автоматически. Нефункциональные: Минимум влияния на Runtime. Среди услуг, предоставляемых нашими сервисами, есть критичные. Снижения Latency, неконтролируемый захват соединений из пула БД, RuntimeExceptions через часы после запуска — такого нам не надо. Минимум влияния на Runtime. Среди услуг, предоставляемых нашими сервисами, есть критичные. Снижения Latency, неконтролируемый захват соединений из пула БД, RuntimeExceptions через часы после запуска — такого нам не надо. Неинвазивность. Переписывание стагнирующих приложений, просто «чтобы документацию улучшить», непозволительно с точки зрения бэклога команды. Неинвазивность. Переписывание стагнирующих приложений, просто «чтобы документацию улучшить», непозволительно с точки зрения бэклога команды. Расширяемость. Хотелось заложить точки для расширения решения. Например, описывать не только архитектурные зависимости, но и resilience-паттерны с их настройками, которые используются вместе в каждой интеграцией с внешними системами. Расширяемость. Хотелось заложить точки для расширения решения. Например, описывать не только архитектурные зависимости, но и resilience-паттерны с их настройками, которые используются вместе в каждой интеграцией с внешними системами. Каждый архитектурный компонент, каждая архитектурная зависимость имеет собственное представление внутри кода. Возьмем базу данных. Чтобы оперировать ее сущностями внутри приложения, используются ORM- или FRM-библиотеки. Механизмы этих библиотек оборачиваются в более высокоуровневые интерфейсы, которые как-то манипулируют данными уже на уровне приложения. Аналогично и для других компонентов: хранилищ данных, брокеров сообщений, интеграций со сторонними системами. Интерфейс выглядит интуитивно понятным. Слушаем Kafka-топик, при получении сообщения как-то парсим его и переносим данные со сдедублицированного ID на активный. Фактически это и есть пример того, как мы работаем с архитектурными зависимостями на уровне приложения. Эмпирическая документация.Раз у нас есть код, мы могли бы попробовать обычным поиском по файлам репозитория собрать все наши классы, связанные с данными и архитектурными компонентами, и оформить в табличку. Такой эмпирический подход может сработать, когда есть единый codestyle, но уже в репозитории другой команды все политики придется настраивать заново и тщательно следить, чтобы нейминг не разъехался. Подобное решение звучит как способное сработать здесь и сейчас, но в дальнейшем велик риск, что все превратится в тыкву. Структурная документация.Что, если задокументировать код изнутри? Через рефлексию или макросы у нас есть возможность достучаться до структуры исходного кода. Если же заготовить разметку классов по принципу принадлежности к какому-то архитектурному компоненту, получится буквально «видеть код изнутри кода». Можно было бы использовать фреймворки для вайринга, работающие во время Runtime по примеру Distage, но это нарушило бы требование неинвазивности. Пришлось бы вручную перелопачивать каждый сервис, чтобы затащить подобный фреймворк. Хочу отметить, что это неплохая идея, просто не подходящая для нас. Решение Мы взяли за основу решения структурную документацию. В какой-то момент получилось реализовать прототип на основе рефлексии и макроаннотаций, но был ряд серьезных ограничений. Например, нельзя было размечать интерфейс внутри trait-ов. Так как в наших старых сервисах вайринг был построен через Cake Pattern, пришлось придумывать костыли, чтобы все работало. Потом коллеги подсказали интересный вариант — реализовать плагин компилятора. В отличие от макросов, которые могут оперировать только конкретным кусочком исходного кода в месте вызова макроса, плагины компилятора позволяют манипулировать всей структурой кода. Что такое плагины компилятора.Компилятор Scala (Scalac в 2.X и Dotty в 3.X) — это конвейер, который на вход принимает текстовые файлики кода с указанием параметров компиляции, прогоняет через пару десятков стадий обработки и на выходе выдает JVM-байткод. Плагины компилятора позволяют как-то изменять представленный набор фаз компиляции, в частности встраивать туда новые фазы. Подобный инструмент дает возможность достичь самых безумных результатов: от новых синтаксических правил и конструкций (kind-projector, better-monadic-for) до альтернативных конечных представлений кода (Scala Native, Scala.js). План работы с архитектурными зависимостями Работать с архитектурными зависимостями можно так: Обойти весь наш код и найти нужные классы. Обойти весь наш код и найти нужные классы. Сохранить информацию о найденных классах в единую структуру. Сохранить информацию о найденных классах в единую структуру. Обойти весь наш код еще раз в поисках мест, где необходима информация об архитектурных компонентах. Обойти весь наш код еще раз в поисках мест, где необходима информация об архитектурных компонентах. Достать из единой структуры информацию и вставить ее в эти места. Достать из единой структуры информацию и вставить ее в эти места. Так получится «видеть код изнутри кода». Стоит обратить внимание, что для реализации такого плана необходимо сначала разметить классы, связанные с архитектурными компонентами. Проектирование API.В первую очередь мы подумали о том, как должна выглядеть разметка классов, представляющих архитектурные компоненты. Варианты есть самые разные, но за основу взяли аннотации, так как они неинвазивны и позволяют легко навигироваться по IDE. Наш пример из требований можно записать так: Аннотация @arch размечает архитектурный компонент. Внутри нее указывается тип архитектурного компонента и какая-то дополнительная информация вроде топика, среды запуска и указания, консьюмер это или продюсер. Аннотация @archModel нужна для указания интерфейса, связанного с архитектурной зависимостью, который оперирует этой моделью данных. В данном случае DeduplicationEvent обрабатывается в KafkaDeduplicationConsumer. Стоит помнить о случаях, когда одна и та же модель нужна в разных интерфейсах или один и тот же интерфейс оперирует несколькими моделями. Еще нужно подумать, как будет выглядеть обратная сторона библиотеки —  работа с уже собранными архитектурными зависимостями. В the-seer используется подобный интерфейс: ArchGraph — это упомянутая в плане единая структура, которая хранит в себе архитектурные зависимости и соответствующие им модели. Через метод summonMeta эти зависимости можно достать внутри кода и как-то обработать в Runtime, например конвертировать в JSON и вывести на этапе сборки приложения в пайплайне. Реализация плагина компилятора Чтобы наш проект был именно плагином компилятора, а не каким-то приложением, необходимо удовлетворятьнекоторым требованиям: Добавить Scala-компилятор как зависимость в проект.libraryDependencies += scalaOrganization.value % \"scala-compiler\" % scalaVersion.value Добавить Scala-компилятор как зависимость в проект. Наследовать интерфейс Plugin и определить все нужные параметры, в частности список новых фаз компилятора (components), название, описание, параметры плагина.class TheSeer(val global: Global) extends Plugin {\n\n  override val name: String = \"the-seer\"\n  \n  val graph: ArchGraphInner = new ArchGraphInner\n  \n  override val components: List[PluginComponent] = {\n   new TheSeerMetaInliner(global, graph) ::\n     new TheSeerMetaCollector(global, graph) :: Nil\n }\n\n  override def init(options: List[String], error: String => Unit): Boolean = true\n\n  override val description: String = \"\"\"\"/╲/\\\\╭༼ ººل͟ºº ༽╮/\\\\╱\\\\\"\"\"\n} Наследовать интерфейс Plugin и определить все нужные параметры, в частности список новых фаз компилятора (components), название, описание, параметры плагина. Создать файл scalac-plugin.xml и определить в нем входную точку в плагин.<plugin>\n  <name>marker</name>\n  <classname>prototype.the.seer.compiler.plugin.TheSeer</classname>\n</plugin> Создать файл scalac-plugin.xml и определить в нем входную точку в плагин. Вуаля! У нас есть скелет для плагина компилятора. В The-seer-prototype дополнительно есть еще sbt-проектexamples, в котором можно оперативно тестировать работу плагина. Чтобыexamplesзависел от sbt-проекта the-seer-compiler-plugin именно как от плагина, а не как от обычного модуля, необходимо прописать дополнительные настройки компилятора. Так новые фазы компиляции будут учтены при компилированииexamples. Реализация TheSeerMetaCollector В первой новой фазе компилятора хочется обойти весь наш код и найти все классы, аннотированные через @arch и @archModel, и сохранить в инстанс структуры ArchGraph. В этом нелегком деле нам помог интерфейс Traverser. Создадим свой Traverser, который будет путем паттерн матчинга по Abstract Syntax Tree (AST) кода искать сущности вроде classes, objects, traits, vals, defs, аннотированные через prototype.the.seer.api.arch. Наш CallerTraverser применяется к одной единице компиляции (compilation unit), представляющей собой обычно один файл исходного кода. Допустим, мы нашли один из размеченных классов. А далее его структураразбирается с помощью Quotation Syntaxв методах вроде refineImplDefToMetaInfoEnriched. Здесь есть несколько интересных моментов: Quotation Syntax. Фактически это инструмент, позволяющий устраивать Pattern Matching на структуре кода. Например, кодclass MyShinyClass[MyTypeParam](myArg: Arg) {def hello = \"world\"}будет соответствовать веткеcase q\"..$mods class $className[..$typeParams](..$fields) extends ..$parents { ..$body }\" Quotation Syntax. Фактически это инструмент, позволяющий устраивать Pattern Matching на структуре кода. Например, код будет соответствовать ветке Выходной тип refineImplDefToMetaInfoEnriched (и его аналогов) — Set[Tree], где Tree — это AST кода. Почему именно такой тип, а не какой-то красивый класс с информацией об аннотациях вроде Kafka, Database, ExternalService и так далее? Потому что при разборе структур в Compile Time мы вынуждены работать с AST — и ничем другим. Код нашего приложения — это просто набор хитростуктурированного текста. У нас под рукой нет JVM, которая могла бы запустить код и инстанциировать классы. Мы вынуждены здесь отпустить попытки в строгую типизацию и просто протаскивать везде AST. Выходной тип refineImplDefToMetaInfoEnriched (и его аналогов) — Set[Tree], где Tree — это AST кода. Почему именно такой тип, а не какой-то красивый класс с информацией об аннотациях вроде Kafka, Database, ExternalService и так далее? Потому что при разборе структур в Compile Time мы вынуждены работать с AST — и ничем другим. Код нашего приложения — это просто набор хитростуктурированного текста. У нас под рукой нет JVM, которая могла бы запустить код и инстанциировать классы. Мы вынуждены здесь отпустить попытки в строгую типизацию и просто протаскивать везде AST. Затем мы складируем найденную информацию в ArchGraphInner. Реализация ArchGraphInner Если присмотреться к API, который мы объявляли ранее, можно заметить, что там используется структура ArchGraph. Но в TheSeerMetaCollector речь идет о некой структуре ArchGraphInner. Зачем нам понадобилось промежуточное представление? На самом деле разница между этими объектами довольно концептуальная. То есть TheSeerMetaCollector заполняет ArchGraphInner сырыми данными (AST кода, отвечающего архитектурным зависимостям). А во время следующей фазы, о которой поговорим чуть ниже, ArchGraphInner превращается в не что иное, как в инстансы ArchGraph и вставляется в места вызова. Потом уже во время Runtime приложения мы можем оперировать инстансами ArchGraph как самыми обычными классами. Реализация TheSeerMetaInliner Согласно плану наш следующий шаг — вторичный проход исходного кода с целью обнаружения точек, в которых используется TheSeerAPI.summonMeta. Мы хотим обнаружить все подобные места и буквально заменить вызов метода самим телом инстанса ArchGraph. Сейчас имплементация summonMeta выглядит так: Вопрос: как же заменить пустышку на настоящую имплементацию? Ответ: никак, нет необходимости что-либо менять. Столь странный стиль работы с кодом характерен для метапрограммирования. Нам не надо менять имплементацию, потому что в ходе TheSeerMetaInliner мы хотим найти все точки вызова summonMeta и в буквальном смысле заменить их методом ArchGraph.apply: Чтобы провернуть такой фокус, нам уже не хватит просто Traverser из предыдущей созданной нами фазы TheSeerMetaCollector, потому что на этот раз нам необходимо не просто пройтись по коду, но и изменить его отдельные участки. Благо на такой случай заготовлен интерфейс AstTransformer. Код TheSeerMetaInliner говорящий. Отлавливаем вызовы TheSeerAPI.summonMeta, берем нашу структуру ArchGraphInner и вставляем данные из нее в место вызова метода через quotation-синтаксис, предварительно типизировав его через localTyper.typed, так как наши фазы объявлены уже после главной фазы \"typer\". Реализация структуры плагина Теперь, когда есть представление об устройстве основных компонентов плагина, настало время поговорить, как связать все воедино. При объявлении плагина мы обязаны указать все фазы компилятора, которые должны примениться во время компиляции. В TheSeer мы объявляем эти компоненты, предварительно создав пустой инстанс ArchGraphInner и передав его в инстансы TheSeerMetaCollector и TheSeerMetaInliner. Сами же новые фазы обязаны наследовать интерфейс PluginComponent. Более того, в них необходимо написать название фазы и фазу, после которой ее следует запускать. В данном случае, \"meta-collector\" запускается после \"typer\", а \"meta-inliner\" — после \"meta-collector\". Стоит заметить, что sbt запускает компиляцию на каждом подпроекте в соответствии с их зависимостями между собой. Компиляция каждого подпроекта начинается заново, так что таким наивным способом передать ArchGraphInner не получится. Можно записывать промежуточное представление в какой-нибудь файл, но в прототипе такое не реализовано. За рамками статьи Полученная диаграмма — один из способов представления ArchGraph. Для документации наших архитектурных зависимостей сделали два вспомогательных модуля: SBT-плагин, позволяющий на этапе пайплайнов прогонять компиляцию проекта и интерпретацию в UML-код. Этот код кладется в артефакты пайплайна и затем попадает в Gitlab Pages, где автоматически рендерится в диаграмму. Таким образом, можно автоматически на этапе мерджа в master-ветку генерировать диаграммы. SBT-плагин, позволяющий на этапе пайплайнов прогонять компиляцию проекта и интерпретацию в UML-код. Этот код кладется в артефакты пайплайна и затем попадает в Gitlab Pages, где автоматически рендерится в диаграмму. Таким образом, можно автоматически на этапе мерджа в master-ветку генерировать диаграммы. Модуль, интерпретирующий ArchGraph в лейблы метрик. Метрики затем попадают на специальный дашборд Grafana и отображают информацию в виде таблиц. Модуль, интерпретирующий ArchGraph в лейблы метрик. Метрики затем попадают на специальный дашборд Grafana и отображают информацию в виде таблиц. Более того, получилось без каких-либо препятствий разметить код в ряде наших сервисов. Серьезных проблем, кроме минорных багов и увеличения продолжительности компиляции на проектах на время порядка десятка секунд (~50K LOC), замечено не было. Демонстрация Чтобы «потрогать руками» работу плагина,можно вызвать ExampleMainиз директорииexamples. В консоль должно выйтиUML-представление примеров из examples.suits Если отрендерить выведенную в консоль UML-нотацию примеров, то диаграмма должна выглядеть примерно так: Библиотека, о которой я столько рассказал.Если у вас остались вопросы по реализации или идеи — залетайте в комментарии! "
  },
  {
    "article_id": "https://habr.com/ru/companies/basis/articles/875942/",
    "title": "Docs-as-Code в технической документации: переход от reStructuredText к AsciiDoc",
    "category": "Разное",
    "tags": "docs-as-code, restructured text, asciidoc, техническая документация, технический писатель, devops-инженер, git, markdown",
    "text": "Хабр, привет! На связи коллективный разум технических писателей компании «Базис». Над представленным в этой статье проектом мы работали вместе, так что и рассказывать о нем будем вместе. Если точнее, расскажем о том, как познавали методологию Docs-as-Code, зачем техническим писателям дружить с DevOps-инженерами, а главное, почему мы перешли от reStructuredText к AsciiDoc и что нам это принесло. Да кто такой этот ваш Docs-as-Code? Начнем с аксиомы: техническая документация на ПО всегда должна описывать актуальную версию софта. Даже в случае интенсивной разработки, когда обновления появляются несколько раз в день, документацию тоже нужно постоянно дополнять. В нашем случае речь идет не об одном продукте, а о целой экосистеме, включающей решения серверной виртуализации и VDI, средства защиты виртуальной инфраструктуры, резервного копирования и т. д. Это сравнительно сложные продукты, и для каждого из них нужно подготовить подробные руководства по установке и эксплуатации, а это десятки и сотни страниц (с картинками). Ошибаться в документации, кстати, не рекомендуется: у заказчика может пострадать инфраструктура, а у нас — репутация. В такой ситуации технические писатели уже не могут существовать в стороне от разработчиков, нам нужно глубоко погружаться в процесс создания продукта, чтобы успевать правильно фиксировать изменения. Поэтому нас заинтересовала концепция Docs-as-Code, позволяющая работать с текстовыми файлами так же, как с кодом. В рамках этой концепции технические писатели пользуются трекером и git-репозиторием, а их тексты участвуют в code review и автоматических тестах. Вместо того, чтобы хранить все в отдельных файлах, которые редактируются вручную и рассредоточены по разным компьютерам, вся документация складывается в единое хранилище — в тот же репозиторий, что и основной код. Release notes и другие тексты представляют собой не громоздкие документы Word или PDF, а файлы в форматах Markdown или Acsiidoc. Потом с помощью других инструментов (например, конвертера Pandoc) эти файлы автоматически собираются в красивый и удобный документ в нужном формате. И поскольку документы хранятся в общем репозитории, мы можем в любой момент увидеть историю всех изменений: кто вносил правки, какие это были правки и когда это произошло. А если кто-то внес неверную информацию, можно откатиться на старую версию. Как и код, текст документов можно проверять до релиза: другие участники команды могут просмотреть изменения, обсудить их и одобрить либо «завернуть». Единая база документации, как и единая кодовая база, позволяет легко обмениваться информацией между разработчиками, тестировщиками, техническими писателями, аналитиками — в общем, между всеми теми, кому нужна документация. Тесное взаимодействие с коллегами и постоянная обратная связь помогают нам готовить более качественную документацию, в итоге все довольны и вместе идут в ресторан отмечать новый релиз. Или не идут. Потому что технические писатели заняты освоением новых инструментов, инженеры размышляют над тем, как обучение техписов этим инструментам стало их задачей, а аналитики вежливо интересуются, почему сделанные в рамках новой концепции Docs-as-Code документы отличаются от привычных. Сейчас сложности, связанные с реализацией подхода Docs-as-Code в нашей команде, уже позади. Мы попробовали формат reStructuredText, но затем перешли на AsciiDoc; перебрали несколько инструментов, чтобы остановиться в итоге на платформе GitLab, редакторе VSCodium и некоторых других. Но путь даже к такому небольшому набору был непростым. Первый подход к снаряду Идея внедрения Docs-as-Code зародилась внутри команды, поэтому изучать доступные возможности мы начали своими силами. Все это вылилось в самостоятельную настройку, поддержку и использование следующих инструментов: Язык разметки reStructuredText (RST)  — был выбран из-за простого синтаксиса, плюс он хорошо поддерживался различными текстовыми редакторами, в частности, Geany. Язык разметки reStructuredText (RST)  — был выбран из-за простого синтаксиса, плюс он хорошо поддерживался различными текстовыми редакторами, в частности, Geany. Geany — это стабильный и легкий текстовый редактор, который предоставляет массу полезных функций, не усложняя рабочий процесс. Geany — это стабильный и легкий текстовый редактор, который предоставляет массу полезных функций, не усложняя рабочий процесс. Pandoc — универсальный конвертер документов, поддерживает множество различных форматов, включая reStructuredText и AsciiDoc. Мы использовали его вместе с кастомными решениями с фильтрами на языке Lua для получения документов целевого формата (DOCX и PDF). Pandoc — универсальный конвертер документов, поддерживает множество различных форматов, включая reStructuredText и AsciiDoc. Мы использовали его вместе с кастомными решениями с фильтрами на языке Lua для получения документов целевого формата (DOCX и PDF). Sphinx — инструмент, который использовался для генерации документации из исходных файлов разметки. Sphinx — инструмент, который использовался для генерации документации из исходных файлов разметки. Для автоматизации процессов написания и обновления документации использовались самописные Python-скрипты. Они позволяли выполнять различные задачи, такие как конвертация документов в нужный формат, автоматическое добавление метаданных и т. д. Для автоматизации процессов написания и обновления документации использовались самописные Python-скрипты. Они позволяли выполнять различные задачи, такие как конвертация документов в нужный формат, автоматическое добавление метаданных и т. д. Метод «Scripts run scripts» предполагал использование цепочки скриптов для автоматизации процессов написания документации. Он позволял нам упростить и ускорить процесс создания и обновления текстовой информации, а также обеспечить её единообразие и стабильность. Метод «Scripts run scripts» предполагал использование цепочки скриптов для автоматизации процессов написания документации. Он позволял нам упростить и ускорить процесс создания и обновления текстовой информации, а также обеспечить её единообразие и стабильность. Все перечисленное — после освоения и настройки — действительно сильно упростило нам работу. Однако мы быстро поняли, что хранение по принципу «один документ = один файл .rst» — это не самый удобный для нас вариант. Да и при оформлении итоговой документации то и дело возникали сложности: отсутствие TOC (Table of Content) на гиперссылках; отсутствие TOC (Table of Content) на гиперссылках; отсутствие автонумерации картинок и таблиц; отсутствие автонумерации картинок и таблиц; наличие «битых ссылок»; наличие «битых ссылок»; разнородность стилей в документе; разнородность стилей в документе; проблемы с генерацией титульной страницы. проблемы с генерацией титульной страницы. В результате всего этого возникала критика со стороны продуктовых команд, да и нас самих итоговое качество оформления не устраивало. Постепенно мы вместе с коллегами, отвечающими за ревью документации, пришли к выводу, что процесс документирования необходимо изменить. Инженеры спешат на помощь На этот раз решили не обходиться только своими силами, а позвали на помощь наших коллег, DevOps-инженеров, которые: провели экспертизу и рекомендовали формат AsciiDoc; провели экспертизу и рекомендовали формат AsciiDoc; помогли организовать новый репозиторий и установить все необходимые инструменты, тот же редактор VSCodium и его расширения; помогли организовать новый репозиторий и установить все необходимые инструменты, тот же редактор VSCodium и его расширения; подготовили стили для документов с учетом принятых в компании стандартов; подготовили стили для документов с учетом принятых в компании стандартов; написали CI/CD-конвейер для сборки документов; написали CI/CD-конвейер для сборки документов; сопровождали и обучали технических писателей на первых этапах перехода. сопровождали и обучали технических писателей на первых этапах перехода. Возможности конвейера нас действительно впечатлили: процесс работы с текстами был автоматизирован, мы смогли быстрее вносить изменения в документы, проверять их и внедрять. CI предполагает постоянное добавление изменений в основную ветку и автоматическое тестирование для выявления проблем на ранних этапах. CD включает в себя автоматическое развертывание изменений, что ускоряет процесс подготовки новых версий документации. Этапы перехода от reStructuredText к AsciiDoc Дальше нужно было организовать процесс переноса документации из RST в ADOC. Задача кажется несложной, ведь это просто конвертация из одного формата в другой. Но на практике пришлось разбить процесс на несколько этапов: Заморозка актуальной версии (выпуска/релиза), оформленной с применением разметки reStructuredText, чтобы зафиксировать текущее состояние документации и подготовиться к ее обновлению. Мы с командой разработки продукта пришли к соглашению о «заморозке» работы над обновлением документации сроком на три месяца. Заморозка актуальной версии (выпуска/релиза), оформленной с применением разметки reStructuredText, чтобы зафиксировать текущее состояние документации и подготовиться к ее обновлению. Мы с командой разработки продукта пришли к соглашению о «заморозке» работы над обновлением документации сроком на три месяца. Разработка структуры документации. Вместе с продуктовыми аналитиками мы определили, какие главы и разделы будут включены в документ. Это помогло нам правильно организовать информацию и сделать ее более доступной для пользователей. Разработка структуры документации. Вместе с продуктовыми аналитиками мы определили, какие главы и разделы будут включены в документ. Это помогло нам правильно организовать информацию и сделать ее более доступной для пользователей. Составление карт документов. На этом этапе мы утвердили состав и порядок сбора основного документа. Карты документов помогли организовать информацию и упростили создание документации. Составление карт документов. На этом этапе мы утвердили состав и порядок сбора основного документа. Карты документов помогли организовать информацию и упростили создание документации. Конвертация исходных текстов из reStructured в AsciiDoc. С этой задачей мы справились самостоятельно, используя инструмент Pandoc, который позволяет конвертировать тексты из одного формата в другой. На выходе получился алгоритм «1 документ = 1 файл .adoc». Конвертация исходных текстов из reStructured в AsciiDoc. С этой задачей мы справились самостоятельно, используя инструмент Pandoc, который позволяет конвертировать тексты из одного формата в другой. На выходе получился алгоритм «1 документ = 1 файл .adoc». Оформление полученных текстов. На этом этапе наша команда ознакомилась с официальной документацией AsciiDoc и выбрала «лучшие практики», а заодно сформировала базовые навыки работы с форматом. Все это стало частью регламента разработки документации и в итоге помогло создать качественные тексты. Оформление полученных текстов. На этом этапе наша команда ознакомилась с официальной документацией AsciiDoc и выбрала «лучшие практики», а заодно сформировала базовые навыки работы с форматом. Все это стало частью регламента разработки документации и в итоге помогло создать качественные тексты. Дробление исходных текстов на главы. Этот этап очень важен, так как позволяет изолировать взгляд от большого объема информации, сделать документ более удобным для чтения и навигации. Мы применили принцип «1 глава = 1 файл .adoc» и придерживаемся его до сих пор. Дробление исходных текстов на главы. Этот этап очень важен, так как позволяет изолировать взгляд от большого объема информации, сделать документ более удобным для чтения и навигации. Мы применили принцип «1 глава = 1 файл .adoc» и придерживаемся его до сих пор. Потратив на все перечисленное три месяца, мы добились того, чего и хотели изначально, — качественно оформленной и хорошо воспринимаемой (читаемой) документации. Причём это оценка не наша, а заказчиков — продуктовой команды. Старое vs Новое Новая структура проекта документации получилась более полной и лучше организованной. Это помогло нам структурировать процессы управления и развития документации, сопровождающей продукты компании. Для наглядности — общий вид структуры до и после изменения процессов документирования: Особенности старой структуры проекта Одна основная директория разработки документации. Одна основная директория разработки документации. В этой директории содержатся файлы JSON для настройки титульного листа и версии продукта, а также директория для хранения медиаконтента. В этой директории содержатся файлы JSON для настройки титульного листа и версии продукта, а также директория для хранения медиаконтента. Готовый файл документации в формате .rst. Готовый файл документации в формате .rst. Особенности новой структуры проекта Наличие главной (корневой) директории, внутри которой лежит комплект документации. Наличие главной (корневой) директории, внутри которой лежит комплект документации. Комплект документации включает директорию разработки документации, внутри которой находятся директория для разработки глав (chapters), карта разработки документации (main) и директория для разработки приложений к документации (appendix). Комплект документации включает директорию разработки документации, внутри которой находятся директория для разработки глав (chapters), карта разработки документации (main) и директория для разработки приложений к документации (appendix). Для хранения медиаконтента используется отдельная директория с поддиректориями по группам скриншотов (media). Для хранения медиаконтента используется отдельная директория с поддиректориями по группам скриншотов (media). Наличие директорий с PDF-темами и CSS-стилями, а также файла с версией документируемого продукта. Наличие директорий с PDF-темами и CSS-стилями, а также файла с версией документируемого продукта. Что мы получили в результате Использование нового языка разметки AsciiDoc в сочетании с VSCodium помогло улучшить скорость написания и согласования release notes, а также в целом повысило качество документации в компании — на радость нашим коллегам. В частности, благодаря формату AsciiDoc мы смогли: автоматизировать нумерацию элементов — картинок и таблиц; автоматизировать нумерацию элементов — картинок и таблиц; автоматизировать сборку PDF и упростить процесс выходного контроля документов; автоматизировать сборку PDF и упростить процесс выходного контроля документов; обучиться простому синтаксису и применить облегченный подход к верстке документов; обучиться простому синтаксису и применить облегченный подход к верстке документов; автоматизировать генерацию титульных страниц с учетом корпоративного стиля компании; автоматизировать генерацию титульных страниц с учетом корпоративного стиля компании; декомпозировать документ на главы для удобной работы над частью документа (изменение и ревью); декомпозировать документ на главы для удобной работы над частью документа (изменение и ревью); создать удобную навигацию по документу. создать удобную навигацию по документу. Что касается VSCodium, то одним из ключевых преимуществ редактора для нас стало использование сниппетов — заранее определенных фрагментов кода, которые можно быстро вставлять в текст. Кроме того, с VSCodium можно генерировать превью документов в реальном времени, т. е. мы получили возможность сразу видеть, как будет выглядеть итоговый документ в HTML- или PDF-формате, и оперативно вносить необходимые коррективы. В процессе перехода на AsciiDoc мы разработали два внутренних регламента: Регламент оформления текста в формате AsciiDoc. Он позволил прийти к единому стилю оформления документа — это важно, когда над одним документом работают несколько технических писателей. Плюсом команде удалось сократить время на подготовку документации параллельно выпуску релиза продукта. Регламент оформления текста в формате AsciiDoc. Он позволил прийти к единому стилю оформления документа — это важно, когда над одним документом работают несколько технических писателей. Плюсом команде удалось сократить время на подготовку документации параллельно выпуску релиза продукта. Регламент взаимодействия технических писателей и аналитиков, которые ставят задачи на документирование. Регламент взаимодействия технических писателей и аналитиков, которые ставят задачи на документирование. В качестве бонуса — новые инструменты и чёткий регламент работы помогли нам сократить время на обучение новых сотрудников. Это пригодилось позже, когда компания начала добавлять в портфель новые продукты, а нам нужно было оперативно приводить их документацию к принятому стандарту и нанимать в штат новых техписов для поддержки будущих релизов. Но это уже совсем другая история. "
  },
  {
    "article_id": "https://habr.com/ru/companies/netologyru/articles/873674/",
    "title": "Как написать грамотный гайд: правила для техписов и разрабов",
    "category": "Разное",
    "tags": "гайд, руководство, техпис, технический писатель, техническая документация, технические тексты, технический гайд, формулировка, хорошие примеры, как надо писать",
    "text": "Большинство гайдов по программному обеспечению написаны трагически плохо. В них не хватает важной информации, и это мешает пользователям повторить описанные в руководстве процессы. Иногда автор исходит из скрытых предпосылок, которые не соответствуют ожиданиям читателя. Но есть и хорошая новость: научиться писать грамотные руководства проще, чем вы думаете. Следуйте нескольким простым правилам, и ваши тексты будут выделяться на фоне повсеместной посредственности. Правила Пишите для чайников Пишите для чайников Пообещайте в заголовке чёткий результат Пообещайте в заголовке чёткий результат Во введении объясните цель статьи Во введении объясните цель статьи Покажите конечный результат Покажите конечный результат Разрешите копировать сниппеты кода Разрешите копировать сниппеты кода Используйте длинные варианты флагов командной строки Используйте длинные варианты флагов командной строки Разделяйте пользовательские значения и переиспользуемую логику Разделяйте пользовательские значения и переиспользуемую логику Не загружайте читателя бессмысленными задачами Не загружайте читателя бессмысленными задачами Поддерживайте код в рабочем состоянии Поддерживайте код в рабочем состоянии Посвятите гайд одной теме Посвятите гайд одной теме Не приукрашивайте Не приукрашивайте Сократите зависимости по максимуму Сократите зависимости по максимуму Чётко указывайте имена файлов Чётко указывайте имена файлов Используйте единообразные содержательные заголовки Используйте единообразные содержательные заголовки Покажите, что ваше решение работает Покажите, что ваше решение работает Увяжите конкретику с комплексным примером Увяжите конкретику с комплексным примером Пишите для чайников Самая распространённая ошибка гайдов — когда объяснение адресовано новичкам, а написано сложным экспертным языком. Руководства чаще всего читают начинающие. Или программисты с опытом, изучающие новое для них направление. ❌ Плохо: Использовать понятия, непонятные новичкам В этом руководстве я расскажу, как создать первый SPA «Hello world» на React. Откройте приложенный файлhello.jsxи измените приветствие сHello worldнаHello universe. Для отображения нового текста браузер не перезагружается. За счёт эффективной транспиляции JSX в React изменения видны сразу же. Браузеру даже не нужна мягкая перезагрузка, потому что механизм сверки React сравнивает виртуальный и текущий DOM и обновляет только те элементы DOM, которым требуются изменения. Приведённый выше пример отпугнёт новичков. Разработчик, который ещё не работал с веб-фреймворком React, не понимает, что такое транспиляция JSX или механизм сверки. А если он не работал с другими фреймворками JavaScript, ему также будут незнакомы SPA, мягкая перезагрузка и виртуальный DOM. Когда вы пишете гайд, помните: вы обращаетесь не к экспертам. Избегайте профессионального жаргона, сокращений или терминов, которые ничего не значат для новичка. Это введение в руководство по React написано языком, который будет понятен читателям, даже если у них нет опыта в программировании. ✅Хорошо: Применяйте термины, понятные новичкам В этом руководстве я покажу вам, как создать простую веб-страницу с помощью современных инструментов веб-разработки. Для этого я использую React, бесплатный популярный инструмент для создания сайтов. React отлично подойдёт для первого в вашей жизни сайта. В то же время это мощное полнофункциональное решение для создания сложных приложений, которыми пользуются миллионы людей во всём мире. Если вы пишете для новичков, это вовсе не значит, что вы оттолкнёте опытных пользователей. Компетентный читатель может бегло просмотреть руководство и пропустить разделы, с которыми уже знаком. Но новичок не сможет прочитать гайд для экспертов. Пообещайте в заголовке чёткий результат Если читатели ищут ответ в Google, подскажет ли им заголовок вашей статьи искомое решение? Если они увидят ваше руководство в соцсетях или в рассылке, убедит ли их заголовок перейти по ссылке? Полное руководство о том, как стать гуру в CSV на Python Полное руководство о том, как стать гуру в CSV на Python Как создать свой собственный TwitterKey Mime Pi: сделайте классный гаджетКак создать компилятор Как создать свой собственный TwitterKey Mime Pi: сделайте классный гаджетКак создать компилятор Вот примеры слабых заголовков: ❌Плохо • Полное руководство о том, как стать гуру в CSV на Python• Как создать свой собственный Twitter• Key Mime Pi: сделайте классный гаджет• Как создать компилятор Это слабые заголовки, потому что они не поясняют, о чём текст. Из такого заголовка неясно, чему научит руководство. Заголовок должен лаконично объяснять, чего читатель может ожидать, ознакомившись с руководством. Вот как можно переписать предыдущие заголовки: ✅Хорошо: Используйте заголовки, обещающие чёткий результат • Как читать CSV-файл в Python• Создайте клон Twitter в реальном времени за 15 минут — с помощью Phoenix LiveView• Key Mime Pi: превратите Raspberry Pi в удалённую клавиатуру• Как написать компилятор на Python и уложиться в 500 строк кода Из этих заголовков совершенно ясно, чему научится читатель, ознакомившись с руководством. Во введении объясните цель статьи Если читатель открыл ваш гайд, это уже отличный старт: кому-то интересно, о чём вы пишете. Но вам всё ещё нужно убедить его, что стоит читать руководство дальше. В начале руководства читатель стремится максимально быстро найти ответ на два важных вопроса: Надо ли изучать эту технологию? Надо ли изучать эту технологию? А если надо, подходящее ли руководство я открыл? А если надо, подходящее ли руководство я открыл? Первые предложения статьи должны дать ответы на эти вопросы. Вот просто ужасное введение в статью о том, как использовать контейнеры Docker. ❌ Плохо Введение в контейнеры Docker Docker — чрезвычайно мощная и универсальная технология. Она позволяет запускать приложение в контейнере, то есть отдельно от всех остальных элементов системы. В этом руководстве я покажу вам, как с помощью Docker запускать контейнеры в локальной инфраструктуре и в облаке. Если судить по этому введению, какую проблему решает Docker? Кому стоит им пользоваться? Во введении нет ответа на эти вопросы. Зато есть приветственные помахивания с туманными терминами и игнорирование моментов, актуальных для читателя. Ниже формулировка, в которой объясняется, как Docker устраняет боли читателя: ✅Хорошо: Объяснять конкретный результат и преимущества Как использовать Docker для надёжного деплоймента приложений Боитесь трогать свой продакшн-сервер, потому что никто не знает, как наладить его работу после сбоя? Рвёте на себе волосы, потому что не понимаете, почему промежуточная рабочая среда работает не так, как продакшн-среда? Docker помогает упаковать приложение так, чтобы оно всегда выполнялось в единообразной воспроизводимой среде. С помощью Docker можно задать среду приложения и его зависимости в исходном коде, так что вы будете точно знать, что к чему, даже если ваше приложение годами отлаживали разные команды. В этом руководстве я покажу вам, как использовать Docker для упаковки простого веб-приложения и не допустить распространённых ошибок. В этом введении говорится, какие проблемы можно устранить с помощью Docker и какую конкретно задачу помогает решить гайд. Во введении не сказано: «Это руководство для тех, кто ничего не знает про Docker». Но это и не нужно. В нём Docker описывается как новая концепция, так что всем понятно, что гайд для новичков. Покажите конечный результат Как можно скорее покажите на демо или скриншоте, что читатель сможет создать к концу гайда. Не то чтобы этот конечный результат должен поражать воображение. Вот как я показал UI терминала, который пользователь увидит в конце руководства: Такой конечный результат повышает определённость решаемой задачи. Так читателю проще понять, подходит ли ему ваш гайд. Разрешите копировать сниппеты кода Когда читатель будет выполнять пункт за пунктом гайда, ему в какой-то момент понадобится скопировать сниппеты кода в редактор или терминал. Удивительно, сколько гайдов не разрешают копирование. А ведь без этого читателю гораздо труднее проверить в деле примеры из статьи. Разрешите копировать команды оболочки Очень распространённая ошибка, связанная со сниппетами кода, — использование символа shell prompt. Сниппет оболочки с символом $ не будет работать, когда пользователь попытается скопировать его в терминал. Даже в Google не удаётся скопировать её без косяков. В некоторых статьях услужливо предусмотрена кнопка «Скопировать образец кода». Если нажать кнопку копирования, она скопирует $, символ приглашения терминала, так что вам не удастся вставить код: Есть и другой вариант этой ошибки, не такой заметный: ❌ Плохо: Последовательность команд, которую пользователю приходится вводить sudo apt update sudo apt install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa sudo apt install python3.9 Если я попытаюсь скопировать этот сниппет, вот что я увижу в терминале: Что случилось? При выполнении команды apt install software-properties-common требуется пользовательский ввод. А пользователь не может выполнить это действие, потому что apt просто считывает буфер обмена дальше. В большинстве инструментов командной строки поддерживаются флаги или переменные среды, избавляющие пользователя от необходимости интерактивного взаимодействия. С помощью неинтерактивных флагов можно упростить копирование сниппетов команд в терминал пользователей. ✅Хорошо: Используйте флаги командной строки, чтобы отказаться от интерактивного пользовательского ввода sudo apt update sudo apt install --yes software-properties-common sudo add-apt-repository --yes ppa:deadsnakes/ppa sudo apt install --yes python3.9 Объединяйте команды оболочки с помощью && Давайте вернёмся к примеру установки Python, который я приводил выше. В нём есть ещё одна проблема: ❌ Плохо: Игнорируются невыполненные команды sudo apt update sudo apt install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa sudo apt install python3.9 Пользователи не всегда замечают, что одна из команд завершилась с ошибкой. Например, если первая команда — sudo apt cache ppa:dummy:non-existent, она завершится с ошибкой, но оболочка радостно выполнит следующую команду, как будто до этого всё было отлично. В большинстве оболочек Linux можно объединять команды с помощью && и продолжать строки обратным слешем. В этом случае оболочка прерывает работу, если команда завершается с ошибкой. Вот удобный способ представить последовательность копируемых команд: ✅Хорошо: Объединить команды с помощью && sudo apt update && \\ sudo apt install --yes software-properties-common && \\ sudo add-apt-repository --yes ppa:deadsnakes/ppa && \\ sudo apt install --yes python3.9 Так пользователь может копировать и вставлять всю последовательность: ему не придётся возиться с промежуточными этапами. Если в какой-то команде возникает ошибка, вся последовательность сразу же останавливается. Показывайте shell prompt, только чтобы продемонстрировать результат Иногда символ shell prompt можно использовать для пользы читателя. Если вы показываете команду и результат её выполнения, символ shell prompt помогает читателю различать, что он вводит и что получается в результате. Например, в руководстве об утилите jq можно представить результаты следующим образом: ✅Хорошо: Используйте символ shell prompt, чтобы провести различия между самой командой и результатом её выполнения Утилита jq позволяет элегантно реструктурировать данные JSON: $ curl \\\n  --silent \\\n  --show-error \\\n  https://status.supabase.com/api/v2/summary.json | \\\n  jq '.components[] | {name, status}'\n{\n  \"name\": \"Analytics\",\n  \"status\": \"operational\"\n}\n{\n  \"name\": \"API Gateway\",\n  \"status\": \"operational\"\n}\n... Исключите номера строк из копируемого текста Указывать номера строк в сниппетах кода — это нормально. Но убедитесь, что они не мешают корректно копировать код. Например, если пользователь пытается скопировать функцию count_tables из следующего сниппета, ему придётся удалять номера строк из копируемого текста. ❌ Плохо: Номера строк включены в копируемый текст 123 def count_tables(form): 124   if not form: 125     return None Используйте длинные варианты флагов командной строки В утилитах командной строки часто предусмотрено два варианта одного и того же флага: короткий и длинный. Всегда используйте длинные флаги в гайдах. Они более содержательные, так что новичкам будет проще читать такой код. ❌ Плохо: Использовать короткие непонятные варианты флагов командной строки Запустите эту команду, чтобы найти все страницы с элементами<span>: grep -i -o -m 2 -r '<span.*</span>' ./ Даже если читатель знаком с инструментом grep, скорее всего, он не помнит наизусть все его флаги. Используйте длинные флаги, чтобы примеры были понятны и опытным пользователям, и новичкам. ✅Хорошо: Используйте подробные описательные варианты флагов командной строки Запустите эту команду, чтобы найти все страницы с элементами <span>: grep \\\n  --ignore-case \\\n  --only-matching \\\n  --max-count=2 \\\n  --recursive \\\n  '<span.*</span>' \\\n  ./ Разделяйте пользовательские значения и переиспользуемую логику Пример кода часто содержит как элементы, присущие решению, так и элементы, которые каждый пользователь может настроить на своё усмотрение. Объясните читателю, что есть что. Возможно, вам различия между пользовательским значением и остальным кодом очевидны, но они непонятны человеку, который только вникает в тему. Используйте переменные среды в примерах командной строки Сервис логирования, которым я пользуюсь, приводит следующий пример кода для извлечения логов в командной строке. ❌ Плохо: Выполните в коде действие bake с пользовательскими переменными LOGS_ROUTE=\"$(\n  curl \\\n    --silent \\\n    --header \"X-Example-Token: YOUR-API-TOKEN\" \\\n    http://api.example.com/routes \\\n    | grep \"^logs \" \\\n    | awk '{print $2}'\n    )\" && \\\n  curl \\\n    --silent \\\n    --header \"X-Example-Token: YOUR-API-TOKEN\" \\\n    \"http://api.example.com${LOGS_ROUTE}\" \\\n    | awk \\\n        -F'T' \\\n        '$1 >= \"YYYY-MM-DD\" && $1 <= \"YYYY-MM-DD\" {print $0}' Какие значения заменять в этом примере? Очевидно, нужно заменить плейсхолдер YOUR-API-TOKEN. А как насчёт YYYY-MM-DD? Вставлять ли на его месте реальные даты вроде 2024-11-23? Или он указывает на схему даты, в том смысле, что YYYY-MM-DD — это буквенное значение, которое надо оставить как есть? В этом примере есть ещё несколько чисел и строк. А их заменять надо? Не заставляйте пользователя гадать, какие значения в вашем примере нужно менять. Просто укажите чёткие отличия одного от другого. Начните с редактируемых значений, потом приведите сниппет, который читатели могут скопировать без изменений. Вот как я переписал пример выше: ✅Хорошо: Используйте переменные среды для пользовательских значений API_TOKEN='YOUR-API-KEY' # Replace with your API key.\nSTART_DATE='YYYY-MM-DD'  # Replace with desired start date.\nEND_DATE='YYYY-MM-DD'    # Replace with desired end date.\n\nLOGS_ROUTE=\"$(\n  curl \\\n    --silent \\\n    --header \"X-Example-Token: $API_TOKEN\" \\\n    http://api.example.com/routes \\\n    | grep \"^logs \" \\\n    | awk '{print $2}' \\\n    )\" && \\\n  curl \\\n    --silent \\\n    --header \"X-Example-Token: $API_TOKEN\" \\\n    \"http://api.example.com${LOGS_ROUTE}\" \\\n    | awk \\\n      -F'T' \\\n      -v start=\"$START_DATE\" \\\n      -v end=\"$END_DATE\" \\\n      '$1 >= start && $1 <= end {print $0}' В новом варианте указаны значения, которые нужно заменить, и код, который остаётся без изменений. Использование переменных среды проясняет назначение пользовательских значений. Благодаря этому пользователю придётся вводить каждое уникальное значение только один раз. Используйте именованные константы в исходном коде Представьте, что вы пишете руководство о том, как обрезать изображение, чтобы оно по формату подходило для постов в Bluesky и Х (ранее Twitter): Вот как можно показать код для обрезки рисунка под размер картинки для поста. ❌ Плохо: Оставить читателя гадать, какие числа нужно заменить func CropForSocialSharing(img image.Image) image.Image {\n  targetWidth := 800\n  targetHeight := int(float64(targetWidth) / 1.91)\n  bounds := img.Bounds()\n\n  x := (bounds.Max.X - targetWidth) / 2\n  y := (bounds.Max.Y - targetHeight) / 2\n\n  rgba := image.NewRGBA(\n    image.Rect(x, y, x+targetWidth, y+targetHeight))\n  draw.Draw(\n    rgba, rgba.Bounds(), img, image.Point{x, y}, draw.Src)\n  return rgba\n} В примере четыре числа: 800; 800; 1,91; 1,91; 2; 2; 2 (ещё раз). 2 (ещё раз). Какие числа можно спокойно менять? В примерах исходного кода чётко указывайте, какие значения — неотъемлемая часть решения, а какие — произвольные. Поменяем формулировки, чтобы прояснить назначение чисел. ✅Хорошо: Используйте переменные среды для пользовательских значений // Use a 1.91:1 aspect ratio, which is the dominant ratio\n// on popular social networking platforms.\nconst socialCardRatio = 1.91\n\nfunc CropForSocialSharing(img image.Image) image.Image {\n  // I prefer social cards with an 800px width, but you can\n  // make this larger or smaller.\n  targetWidth := 800\n\n  // Choose a height that fits the target aspect ratio.\n  targetHeight := int(float64(targetWidth) / socialCardRatio)\n\n  bounds := img.Bounds()\n\n  // Keep the center of the new image as close as possible to\n  // the center of the original image.\n  x := (bounds.Max.X - targetWidth) / 2\n  y := (bounds.Max.Y - targetHeight) / 2\n\n  rgba := image.NewRGBA(\n    image.Rect(0, 0, targetWidth, targetHeight))\n  draw.Draw(\n    rgba, rgba.Bounds(), img, image.Point{x, y}, draw.Src)\n\n  return rgba\n} В этом примере значение 1.91 становится именованной константой, а код содержит комментарий, в котором объясняется, что это за цифра. Таким образом, читатель понимает, что значение не надо менять, поскольку в этом случае у изображения будут пропорции, не подходящие для картинки для поста. С другой стороны, ширину можно поменять, и из комментария читателю становится ясно, что вместо 800 можно указать другую цифру. Не загружайте читателя бессмысленными задачами Читатель по достоинству оценит гайд, который экономит его время. Не перегружайте пользователя утомительными итерациями, когда в результате выполнения сниппета командной строки получается один и тот же результат. ❌ Плохо: Загружать читателя ненужными утомительными действиями Выполните следующие утомительные действия: 1. Запуститеsudo nano /etc/hostname2. Удалите имя хоста3. Введитеawesomecopter4. Нажмите Ctrl + o, чтобы сохранить содержимое5. Нажмите Ctrl + x, чтобы выйти из редактора Из-за таких пунктов руководство становится скучным, а пользователь легко может допустить ошибку. Кто захочет раз за разом вручную редактировать текстовый файл? Лучше покажите читателю сниппет командной строки, который делает то, что ему нужно. ✅Хорошо: Создайте скрипт для неинформативных и скучных действий Вставьте следующую простую команду: echo 'awesomecopter' | sudo tee /etc/hostname Поддерживайте код в рабочем состоянии Некоторые авторы готовят руководства, как будто это инструкции по складыванию оригами: эдакая таинственная последовательность поворотов и изгибов. Пока не доберёшься до конца, эта свёрнутая бумажка не вызывает никаких ассоциаций, и вдруг — о чудо, вот он, прекрасный лебедь! Возможно, любители оригами ценят непредсказуемость, но читателя такой процесс нервирует. Он хочет знать, что всё делает правильно. А для этого код должен быть в рабочем состоянии. ❌ Плохо: Ссылаться на код, который читатель ещё не видел Вот код для примера. Но вы не занимайтесь его компиляцией. В нём не хватает функцииparseOption, ха-ха! // example.c\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n#define MAX_LINE_LENGTH 256\n\nint main() {\n    char line[MAX_LINE_LENGTH];\n    char key[MAX_LINE_LENGTH];\n    char value[MAX_LINE_LENGTH];\n\n    while (fgets(line, sizeof(line), stdin)) {\n      // Don't do this!\n      parseOption(line, key, value); // <<< Not yet defined\n\n      printf(\"Key: '%s', Value: '%s'\\n\", key, value);\n    }\n\n    return 0;\n} Если читатель пытается скомпилировать пример выше, возникает ошибка: Как можно раньше покажите читателю пример, с которым он может работать. Опирайтесь на этот постулат и поддерживайте код в рабочем состоянии. ✅Хорошо: Покажите пример, который пользователь сразу же может повторить, не забегая вперёд // example.c\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n#define MAX_LINE_LENGTH 256\n\nvoid parseOption(char *line, char *key, char *value) {\n    // Fake the parsing part for now.\n    strncpy(key, \"not implemented\", MAX_LINE_LENGTH - 1);\n    strncpy(value, \"not implemented\", MAX_LINE_LENGTH - 1);\n}\n\nint main() {\n    char line[MAX_LINE_LENGTH];\n    char key[MAX_LINE_LENGTH];\n    char value[MAX_LINE_LENGTH];\n\n    while (fgets(line, sizeof(line), stdin)) {\n        parseOption(line, key, value);\n        printf(\"Key: '%s', Value: '%s'\\n\", key, value);\n    }\n\n    return 0;\n} Теперь я протестирую программу, чтобы убедиться, что она работает: $ gcc example.c -o example && \\\n    printf 'volume:25\\npitch:37' | \\\n    ./example\nKey: 'not implemented', Value: 'not implemented'\nKey: 'not implemented', Value: 'not implemented' Пока что в коде используются выдуманные параметры парсинга, но фиктивный код подтверждает, что всё остальное работает. Если код работает, пользователь понимает, правильно ли он выполняет рекомендации из статьи. Ему не нужно беспокоиться, не допустил ли он несущественную ошибку, из-за которой позднее придётся всё перепроверять. Посвятите гайд одной теме Хорошее руководство должно объяснять только одну вещь — и объяснять её хорошо. Распространённая ошибка — заявить конкретную тему гайда, а потом смешать коктейль из разных технологий. ❌ Плохо: Рассказывать одновременно о нескольких не связанных между собой идеях В этом руководстве я покажу вам, как добавить в блог Hugo поиск на стороне клиента, чтобы ваши читатели могли мгновенно выполнять полнотекстовый поиск по всем статьям блога даже при слабом мобильном интернете. Но это ещё не всё! Помимо полнотекстового поиска, я покажу вам, как использовать локальное хранилище браузера для хранения истории пользовательского поиска и как применить дорогой ИИ-сервис для выводов о том, какая тема больше нравится пользователям вашего сайта: тёмная или светлая. В приведённом примере руководство сначала обещает нечто полезное для множества читателей: полнотекстовый поиск по блогу. Сразу вслед за ним в сборную солянку попадают не связанные друг с другом идеи. Теперь читатель, которому нужен полнотекстовый поиск, должен вычленить интересующую его тему из статьи. Люди открывают гайд, потому что их интересует один конкретный вопрос. Предоставьте им возможность получить на него ответ, и всё. ✅Хорошо: Изложить в руководстве одну новую концепцию В этом руководстве я покажу вам, как добавить в блог Hugo поиск на стороне клиента, чтобы ваши читатели могли мгновенно выполнять полнотекстовый поиск по всем статьям блога даже при слабом мобильном интернете. В этом гайде я остановлюсь только на этом вопросе. Если вам нужно сформировать стек технологий, подождите до конца гайда Иногда в руководстве приходится сочетать несколько технологий. Например, у языка веб-программирования PHP нет встроенного рабочего веб-сервера. Чтобы показать, как задеплоить PHP-приложение в веб, вам придётся выбрать какой-то сервер, например: Apache, nginx или Microsoft IIS. Какой бы вариант вы ни выбрали, это оттолкнёт пользователей, которые предпочитают другой сервер. Если вам нужно объединить несколько технологий, отложите это на конец гайда. Если пишете о PHP, постарайтесь как можно дольше продержаться на сервере разработки PHP. Если вы показываете, как задеплоить PHP-приложение в продакшн-среде на nginx, перенесите эти этапы на конец статьи. Тогда все, кто работает с другими серверами, смогут пройтись по этапам руководства до момента, когда вы дойдёте до раздела про серверы. Не приукрашивайте Вот отрывок из статьи, которую я недавно прочитал. Можете догадаться, что это был за гайд? ❌ Плохо <div class=\"flex flex-row mb-4 overflow-hidden bg-white\">\n  <div class=\"flex flex-col w-full p-6 text-light-gray-500\">\n    <div class=\"flex justify-between mb-3\">\n      <span class=\"uppercase\">{{ title }}</span>\n    </div>\n    <slot></slot>\n  </div>\n</div> Думаете, это статья о фреймворке CSS? А вот и нет. Этот сниппет — из руководства об использовании элемента <slot> в веб-фреймворке Vue. Так почему же половина кода приходится на классы CSS? Автор добавил их, чтобы пример смотрелся привлекательнее. Вот тот же сниппет, в котором мы оставим только код, необходимый для объяснения главного понятия: ✅Хорошо: Придерживайтесь простых демонстрационных UI, не зависящих от стиля <div class=\"card\">\n  <p class=\"card-title\">{{ title }}</p>\n  <slot></slot>\n</div> Да, из этого упрощённого кода не получится красивая картинка, которая хорошо смотрится в браузере. Но это и не нужно. Зато таким образом можно доходчиво рассказать об элементе <slot>, не отвлекая читателя на посторонние моменты. Читателям неважно, красиво ли смотрится приложение, которое вы приводите для примера. Им нужен гайд, который однозначно объясняет новое понятие. Сократите зависимости по максимуму В каждом руководстве попадаются зависимости. Как минимум, читателю нужна операционная система. Скорее всего, для выполнения предложенных примеров ему ещё понадобится конкретный компилятор, библиотека или фреймворк. Каждая зависимость добавляет читателю работы. Ему нужно разобраться, как установить и настроить всё это в системе, а это снижает шансы пройти руководство до конца. Упростите руководство, максимально сократив число необходимых зависимостей. ❌ Плохо: Удивить читателя зависимостями, которые трудно установить Итак, мы добрались до пункта 12. Самое время установить ещё букет раздражающих вас программ, о которых я не упомянул раньше: • Ffmpeg вместе с расширением libpita (скомпилированные двоичные файлы недоступны);• специальная ветка Node.js, которую мой друг Марк Душнилз опубликовал в 2010 (для её компиляции понадобится Ubuntu 6.06);• Perl 4. Самые распространённые и фривольные зависимости, которые я вижу, — это библиотеки для парсинга дат. Вам тоже встречались подобные инструкции? ❌ Плохо: Добавлять сторонние зависимости для решения тривиальных задач CSV-файл содержит даты в форматеYYYY-MM-DD. Чтобы спарсить его, установите эту библиотеку на 400 МБ, предназначенную для парсинга любой строки данных в любом формате на любом языке и с любыми региональными настройками. На самом деле, вам совершенно не нужна целая сторонняя библиотека, чтобы спарсить простую строку данных в коде, который мы берём для примера. В худшем случае вы можете спарсить её самостоятельно пятью строчками кода. Мало того, что со сторонними инструментами сложнее следовать инструкциям в гайде, так ещё и каждая зависимость сокращает «срок службы» руководства. Возможно, через месяц внешняя библиотека выпустит обновление, из-за которого ваш код перестанет работать. Или издатель снимет библиотеку с публикации, и тогда ваш гайд станет вообще бесполезным. Полностью избавиться от зависимостей не удастся, но применять их нужно стратегически. Если в руководстве вы рассказываете, как изменить размер изображения, не стесняйтесь, воспользуйтесь сторонней библиотекой изображений, а не создавайте с нуля декодирование JPEG. Но если вместо зависимости можно написать 20 строк кода, почти всегда лучше обойтись без зависимости. Зафиксируйте определённые версии зависимостей Чётко проговаривайте, какие версии инструментов и библиотек используются у вас в руководстве. Библиотеки издают обновления без обратной совместимости, так что обязательно убедитесь, что пользователь знает, какая версия точно работает. ❌ Плохо: Использовать плохо определённые зависимости Установите стабильную версию Node.js. ✅Хорошо: Однозначно обозначить рабочие версии используемых зависимостей Установите Node.js 22.x. Я тестировал это на Node.js v22.12.0 (LTS). Чётко указывайте имена файлов Больше всего меня бесит, когда автор руководства небрежно указывает: «Добавьте эту строку в файл конфигурации». В какой именно файл конфигурации? Где именно? ❌ Плохо: Давать туманные инструкции о редактировании файла Чтобы включить удаление неиспользуемого кода, добавьте эту настройку в файл конфигурации: optimization: {\n  usedExports: true,\n  minimize: true\n} Если читателю нужно отредактировать файл, дайте ему полный путь к файлу и точно покажите, какую строку отредактировать. Указать имя файла можно множеством способов: в комментариях к коду, в заголовке или даже в предыдущем абзаце. Сгодится любой метод, который однозначно показывает пользователю, где вносить изменения. ✅Хорошо: Конкретно указать, какой файл редактировать и где в нём будут изменения Чтобы включить удаление неиспользуемого кода, добавьте следующий параметр оптимизации в module.exports файла конфигурации webpack: // frontend/webpack.config.js\n\nmodule.exports = {\n  mode: \"production\",\n  entry: \"./index.js\",\n  output: {\n    filename: \"bundle.js\",\n  },\n  // Enable tree-shaking to remove unused code.\n  optimization: {\n    usedExports: true,\n    minimize: true,\n  },\n}; Используйте единообразные содержательные заголовки Большинство читателей сначала бегло просматривают гайд, чтобы определить, стоит ли читать его внимательно. Такое чтение по верхам помогает читателю решить, найдёт ли он в гайде ответы на свои вопросы и трудно ли будет выполнить описанные в нём действия. Если убрать заголовки, перед читателем расстилается сплошное полотно текста. Заголовки же структурируют статью. Большое руководство со множеством пунктов лучше разбить на несколько разделов, в каждом из которых будет по 5–6 подпунктов. Пишите чёткие заголовки Недостаточно просто разделить длинное полотно текста парой заголовков. Продумайте формулировки заголовков: не жертвуйте их лаконичностью, но постарайтесь вложить в них как можно больше содержательности. Какие из этих руководств вы бы предпочли? Эти? ❌ Плохо 1. Go2. Установка3. Hello, world!4. Деплоймент Или эти? ✅Хорошо 1. Чем хорош Go2. Установите Go 1.233. Создайте простое приложение Hello, World на Go4. Как задеплоить приложение в веб Во втором примере намного больше информации, которая помогает читателю решить, подходит ли ему этот гайд. Сделайте заголовки единообразными Перед публикацией гайда проверьте, точно ли у него единообразные заголовки. ❌ Плохо: Использовать заголовки вразнобой 1. Как я установил Go 1.232. Этап 2: ваше первое приложение3. Как я упаковываю приложения на Go4. Раздел D: как задеплоить приложение Проверяя заголовки, обращайте внимание на следующие моменты: Грамматические формы.Ваши заголовки — это именные или глагольные предложения? Грамматические формы.Ваши заголовки — это именные или глагольные предложения? От чьего лица ведётся повествование.Можно писать от первого лица («Я сделал X»), обращаться к читателю как к собеседнику («Сделайте X») или выбрать нейтральную манеру. От чьего лица ведётся повествование.Можно писать от первого лица («Я сделал X»), обращаться к читателю как к собеседнику («Сделайте X») или выбрать нейтральную манеру. Грамматическое время.Вы пишете в настоящем, прошедшем или будущем времени? Грамматическое время.Вы пишете в настоящем, прошедшем или будущем времени? Создайте с помощью заголовков логическую структуру Убедитесь, что заголовки отражают логическую структуру руководства. Я часто вижу статьи, в которых заголовки смотрятся как бессмысленный набор фраз. ❌ Плохо: Использовать нелогичную структуру заголовков 1. Почему Go1. История nginx2. Конфигурируем локальный доступ в nginx2. Создайте своё первое приложение на Go1. Почему Go лучше, чем Perl3. Поддержка базовой страницы В приведённом примере у заголовка «Почему Go» есть подзаголовок «История nginx», хотя история nginx логически не является подтемой темы про Go. Покажите, что ваше решение работает Если в руководстве объясняется, как установить инструмент или интегрировать несколько компонентов, покажите, как использовать результат. ❌ Плохо: Показать установку и этим ограничиться Наконец, выполните команду, чтобы активировать сервис nginx: sudo systemctl enable nginx Поздравляем! Готово! Я предполагаю, что вы и так знаете, как действовать дальше, так что на этом моя миссия выполнена. Если вы объясняете, как что-то установить, покажите читателю, как работает достигнутый результат. Иногда это элементарное действие, например распечатать строку с версией. Просто покажите, как использовать инструментдля чего-то, чтобы читатель знал, есть ли польза от гайда. ✅Хорошо: Показать читателю, как взаимодействовать с инструментом Наконец, выполните команду, чтобы активировать сервис nginx: sudo systemctl enable nginx Далее перейдите по этому URL-адресу в браузере: •http://localhost/ Если всё работает, вы увидите nginx success page по умолчанию. В следующих разделах я покажу, как заменить веб-страницу nginx по умолчанию и выполнить настройки nginx для решения ваших задач. Увяжите конкретику с комплексным примером Даже если на протяжении всего гайда вы придерживались одной темы, не помешает показать читателю, как всё работает в комплексе. Дайте ему ссылку на репозиторий со всем кодом, который вы показывали в руководстве. В идеале репозиторий лучше устроить в системе непрерывной интеграции вроде CircleCI или GitHub Actions, чтобы было видно, что ваш пример создан в актуальной среде. Бонус: показывайте полный код на каждом этапе Я охотно разбиваю репозиторий на ветки Git, чтобы читатель видел не только конечный результат, но и общее состояние проекта в каждом разделе руководства. Например, в руководстве «Как протестировать PDF-парсер с помощью Nix» я показываю читателю первую собираемую версию репозитория в собственной ветке: https://gitlab.com/mtlynch/fuzz-xpdf/-/tree/01-compile-xpdf https://gitlab.com/mtlynch/fuzz-xpdf/-/tree/01-compile-xpdf В конце руководства даю ссылку на конечный результат: https://gitlab.com/mtlynch/fuzz-xpdf https://gitlab.com/mtlynch/fuzz-xpdf Если размер файлов слишком велик, чтобы показывать их после каждого изменения, приводите ссылки на ветки, чтобы читатель видел, как компоненты руководства работают в комплексе. Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий: Как достичь своих целей в новом году; Как достичь своих целей в новом году; 1C-аналитик: погружение в профессию на практике; 1C-аналитик: погружение в профессию на практике; Основы разработки на Java; Основы разработки на Java; Специалист по информационной безопасности: старт карьеры; Специалист по информационной безопасности: старт карьеры; Системный аналитик: первые шаги к профессии. Системный аналитик: первые шаги к профессии. Или открыть перспективы и получить повышение с профессиональным обучением: Java-разработчик с нуля; Java-разработчик с нуля; Специалист по информационной безопасности: расширенный курс; Специалист по информационной безопасности: расширенный курс; Python-разработчик с нуля; Python-разработчик с нуля; Разработчик на C++; Разработчик на C++; SQL и получение данных. SQL и получение данных. "
  },
  {
    "article_id": "https://habr.com/ru/companies/sportmaster_lab/articles/875036/",
    "title": "Проектировочная документация: практический опыт и проверенные шаблоны",
    "category": "Разное",
    "tags": "документация, системный анализ, аналитик, graphical user interface, проектирование, проектирование систем",
    "text": "Привет! Меня зовут Павел Астахов, я работаю в департаменте системного анализа SM Lab. Сегодня расскажу про проектировочную документацию и её стандартизацию в нашей компании. Причины внедрения стандартизации Причина 1. Сотрудники Департамент системного анализа появился в 2020 году: на тот момент нас было 50 человек в 20 командах; к 2024 году мы сильно разрослись и нас стало уже 260 системных аналитиков, которые трудились в 85 командах. Рост и увеличение масштаба департамента выявили проблемы, которые ранее не были видны и постепенно стали выходить на первый план. В частности, стали появляться вопросы от системных аналитиков(особенно новеньких): «А какие вообще требования к постановке задачи?», «Как я пойму, что моя работа над задачей закончена и я могу передавать её дальше по потоку разработчику и тестировщику?» При запуске новых команд возникали следующие вопросы: «Как нам организовать свое пространство в Confluence?», «Что от нас ждут разработчики и тестировщики?», «А каково текущее актуальное состояние системы?» Причина 2. Компания И тут уже появились вопросы со стороны компании: «Как адаптировать по 60 человек в год так, чтобы ребятам было комфортно вливаться в рабочий процесс?» Более того, увеличение числа сотрудников и команд вызвало увеличение взаимодействия между продуктами. Ребята стали часто наведываться друг к другу в пространство Confluence для того, чтобы почитать как работает та или иная система. И из-за того, что ребята работали в несколько разных подходах, ориентироваться в чужом пространстве было часто затруднительно. А ещё у нас есть сотрудники, которые работают в компании уже более 10 лет и хотят перейти в другой продукт потому, что текущий знают уже досконально и хотят попробовать новую предметку и технологии. Для того, чтобы все три процесса протекали гладко и не занимали много времени, мы решили стандартизировать всю документацию, построив единую систему ведения проектировочной документации, которая отражала бы все сведения, состояния и накопленные знания о всех наших системах на ландшафте компании «Спортмастер». Первые шаги к решению проблемы: аудит и постановка целей Первое, что мы сделали – провели аудит. Мы прошлись по нашим командам и сделали следующие выводы: документация велась правильно там, где были хорошие инженерные практики; документация велась правильно там, где были хорошие инженерные практики; то же самое касаемо структуры (однако в большинстве своём она велась стихийно); то же самое касаемо структуры (однако в большинстве своём она велась стихийно); инструменты для постановки задач использовались разные. В среднем это был либо Confluence, либо Jira, либо их некая смесь. Confluence был для нас предпочтительным вариантом, т.к. в Jira информация может уехать в бэклог, закрыться в задачах и потеряться. Смесь Confluence и Jira, когда ребята вели основную доку в Confluence, а доработки добавляли в Jira, тоже была таким себе вариантом: документация разъезжалась по разным системам, что не позволяло отслеживать её текущее состояние. инструменты для постановки задач использовались разные. В среднем это был либо Confluence, либо Jira, либо их некая смесь. Confluence был для нас предпочтительным вариантом, т.к. в Jira информация может уехать в бэклог, закрыться в задачах и потеряться. Смесь Confluence и Jira, когда ребята вели основную доку в Confluence, а доработки добавляли в Jira, тоже была таким себе вариантом: документация разъезжалась по разным системам, что не позволяло отслеживать её текущее состояние. Проанализировав результаты работы, мы наметили следующие цели: повысить характеристики постановок задач; повысить характеристики постановок задач; выработать систему проектировочной документации; выработать систему проектировочной документации; создать шаблоны. создать шаблоны. Повышение характеристик постановок задач Во-первых, поддержка актуальности документации: она должна отражать реальное состояние системы, которая постоянно дорабатывается и изменяется. Во-вторых, точность и полнота. Системный аналитик должен предоставить полный набор данных о том или ином компоненте системы для того, чтобы все последующие специализации – будь то системный аналитик из другой команды или разработчик, или тестировщик – могли выполнять свои работы без дополнительных уточнений и требований. В-третьих, нашей документацией пользуется широкий круг потребителей: от службы эксплуатации и дизайнеров до владельцев продуктов, поэтому она должна быть доступна и понятна. Проектировочная документация Для работы с шаблонами мы построили систему проектирования нашей документации: связывание документов; связывание документов; изменение документов; изменение документов; раздел описательной аналитики. раздел описательной аналитики. Связывание документов делается для того, чтобы оценить влияние изменений на компоненты наших систем. Перелинковки – неотъемлемая часть процесса. Все необходимые вещи, которые следует уточнить, выполнены в виде гиперссылок: перейдя по ним, вы можете почитать следующую статью –  совсем как в Википедии. Как мы это делаем? Каждый наш документ имеет свой уникальный код. Код формируется с помощью макроса Requirements и состоит из следующих блоков: тип документа, например, АРІ; тип документа, например, АРІ; код системы, например, MPSM; код системы, например, MPSM; мобильное приложение «Спортмастер». мобильное приложение «Спортмастер». Далее идет порядковый номер, который автогенерируется с помощью макроса Requirements. Мы используем эти коды для того, чтобы перелинковыватьсямежду системами. Для того, чтобы проставлять связи между компонентамивнутри системы, мы используем обычные ссылки Confluence, т.к. при использовании макроса Requirements Link серьезно снизило читаемость наших постановок. Изменение документов влияет на такую характеристику, как актуальность и поддерживаемость: наши системы постоянно дорабатываются, регулярно вносятся изменения, и мы выработали для себя несколько способов работы с этими изменениями. Наверное, самый простой и популярный способ, с которым сталкивалось большинство системных аналитиков, – это цвета и зачеркивания. Он для нас является основным(хотя и не самым удобным)как самый простой и доступный. Работает это следующим образом. В постановке задачи изменяемый фрагмент выделяется цветом и зачеркивается. Новые требования дописываются тем же цветом ниже. В историю изменений вносится задача, по которой были выполнены изменения, и её описание. Далее это отправляется в разработку. Разработчик видит ту дельту, которую необходимо внести, видит задачу, по которой необходимо внести изменения. И, собственно, работает с постановкой задачи. Есть и аналогичный способ – это блоки Old и New. Он является аналогом для цветов и зачеркиваний. По сути, это некое тегирование: мы заключаем предыдущую версию требований в блок Old, новую версию требований в блок New, закрашиваем цветом и также вносим в историю изменений. После передаем в разработку. Если в системе мало доработок и эти доработки вносятся в одну функцию в одну единицу времени, мы можем описать все изменения технически в рамках Change Request. Далее запрос передается разработчику, который видит его единым документом – это проще, чем переключаться между разными постановками. И после того, как разработчик внес изменения в код, аналитика переносит данные из Change Request в постановку задач. Структура раздела описательной аналитикивлияет на понятность и доступность. Мы хотели стандартизировать правила размещения документации для того, чтобы ребята, приходя в чужое пространство, могли достаточно быстро навигировать по документации смежной команды и находить ту информацию, которая им необходима. Структура раздела иерархическая. Пройдем по ней снизу вверх. На самом нижнем уровне мы группируем наши документы согласно их функциональному назначению. К примеру, если у нас есть АРІ, то все АPI будут лежать в конкретном разделе Confluence, и если вам необходимо обратиться к АРІ, вы идете в раздел с АРІ. Но не все наши системы являются простыми: есть системы сложные(их на самом большинство), поэтому в случае, если система является сложной, в плоском списке затруднительно вести все АРІ, например, или экранную форму – их очень много, мы разбиваем нашу систему на функциональные блоки. Далее в рамках каждого функционального блока агрегируем информацию о тех компонентах, которые обеспечивают его работу. Например, если у нас есть корзина, то все API, доступные для корзины, будут лежать в разделе корзина, а все экранные формы, присущие разделу заказов, будут лежать в разделе заказов – это упрощает нашу навигацию. Ну и группировка на уровне информационных систем. В ряде случаев наши продуктовые команды ведут не одну информационную систему, а несколько. В этом случае мы на самом верхнем уровне: уже разбиваем пространство на информационные системы, под ними идут функциональные блоки и под функциональными блоками конкретные компоненты. В иерархической структуре можно увидеть и Change Request («запрос на изменения» – это, по сути, некий аналог Jira-задачи, который включает в себя смысл цели изменения, а также состав работ, которые необходимо выполнить для того, чтобы изменения можно было внести в систему). Если их не очень много, они хранятся на уровне информационной системы, если же их достаточно много, мы опускаем запросы под каждый функциональный блок. А теперь перейдем к самому важному – шаблонам. Шаблоны Шаблон – это некий строительный кирпич, из которого мы собираем все описания наших информационных систем. Он обозначает тот набор знаний, который необходимо описать о компоненте и помогает нашим аналитикам ориентироваться в тех знаниях, которыми они должны владеть о системе, выявлять свои пробельные зоны и заполнять их. В каждом шаблоне присутствуют следующие элементы: Содержание. Для содержания используем макрос «Оглавление». Не скажу, что это самый важный общий элемент наших шаблонов, но это скорее дань хорошим практикам ведения технической документации, которая позволяет вновь пришедшему читателю быстро сориентироваться «А тот ли это документ и нужно ли мне в него погружаться более детально?» Содержание. Для содержания используем макрос «Оглавление». Не скажу, что это самый важный общий элемент наших шаблонов, но это скорее дань хорошим практикам ведения технической документации, которая позволяет вновь пришедшему читателю быстро сориентироваться «А тот ли это документ и нужно ли мне в него погружаться более детально?» История изменений. Важная часть наших документов, позволяет работать с изменениями и отслеживать их. Она состоит из ссылки на задачу, цвета правок, даты внесения изменений, их описания и автора. Как только мы видим, что статус задачи изменился на «Закрыт», это значит, что автор задачи или следующий аналитик, который пришел в постановку, должен перекрасить цвета изменений в дефолтный черный цвет, а записи в истории изменений перенести в архив. История изменений. Важная часть наших документов, позволяет работать с изменениями и отслеживать их. Она состоит из ссылки на задачу, цвета правок, даты внесения изменений, их описания и автора. Как только мы видим, что статус задачи изменился на «Закрыт», это значит, что автор задачи или следующий аналитик, который пришел в постановку, должен перекрасить цвета изменений в дефолтный черный цвет, а записи в истории изменений перенести в архив. Информация о документе содержит как раз тот самый уникальный код документа, набор описательных атрибутов, которые этому документу присущи, и они зависят от того типа документа, который мы описываем, а также списка всех потребителей, которые обращаются к этой постановки задачи для того, чтобы отслеживать на кого повлияют изменения, если мы этот документ поправим. Информация о документе содержит как раз тот самый уникальный код документа, набор описательных атрибутов, которые этому документу присущи, и они зависят от того типа документа, который мы описываем, а также списка всех потребителей, которые обращаются к этой постановки задачи для того, чтобы отслеживать на кого повлияют изменения, если мы этот документ поправим. Теперь рассмотрим четыре наиболее популярных шаблона, которые используют 90-95% наших команд. 1. Graphical User Interface (GUI) Graphical User Interface (GUI), или графический интерфейс пользователя. Используется для того, чтобы описывать все экраны, все графические интерфейсы, будь то экран мобильных, декстопных или веб-приложений. Первый раздел – это входные-выходные параметры экранной формы. Они используются в тех случаях, если ваша экранная форма перед вызовом должна какие-то данные подгрузить, инициализироваться; например, вы хотите отредактировать какую-то запись из каталога записей, кликаете на нее и у вас открывается окно редактирования. Для того, чтобы окно редактирования открыло корректную запись, нужно передать ей параметры этой записи и она подгрузит данные. Ну или, например, у вас есть диалоговое окно, которое может вывести разные сообщения и текст на кнопках. Для того, чтобы можно было писать его единожды, а потом вызывать различными параметрами, можно использовать параметры формы. Второй раздел – это экранные формы. Они могут работать в разных режимах и для того, чтобы не плодить сущности, можно всё описать в одной постановке. Например, экранная форма позволяет открывать запись в режиме просмотра или редактирования. Поведение полей и управляющих элементов в этих режимах может отличаться. Для этого мы вводим такой блок, как режимы работы экранной формы, описываем их, и в дальнейшем имеем возможность единожды описать экранный интерфейс и ту логику, которая присуща тому или иному режиму. Третий раздел – это макеты. Мы ведем их в Figma и разбиваем на разделы для того, чтобы можно было в дальнейшем эти разделы описать. И, например, в макете на картинке строка поиска под цифрой 1 является отдельной постановкой задачи с отдельной логикой. В структуре выбранной формы мы сослались на макет, прописав, что единичка – это строка поиска, и в описании приложили ссылку на ту постановку, которую необходимо реализовать. Разработчик может перейти и  посмотреть, что этот компонент уже кем-то ранее реализовывался по определенной задаче, найти его в коде и приложить на нужный экран. Ну и самый главный раздел – это описание элементов экранной формы. В принципе, описываем все достаточно подробно – до полей, как говорится. Все поля экранной формы, все управляющие элементы расписаны и внесены в табличку. Для них описаны источники данных, типы данных, форматы данных, размерности, значения по умолчанию и те действия системы, которые она выполнит при взаимодействии клиента с управляющим элементом. С самым сложным шаблоном закончили. Поговорим об алгоритмах. 2. Algorithm (ALG) Алгоритм – это один из тех шаблонов, который может использоваться в рамках других шаблонов, подтягиваться туда. С помощью него мы описываем интерфейсную логику, например, различные вычисления там. Были случаи, когда ребята описывали сложные вычислительные алгоритмы похожие на некий курсач, для описания логики интеграции и бизнеса которого использовали алгоритмы. Выглядит он следующим образом – это просто текстовое описание в виде иерархического списка. Если алгоритм сложный, то разбиваем эти списки на подразделы. Иногда ребята применяют табличные описания, где для каждого шага используется строка таблицы. Ну и для того, чтобы в некоторых алгоритмах упростить читаемость, мы применяем блок-схемы и диаграммы активности для визуализации алгоритмов. Везде их не вставляем, только там, где решения уже устаканенные и к этим алгоритмам часто обращаются разные потребители. 3. Data definition (DD) С помощью определения данных мы описываем любые объекты хранения данных, будь то, например, бизнес-объекты, табличка «База данных» или некоторые сообщения, которые отправляем в брокеры. Для описания используем следующие атрибуты: наименование атрибута, которое присваиваем мы, обычно вкладывая в него бизнес-смысл. Если нам необходимо добавить наименование атрибута, которое придумал разработчик, у нас в таблице есть раздел физическая таблица, куда можно внести наименование таблицы и наименование соответствующего поля; наименование атрибута, которое присваиваем мы, обычно вкладывая в него бизнес-смысл. Если нам необходимо добавить наименование атрибута, которое придумал разработчик, у нас в таблице есть раздел физическая таблица, куда можно внести наименование таблицы и наименование соответствующего поля; системы и источники, а также алгоритмы заполнения данных, если эти данные откуда-то в таблицу поступают, например, интеграция со внешней системой, вызов API или данные, которые поступают из экранной формы; системы и источники, а также алгоритмы заполнения данных, если эти данные откуда-то в таблицу поступают, например, интеграция со внешней системой, вызов API или данные, которые поступают из экранной формы; описание атрибута, его тип и обязательность и ключи, если это необходимо. описание атрибута, его тип и обязательность и ключи, если это необходимо. Обязательно прописываем логику заполнения таблицы, если эта таблица работает с интеграционными взаимодействиями или экранными формами. 4. Application Programming Interface (API) Вот здесь будем в основном говорить про REST API, т.к. у нас в компании он является стандартом взаимодействия между системами. При описании обязательно определяем заголовки, описываем параметры строки и пути, прописываем для этих параметров наименования, типы, обязательности, даем описания, что это за параметр ну и, соответственно, для тела запроса описываем то же самое, что мы ожидаем на вход и с какими типами параметров. Также описываем ответы, которые ожидаем от нашего интерфейса в зависимости от ситуации, как успешные ответы, так и те, когда что-то пошло не так с добавлением альтернативных сценарииев. В большинстве случаев у нас в теле ответа есть специальный блог для сообщения об ошибках. Но в некоторых случаях, например, для каких-то исключительных ситуаций тело может отключаться, тогда тоже его отдельно опишем у себя в постановке на API. Если необходимо выполнить преобразования данных, указываем, из каких внешних систем можно получить эти данные. Для разработчиков описываем алгоритм формирования ответа. Результаты Перед тем, как проанализировать полученные результаты, пара слов о тех сложностях, с которыми мы столкнулись в первую очередь. Самым сложным было первичное сопротивление от ребят, привыкших работать по определенным подходам. Им, конечно, было сложно адаптироваться, но где-то через месяц-два они привыкли и стали нормально воспринимать те изменения, которые принесли в их команды. Также потребовалось дообучить часть сотрудников, погрузить их в технические аспекты, поскольку не всегда все аналитики описывали тот набор информации о системе, который мы требовали теперь описывать для повышения точности и полноты. Было трудно, но в конце концов мы получили положительную обратную связь: ребята профессионально подросли, им стало проще работать. Ну и, если посмотреть ретроспективно, мы понесли достаточно высокие трудозатраты на разработку этой системы и её внедрение – около года в рабочей группе из 10 человек. Теперь о результатах. Мы провели опрос пилотных команд, и большинство ребят отметили, что качество ведения их документация улучшилось: точность и полнота подросла в 60% случаев, доступность и понятность документации достигли 85%, т.к. стало проще ориентироваться, навигироваться как в своей, так и смежных командах. Хочу отметить, что на картинке со статистикой нет ответа «Все стало очень плохо», но в вопросе такой вариант ответа присутствовал и было радостно видеть, что никто его не выбрал. Планы на будущее Прежде чем делиться планами на будущее, поделимся небольшим бонусом о связывании документов. Мы используем их для создания контекстных диаграмм, которые отображают, откуда ваш продукт получает данные, кто получает их из вашего продукта и, что немаловажно, с помощью каких методов. В дальнейшем мы также хотим попробовать построить и схему ландшафта. Если мы все наши АРІ между собой перевяжем, сможем построить её достаточно быстро и в актуальном состоянии. А также есть идея попробовать использовать LLM для того, чтобы автоматизировать некоторые ежедневные рутины системного аналитика. В частности, ревью постановок. Более того, мы попробовали уже в наши шаблоны АРІ позакидывать и погенерить swagger-файлы и получилось достаточно неплохо, закидывая постановку с полным описанием. В чате GPT можно на выходе получить рабочий swagger-файл. "
  },
  {
    "article_id": "https://habr.com/ru/articles/873846/",
    "title": "Как я замучился с пресейлами и решил создать SaaS сервис себе в помощь",
    "category": "Разное",
    "tags": "Presale.Ninja, saas-сервис, estimation, automation, project management, projects, features, tasks",
    "text": "Хочу поведать свою историю, как большая нагрузка, множество рутинной работы и постоянное отвлечение меня от основной работы, разработки архитектуры софта, толкнули меня на создание системы, которая бы автоматизировала ряд процессов в пресейле новых проектов. Что позволило мне сэкономить в итоге много времени и сил. Скажу пару слов о том, чем я занимаюсь. Более 25 лет я занимаюсь разработкой программных продуктов под WEB. Начинал еще в 90х, как разработчик десктопных приложений. Писал, практически на всем. На разных ЯП, CMS, фреймворках. Перепробовал все что только можно. Искал себя и смысл жизни (себя нашел, а смысл жизни - нет). Меня всегда увлекала инженерная и архитектурная часть, поэтому я хотел докопаться до самой сути вещей, уделяя огромное внимание не технологиям и инновациям, а деталям, логике и здравому смыслу. Лет через 10 после старта, я устал от однотипных задач и непонимания, что я делаю и зачем, не имея возможности влиять на конечный продукт. И ушел в архитектуру. Но к большому сожалению, чудеса бывают только в сказках. Поэтому компании (это, как правило, был либо украинский аутсорс на Запад, либо Западный продукт по прямому контракту) нагружали меня дополнительно всем чем только можно. Мол ты же не разрабатываешь архитектуру по 8 часов в день каждый день, так что будь: тим лидом/тех лидом, системным аналитиком, помоги с кодом той или иной команде, поработай DevOps и самое противное - это, к нам зашел новый клиент на пресейл, поэтому нужно очень быстро оценить объем работы, расписать и заэстимейтить фичи с тасочками (извиняюсь за американизмы, все работа ведется только на английском языке). Вот пресейлы я реально не люблю, так как для того, чтобы объективно оценить проект, нужно создать под него архитектуру. А за архитектуру на этапе дискавери (исследования бизнес требований) никто не хочет платить. Но очень часто бывает так, что в компании вообще нет таких людей, как солюшен архитекторы или системные архитекторы. И тогда пресейл менеджеры бегут к разработчикам и начинают их мучить, чтобы те быстро сделали эстимейты, желательно на вчера. Про чувства программистов писать не буду, уверен вы и так это знаете. В итоге рождаются для клиента такие чудесные коммерческие предложения в Excel таблицах, как \"Форма регистрации на сайте: 40 - 120 часов, $1600 - $4800\". Вы можете это даже в Гугл поиске найти, на англоязычных сайтах. Это реальный пример из жизни. На уточняющий вопрос заказчика, что это вообще значит, менеджеры просто пожимают плечами, а клиент идет дальше искать \"надежду\" и здравомыслие в других компаниях. Вот на пресейлах я и остановлюсь подробнее и расскажу всю боль, и как я решил ее уменьшить. Стоимость создания IT продукта напрямую зависит от времени, затраченного на его создание. А время уже в итоге умножается на рейт компании ($40 - $80), что дает в результате стоимость в деньгах. Поэтому, сосредоточимся на том, как правильно оценить временные затраты. Самый точный метод — это декомпозиция функционала на самые маленькие элементы, которые уже можно достаточно просто оценить. Как это сделать правильно? Первое, что нужно сделать, — это собрать все бизнес-требования клиента и проанализировать их. Залог успеха и правильной оценки заключается в том, чтобы полностью прояснить для себя весь проект. Какова его цель, какие функции должны в нем быть, почему клиент видит реализацию своих идей именно так и т.д. Для того чтобы собрать все это в своей голове, необходимо создать бизнес-диаграмму (workflow), на которой графически изобразить весь функционал, связи между этим функционалом и все алгоритмы действий, которые должны работать в проекте. Этот воркфлоу нужно делать на высшем уровне, без углубления в детали. Это поможет сразу выделить очевидные места, которые можно относительно легко оценить. Например, система регистрации и авторизации, форма обратной связи и т.д. Для более сложных частей проекта придется делать более глубокую декомпозицию, разбивая все на отдельные функции, а функции — на более мелкие задачи. С помощью такого подхода можно будет получить подробную документацию с множеством мелких задач, которые уже можно более или менее точно оценить, дав минимальную и максимальную временную оценку. В таком подходе есть как плюсы, так и минусы. Главныеплюсы— это максимально точная оценка и подробная проектная документация.Минус— большие временные затраты на оценку всех задач, и как следствие - большая стоимость затрат на дискавери сессию, которая может быть не оплачена. А также необходимо наличие специалистов с соответствующей квалификацией. Есть еще ряд трудностей, с которыми сталкиваются разработчики при оценке проекта. Это сохранение и переиспользование оцененных ранее задач. Использование для расчетов Excel или других таблиц, что неудобно для работы и выглядит не слишком серьезно в глазах клиента. Плюс невозможность затем превратить полученный документ в реальный проект, например, в системе управления проектами Jira, Trello или другой. Также непонятно, где хранить наработки в виде диаграмм, схем и другой документации, созданной для задачи. Иногда я встречал, что компании заводят новый проект на пресейл в систему прожект менеджмента, например в Jira. Делают там эстимейты, а потом строят отчеты. Но это отнимает огромное количество времени, и если проект в работу так и не взяли, то он становиться бесполезным грузом. Мне очень часто приходилось сталкиваться с оценкой, как частей проекта и конкретного функционала, так и целых проектов, начиная от невнятной идеи заказчика. Это реально очень сложно и утомительно. Также это осложнялось тем, что компания, где я работал, а вернее все компании, где я работал, пытались повесить на меня кучу лишних обязанностей, которые я не очень люблю. И вместо создания архитектуры для продукта и помощи разработчикам в его создании, и доведения его до продакшена, меня вечно отвлекали на пресейлы. Я много лет реально промучался с этой рутиной, каждый раз выполняя одни и те же действия, как обезьяна, оценивая проекты, фичи и таски для клиентов. И всегда бизнесу это нужно было на вчера. Меня из архитектора превратили в какого-то сейлза, от чего у меня невероятно начинало пригорать. И в какой-то момент я понял, что уже начинаю сдавать морально, и что пора что-то с этим делать. И я стал продумывать, как мне упростить свою жизнь. Вначале я пытался создавать проект в Jira, как я встречал это в других компаниях. Но это отнимало много времени и сил на настройку и структуру проекта. Стало скапливаться много независимых друг с другом проектов, многие из которых не уходили в работу. Это было жутко неудобно. Потом я начал сохранять эстимейты в виде текстовых закладок, потом создавал мелкие файлы и перепробовал много чего еще. Все это было неудобно и не давало гибкости и простоты в быстром переиспользовании уже проделанной работы. И тогда я понял, что придется делать что-то свое, дабы убрать с моих плеч эту рутину. Задачу я себе поставил такую: нужно разбивать бизнес требования на маленькие задачи, которые я могу оценить в часах, как минимальное и максимальное время выполнения. Задачи должно быть легко создавать, сохранять и быстро находить. Из этих задач можно быстро собирать фичи (функционал). Я это видел как редактор, создал фичу и накидал туда готовые таски. При этом мне нужно было, чтобы эта фича уже сама посчитала все эстимейты исходя из тех задач, из которых она состоит. Таким образом я смогу накапливать функционал, который я уже описал и оценил. Следующий момент - это возможность создать новый проект для пресейла нового запроса от клиента. Быстро заполнить его фичами, а если чего-то у меня в базе нет, то создать только это и не более того. И конечно же быстро это сохранить вPDFфайл, с простой, понятной структурой, где будет все описано подробно и понятно для заказчика. Ну а по мере необходимости я буду добавлять уже новый функционал. И я сделал для себя черновой проект на NodeJS и ReactJS, и использовал его для своих целей. Потом понадобилось подключать и других инженеров для оценки времени, так как большие проекты декомпозировать и оценивать в одиночку очень сложно. И для этого пришлось добавлять возможность приглашения конкретного сотрудника, а сам проект сделать онлайн. После чего нужен был экспорт уже готового проекта в какую-нибудь систему прожект менеджмента. Но проект стал усложняться и разрастаться, и в итоге, я решил сделатьSaaS-сервис. Назвал его соответственно -Presale.NinjaЧтобы название проекта говорило само за себя. А нинзя, потому, что это крутой уровень мастерства :-) На самом деле домен .com и все другие были заняты, а нинзя свободен, вот и все дела. Весь функционал полностью бесплатный. Единственное ограничение я сделал на количество итоговых задач в самом проекте. Это 30 задач, которые высчитываются из фич. Для большинства нужд небольших проектов этого достаточно, а фрилансерам так и подавно. А если нужно больше, то можно докупить, стоит это копейки. Для крупных проектов компаний, стоимость создания большого проекта будет обходится в 10 - 20 раз дешевле, чем эстимейтить по старинке. Как это работает? Когда вы только начинаете пользоваться сервисом, вам не уйти от стандартного подхода декомпозиции и оценки задач. Но зато ваши инженеры могут работать не по одному, а сразу командой, создавая в сервисе задачи, которые будут оценены с минимальным и максимальным эстимейтом на выполнение. К задаче можно прикрепить все файлы, связанные с ней. Позднее это пригодится, когда проект нужно будет импортировать в систему управления проектами. Задача (task) — это минимальный элемент, являющийся продуктом нашей декомпозиции. У задачи есть заголовок и описание (title и description). Также, как было сказано выше, к задаче можно добавить файлы, которые к ней относятся. У задачи есть автор. Для быстрого поиска задач можно добавлять к ним различные теги (tags), которые можно создать в разделе тегов. Теги сильно облегчают поиск, когда нужно что-то быстро найти. И, естественно, у задачи есть два поля для эстимейтов в часах — минимальный и максимальный. Теперь, когда у нас есть большое количество мелких и хорошо описанных задач, мы можем создавать фичи (feature). Фича состоит из ряда этих мелких задач. Например, функционал \"Регистрация\". Он включает такие задачи, как создание миграции в базе данных, создание моделей для таблиц, создание контроллеров и API для регистрации и авторизации, формы регистрации и авторизации на фронтенде, дизайн, верстка и т.д. Создание фичи также легко, как и создание задачи, даже проще. Нужно создать фичу, заполнить заголовок и описание, прикрепить теги для быстрого поиска, а затем в разделе билдера (builder) наполнить фичу соответствующими задачами. Это, также, задача инженера. В итоге будет создана фича с описанием, списком задач, автором и суммарным эстимейтом от минимума до максимума. Все это сохраняется в облаке сервиса для компании, которая создала этот функционал. В дальнейшем не нужно будет снова проводить эстимейты того, что уже содержится в базе. Можно все это переиспользовать в новых проектах. Далее нужно создать проект, который может создать сам менеджер по работе с клиентом. Проект также содержит заголовок, описание, возможность добавить теги. После этого можно перейти в билдер проекта и выбрать фичи, которые нужны клиенту для этого проекта. Менеджер может сделать все это самостоятельно, не отвлекая инженеров от их работы. Если окажется, что нужной фичи нет в базе знаний, менеджер может привлечь инженеров, чтобы те помогли создать недостающие задачи и фичи для проекта. Можно приглашать коллег через раздел инвайтов (invite), назначая им соответствующие роли для доступа к определенному функционалу сервиса. После этого можно просмотреть, как будет выглядеть финальный документ в предпросмотре (preview), и сохранить его в форматеPDFдля передачи клиенту. Это будет полноценный документ, содержащий заголовок и описание проекта, список всех фич и задач, а также эстимейты — для фич, задач и для всего проекта. Фича в документе будет отображать сумму всех эстимейтов своих задач. Документ будет содержать заголовки и описания для всех фич и задач в иерархическом виде, что поможет клиенту понять, за что ему предстоит заплатить. Чем более подробно будут декомпозированы задачи с минимальными эстимейтами, тем выше вероятность устранить вопросы клиента о стоимости или сроках. Все будет перед глазами, ясно и очевидно. Это значительно повысит вероятность получения этого заказа. Меня попросили добавить в сервис еще одну возможность. А именно шапку и футер для проекта (Header & footer). Это нужно, чтобы в шапке указать логотип и контакты компании, которая произвела пресейл проекта. А в футере, внизу документа, добавить дополнительную информацию. Например, что это не окончательные расчеты, и все подлежит обсуждению. Что в эстимейты не входит время на прожект менеджмент и тестирование, и т.д. Когда проект будет принят в работу, возникнет вопрос о том, какие фичи и задачи нужно создать для разработчиков и на какую документацию им опираться. Для этого проект можно экспортировать в форматеJSON, а затем импортировать его в систему управления проектами. При экспорте проекта вы получитеZIP-файл, содержащий ваш проект в форматеJSON, а также папки с названиями задач из проекта, в которых будут находиться файлы, относящиеся к соответствующим задачам. Сервис не заставляет вас действовать по определенным шаблонам, и вы можете построить гибкую систему, применяя разные подходы в создании задач и фич, которые будут подходить именно для вашей компании и команды. Все зависит от вашего воображения. Система поиска по тегам и строка поиска по текстам в заголовках и описаниях значительно упростят вашу работу. Вот и все — очень просто и эффективно. В итоге сервис может сохранять всю базу знаний по предыдущим эстимейтам, уменьшить зависимость от инженеров, быстро создавать новые проекты, создавать профессиональную документацию, а также переносить проекты в  систему управления проектами. Лично меня это полностью избавило (почти избавило) от рутинных операций и сократило потери времени. А для компании повысило экспертизу в глазах клиентов. P. S. Был один курьезный случай. Маленькая WEB студия из трех человек сделала через сервис оценку проекта, по запросу заказчика. И заказчик согласился с ними работать. Они были в шоке, как так получилось. Этот заказчик делал запрос также в среднюю аутсорсинговую компанию. И та выдала ему эксел файлик на одну страничку, примерно с такими же эстимейтами, и в таком же виде, как я рассказывал вначале статьи. И заказчик решил, что эта аутсорсинговая компания и есть подвальная студия, а эти ребята из студии - это реальные профессионалы. Как гласит старая мудрость: \"Встречают по одежке\". Всем удачи в ваших начинаниях! Уже после написания статьи я добавил ИИ ассистента, который помогает создать описание для тасок, фич и проектов. Работает очень просто. Нажимаете на кнопку ИИ ассистента и вводите запрос, какое описание нужно создать. Например, \"Создай описание для задачи создания формы регистрации на фронтенде на реакт. Описание должно быть в сжатом виде не более 600 символов.\" После небольшой обработки запроса, в форму описания будет вставлен текст, созданный ИИ. Это очень упрощает жизнь, чтобы из головы не придумывать что-то непонятное. Текст потом можно поправить или опять воспользоваться ИИ ассистентом, переделав запрос. "
  },
  {
    "article_id": "https://habr.com/ru/companies/alfa/articles/873720/",
    "title": "Как сделать BPMN-диаграмму чуточку лучше",
    "category": "Разное",
    "tags": "bpmn, bpmn 2.0, bpmn диаграммы обучение uml, бизнес-процессы, бизнес-анализ, планирование проектов, аналитика, анализ, анализ и проектирование систем",
    "text": "Всем привет! Сегодня хочу затронуть холиварную тему: как сделать диаграмму BPMN немного читабельнее и как избежать логических ошибок. Если вы только начинаете путь в бизнес-аналитике или в системной аналитике, для начала рекомендую ознакомитьсясо статьёй, которая подробно описывает базовые постулаты BPMN. В данной статье будет упоминаться одна очень важная сущность BPMN — токен. Токен— абстракция, которая определяет, на каком этапе находится бизнес‑процесс. Она показывает, какой блок процесса сейчас выполняется.  Мы рассмотрим несколько «проблемных» BPMN‑диаграмм, с которыми я встречался в своей практике, и узнаем, как их можно улучшить (если вы не персонаж с картинки ниже и хотите улучшать процесс, чтобы свести к минимуму негативные последствия). Проблема №1. Использование одного завершающего события На первой схеме не указаны явно варианты завершения событий. При детализации бизнес‑требований вы можете упустить, какие могут быть варианты решений по заявке. Схема усложнит оценку сроков реализации, так как решение может быть небинарным («Одобрено» или «Отказано»). Вместо одного завершающего события лучше использовать несколько с явным указанием результата. Так мы упростим системную аналитику, избавимся от корнер‑кейсов и точнее оценим сроки реализации: Мы доработали схему и выяснили, что сумма кредита может быть частично одобрена. Ну всё, можно звонить клиенту примерно с таким запросом: Проблема №2. Несколько входов/выходов элемента Activity В данном примере три выходящих потока из «Task 2». Ещё нет явного указания, токен должен выходить по всем трём потокам по принципу «И», либо в сторону одного потока по принципу «ИЛИ». Здесь возможно расхождение между «Ожидаемым» и «Реализованным». Вы избежите проблем, если постараетесь использовать шлюзы. Вышеупомянутую схему можно исправить так: В обновлённом варианте явно отображён шлюз, который определяет, что после «Task 2» токен перенаправится по принципу «ИЛИ» только в один из Activity. Мы исключили двоякое чтение схемы и снизили вероятность ошибки при реализации бизнес‑требований. Когда рисуете BPMN‑диаграмму, учитывайте, что зимой медведи спят и не могут напомнить вам про шлюзы — именно от них зависит бизнес‑логика. Проблема №3. Построение диаграммы только с шлюзами  С точки зрения работоспособности такие схемы BPMN — корректные, но читать их сложно из‑за нагромождения элементов. На более разветвлённых BPMN‑диаграммах вы это точно заметите. Вместо нагромождения шлюзов используйте элементы событий, так вы разгрузите BPMN‑схему и улучшите её читабельность. Обновим схему без изменения логики: Я показал небольшой, но полезный для громоздких схем лайфхак, о нём даже Илон Маск узнал совсем недавно. Проблема №4. Постановка задачи на клиента Мне приходилось встречать диаграммы, на которых задачи ставились на внешнего с точки зрения бизнеса клиента, но это не совсем корректно. Мы не можем контролировать действия пользователя, поэтому мы должны расценивать их как стартовый или промежуточный event. Что будет, если клиент заболеет, передумает, уйдёт к конкурентам? Наш процесс (токен) «зависнет», и сотрудники будут бесконечно ожидать звонка. Обновим диаграмму: Теперь мы не ждём, пока клиент сам позвонит нам и сообщит об удобном ему времени визита, мы сами контролируем процесс. В результате действия клиента из шлюза выйдет один из токенов (первый или второй — на рисунке они помечены цифрами), и процесс не зависнет. Если ставить задачи на клиента, токены могут зависнуть, а вы можете навлечь на себя гнев ваших коллег: На одном проекте я оптимизировал схему таким образом и повысил конверсию завершения процесса на 70%. Проблема №5. Нет собирающих шлюзов Без собирающих шлюзов вы рискуете исполнить задачу дважды. В качестве примера — покупка в онлайн‑магазине на бонусные баллы. На схеме мы можем ошибочно дважды списать деньги с клиента, так как за разводящим шлюзом будет два токена. Мы проверим, есть ли товар на складе — проведём оплату, спишем бонусы — и снова проведём оплату. Клиент вряд ли будет рад такому поведению нашей системы, поэтому лучше каждый разводящий шлюз затем собирать — так вы избежите логических ошибок. После доработки диаграммы токен отправляется далее из собирающего шлюза, только когда получит токен со всех входящих в него потоков. Так мы избежим повторного списания денег. Если вы не хотите, чтобы к вашему бизнесу приходили клиенты и требовали вернуть деньги, не забывайте собирающие шлюзы: Проблема №6. Разные собирающие и разводящие шлюзы Эту проблему можно разделить на два подпункта. Причина проблем одна, но бизнес‑результаты могут быть разные. Давайте рассмотрим подробнее оба случая. 6.1. Ошибка отличающихся шлюзов приведёт к многократному исполнению задачи Вернёмся к примеру из предыдущего пункта, где клиент хочет купить товар в интернет‑магазине на бонусные баллы, но в этот раз мы не забудем поставить собирающий шлюз. Нюанс в том, что разводящий шлюз генерирует на выход токен в каждый исходящий поток, а собирающий шлюз в схеме пропускает на выход каждый входящий токен. Результат будет таким же, как в предыдущем разделе — у клиента дважды спишутся деньги. 6.2. Из‑за ошибки отличающихся шлюзов токен зависнет, и задача никогда не исполнится Теперь клиент решил купить товар без бонусных баллов. В этой версии схемы два собирающих шлюза, но проблема в том, что разводящий шлюз генерирует на выход только один токен в исходящий поток, а собирающий шлюз ожидает токен на вход каждого потока. Так как этого не происходит, токен зависнет на собирающем шлюзе, и деньги не спишутся. В BPMN множество различных шлюзов, и нужно внимательно следить за типом разводящих и собирающих шлюзов — как правило, они должны быть одинаковыми. В исправленном варианте схемы собирающий и разводящий шлюзы — идентичные, и всё работает как надо. Никогда не путайте шлюзы (и детей с кошками тоже): Проблема №7. Отсутствие дефолтного флоу Использовать инклюзивный шлюз без дефолт‑флоу не самое правильное решение. На схеме на разводящем шлюзе проверяются условия исходящих из шлюза потоков. Для всех истинных исходящих потоков появляется новый токен (на выход могут быть созданы несколько токенов или только один токен). Но может получиться так, что желаемый товар отсутствует, и у клиента день рождения не сегодня — в этом случае токен зависнет. Исправить такой корнер‑кейс можно при помощи дефолт‑флоу — это выходящий из шлюза поток, по которому пойдёт процесс, если не выполняется ни одно из условий на выходе из шлюза. Это решение спасёт процесс от падения с ошибкой. В исправленной диаграмме в случае, если у человека не день рождения и товара нет, BPMN‑схемой предусмотрена отправка промокода. А, например, если клиент делает заказ не в день рождения и товар в наличии — мы просто сделаем доставку товара. Если забывать ставить дефолт-флоу, ваш токен будет не динозавриком из примеров в статье. Он зависнет и будет выглядеть примерно так:  Что в итоге В статье мы рассмотрели несколько частых ошибок, которые я встречал в BPMN‑диаграммах. Мы разобрали на конкретных примерах, к чему приводят такие ошибки. Иногда это гарантированный конфликт с клиентом. Теперь вы знаете, как решить вопрос, добавив в схему отправку промокода, как показано в примере 7. Иногда это простой персонала, как при постановке задач на клиента в примере 4. Иногда ошибки BPMN‑диаграмм блокируют платёж, как во втором примере проблемы № 6. Общий итог один: ошибки и неточности в BPMN‑диаграммах негативно влияют на бизнес. Расскажите в комментариях, с какими ошибками вы встречались в BPMN‑диаграммах? К каким последствиям и бизнес‑результатам приводили эти ошибки? "
  },
  {
    "article_id": "https://habr.com/ru/companies/rostelecom/articles/873452/",
    "title": "Вредные советы начинающему аналитику",
    "category": "Разное",
    "tags": "личный опыт, аналитика, системный анализ, бизнес-анализ, новичкам",
    "text": "Вступление Приветствую, дорогие читатели. В этой статье хочу поделиться множеством «вредных советов», которые покажутся не только забавными, но и крайне полезными для начинающих аналитиков и технических писателей. Эти советы основаны на личном опыте и на опыте коллег в начале карьерного пути. Надеюсь, что мои размышления помогут вам избежать этих ошибок. Как показывает практика, начинающие специалисты, как и я когда-то, сталкиваются с типичными подводными камнями, которые затрудняют успешное выполнение задач. Если у вас есть свои «вредные советы», которые не попали в этот список, пожалуйста, поделитесь в комментариях к публикации. Давайте создавать нашу копилку знаний, делясь опытом и находками друг с другом. Список вредных советов Верь на слово коллегам и пользователям. Они опытнее. Зачастую они проработали в этой компании много лет и знают лучше, как должна работать та или иная фича. Поэтому не углубляйтесь, сэкономьте время и пишите сразу. В начале работы аналитиком на одной из встреч я начал интервьюировать стейкхолдера для уточнения требований по задаче, которую необходимо было реализовать. Интервью сразу пошло не по плану, и сотрудник очень убедительно начал рассказывать, как реагирует система на те или иные действия пользователя и что система должна уметь. Воодушевленный сразу переложил полученную информацию в ТЗ. Как итог потратил свое время, описывая что-то неведомое, потратил время своего куратора, с которым мы проговорили про ограничения системы, проверили на соответствие предложенное решение. В общем, показал себя не с лучшей стороны. Опиши все что только можешь в ТЗ: и текущую работу системы, и новые фичи. Так разработчик сразу поймет, что именно нужно сделать. На доработках посложнее испугался, что разработчик не уловит контекст и не поймет, что именно нужно дорабатывать. Решил описать как сейчас документ доходит до своего текущего состояния и что для этого необходимо сделать пользователю. Документ получился объемным и совершенно бестолковым: вопросов к такому ТЗ было куда больше. Не структурируй документ. Иначе читатель не будет перечитывать его многократно, вследствие чего, не вникнет в суть доработки. К тому же лишишь его приятного чтения. Помни, что твой документ – это задание на разработку, а не художественный шедевр. Личного примера у меня нет, но такие документы с простыней текста одним абзацем попадались в руки. Используй сложносочиненные, сложноподчиненные предложения с множеством причастных и деепричастных оборотов. Ты в первую очередь писатель. Аналогичная подсказка. На самом деле технический документ должен быть понятным и точным. Старайся сконцентрироваться на результате доработки, а не на Пулитцеровской премии для своего документа. Разбивай сложные мысли на более простые. Не используй макеты, схемы и таблицы. Они будут только мешать. Лучше напиши, что новое поле должно находиться между полями 2 и 4, слева, с неймингом в две строки. Чем больше текста, тем круче специалист. Почему-то не для всех это очевидный момент и часто некоторые новые сотрудники буквально все пытаются описать текстом. Зачем читать два лишних абзаца, если можно взглянуть на информативный макет с новым полем? Зачем читать два лишних абзаца, если можно оформить атрибуты в удобную для восприятия таблицу? Не бойся использовать эти инструменты, визуализация помогает лучше понять информацию Форматирование? Это не курсовая работа. Оставь это студентам и школьникам с рефератами. Мы уже на серьезном уровне. Считаю, что форматирование документа показывает отношение к выполненной работе. С большой долей вероятности, в твоей компании/команде есть определенные требования к документации и к тому как она должна выглядеть. Изучи их и приведи документ к утвержденному виду. Оффтоп. Когда мне нужно объяснить человеку далекому от ИТ отрасли чем занимается системный аналитик, то привожу в пример курсовые работы, которые дают общее представление как выглядят зафиксированные требования в ТЗ к разработке. Не номеруй макеты и таблицы. Номера придется актуализировать, а читатель и так поймет про что конкретно ты говоришь. Близкий пункт к предыдущему и касается удобства восприятия и написания. К нумерованным таблицам проще ссылаться в тексте. К тому же, это значительно упростит процесс коммуникации между членами команды, при обсуждении конкретной части документа. Сравни: - Радик, у тебя в таблице номер 9 какие-то странные наименования параметра в строке 4 - Радик, у тебя на странице 15, в той большой таблице какие-то странные наименования параметра на 4 или 5ой строке, та что под ID Не оформляй перечисления нескольких элементов списком. Зачем увеличивать в объеме текст, который умещается в три строки? Список воспринимается легче. Ты ничего не упустишь, разработчик ничего не упустит, тестировщик ничего не упустит. Взял себе за правило оформлять элементы в виде списка, если их больше одного. Не заморачивайся над использованием одних и тех же понятий. «Документ», «Сущность», «Объект» - в целом все одно и то же. Разберутся. При написании технической документации всегда помни, что она не художественная. Техническая документация должна быть ясной и последовательной. Употребление одних и тех же терминов в одном и том же контексте поможет избежать путаницы и повысит качество твоей работы. Для того чтобы показать на сколько богат лексикон пишите статьи на Хабре Если в процессе разработки ты внес какие-то правки в документ, то не надо говорить об этом тестировщику (или другому участнику доработки). Он должен сам всегда находить актуальную версию документа и видеть изменения. Пожалейте время коллег. В своей работе забывал, что тестировщики работают параллельно с разработкой, и сжигал их время в трубу. Прости, Влад! Никогда не задавай вопросов коллегам. Они посчитают, что ты глупый и ничего не понимаешь. Универсальный совет, для всех сотрудников. Если в голове возникли вопросы и не смог на них найти ответ – задай их коллеге. Товарищ направит тебя в нужную сторону, скинет руководство, объяснит. На самом деле, задавать вопросы — нормально. Ненормально, если ты из раза в раз задаешь один и тот же вопрос. Не продумывай отрицательные пользовательские сценарии. Только положительные: это проблема пользователя, что он может нажать не туда и сломать процесс Зачастую проработка отрицательных сценариев поможет тебе продумать реализацию задачи до мельчайших подробностей и сделать конечный продукт готовым и качественным. Не участвуй в обсуждениях и митингах. Чем меньше взаимодействий, тем лучше. Отказ от участия в командных обсуждениях может привести к тому, что ты упустишь важную информацию и идеи, которые родятся во время общения. Командная работа и обсуждение - это важные компоненты успешного развития продукта. Не бойся делиться своими мыслями и задавать вопросы, твое мнение обязательно окажет влияние на конечный результат. Оставь документацию на потом. Лучше решить текущие задачи, а на документы всегда найдется время. Часто это приводит к неразберихе и трудностям в будущем. Зачастую после релиза доработки ко мне приходили пользователи или коллеги с вопросами «Как это работает?» / «Как это должно работать?». Куда проще отправить ссылку на актуальное ТЗ или руководство пользователя. А еще ты можешь быть в отпуске, а кроме тебя никто не знает про доработку. Поверь, этот совет поможет не только тебе, но и всей команде. Не думай о том, как можно автоматизировать рутинные задачи. Повторяющаяся работа только укрепит твои навыки. Это стереотип, который на самом деле снижает продуктивность. Научись выявлять рутинные процессы и ищи способы их автоматизации. Во-первых, это высвобождает время, во-вторых, это крутой навык, которым можно будет поделиться. Считай, что твои идеи всегда самые лучшие. Не слушай критику. Игнорирование мнений других может привести к созданию неудачных решений. Обратная связь от твоих коллег, пользователей и других сторон может очень сильно помочь. Главное соблюдать баланс, когда критика конструктивная, а когда ты получаешь ушат токсичного монолога. Не бойся принимать критику конструктивно и использовать ее для своего профессионального роста. Итоги В этой статье собрал ряд «вредных советов», которые, к сожалению, могут поджидать каждого начинающего аналитика или технического писателя. Избегайте этих ошибок, и ваш путь к успешной карьере станет заметно легче. Помните, что наработка навыков и осознание важности сотрудничества и анализа — ключ к успехам. Каждый из приведенных советов основан на практическом опыте, поэтому старайтесь критически осмысливать, что может помочь вам в работе. Давайте учиться на ошибках друг друга.  *все изображения сформированы при помощи нейросети Flux "
  },
  {
    "article_id": "https://habr.com/ru/articles/872954/",
    "title": "Вопросно-ответные системы в области кода: часть 1",
    "category": "Разное",
    "tags": "Документация, вопросно-ответные системы, код, программирование, gpt, чат",
    "text": "Всем привет, желаю вам хорошего дня и настроения Было ли когда-то у вас желание получить документацию к своему проекту в пару кликов? У меня — регулярно. Жила была проблема, которая преследует меня с начала жизни — я понятия не имею, что происходит и зачем тут написано так много букв которые делают какие-то умные штуки. И я не только про код, с квантовой физикой да и с жизнью в целом такая же проблема. Контекст, а не реклама Так уж получилось, что большую часть жизни я в итоге занимаюсь кодом и после прорыва LLM, первая вещь которую мы попытались сделать — документацию к коду. Простая идея: жмешь 2 кнопки и у тебя готова документация к проекту. Собственноhttps://nextdocs.ai/пытается это сделать. Тут пример готовой документации для: фронтовой библиотекиhtml2pdf, тут мы задокументировалиDocker Compose, а это проект про который дальше пойдет речь —codeqai. Но самое забавное, что мы поняли во время разработки, что мы сделали инструмент для исследования кода, и ничто не мешает нам, использовать проект как инструмент для описания вообще любого неизвестного нам кода. Проще говоря, перед вами поставили задачу, и вы, прежде всего вы пойдете искать уже готовые решения, тут и происходят регулярные проблемы связанные с тем, что даже если на словах проект решает обозначенную проблему, запустили и проверили проект — тоже решает заявленную проблему, но чего-то не хватает. Количество усилий которые нужно потртаить на исправления одной мелочи иногда не соразмерно больше чем написать с нуля, хотя основная проблема именно в понимании. Теперь, мы можем к любой заинтересовавшей нас библиотеке создать документацию за пару кликов и 5 минут ожидания, но что не мало важно, теперь у нас больше контекста для кода. Можете думать об этом как об обучении большой языковой моделью маленькую с целью решение более узкой и специфичной задачи более эффективным и дешевым способом.Да, описание проекта, это хорошо, но как правило, угадать, что конкретно в определенный момент времени хочет человек от проекта довольно сложно, хоть и стараемся это сделать, более простой вариант — дать пользователю возможность спросить о коде используя контекст который у нас есть. codeqai Исходный код доступен поссылке. Дальше разберем, как codeqai устроен внутри CodeQAI использует Python >=3.9,<3.12 как основной язык. Для того, чтобы обрабатывать код на различных языках, он использует библиотекуTreesitter.Тутобщая документация, атутдля python. Treesitter- это инструмент для создания парсеров и библиотека инкрементального парсинга, простой пример, где вы могли встречаться с результатом его работы — подсветка синтаксиса.Статьяс примерами на эту тему, с визуальным объяснением того, как tree-sitter уже влияет на нашу жизнь. Из коробки Treesitter поддерживает: Haskell Haskell JavaScript (Node.js) JavaScript (Node.js) JavaScript (Wasm) JavaScript (Wasm) Lua Lua OCaml OCaml Python Python Ruby Ruby Rust Rust Swift Swift Kotlin Kotlin Java Java Также библиотеки, которые используются в codeqai: LangChain— это фреймворк для разработки приложений на основе больших языковых моделей (LLM). Благодаря этому фреймворку, можно работать с большим количеством различных языковых моделей. Например OpenAI, Azure, Google, Anthropic и другие.Streamlit— это библиотека Python для построения интерактивных веб-приложений, управляемых данными. Она используется для создания приложения, которое позволяет более удобно общаться с ботом. Запускается так: FAISS— это библиотека для эффективного поиска сходства и кластеризации плотных векторов. В данном случае, для codeqai, FAISS выступает в роли базы данных для векторов, то есть локально хранит вектора, которые извлек treesitter Несмотря на большую поддержку различных языков, моделей удаленных чатов и т.п. CodeQAI поддерживает далеко не все, а именно:OpenAI, Azure OpenAI и Anthropic удаленные модели, а также такие языки программирования как: Python, Typescript, Javascript, Java, Rust, Kotlin, Go, C++, C, C#, Ruby. Этапы работы 1. Сперва весь репозиторий разбивается на векторы с помощью treesitter. 2. Далее результаты разбития сохраняются локально в векторную базу данных FAISS с использованием либо трансформаторов предложений, либо инструкторов-эмбеддингов, либо OpenAI's text-embedding-ada-002. Все это позволяет преобразовывать текст в векторные представления, для использования в приложениях, как поиск, рекомендации и кластеризация. 3. Вся база будет сохранена в файловой системе компьютера и при следующем запуске, будет подгружаться. 4. Для общения можно использовать локальные варианты LLM, например llama.cpp или Ollama, а также удаленные модели чата, которые перечислялись выше. 5. Чтобы синхронизировать изменения в репозитории, codeqai сохраняет хэши коммитов, если данные устарели (хэши не совпадают), то они удаляются и создаются заново с обновленной информацией. 6. Если используется локальный контейнер Ollama, то его нужно запустить заранее на порту 11434 Можно сказать, что необходимые ресурсы сильно зависят от той технологии, что будет использована в конечном итоге. Например, можно локально разворачивать llama модель, которая есть в открытом доступе и спокойно работает без вложений (но не все версии). То есть, в таком случае, нужна только машина достаточной мощности, которая сможет все это обрабатывать. Если же использовать платные модели для чата, то тут все зависит от объема и сложности запросов, к платной модели. Одна из машин на которой мы тестировали:Intel(R) Xeon(R) CPU E5-2640 0 @ 2.50GHz   2.50 GHz; 16 gb ddr3; RTX 2060 Superможно развернуть локальную модель, а также запустить codeqai. Пример чата Тестировали на проекте, который сам публикует видео в TikTok, да, мы ленивые, а про остальное лучше не спрашивать. В общем, мы проверили — работает. Но нам не подходит, по нескольким причинам: Потребленние оперативной памяти, 1.6 GB только для небольшого проекта и весь индекс будет занимать оперативной памяти. Мягко говоря на сервер в 64 RAM много проектов не поместится. Потребленние оперативной памяти, 1.6 GB только для небольшого проекта и весь индекс будет занимать оперативной памяти. Мягко говоря на сервер в 64 RAM много проектов не поместится. Блокирующая система индексации, очень долго, мы писали свой парсер кода и приложили усилия к тому, чтобы этот процесс был эффективным и быстрым, хотим продолжить в том же духе, тут нам прийдется и долго ждать и много отдать памяти. Блокирующая система индексации, очень долго, мы писали свой парсер кода и приложили усилия к тому, чтобы этот процесс был эффективным и быстрым, хотим продолжить в том же духе, тут нам прийдется и долго ждать и много отдать памяти. Код внутри не плохой, но сильно \"про код\", да, мы делали MVP основываясь только на коде, но есть огромное желание эксперементировать с дополнительным контекстом: существующая документация, таск-трекеры, вообщем все что может ответить на вопрос \"почему\" тут эта функция, а не только на вопрос \"что делает\".Вотзапускают подкаст на основе GitHub репозитория, идея не плохая, сам бы с радостью на пробежке слушал бы про новые коммиты или ядро Linux, но пока рано, к чему я — хочется гибкости, хочется иметь возможность эксперементировать, чтобы работало как конструктор. Код внутри не плохой, но сильно \"про код\", да, мы делали MVP основываясь только на коде, но есть огромное желание эксперементировать с дополнительным контекстом: существующая документация, таск-трекеры, вообщем все что может ответить на вопрос \"почему\" тут эта функция, а не только на вопрос \"что делает\".Вотзапускают подкаст на основе GitHub репозитория, идея не плохая, сам бы с радостью на пробежке слушал бы про новые коммиты или ядро Linux, но пока рано, к чему я — хочется гибкости, хочется иметь возможность эксперементировать, чтобы работало как конструктор. Личные выводы, codeqai может быть хорошим для локального и собственного использования, но нас куда сильнее привлект проектtextaiдавая ту гибкость которую бы мы хотели получить от семантического поиска.Про textai речь пойдет в следующей статье. "
  },
  {
    "article_id": "https://habr.com/ru/articles/872298/",
    "title": "Как мы модернизировали «мешалку» для пульпы",
    "category": "Разное",
    "tags": "реверс-инжиниринг, перемешивающее устройство, пульпа, конструкторское бюро, разработка кд, расчёты, инжиниринг, глинозёмное производство",
    "text": "Мы отправляемся на крупнейший в России глинозёмный комбинат. Здесь из бокситов получают глинозём - важный сырьевой компонент для производства алюминия. Это место, где промышленность соединяет науку и технологии, превращая сырьё в основу для будущих высокотехнологичных изделий. Классическое для таких мест фото: живые растения, брэндированный календарик с девчонками и объявление об обязательном ношении масок (проект реализовывался во времена \"ковида\"). Настоящий производственный шик! Одним из ключевых элементов оборудования такого производства является устройство для перемешивания пульпы, которое и подверглось модернизации с нашим участием. Давайте разберёмся, что это такое и зачем оно нужно. Статью писал инженер-конструктор с претензией на SMM-щика, а не химик-технолог, поэтому в части названий растворов и химических процессов, протекающих в них, могут содержаться досадные оплошности. Пульпа - это суспензия, состоящая из глинозёмной руды, воды и химических реагентов.  Она является важным этапом в процессе производства глинозёма по так называемомуБайеровскому методу.  По сути, это \"коктейль\", который никто не закажет в баре, но который обязателен на заводе. В этом \"коктейле\" происходят химические реакции, поэтому важно обеспечить его равномерное перемешивание. Процесс переработки бокситов требует высокой точности и однородности. Если пульпа недостаточно перемешана, это может привести к неравномерному распределению реагентов, снижению эффективности реакций, увеличению отходов производства. Устройство представляет собой бочку, диаметром окололо 4...5 метров, в которой установлена мешалка в виде вращающегося вала с траверсами. На траверсах закрепелены цепи. Сверху на бочке стоит тихоходный мотор-редуктор, который через карданный шарнир приводит мешалку в движение. Особенность конструкции в том, что траверсы и цепи со временем обрастают значительным количеством отложений. Остановка оборудования для чистки производится раз в шесть месяцев, т.е. примерно, как генеральная уборка у холостяка в квартире. Это значит, что элементы конструкции и привод должны быть рассчитаны на долговременную работу в таких суровых условиях. Никаких подшипников качения и манжет в конструкции не предусмотрено, т.к. они попросту не выживут. Ниже приведена фотография сосуда с мешалкой (слева), а также мотор-редуктор на 55 кВт, стоящий на крышке этого самого сосуда (справа). Стабильная работа таких устройств напрямую влияет на экономическую эффективность предприятия. Чем выше качество перемешивания, тем меньше потерь, выше производительность и, соответственно, конкурентоспособность продукции. Умные химики-технологи провели некоторые реологические расчёты на компьютере и высказали гипотезу о том, как нужно поменять конструкцию траверс и цепей, чтобы увеличить эффективность процесса. Нам, простым инженерам, остаётся лишь одно:не облажатьсявыполнить реверс-инжиниринг существующего оборудования и разработать чертежи на обновлённую мешалку с учётом данных от химиков-технологов. Отправка конструкторов на большой производственный объект в первую очередь тянет за собой кучу бюрократических процедур: согласование дат поездки с принимающей стороной; согласование дат поездки с принимающей стороной; сдача тестов на \"ковид\"; сдача тестов на \"ковид\"; уточнение требований к спецодежде (спойлер: ничего яркого и красивого); уточнение требований к спецодежде (спойлер: ничего яркого и красивого); подготовка необходимого для работы инструмента; подготовка необходимого для работы инструмента; согласование порядка заноса и выноса инструмента на территорию; согласование порядка заноса и выноса инструмента на территорию; гостиница и перелёт; гостиница и перелёт; вводный инструктаж по безопасности; вводный инструктаж по безопасности; подготовка протокола по результатам поездки, согласование его с заказчиком. подготовка протокола по результатам поездки, согласование его с заказчиком. Какие-то старые бумажные чертежи нам удалось разыскать на самом предприятии. Размеры с существующего прототипа снимали рулеткой. Насколько работа конструктора может быть тёмной и грязной, можно оценить по фото ниже. Также уточнили марку существующего мотор-редуктора. Померяли ток на включенных мешалках, чтобы оценить запас мощности. Ну а далее - дело техники. Строим 3D модель и согласуем технические решения с заказчиком. Расчёты на прочность и выпуск КД Усложнение конструкции траверс с цепями поставило под вопрос прочность карданнного шарнира, который соединяет вал с мотор редуктором, поэтому было принято решение проверить его на прочность. Расчёт выполнялся с применением МКЭ (метода конечных элементов). В качестве максимальной нагрузки был принят номинальный момент, который выдаёт мотор-редуктора, а он в данном механизме составляет внушительные 30 кНм. Для протокола отметим, что, несмотря на простоту конструкции, наши конструкторы выпустили 77 листов чертежей по ЕСКД, пояснительую записку, выписку о расчётах на прочность. Как всегда: проект вроде бы не большой, а нюансов столько, что инженеры начинают думать, не проще ли было стать баристой. FAQ Чем в итоге закончилаь модернизация?В данном случае мы выступаем внешним конструкторским бюро для завода, а после нас туда ещё \"зайдёт\" изготовитель и монтажная организация. А потом завод сам будет проводить испытания и делать заключения. Поэтому мы не можем знать, как в итоге изменились показатели по выпуску глинозёма после запуска новой мешалки в работу. Это - закрытая информация. Чем в итоге закончилаь модернизация?В данном случае мы выступаем внешним конструкторским бюро для завода, а после нас туда ещё \"зайдёт\" изготовитель и монтажная организация. А потом завод сам будет проводить испытания и делать заключения. Поэтому мы не можем знать, как в итоге изменились показатели по выпуску глинозёма после запуска новой мешалки в работу. Это - закрытая информация. А что поменяли в конструкции?Увы, как и всегда, мы должны соблюдать коммерческую тайну. Дать больше конкретики по поводу фактических изменений мы не можем. А что поменяли в конструкции?Увы, как и всегда, мы должны соблюдать коммерческую тайну. Дать больше конкретики по поводу фактических изменений мы не можем. Да кто вы такие, чёрт побери? Почему шифруетесь?Мы - частное конструкторское бюро. Реклама на Хабре на бесплатных аккаунтах запрещена, поэтому если кому-то нужны наши контакты, то они есть в нашем профиле. А ещё про нас можно почитать в статье, которую недавно опубликовала компания Асконна Хабре. Да кто вы такие, чёрт побери? Почему шифруетесь?Мы - частное конструкторское бюро. Реклама на Хабре на бесплатных аккаунтах запрещена, поэтому если кому-то нужны наши контакты, то они есть в нашем профиле. А ещё про нас можно почитать в статье, которую недавно опубликовала компания Асконна Хабре. "
  },
  {
    "article_id": "https://habr.com/ru/articles/878620/",
    "title": "Заговор разработчиков против корпораций: архитектура и принципы",
    "category": "Дизайн",
    "tags": "web разработка, oop, функциональное программирование, solid, dry, system design, архитектура, clojure, совершенный код, монорепозиторий",
    "text": "С момента написанияпредыдущей статьия находился под пристальным вниманием. Попытка опубликовать материалы на англоязычных платформах обернулась фиаско — в первые же минуты легионы последователей тайного братства обрушились с критикой: — Нет никакой организации! — вопили они. Подозреваю, что слежка велась через мойтелеграм-канал. Тем не менее я жив, а значит, пора поведать об архитектурной подлости неимоверных масштабов. Вы узнаете, как ведется борьба с крупными корпорациями изнутри и снаружи, как умы разработчиков заражают деструктивными идеями в обертке сакральных истин. Тактика работы с архитектурой Чего бы нам хотелось получить от архитектуры проекта глобально? Предельно упрощая, можно выделить 2 требования: Хотим соблюдатьSLA(время ответа сервиса, RPS, минимальный latency, uptime и т.п.) Хотим соблюдатьSLA(время ответа сервиса, RPS, минимальный latency, uptime и т.п.) Хотим, чтобы фичи быстро релизились (Time to market,TTM). Хотим, чтобы фичи быстро релизились (Time to market,TTM). Остальные хотелки — производные. Например, хочется, чтобы сложность кодовой базы росла как можно медленнее. Но зачем? Чтобы сократитьTTM. Хочется, чтобы решения масштабировались, но зачем? ЧтобыSLAсоблюдать, когда аудитория сервиса продолжит расти. Хочется, чтобы нас не могли «задедосить», но зачем? Чтобы соблюдатьSLA(Uptime guarantee). Из предыдущейстатьи про кодовую базумы уже видели, как адепты анархии влияют на оба пункта. Внедрение проблем с производительностью влияет наSLA, а когнитивное истощение и ловушки в коде — наTTM. Архитектурные решения позволяют точно так же влиять на оба требования, только масштабы выше и последствия страшнее. Бойтесь идолов! Хочу раскрыть жуткую тайну, тщательно оберегаемую оккультным братством десятилетиями. Правду о том, как корпорации по всему миру уничтожаются чужими руками. И́дол(греч. εἴδωλον, др.-рус. идолъ; также, кумир, фантом, истукан) — материальный предмет, который служит объектом религиозного поклонения и магических действий. —Wikipedia. Мало кто знает, но апологеты техно-анархии насаждают свою религию и идолопоклонство, чтобы продвигать идеи в массы, минуя «рацио». Сложно убедить человека вредить кодовой базе, наворачивая килограммы абстракций или разбивая функцию из 5 строк на 5 функций. Но если сказать, что так велел господь или что без этого не пойдет дождь — происходит чудо. Разработчик начинает верить, в том числе в то, что вера укрепит его техническое решение: Иди, вера твоя спасла тебя. И он тотчас прозрел и очистил себя и код свой. — Потерянные стихи из Техно-Евангелия братства технического угнетения. Уловка в том, что принципы можнотрактовать достаточно широко. Двоякое толкование и одновременно безрассудное слепое поклонение принципам приводят к спорам на code review со ссылками на авторитеты; к спорам на code review со ссылками на авторитеты; к потере времени на чтение очередной статьи о том, какдействительнонужно трактовать эти принципы; к потере времени на чтение очередной статьи о том, какдействительнонужно трактовать эти принципы; к отвлечению от написания простого работающего кода; к отвлечению от написания простого работающего кода; к ограничению восприятия (делай так, а не иначе, независимо от контекста). к ограничению восприятия (делай так, а не иначе, независимо от контекста). Что же это за принципы, о которых идет речь? Вы готовы узреть ложь в одеянии правды? Это принципы, сформулированные изначально под аббревиатуройIDOLS! I.D.O.L.S! I.D.O.L.S! Бойтесь этого слова, ведь оно сутьS.O.L.I.D! Позволю себе сослаться на так удачно вышедшую статью «Перестаньте молиться на принципы S.O.L.I.D». Согласен со всем, кроме примеров Liskov Substitution и Interface Segregation — там, на мой взгляд, критикуемый код с интерфейсами выглядит лучше. В статье не было примера необязательности Dependency Inversion Principle, приведу свой. Возьмём сущностьProductsDataSource. Вместо того, чтобы выделять интерфейсProductsDataSourceи реализациюSomeProductsDataSourceImpl, можно оставитьProductsDataSourceединственной реализацией. Было: Если понадобится другая реализация, просто заведите интерфейсProductsDataSource, а единственную реализацию переименуйте вSomeProductsDataSourceImpl— никакие изменения в других местах не нужны, всё теперь завязано на интерфейс. Хотелось бы добавить и про Opened/Closed. Боб всвоих лекцияхкритикуетswitchвнутри функции и приводит преимущества решения через расширение классов. Проблема тут в том, что такой подход работает только при добавлении новых классов. Если же постоянно добавляются новые функции, нам придется изменять уже написанные классы, нарушая этот же самый O/P. Проблема известна с 1975 под именемexpression problem. S.O.L.I.D. — это всего лишь набор случайных принципов, применение которых более или менее актуально в зависимости от контекста задачи и языка программирования. Злоупотребление DRY Можно оправдать любое злодеяние, если подобрать нужные слова. Одно из таких слов —DRY(Don't Repeat Yourself). Если кто-то усомнился в адекватности автора, попробуйте, не читая дальше, ответить на вопрос: зачем избегать дублирования? Назовитеоднудействительно важную причину. Дело не в желании печатать меньше. Страшнозабыть про все места, когда общая логика изменится. ПринципDRYвпервые был сформулирован в 1999 (в книге «The Pragmatic Programmer»), когда разработчики не имели возможности открыть GitHub, посмотреть Issue, PR и все измененные файлы. Не было умных редакторов кода, которые с LLM или другой реализацией паттерн-матчинга позволили бы пробежаться по коду и сделать автозамену. Не было решений, вродеsealed classв Kotlin и Java с исчерпывающимwhenиswitch(с 17 версии). Архитектурные террористы, игнорируя развитие инструментов, до сих пор пропагандируют злоупотреблениеDRYв ожидании предсказуемых негативных последствий. Первая проблемаDRY— код становится менее читаем.Чем больше переходов нужно, чтобы добраться до логики сквозь дебри абстракций, тем сложнее.В этом плане можно смотреть наDRYкак на меру нарушения другого принципа —LoC(Locality of Behaviour), облегчающего восприятие кода. Вторая проблемаDRY— возможность внести ошибку при изменении общего кода, когда только в подмножестве нужны изменения. Допустим, была функция, возвращающая сущностьUserиз базы, а затем ей добавили колонкуdeleted. В большинстве мест нужно отфильтровать по признакуdeleted, но не везде, а программист, меняющий код, может об этом не подумать и добавить изменение только в общий код. Как результат — неявные баги, которые могут обнаружиться на проде. Третья проблемаDRY— связанность всего со всем. Как в старом анекдоте про ООП: захотели банан — получили обезьяну, держащую этот банан, и джунгли в придачу. И проблема не только в отсутствии возможности в будущем безболезненно вынести часть кода в отдельный сервис. Связный код сложнее тестировать и сложнее рефакторить, что напрямую влияет наTTM. Если начать говорить об этом с техно-анархистом, можно услышать про high cohesion, low coupling. Делается это для того, чтобы уйти от необходимости обсуждения проблем по существу в софистику, ведь low coupling может быть определен субъективно, а в реальных приложения и вовсе неосуществим: High coupling must be unavoidable, statistically speaking, apparently contradicting standard ideas about software structure. - research paper,Can We Avoid High Coupling? Четвертая проблемаDRYпроявляется в крупных проектах, если общий код (какой-нибудь наборcommon_utils) между двумя сервисами или приложениями выносится в отдельный репозиторий. Теперь, чтобы зарелизить задачу, нужно сперва зарелизить библиотеку с общим кодом. Будет два независимых пулреквеста. Разные релизные ветки будут зависеть от разных релизов библиотеки. Придется думать об обратной совместимости. Как результат,TTMвозрастет на порядок. Не призываю отказываться отDRY, призываю не злоупотреблять.DRYхорош, но лучшее — враг хорошего. Low coupling — тоже хорошо, но 0 coupling — это бесполезный код, который можно удалить. Решения, убивающие скорость получения обратной связи The quicker I get the feedback, the happier I am. — Martin Fowler У Мартина Фаулера наберётся не один десяток высказываний про важность петли обратной связи. В том числе про то, что не всё можно спроектировать сразу, и что существует «степень понимания, которая достигается только выполнением программы, когда можноувидеть, что действительно работает» Безусловно, есть задачи, которые на языках со статической типизацией выполняются «с первого раза» — написал, запустил, работает. Это как правило какая-то стандартная рутина. Всегда найдется очень умный человек или адепт технического угнетения, который скажет, что частые запуски ему не нужны, ведь можно прочитать документацию и сразу всё написать как надо. Но я наблюдал, как разработчикичасами не могли решить задачу, пренебрегая скоростью обратной связи. Выглядит это так: Разработчик делает ПР (потому что локально не воспроизводится). Разработчик делает ПР (потому что локально не воспроизводится). Ждет CI 45 минут и получает очередную ошибку. Ждет CI 45 минут и получает очередную ошибку. Думает над новым решением и Возвращается к 1. Думает над новым решением и Возвращается к 1. За день так можно сделать около 8-10 попыток. Если же потратить полчаса (полдня) и добиться воспроизведения проблемы локально, можно сделать те же 10 попыток за 10 минут. Уничтожители корпораций — опытные разработчики, поэтому легко могут найти способы замедления петли обратной связи на проекте, чтобы не попасть под подозрение в саботаже. Например, выбрать библиотеку для DI (вродеDagger 2), использующую кодогенерацию. На больших проектах каждый git pull будет сопровождаться чистой сборкой. Особенно легко внедрить Dagger в мобильной разработке, где можно давить на важность быстрого старта приложения для обоснованияDI с кодогенерацией. И не важно, что для большинства девайсов это вопрос 0.45-20ms (замеры из прошлого, уверен, на новых моделях еще быстрее). Есть проекты вродеdagger-reflect, чтобы в compile time была кодогенерация, а во время разработки — reflection, но техно-террористы каким-то образом отвлекли Джейка Вортона от проекта. Другой отличный пример — упомянутый впредыдущей статьеAspectJ. Тут и кодовая база будет обрастать неявно описанной логикой, и компиляция замедлится. В Android-разработке феноменально хорошо себя зарекомендовалrobolectric— как инструмент для выключения TDD. Тесты будут запускаться так долго, что никто их запускать не будет. В каждом стеке есть свои приемы увеличения петли обратной связи, описание каждого выходит за рамки статьи. Монорепозитории и гравитация Как мы уже проговорили выше, для счастья и продуктивности разработчика критически важна скорость получения «фидбека». Соответственно, витязи технического упадка хотят организовать монорепозиторий так, чтобы обратную связь разработчик получал как можно медленнее. Давайте сразу введем термин«гравитации репозитория»— то, насколько время решения задачзамедляетсяв результате плохой организации проекта. Пример из опыта. Я работал в российском «FAANG» в 2021, когдалид мобильной разработки решил прошить«гредловую» нитьсквозь все gradle-репозитории. Последние уже были частью «монорепы», но оставались свободными — то есть имелась возможность открыть в редакторе кода только нужную папку и работать с ней, независимо от остальных. Я жил счастливо и буквально парил над землей, ведь гравитация не ощущалась — сборка проекта занимала 2-5 секунд, clean build — секунд 20-30. Добавление новой зависимости и синхронизация системы сборки — секунд 10-15. Вернувшись из отпуска, я узнал, что репозиторий переехал на «multi project builds». Под тяжестью гравитации вся команда оказалась на лопатках — время сборки проекта увеличилось в 5-10 раз. До меня дошли слухи, что кто-то (возможно, тот самый лид) потерял 3 часа из-за того, что версии библиотек у двух репозиториев не совпали. Чтобы такое не повторилось, было принято решение, из-за которогопо 3 часа в день теперь терял каждый из 50+ человек. Дополнительной мотивацией лида могло быть желание контролировать стек. Однако по результату ущерба, нанесенного внедренным решением, я предполагаю вмешательство организации техно-анархистов. Также не могу не добавить, что бывший коллега, ревьюивший статью, матерился только в этой части текста: «я это *** *** по 40 минут синкал ***» — написал потерпевший. Переход на микросервисы без нужды Ибо истинно говорю вам: если вы будете иметь веру с горчичное зерно и скажете монолиту сему: «перейди на микросервисы», и он перейдёт; и ничего не будет невозможного для вас; — Потерянные стихи из Техно-Евангелия братства технического угнетения. Братство архитекторного терроризма в очередной раз воспользовалось мистическими практиками, чтобы насадить веру, что микросервисы — лекарство от всех болезней. И это работает. На профессиональных конференциях уже не принято говорить, зачем: «Как продать микросервисы: пошаговый план для разработчика» (Москва, Highload++ 2024). Давайте сразу разделим понятия микросервисы и сервисы (service-based architecture). Сервисы — это разбитый по доменным областям монолит (например, сервис доставки). Микросервисы — уже разбитые по функциям сервисы (например, микросервис нотификации о доставке). За подробностями можно обратиться к архитектору Марку РичардсуMicroservices vs. Service Based Architecture. Не утверждаю, что переход на микросервисы — это всегда плохо. Существуют успешные примеры,но ведь и сломанные часы иногда показывают правильное время,но не следует забывать, что за микросервисы мы платим огромную цену: Увеличение сложности системы: будет сложнее деплоить, мониторить, поддерживать и тестировать. Увеличение сложности системы: будет сложнее деплоить, мониторить, поддерживать и тестировать. Увеличение накладных расходов: каждому микросервису нужна своя база данных, кеш, аналитика, репозиторий конфигов, и т.д. Увеличение накладных расходов: каждому микросервису нужна своя база данных, кеш, аналитика, репозиторий конфигов, и т.д. Увеличение Latency: вместо вызова функции будет поход по сети. Увеличение Latency: вместо вызова функции будет поход по сети. Увеличение стоимости разработки: понадобится больше усилий от дефопсов, больше суеты и ресурсов для организации e2e тестирования, больше паттернов для реализации бизнес-логики (следующий пункт). Увеличение стоимости разработки: понадобится больше усилий от дефопсов, больше суеты и ресурсов для организации e2e тестирования, больше паттернов для реализации бизнес-логики (следующий пункт). Микросервисы усложняют поддержание целостности данных: вместо одной транзакции в базу — SAGA, TCC, XA, Workflow, Outbox pattern, и т.п. Микросервисы усложняют поддержание целостности данных: вместо одной транзакции в базу — SAGA, TCC, XA, Workflow, Outbox pattern, и т.п. Как результат вышеперечисленного — микросервисы будут стоить бизнесу на порядок больше. Как результат вышеперечисленного — микросервисы будут стоить бизнесу на порядок больше. Критикуя микросервисную архитектуру, я не отказываюсь от гибридных подходов, развивающихся изModulith-архитектуры, когда мы разделяем код по модулям по доменным областям. Подобная архитектура, не имея перечисленных выше проблем, позволит получить некоторые плюсы микросервисов, вроде параллельной работы над разными модулями и быстрой сборки проекта. Ничего не помешает с течением времени, при необходимости, выделить модуль в сервис или микросервис. Кроме того, до выделения микросервисов у нас будет больше времени, чтобы «нащупать» границы. Адепта же оккультного братства можно отличить по слепому желанию все переписать на микросервисы или сразу начать с микросервисов. Запрещаем рефакторинг обилием Unit-тестов Тесты в умелых руках — смертельное оружие. Авторитеты (Майк Кон, книга «Succeeding with Agile») говорят, что правильная пирамида тестирования — это 70% unit-тестов, 20% интеграционных и 10% UI. Возможно, Майк один из вдохновителей данной статьи, но что-то подобное пропагандировал и Мартин Фаулер, которого я выше цитировал. Ох, неловко вышло. В любом случае, кажется, что Мартин осознал свои ошибки ипопытался спрятаться за размытостью формулировок. Проблема в том, что Unit-тесты имеют очень низкий КПД (в сравнении с интеграционными тестами), при этом значительно усложняют рефакторинг. Представьте, у вас есть http-endpoint (далее «ручка»)/link. Ручку можно рассмотреть как черный ящик, на вход которому идут две вещи: явно: запрос пользователя; явно: запрос пользователя; неявно: текущий стейт системы (базы данных, кешей). неявно: текущий стейт системы (базы данных, кешей). А на выходе: явно: ответ пользователю; явно: ответ пользователю; неявно: текущий стейт системы (базы данных, кеши, записи в очереди). неявно: текущий стейт системы (базы данных, кеши, записи в очереди). Внутри ручки/linkмогут использоваться 50 функций. Какие-то из них могут содержать сложную логику, и на них можно написать unit-тесты. Но я уверен, что большая частью функций не так уж и сложны. Для демонстрации идеи давайте посмотрим на крайности. Имеятолькоинтеграционный тест, разработчик может со спокойной душой переписать 50 фукнций в 30, избавиться от лишних запросов, переписать другие для оптимизации. Менять код ничего не мешает, а корректность кода будет проверена. Но представьте, еслина все имеющиеся 50 функций написаны unit-тесты. Рефакторинг будет невозможен. Когда unit-тестами покрыто всё, придется переписывать еще и каждый тест. Уже звучит как муторная рутина. Удаление теста — всегда подозрительно на ревью, и к этому с большой вероятностью придерутся. Кроме того, 50 unit-тестов не всегда могут заменить один интеграционный тест из-за эмерджентности: сумма частей не равна целому. Даже если все 50 тестов гарантируют правильность 50-ти функций, нужно все равно протестировать работу этих функций друг с другом в рамках ручки. Каким же должно быть соотношение тестов? На мой взгляд, ни один адекватный программист не будет давать такие соотношения, надо смотреть на контекст. Вполне можно жить без unit-тестов вообще. Интеграционные тесты дают свободу, юнит-тесты — душат. — Неизвестный мученик, павший во время рефакторинга. На велосипеде по зоопарку Прежде чем двигаться дальше, замечу, что Clojure — мой любимый ЯП. В качестве неопровержимого доказательства привожуэтот пост. Clojure — язык свободы, в том числе свободы для саботажа, поэтому многие ярые сторонники техно-анархизма приходят за идеями и примерами решений, чтобы потом насаждать практики в другие проекты. Пишуткниги, распространяя идеи на ЯП вроде Java, Go, C#, где они не применимы. Мне довелось поработать на Clojure в австралийском стартапе, и признаюсь, что после этого проекта я наконец-то понял, чем хорош Go. Вот что приносило боль на работе: Злоупотребление примитивами теории категорий. Злоупотребление примитивами теории категорий. Каждая интеграция с партнерами написана в своем стиле, с разными подходами к валидации, документации, описанию запросов, тестированию. Каждая интеграция с партнерами написана в своем стиле, с разными подходами к валидации, документации, описанию запросов, тестированию. Каждая задача решалась множеством способов. Например, несколько библиотек для переменных окружения; несколько реализаций для валидации; несколько решений для запросов к базе (макросы, dsl на ними, honeysql, просто строки). Каждая задача решалась множеством способов. Например, несколько библиотек для переменных окружения; несколько реализаций для валидации; несколько решений для запросов к базе (макросы, dsl на ними, honeysql, просто строки). Десятки способов работы с concurrency. Прибавьте к concurrency-примитивам Java примитивы Clojure,core asyncи библиотеку, имитирующуюэрленговский otplike. Десятки способов работы с concurrency. Прибавьте к concurrency-примитивам Java примитивы Clojure,core asyncи библиотеку, имитирующуюэрленговский otplike. Набор огромных макросов, дающий близкую к GraphQL функциональность, с неявными ограничениями в неявных местах. Набор огромных макросов, дающий близкую к GraphQL функциональность, с неявными ограничениями в неявных местах. Редактор кода с LSP не поддерживал подсветку синтаксиса макросов на проекте. Потратив два дня, так и не смог завести «кастомную» логику статического анализа для каждого макроса через популярнуюclj-kondo. Редактор кода с LSP не поддерживал подсветку синтаксиса макросов на проекте. Потратив два дня, так и не смог завести «кастомную» логику статического анализа для каждого макроса через популярнуюclj-kondo. Никакой унификации стиля и организации кода. Где-то строчки по 400 символов в длину, где-то код тестов написан прямо в файлах с бизнес-логикой (даже не вcomment-форме). Никакой унификации стиля и организации кода. Где-то строчки по 400 символов в длину, где-то код тестов написан прямо в файлах с бизнес-логикой (даже не вcomment-форме). Архитектурный терроризм осуществлялся написанием велосипедов и добавлением новых технологий и подходов для решения уже решенных на проекте задач. Выбор языка программирования В предыдущем разделе про зоопарк и велосипеды мы посмотрели, как поборники архитектурного саботажа воспользовались свободой на проекте и сумели навести беспорядок. Теперь давайте сконцентрируемся на языке. Как бы я ни любил Clojure, не хочу позволять коллегам отнимать моё время. Когда каждый день находишь новый способ реализации цикла или разбираешься с кодом, содержащимmonoidиreducer, которые оказываются простым фильтром или циклом — это начинает надоедать. Вот забавная картинка, которая не подразумевалась, как забавная картинка: Это только типы, есть еще мультиметоды и интерфейсы:defprotocol,gen-interface. Такое обилие возможностей отвлекает от решения реальных задач. Например, я хочу добавить валидацию черезSpecи пишу ее как дляmapилиdefrecord. Но если кто-то уже описал типы черезdeftypeдля оптимизации (илиgen-class,proxy,reifyдля совместимости с java), придется это отдельно поддерживать. Всё описанное выше является примером проблемы «проклятья лиспа». Хотите верьте, хотите нет, нооккультное братство наложило проклятье на целое семейство языков. The power of Lisp is its own worst enemy. — Rudolf Winestock. С Kotlin, например, возможностей меньше, а значит, и саботаж устраивать сложнее, но ситуация не идеальная. Впредыдущей статьеуже обсуждалась «маскировка проблем производительности через циклы», но на Kotlin можно десятком способов реализовать и другие идеи, не только циклы. Java кажется более предпочтительным вариантом, так как менее экспрессивна, чем Kotlin, но огромный минус языка в том, что у адептов обсуждаемой организации было очень много времени, чтобы пристреляться. Сделано всё, чтобы проекты собирались долго (аннотации и кодогенерация) и реальная логика терялась в тумане абстракций (аннотации, OOP, SOLID, DRY): Пример из статьи «Annotations nightmare»: Согласен, пример старенький. Тут еще не хватает аннотаций AspectJ и Dagger, так бы получилось добавить еще 4 штуки. Подробное обсуждение языков программирования выходит за рамки статьи, оставлю только последнюю идею, прежде чем двигаться дальше. Существует книга «Java Concurrency in Practice», но нет книги «Go Concurrency in Practice». В Go всегда пишешьtype, когда нужно объявить тип,for— когда нужен цикл. Что замечательно,компилятор просто не будет работать, если используется нестандартное форматирование. А это значит, что куча бесполезной мишуры пропадет, так и не появившись: Настройки, которые нужно устанавливать, приходя на новый проект; Настройки, которые нужно устанавливать, приходя на новый проект; Утилиты вродеktfmt,js-beautifyи тому подобные; Утилиты вродеktfmt,js-beautifyи тому подобные; Споры о том, какое форматирование использовать на проекте; Споры о том, какое форматирование использовать на проекте; «Кожаный» линтинг на code review: «поставь/убери, пожалуйста, запятую/пробел». «Кожаный» линтинг на code review: «поставь/убери, пожалуйста, запятую/пробел». Подход с решением каждой задачи одним способом приводит к тому, что любая кодовая база становится читабельной сразу. Нет риска нарваться на DSL, макросы или аннотации, с магией которых придётся разбираться и терять время, добираясь до действительно значимого кода. Поэтому в большой команде, где будут джуны, мидлы и техно-анархисты, я бы хотел писать на Go. Язык защитит от коллег во многих аспектах. Компиляция будет быстрой даже в монорепозитории. Тесты можно запускать, компилируя только нужный файл (и зависимости, если они изменились). Если проект мой личный, всё ещё хотелось бы писать на Clojure, используяREPL(моментальный фидбек) и отличнуюабстракцию над sequence. По моему опыту* на Clojure намного быстрее получаешь готовое решение. Больше всего работал на Kotlin и Clojure, но хотя бы более трех месяцев (по 2+ часа в день) писал на следующих языках: Java, Scheme, Fennel (Lua), Dart, Go, JavaScript, TypeScript, Python.Назвал бы еще Bash и Groovy, но использовал их скорее бессознательно. На практике главное, чтобы язык был адекватен задаче. Если задача — написать Launcher для Android, то ни Go, ни Clojure не подойдут, а Kotlin будет отличным выбором. Game development уровня AAA — C++. Что-то возле ML — Python. Распознать адепта уничтожения корпораций можно в момент, когда для мобильного приложения предлагается Scala или OCaml. Заключительные слова Выше приведен далеко не полный список подходов, принципов и приемов, способных повлиять негативно на архитектуру проекта. Организация, чья цель — добиться упадка корпораций, не стоит на месте и развивается точно так же, как и вся индустрия. Допускаю false positive: что-то из перечисленного выше могло попасть в моё поле зрения исключительно из-за неудачного опыта применения технологии. Прошу не обижаться и приходить дискутировать в комментарии. "
  },
  {
    "article_id": "https://habr.com/ru/companies/sportmaster_lab/articles/872204/",
    "title": "«База» по метрикам в Prometheus",
    "category": "Дизайн",
    "tags": "prometheus, мониторинг, мониторинг производительности, prometheus grafana, monitoring, promql",
    "text": "Привет, Хабр! Меня зовут Глеб Типсин, я являюсь ведущим разработчиком продукта «Системы персонализации и геймификации» в SM Lab. Как разработчик, я часто сталкиваюсь с оптимизацией производительности приложений и инфраструктуры.Пройдя через несколько кругов ада оптимизаций различных продуктов, я обрел опыт работы с Prometheus и метриками и готов поделиться им. В этой статье мы разберём, как устроены метрики в Prometheus, как происходит их сбор, а также что с ними можно делать. Кроме того, обсудим подходы к мониторингу, архитектуру Prometheus и её ограничения. Материал подойдёт тем, кто только знакомится с мониторингом и хочет освоить «базу», а также тем, кто уже работает сPrometheus, но хочет \"освежить\" свои знания и узнать что-то новое. Мониторинг и подходы Мониторинг— это процесс сбора, обработки, агрегирования и отображения количественных показателей системы в реальном времени. Например, общее количество и тип http запросов, число и виды ошибок, время обработки запросов, а также показатели доступности серверов. Мониторинг дает: Повышение уровня прозрачности: понимание того, как система ведет себя в условиях эксплуатации (production среда); Повышение уровня прозрачности: понимание того, как система ведет себя в условиях эксплуатации (production среда); Оповещение о проблемах: если что-то вышло из строя, то требуется немедленное вмешательство. Также могут поступать оповещения о будущих сбоях, которые стоят предупредить заранее; Оповещение о проблемах: если что-то вышло из строя, то требуется немедленное вмешательство. Также могут поступать оповещения о будущих сбоях, которые стоят предупредить заранее; Долгосрочный и ретроспективный анализ: насколько быстро растет нагрузка на систему? Насколько быстро растет хранимый объем данных? Ответы на эти вопросы позволяют заблаговременно принять необходимые меры; Долгосрочный и ретроспективный анализ: насколько быстро растет нагрузка на систему? Насколько быстро растет хранимый объем данных? Ответы на эти вопросы позволяют заблаговременно принять необходимые меры; Сравнение с предыдущими версиями системы: оценка изменений в состоянии системы после проноса очередной доработки — улучшилось ли её «самочувствие» или ухудшилось. Сравнение с предыдущими версиями системы: оценка изменений в состоянии системы после проноса очередной доработки — улучшилось ли её «самочувствие» или ухудшилось. Можно выделить два подхода в мониторинге: Метод белого ящика— мониторинг на основе внутренних метрик и показателей системы; Метод белого ящика— мониторинг на основе внутренних метрик и показателей системы; Метод черного ящика— наблюдение за внешним поведением системы с точки зрения пользователя. Метод черного ящика— наблюдение за внешним поведением системы с точки зрения пользователя. Метод белого ящика зависит от возможности исследовать систему изнутри. Поэтому метод белого ящика позволяет обнаруживать будущие проблемы, замаскированные повторными попытками и прочими сбоями.Этот подход может быть как симптомо-ориентированным, так и причинно-ориентированным, в зависимости от степени информативности. Метод черного ящика заключается в наблюдении за симптомами, позволяя выявлять уже возникшие, а не прогнозируемые проблемы: «Система работает некорректно прямо сейчас». Его ключевое преимущество — система обращается к человеку только при наличии реальной, ощутимой проблемы. Однако этот подход малоэффективен для прогнозирования и предотвращения неизбежных проблем, которые еще не проявились. Подходы в сборе показателей RED; RED; USE; USE; 4 Golden Signals. 4 Golden Signals. RED— это метод, который в своей основе представляет собой подход черного ящика, ориентированный на показатели, доступные для внешнего наблюдения: Rate— количество запросов в секунду; Rate— количество запросов в секунду; Errors— количество запросов, которые вернули ошибку; Errors— количество запросов, которые вернули ошибку; Duration— время, затраченное на обработку запроса. Duration— время, затраченное на обработку запроса. USE— метод ориентирован на мониторинг ресурсов. Помогает понять, как эффективно используются системные ресурсы и выявить узкие места. Концептуально представляет собой метод белого ящика, который фокусируется на внутренних показателях системы: Utilization— процент времени, в течение которого ресурс был использован; Utilization— процент времени, в течение которого ресурс был использован; Saturation— количество работы, которую ресурс должен выполнить, но которая в данный момент находится в очереди на обработку; Saturation— количество работы, которую ресурс должен выполнить, но которая в данный момент находится в очереди на обработку; Errors— количество ошибок. Errors— количество ошибок. Четырезолотыхсигнала(4 Golden Signals) включают время отклика, величину трафика, уровень ошибок и степень загруженности системы: Время отклика(Latency) — время, которое требуется для выполнения запроса; Время отклика(Latency) — время, которое требуется для выполнения запроса; Величина трафика(Traffic) — уровень нагрузки на систему, измеряемый в единицах, специфичных для данной системы, таких как RPS (запросы в секунду), RPM (запросы в минуту), TPS (транзакции в секунду) и т.п; Величина трафика(Traffic) — уровень нагрузки на систему, измеряемый в единицах, специфичных для данной системы, таких как RPS (запросы в секунду), RPM (запросы в минуту), TPS (транзакции в секунду) и т.п; Уровень ошибок(Errors) — частота или количество ошибок, такие как некорректные коды ответа, исключения в приложении и другие подобные проблемы; Уровень ошибок(Errors) — частота или количество ошибок, такие как некорректные коды ответа, исключения в приложении и другие подобные проблемы; Степень загруженности(Saturation) — показатель того, насколько полно загружена система. Степень загруженности(Saturation) — показатель того, насколько полно загружена система. Подход с четырьмя золотыми сигналами представляет собой сочетание метода белого и черного ящика. Архитектура Prometheus Одной из самых популярных связок для мониторинга является интеграцияPrometheusиGrafana. Prometheus отвечает за сбор и хранение метрик в виде временных рядов, а Grafana используется для визуализации этих данных. Вместе они образуют мощный инструмент для мониторинга и анализа производительности распределённых систем. Сервер Prometheus включает несколько ключевых компонентов, которые работают совместно для сбора, хранения и отображения метрик: Первый компонент—этоTSDB(база данных временных рядов), которая используется для хранения метрик в виде временных рядов. Каждый временной ряд представлен как набор пар (timestamp, value), где timestamp — это временная метка, а value — значение метрики на данный момент времени. TSDB обеспечивает эффективное хранение метрик и быстрый доступ к ним для дальнейшей обработки и анализа; Первый компонент—этоTSDB(база данных временных рядов), которая используется для хранения метрик в виде временных рядов. Каждый временной ряд представлен как набор пар (timestamp, value), где timestamp — это временная метка, а value — значение метрики на данный момент времени. TSDB обеспечивает эффективное хранение метрик и быстрый доступ к ним для дальнейшей обработки и анализа; Второй компонент— этоretrieval worker, который отвечает за сбор метрик. Prometheus использует механизмpullдля получения данных, при котором через определённые интервалы времени Prometheus отправляет запросы к конечным точкам (target) метрик для получения актуальных значений. Retrieval worker выполняет эти запросы и получает метрики от целевых target. Второй компонент— этоretrieval worker, который отвечает за сбор метрик. Prometheus использует механизмpullдля получения данных, при котором через определённые интервалы времени Prometheus отправляет запросы к конечным точкам (target) метрик для получения актуальных значений. Retrieval worker выполняет эти запросы и получает метрики от целевых target. Третий компонент— этоHTTP server, который предоставляет API для выполнения запросов к метрикам, сохранённым в TSDB. Через HTTP сервер можно запрашивать метрики за определённый период, а также применять функции агрегации, фильтрации и преобразования данных. Третий компонент— этоHTTP server, который предоставляет API для выполнения запросов к метрикам, сохранённым в TSDB. Через HTTP сервер можно запрашивать метрики за определённый период, а также применять функции агрегации, фильтрации и преобразования данных. Четвёртый компонент—конфигурационные файлы, которые содержат настройки, такие как интервалы сбора метрик, статические и динамические источники данных, параметры алертов и другие аспекты работы системы. Четвёртый компонент—конфигурационные файлы, которые содержат настройки, такие как интервалы сбора метрик, статические и динамические источники данных, параметры алертов и другие аспекты работы системы. Сервер Prometheus периодически собирает метрики из различных источников — как статических, так и динамических — и сохраняет их в TSDB. Если настроены правила алертинга, Prometheus генерирует алерты и отправляет их вAlertmanager, который, в свою очередь, перенаправляет эти алерты в различные каналы оповещения, такие как электронная почта, мессенджеры и т.п. Для визуализации данных используется Grafana, которая интегрируется с Prometheus и предоставляет возможность настраивать удобные дашборды для анализа состояния системы и её производительности. Pushgatewayприменяется для сбора метрик от кратковременных или фоново работающих приложений, которые не поддерживаютpullмодель. В таких случаях приложения отправляют свои метрики в Pushgateway, а Prometheus забирает их оттуда. Для работы с динамической инфраструктурой используется механизмservice discovery. Этот процесс автоматически определяет список доступных источников данных, позволяя Prometheus адаптироваться к изменениям в инфраструктуре. Источники данных для Prometheus: Экспортеры— это специальные приложения, которые собирают метрики из различных систем и предоставляют их в формате Prometheus. Экспортеры используются, например, для мониторинга баз данных, операционных систем и сетевого оборудования; Экспортеры— это специальные приложения, которые собирают метрики из различных систем и предоставляют их в формате Prometheus. Экспортеры используются, например, для мониторинга баз данных, операционных систем и сетевого оборудования; Приложения— приложения, которые могут отдавать метрики в формате Prometheus. С помощью клиентских библиотек разработчики могут внедрить в свои приложения сбор и предоставление метрик. Приложения— приложения, которые могут отдавать метрики в формате Prometheus. С помощью клиентских библиотек разработчики могут внедрить в свои приложения сбор и предоставление метрик. Таким образом, Prometheus представляет собой мощный инструмент для сбора, анализа и визуализации метрик, обеспечивая мониторинг как статичных, так и динамических систем. Ограничения Prometheus Prometheus имеет ряд ограничений, которые важно учитывать при его внедрении: Ограниченное долговременное хранение метрик: Prometheus изначально разработан для мониторинга в реальном времени, поэтому встроенные возможности хранения метрик рассчитаны на ограниченный временной период; Ограниченное долговременное хранение метрик: Prometheus изначально разработан для мониторинга в реальном времени, поэтому встроенные возможности хранения метрик рассчитаны на ограниченный временной период; Отсутствие поддержки Global Query View: если у вас несколько независимых инстансов Prometheus, каждый из которых собирает свои метрики, выполнение объединённых запросов по данным со всех инстансов невозможно; Отсутствие поддержки Global Query View: если у вас несколько независимых инстансов Prometheus, каждый из которых собирает свои метрики, выполнение объединённых запросов по данным со всех инстансов невозможно; Ограниченная производительность: производительность Prometheus ограничивается возможностями одного сервера, так как он не поддерживает автоматическое горизонтальное масштабирование на несколько серверов. Для работы с высокими нагрузками требуется ручное шардирование данных или использование дополнительных решений для распределённого мониторинга. Ограниченная производительность: производительность Prometheus ограничивается возможностями одного сервера, так как он не поддерживает автоматическое горизонтальное масштабирование на несколько серверов. Для работы с высокими нагрузками требуется ручное шардирование данных или использование дополнительных решений для распределённого мониторинга. Эти ограничения делают Prometheus менее подходящим для очень крупных систем или сценариев с высокими требованиями к долговременному хранению данных и глобальному доступу к метрикам. Однако они обходятся с помощью дополнительных инструментов и настроек. Для решения ограничений, присущих Prometheus, существует несколько подходов, которые помогают масштабировать систему и улучшить её возможности. Наиболее популярными решениями являются: VictoriaMetrics; VictoriaMetrics; Cortex; Cortex; Thanos; Thanos; Mimir; Mimir; M3. M3. Архитектура мониторинга у нас В каждом кластереKubernetesразвёрнут собственный экземпляр Prometheus. Приложения, размещённые внутриpod'ов, предоставляют данные о метриках на заданном порту и эндпоинте в формате Prometheus. Prometheus с помощьюServiceMonitor, входящего в состав Prometheus оператора, автоматически определяет источники метрик и способы их сбора. Этот механизм упрощает настройку мониторинга, обеспечивая динамическую интеграцию с изменяющейся инфраструктурой кластера. Каждый экземпляр Prometheus передаёт собранные метрики в централизованную системуVictoriaMetricsс использованием протоколаremoteWrite. Это позволяет агрегировать данные мониторинга из различных кластеров в единую высокопроизводительную и масштабируемую инфраструктуру. Вот в целом и все. Приведённая схема представляет собой упрощённую модель, которая намеренно опускает некоторые детали для большей простоты. Модель данных Данные организованы в виде временных рядов, каждый из которых состоит из уникальногоидентификатораи набораsample'ов. Временной ряд однозначно идентифицируется комбинацией имени метрики и набора лейблов (если они присутствуют). Каждый sample представляет собой пару: временную метку (timestamp), выраженную в миллисекундах, и значение метрики в формате float64, которое отражает состояние метрики в соответствующий момент времени. Пример с двумя временными рядами в один и тот же момент времени: Название метрики: skip_records_total — отражает количество пропущенных      сообщений из топика брокера; Название метрики: skip_records_total — отражает количество пропущенных      сообщений из топика брокера; Набор лейблов: app — обозначает имя приложения, а topic — указывает название топика; Набор лейблов: app — обозначает имя приложения, а topic — указывает название топика; Значение: представлено числом в формате float64, отображающим текущее состояние      метрики. Значение: представлено числом в формате float64, отображающим текущее состояние      метрики. Данная метрика является кастомной и увеличивается непосредственно внутри приложения. На представленном изображении показан процесс извлечения значений метрик через пользовательский интерфейс Prometheus с использованием простейшего выражения на языке запросовPromQL. В результате были извлечены два временных ряда, отличающиеся значениями лейблов (название топика). Этот подход наглядно иллюстрирует, как использование лейблов позволяет значительно повысить гибкость и информативность, предоставляя возможность хранить дополнительные контекстные данные при использовании единого имени метрики. Далее рассмотрим подробно, какие типы данных могут возвращать запросы PromQL, сосредоточив внимание на самых распространённых из них —instant vectorиrange vector. Instant vector Данный тип данных представляет собой «снимок» временных рядов в определённый момент времени. Для каждого временного ряда возвращается одно единственное значение, соответствующее последней доступной выборке на момент выполнения запроса. Такой подход позволяет получить актуальное состояние метрик именно на заданный временной срез. Данный тип был представлен в примере раннее (см. модель данных метрики). Если в заданный момент времени данных в TSDB не обнаружено, Prometheus попытается найти ближайшее предыдущее значение. Это поведение регулируется параметромquery.lookback-delta, который по умолчанию установлен на 5 минут. Для наглядности данный механизм продемонстрирован на следующей анимации: Range вектор Этот тип данных представляет собой набор временных рядов, где каждому ряду соответствует массив значений, собранных за указанный временной интервал. Проще говоря,range vector— это совокупность текущего значения метрики и всех предыдущих выборок, попадающих в заданный диапазон времени. Такой формат данных позволяет анализировать изменения метрик во времени. Рассмотрим синтетический пример для лучшего понимания. Для метрикиmy_custom_metricполучим range вектора за 30 секунд для 3 срезов времени. Для момента T2: Для момента времени T1: Для момента времени T0: Для полученияrange vectorв PromQL необходимо добавить квадратные скобки с указанием интервала времени в конце запроса. Этот интервал определяет, за какой период будут собраны данные для каждого временного ряда. Например, для метрики my_custom_metric с интервалом 30 секунд запрос будет выглядеть так:my_custom_metric{}[30s]. Типы метрик В Prometheus всего 4 типа метрик: Примерcounter: Примерgauge: Примерhistogram: ⚠️ Важно отметить, что размерыbucketнеобходимо настраивать вручную, если такая возможность имеется. Если это не будет сделано, существует риск, что фреймворк или библиотека, которую вы используете, автоматически создаст большое количествоbucket, что может привести к неоправданному увеличению объёма данных. В результате вы получите большеbucket, чем реально нужно для ваших задач. Примерsummary: Фильтрация временных рядов Метрики в PromQL можно фильтровать по значениям лейблов, что позволяет получать данные, соответствующие определённым критериям: Для этого в фигурных скобках после имени метрики указываются условия фильтрации через запятую. Пример PromQL запроса с фильтрацией: В данном примере получаем только временные ряды, у которых topic соответствует значениюpg_notification_click. ⚠️ Фильтрацию можно применять как кinstant vector, так и кrange vector. «La сlassique» операции В PromQL к векторам можно применять арифметические, логические операторы, а также операторы сравнения. Операндами в таких выражениях могут быть как сами векторы, так и скалярные значения. ⚠️ Все эти операции применимытолько к instant вектору. Агрегация В PromQL можно выполнять агрегирование векторов, чтобы объединять временные ряды и извлекать агрегированную информацию. Агрегирование позволяет применять функции, такие как sum, min, max, avg, count, quantile, чтобы свести значения временных рядов к одному или нескольким агрегированным результатам: ⚠️ Все эти операции применимытолько к instant вектору. Базовые функции Наиболее часто используемые функции: Эти функции играют ключевую роль в анализе временных рядов, поскольку позволяют оценить, как метрики изменяются и с какой скоростью. ⚠️ Перечисленные функции применимы исключительно кrange векторам, так как они работают с массивами значений, собранных за определённый временной интервал. Кроме того, эти функции предназначены для использования с метриками типаcounter. Increase Данная функция показывает, насколько изменилось значение метрики за заданный интервал. Для наглядности рассмотрим небольшой пример. Для метрикиmy_custom_metricсуществует следующий набор измерений: Теперь применим функциюincreaseдля момента времени T3, используяrange vectorс временным интервалом в 2 минуты. Это позволит вычислить разницу между значениями метрики на границах заданного интервала. Функцияincreaseвозвращает величину роста метрики в пределах указанного диапазона времени. В данном случае, для момента T3, это изменение отражает разницу между последним доступным значением метрики в интервале и её начальным значением. Rate Функцияrateвычисляет среднюю скорость изменения метрики в секунду на основе накопительных данных за указанный временной интервал. Также для наглядности рассмотрим небольшой пример. Теперь применим функциюrateдля момента времени T3, используяrange vectorс интервалом в 2 минуты. Эта функция рассчитает среднюю скорость изменения метрики, используя значения на начальной и конечной границах интервала. Функцияrateвозвращает скорость изменения метрики в секунду, то есть она вычисляет производную метрики по времени. Irate Функцияirateявляется близким аналогомrate, но с одним важным отличием: она вычисляет производную, основываясь только на двух последних значениях, попавших в указанный временной интервал. Это делает её подходящей для анализа мгновенной скорости изменения метрики. Также приведем наглядный пример: Теперь применим функциюirateдля момента времени T3, используяrange vectorс интервалом в 2 минуты. В данном случае функция определит разницу между двумя последними значениями метрики, попавшими в диапазон, и поделит её на временной промежуток между этими значениями. В отличие отrate, которая усредняет изменения по всему интервалу,irateотображает мгновенные изменения, делая её более чувствительной к резким колебаниям. Обработка сбросов counter Приложения, предоставляющие метрики, неизбежно обновляются, например, при редеплое, что приводит к сбросу метрик типаcounterдо нуля. В таких случаях некоторые функции Prometheus, такие какincrease,rate,irateи другие, способны корректно обрабатывать подобные случаи. Метрики типаcounterобладают важным свойством — они монотонно возрастают. Это позволяет легко обнаружить сброс: новое значение счётчика оказывается меньше предыдущего. Prometheus автоматически учитывает такие случаи и корректирует расчёты, добавляя дельту, которая компенсирует разницу, возникшую из-за сброса: Экстраполяция Некоторые функции Prometheus, такие какincreaseиrate, используют экстраполяцию, поскольку значения метрик не всегда совпадают с точными границами временного окна. Это может приводить к появлению дробных значений в результате вычислений, даже если метрика по своей природе должна быть целочисленной. В общем случае экстраполяция работает следующим образом: Prometheus оценивает положение значений метрики относительно границ временного интервала и \"достраивает\" недостающие части. Вот еще интересный случай – Prometheus никогда не будет экстраполировать данные в область отрицательных значений. Это важно, особенно для метрик типа counter, которые должны всегда оставаться положительными: Если одно из крайних значений метрики слишком далеко от границы временного окна, Prometheus не будет экстраполировать на весь интервал: А вот для медленно изменяющихся счетчиков, экстраполяция может привести к не совсем корректным результатам: В этой статье мы разобрали базовые концепции работы с метриками вPrometheus, начиная от подходов в мониторинге и инфраструктуры до функций и особенностей экстраполяции. Этот материал был задуман как вводный, чтобы помочь вам уверенно начать работу с Prometheus. "
  },
  {
    "article_id": "https://habr.com/ru/articles/877312/",
    "title": "System Design для начинающих: всё, что вам нужно. Часть 2",
    "category": "Дизайн",
    "tags": "system design, архитектура, архитектура системы",
    "text": "Вам не нужно изучать какую‑либо теорию, кроме этой статьи, чтобы начать собеседоваться. После прочтения смело приступайте к решению типовыхSystem Designзадач. ИзучаяSystem Design, вы часто видите только теоретические материалы. В этой статье я постарался показать в том числе практическую реализацию многих вещей, чтобы вы не просто готовились к собеседованиям, но и знали, как эти вещи используются в реальном мире. Содержание Зачем изучать проектирование систем? Зачем изучать проектирование систем? Что такое сервер? Что такое сервер? Задержка и пропускная способность Задержка и пропускная способность Масштабирование и его типы+ Вертикальное+ Горизонтальное Масштабирование и его типы+ Вертикальное+ Горизонтальное Автоматическое масштабирование Автоматическое масштабирование Оценка на коленке Оценка на коленке Теорема CAP Теорема CAP Масштабирование базы данных+ Индексирование+ Партиционирование+ Архитектура «master-slave»+ Multi-master+ Шардирование+ Недостатки Шардирования Масштабирование базы данных+ Индексирование+ Партиционирование+ Архитектура «master-slave»+ Multi-master+ Шардирование+ Недостатки Шардирования SQL и NoSQL СУБД. Когда какую базу данных использовать?+ SQL СУБД+ NoSQL СУБД+ Особенности масштабирования+ Когда использовать ту или иную базу данных? SQL и NoSQL СУБД. Когда какую базу данных использовать?+ SQL СУБД+ NoSQL СУБД+ Особенности масштабирования+ Когда использовать ту или иную базу данных? Микросервисы+ Что такое монолит и микросервис?+ Почему мы разбиваем наше приложение на микросервисы?+ Когда следует использовать микросервисы?+ Как клиенты отправляют запросы? Микросервисы+ Что такое монолит и микросервис?+ Почему мы разбиваем наше приложение на микросервисы?+ Когда следует использовать микросервисы?+ Как клиенты отправляют запросы? Load Balancer+ Зачем нам нужен балансировщик нагрузки?+ Алгоритмы балансировщика нагрузки Load Balancer+ Зачем нам нужен балансировщик нагрузки?+ Алгоритмы балансировщика нагрузки Кэширование+ Введение в кэширование+ Преимущества кэширования+ Типы кэшей+ Подробное описание Redis Кэширование+ Введение в кэширование+ Преимущества кэширования+ Типы кэшей+ Подробное описание Redis Хранилище BLOB-объектов+ Что такое BLOB и зачем нам нужно хранилище BLOB?+ AWS S3 Хранилище BLOB-объектов+ Что такое BLOB и зачем нам нужно хранилище BLOB?+ AWS S3 Сеть доставки контента (CDN)+ Знакомство с CDN+ Как работает CDN?+ Ключевые понятия в CDN Сеть доставки контента (CDN)+ Знакомство с CDN+ Как работает CDN?+ Ключевые понятия в CDN Message Broker+ Асинхронное программирование+ Зачем мы добавили посредника для передачи сообщений?+ Queue+ Stream+ Кейсы использования Message Broker+ Асинхронное программирование+ Зачем мы добавили посредника для передачи сообщений?+ Queue+ Stream+ Кейсы использования Apache Kafka Deep dive+ Когда использовать Kafka+ Внутреннее устройство Kafka Apache Kafka Deep dive+ Когда использовать Kafka+ Внутреннее устройство Kafka Pub/Sub Pub/Sub Event-Driven Архитектура+ Введение+ Зачем использовать EDA?+ Система нотификаций с id+ Система с передачей всего состояния Event-Driven Архитектура+ Введение+ Зачем использовать EDA?+ Система нотификаций с id+ Система с передачей всего состояния Distributed Systems Distributed Systems Leader Election Leader Election Big Data Tools Big Data Tools Consistency Deep Dive+ Когда использовать Strong Consistency, Eventual Consistency+ Как добиться Strong, Eventual Consistency Consistency Deep Dive+ Когда использовать Strong Consistency, Eventual Consistency+ Как добиться Strong, Eventual Consistency Consistent Hashing Consistent Hashing Data Redundancy and Data Recovery+ Зачем мы делаем резервные копии баз данных?+ Различные способы резервного копирования данных+ Непрерывное резервное копирование Data Redundancy and Data Recovery+ Зачем мы делаем резервные копии баз данных?+ Различные способы резервного копирования данных+ Непрерывное резервное копирование Proxy+ Что такое прокси сервер?+ Прямой и обратный прокси сервер+ Создание собственного обратного прокси-сервера Proxy+ Что такое прокси сервер?+ Прямой и обратный прокси сервер+ Создание собственного обратного прокси-сервера Как решить любую проблему, связанную с проектированием системы? Как решить любую проблему, связанную с проектированием системы? Мы рассмотрели разделы 1-7 вчасти I. Пришло времяБаз Данных. Масштабирование базы данных Обычно у вас один сервер баз данных. Ваше приложение запрашивает данные из этой БД и получает результат. Когда вы достигаете определённого масштаба, этот сервер баз данных начинает медленно отвечать или может выйти из строя из-за своих ограничений. В такой ситуации необходимо масштабировать базу данных, что мы и рассмотрим в этом разделе. Масштабировать базу данных стоит постепенно. Это значит, что если у нас всего 10 тысяч пользователей, то масштабировать её для поддержки 10 миллионов — пустая трата времени. Это избыточная инженерия. Будем масштабировать только до того предела, который достаточен для нашего бизнеса. Предположим, у вас есть сервер баз данных, в котором есть таблица пользователей. С сервера приложений поступает множество запросов на чтение, чтобы получить информацию о пользователе с определённым идентификатором. Чтобы ускорить запрос на чтение, сделайте следующее: Индексирование База данных проверяеткаждую строкув таблице, чтобы найти запрашиваемые данные. Это называетсяполным сканированием таблицы(full table scan)и может быть медленным для больших таблиц. Проверка каждого идентификатора занимает O(N) времени. При индексировании база данных использует индекс для быстрого перехода к нужным строкам, что значительно ускоряет работу. Вы индексируете столбец «id», после чего база данных создаёт копию этого столбца «id» в структуре данных (называемой B-деревом). B-дерево используется для поиска конкретного идентификатора. Поиск выполняется быстрее, потому что идентификаторы хранятся в отсортированном виде, так что вы можете использовать двоичный поиск для поиска за O(logN)(прим. переводчика. В PostgreSQL - индекс для Primary Key создаётся автоматически) Если вы хотите включить индексирование в каком-либо столбце, вам нужно просто добавить одну строку кода, и все операции по созданию B-деревьев и т.д. будут выполняться БД. Вам не нужно ни о чём беспокоиться. Это было очень короткое и простое объяснение по поводу индексации. Партиционирование Означает разбиение большой таблицы на несколько маленьких таблиц. Вы видите, что мы разделили таблицу пользователей на 3 таблицы:- user_table_1- user_table_2- user_table_3 Эти таблицы хранятся наодном сервере базы данных. Пример запроса При вставке данных PostgreSQL автоматически направляет строки в соответствующую партицию на основе значения столбцаcreated_at: При выполнении запросов PostgreSQL автоматически выбирает нужные партиции на основе условий. Например: В этом случае PostgreSQL будет искать данные только в партицииusers_2023_02. В чём преимущество такого разделения? Когда ваш индекс становится очень большим, при поиске также возникают проблемы с производительностью. Теперь, после разделения на разделы, у каждой таблицы есть свой индекс, поэтому поиск в таблицах меньшего размера выполняется быстрее. Вам может быть интересно, как мы определяем, из какой таблицы выполнять запрос. До этого мы могли выполнить запросSELECT * FROM users where ID=4. Не волнуйтесь, вы можете снова выполнить тот же запрос. PostgreSQL работает за кулисами. Он найдёт нужную таблицу и выдаст результат. Но вы также можете настроить это на уровне приложения, если хотите. Архитектура ведущего-ведомого устройства (Master-Slave) Используйте её, если даже после индексирования, партиционирования и вертикального масштабирования ваши запросы выполняются медленно или ваша база данных не может обрабатывать дальнейшие запросы на одном сервере. Суть подхода заключается в репликации данных на несколько серверов. При выполнении любого запроса на чтение(запросы SELECT)он будет перенаправлен на наименее загруженный сервер. Таким образом вы распределяете нагрузку. Но все запросы на запись(INSERT, UPDATE, DELETE)будут обрабатываться только одним сервером. Узел/сервер/нода, который обрабатывает запрос на запись, называетсяглавным узлом(master). Узлы, которые принимают запросы на чтение, называютсяподчиненными узлами(slaves). Когда вы отправляете запрос на запись, он обрабатывается и записывается на главном узле, а затем асинхронно (или синхронно в зависимости от конфигурации) реплицируется на все подчиненные узлы. Несколько мастеров Если запросы на запись выполняются медленно или один главный узел не может обработать все запросы на запись, вы можете сделать следующее. В этом случае вместо одной главной базы данных для обработки записей используются несколько главных баз данных. Пример Очень распространённая практика — использование двух главных узлов. Один для Европы, а другой для Азии. Запросы из данных регионов обрабатываются на соответствующем ближайшем узле. Узлы периодически синхронизируют свои данные. В системе с несколькими ведущими серверами самая сложная часть — это обработка конфликтов. Если для одного и того же идентификатора в обоих ведущих серверах есть два разных набора данных, то вам нужно написать логику в коде: Хотите ли вы принять оба набора данных Хотите ли вы принять оба набора данных Заменить предыдущий набор данных последним Заменить предыдущий набор данных последним Объединить их и т. д. Объединить их и т. д. Здесь нет единого правила. Всё зависит от бизнес-сценария. Шардирование базы данных Шардинг — это очень сложная процедура(плюсомссылка на хабр статью). Старайтесь избегать этого в реальной жизни и делайте это только в том случае, если всего вышеперечисленного недостаточно и требуется дальнейшее масштабирование. Шардинг похож на партиционирование, что мы видели выше. Но вместо того, чтобы размещать разные таблицы на одном сервере, мы размещаем их на разных серверах. На изображении выше вы видите, что мы разделили таблицу на 3 части и поместили их на 3 разных сервера. Эти серверы обычно называютшардами. Здесь мы выполнили шардирование на основе идентификаторов, поэтому этот столбец с идентификаторами называетсяключом шардирования. Примечание:ключ шардирования должен равномерно распределять данные по сегментам, чтобы избежать перегрузки одного. Каждый раздел хранится на независимом сервере баз данных (называемом сегментом/шардом). Таким образом, теперь вы можете масштабировать этот сервер по отдельности в соответствии с вашими потребностями, например, используя архитектуру «ведущий-ведомый» для одного из шардов, на который поступает много запросов. Почему шардирование является сложной задачей?При партиционирование(сохранении фрагментов таблицы на одном сервере БД) вам не нужно беспокоиться о том, из какой таблицы выполнять запрос. PostgreSQL делает это за вас. Но при шардирование (сохранении фрагментов таблицы на разных серверах БД) вам нужно обрабатывать это на уровне приложения. Вам нужно написать код таким образом, чтобы при запросе от идентификатора 1 до 2 он отправлялся в БД-1, а при запросе от идентификатора 5 до 6 — в БД-3. Кроме того, при добавлении новой записи вам нужно вручную обработать логику в коде приложения, чтобы определить, в какой шард вы собираетесь добавить эту новую запись. Стратегии шардирования Шардированиена основе диапазонов(Range-Based Sharding)Данные делятся на сегменты на основе диапазонов значений в ключе сегментирования.Пример:Шард 1: пользователи сuser_id 1–1000Шард 2: пользователи сuser_id 1001–2000Шард 3: пользователи сuser_id 2001–3000Плюсы: просто в реализации.Минусы: неравномерное распределение, если данные искажены (например, в некоторых диапазонах больше пользователей). Шардированиена основе диапазонов(Range-Based Sharding)Данные делятся на сегменты на основе диапазонов значений в ключе сегментирования.Пример:Шард 1: пользователи сuser_id 1–1000Шард 2: пользователи сuser_id 1001–2000Шард 3: пользователи сuser_id 2001–3000Плюсы: просто в реализации.Минусы: неравномерное распределение, если данные искажены (например, в некоторых диапазонах больше пользователей). Шардированиена основе хеширования(Hash-Based Sharding)К ключу шардирования применяется хеш-функция и результат определяет шард.Пример:HASH(user_id) % number_of_shardsопределяет шардПлюсы: обеспечивает равномерное распределение данных.Минусы: при добавлении новых сегментов сложно выполнить повторную балансировку, так как результаты хеширования меняются. Шардированиена основе хеширования(Hash-Based Sharding)К ключу шардирования применяется хеш-функция и результат определяет шард.Пример:HASH(user_id) % number_of_shardsопределяет шардПлюсы: обеспечивает равномерное распределение данных.Минусы: при добавлении новых сегментов сложно выполнить повторную балансировку, так как результаты хеширования меняются. Географическое шардирование(Geographic/Entity-Based Sharding)Данные разделяются на основе логической группировки, например по региону.Пример:Шард 1: пользователи изАмерики.Шард 2: пользователи изЕвропы.Плюсы: полезно для географически распределенных систем.Минусы: некоторые сегменты могут стать «горячими точками» с неравномерным трафиком. Географическое шардирование(Geographic/Entity-Based Sharding)Данные разделяются на основе логической группировки, например по региону.Пример:Шард 1: пользователи изАмерики.Шард 2: пользователи изЕвропы.Плюсы: полезно для географически распределенных систем.Минусы: некоторые сегменты могут стать «горячими точками» с неравномерным трафиком. Недостатки шардирования Сложно реализовать, потому что вам придётся самостоятельно писать логику, чтобы знать, из какого шарда выполнять запрос и в какой шард записывать данные.(прим. переводчика - шардирование средствами PostgreSQL опишу на канале) Сложно реализовать, потому что вам придётся самостоятельно писать логику, чтобы знать, из какого шарда выполнять запрос и в какой шард записывать данные.(прим. переводчика - шардирование средствами PostgreSQL опишу на канале) Разделы хранятся на разных серверах/шардах. Поэтому при выполнении объединений вам приходится извлекать данные из разных шардов, чтобы выполнить объединение с разными таблицами. Это дорогостоящая операция. Разделы хранятся на разных серверах/шардах. Поэтому при выполнении объединений вам приходится извлекать данные из разных шардов, чтобы выполнить объединение с разными таблицами. Это дорогостоящая операция. Вы теряете согласованность. Поскольку разные части данных находятся на разных серверах. Поэтому поддерживать согласованность сложно. Вы теряете согласованность. Поскольку разные части данных находятся на разных серверах. Поэтому поддерживать согласованность сложно. Подведение итогов масштабирования базы данных Давайте подведём итоги и запомним эти правила: Во-первых, всегда и везде отдавайте предпочтение вертикальному масштабированию. Это просто. Вам достаточно увеличить характеристики одного устройства. Если после этого вас настигнут проблемы с производительностью, выполняйте действия ниже. Во-первых, всегда и везде отдавайте предпочтение вертикальному масштабированию. Это просто. Вам достаточно увеличить характеристики одного устройства. Если после этого вас настигнут проблемы с производительностью, выполняйте действия ниже. Когда вы столкнетёсь с интенсивным трафиком(а лучше чуть заранее), реализуйте архитектуру master-slave. Когда вы столкнетёсь с интенсивным трафиком(а лучше чуть заранее), реализуйте архитектуру master-slave. Если у вас много операций записи, используйте шардирование, потому что все данные не поместятся на одном компьютере. Просто старайтесь избегать запросов между шардами. Если у вас много операций записи, используйте шардирование, потому что все данные не поместятся на одном компьютере. Просто старайтесь избегать запросов между шардами. Если у вас большой трафик, но архитектура «ведущий-ведомый» работает медленно или не справляется с нагрузкой, вы также можете использовать шардирование и распределять нагрузку. Но обычно это происходит в очень больших масштабах. Если у вас большой трафик, но архитектура «ведущий-ведомый» работает медленно или не справляется с нагрузкой, вы также можете использовать шардирование и распределять нагрузку. Но обычно это происходит в очень больших масштабах. Базы данных SQL против NoSQL. Когда какую использовать Выбор подходящей базы данных — важнейшая часть проектирования системы, поэтому внимательно прочитайте этот раздел. База данных SQL Данные хранятся в виде таблиц. Данные хранятся в виде таблиц. Есть предопределённая схема. То есть структура данных (таблицы, столбцы и их типы) должна быть определена до вставки данных. Есть предопределённая схема. То есть структура данных (таблицы, столбцы и их типы) должна быть определена до вставки данных. Соответствуетсвойствам ACID, обеспечивая целостность и согласованность данных. Соответствуетсвойствам ACID, обеспечивая целостность и согласованность данных. Пример: MySQL, PostgreSQL, Oracle, SQL Server, SQLite. Пример: MySQL, PostgreSQL, Oracle, SQL Server, SQLite. База данных NoSQL Они делятся на 4 типа:– Документоориентированные БД(Document-based)Хранят данные в документах, таких как JSON или BSON.Пример: MongoDB.–Хранилища «ключ-значение»(Key-value stores)Хранят данные в парах «ключ-значение».Пример: Redis, AWS DynamoDB–Колоночные СУБД(Column-family stores)Хранят данные в столбцах, а не в строках.Пример: Apache Cassandra,ClickHouse–Графовые СУБД(Graph databases)Ориентированы на взаимосвязи между данными, которые хранятся в виде графа. Полезно в приложениях для социальных сетей, например, для создания общих друзей, друзей друзей и т. д.Пример: Neo4j. Они делятся на 4 типа:– Документоориентированные БД(Document-based)Хранят данные в документах, таких как JSON или BSON.Пример: MongoDB.–Хранилища «ключ-значение»(Key-value stores)Хранят данные в парах «ключ-значение».Пример: Redis, AWS DynamoDB–Колоночные СУБД(Column-family stores)Хранят данные в столбцах, а не в строках.Пример: Apache Cassandra,ClickHouse–Графовые СУБД(Graph databases)Ориентированы на взаимосвязи между данными, которые хранятся в виде графа. Полезно в приложениях для социальных сетей, например, для создания общих друзей, друзей друзей и т. д.Пример: Neo4j. Такие БД обладают гибкой схемой. То есть, мы можем добавлять новые типы данных или поля, которые могут не быть определены в исходной схеме. Такие БД обладают гибкой схемой. То есть, мы можем добавлять новые типы данных или поля, которые могут не быть определены в исходной схеме. Они не следуют строгому принципу ACID. Отдают приоритет другим факторам, таким как масштабируемость и производительность. Они не следуют строгому принципу ACID. Отдают приоритет другим факторам, таким как масштабируемость и производительность. Гибкость схемы В NoSQL базе данных, такой как MongoDB, вы можете хранить данные от разных устройств в одной коллекции, даже если их структура отличается. Пример документа для датчика температуры: Обратите внимание, что структура документов разная, но они могут храниться в одной коллекции. Вспоминаем гибкие в этом плане коллекции питона ) Динамическое добавление полейЕсли датчик начинает отправлять новые данные (например, уровень заряда батареи), вы можете просто добавить это поле в документ без изменения схемы коллекции. Пример обновлённого документа: Запросы к данным Вы можете выполнять запросы к данным, даже если структура документов разная. Например, найти все документы, гдеbattery_levelменьше 20:  Масштабирование в SQL по сравнению с NoSQL SQL СУБД при росте нагрузки в первую очередь стоит масштабировать вертикально - увеличивать аппаратные ресурсы(процессора, оперативной памяти, хранилища)одного сервера для обработки больших объёмов данных. SQL СУБД при росте нагрузки в первую очередь стоит масштабировать вертикально - увеличивать аппаратные ресурсы(процессора, оперативной памяти, хранилища)одного сервера для обработки больших объёмов данных. NoSQL СУБД в первую очередь предназначены для горизонтального масштабирования. То есть, для добавления в кластер дополнительных серверов (узлов) для обработки растущих объёмов данных. NoSQL СУБД в первую очередь предназначены для горизонтального масштабирования. То есть, для добавления в кластер дополнительных серверов (узлов) для обработки растущих объёмов данных. Как правило, шардирование применяется в базах данных NoSQL для обработки больших объёмов данных. Как правило, шардирование применяется в базах данных NoSQL для обработки больших объёмов данных. Разделение на сегменты/шарды(прим. переводчика: shard - a small piece or part) также можно реализовать в базе данных SQL. Но, как правило, мы этого избегаем, потому что используем базу данных SQL для реализацииACID гарантий. А обеспечение согласованности данных(\"C\" в ACID)становится очень сложной задачей, когда данные распределены по нескольким серверам, а запросы к данным с помощью объединений шардов также сложны и затратны. Разделение на сегменты/шарды(прим. переводчика: shard - a small piece or part) также можно реализовать в базе данных SQL. Но, как правило, мы этого избегаем, потому что используем базу данных SQL для реализацииACID гарантий. А обеспечение согласованности данных(\"C\" в ACID)становится очень сложной задачей, когда данные распределены по нескольким серверам, а запросы к данным с помощью объединений шардов также сложны и затратны. Когда использовать какую базу данных? Еслиданные неструктурированыи вы хотите использоватьгибкую схему, выбирайтеNoSQL.Пример: отзывы, рекомендации в приложении для электронной коммерции Еслиданные неструктурированыи вы хотите использоватьгибкую схему, выбирайтеNoSQL.Пример: отзывы, рекомендации в приложении для электронной коммерции Еслиданные структурированыи имеют фиксированную схему, используйтеSQL.Пример: таблица учётных записей клиентов в приложении для электронной коммерции Еслиданные структурированыи имеют фиксированную схему, используйтеSQL.Пример: таблица учётных записей клиентов в приложении для электронной коммерции Если вам нужна целостность и согласованность данных, выбирайте SQL БД, потому что она поддерживаетсвойство ACID.Пример:+ Финансовые транзакции, операции с остатками на счетах в банковском приложении+ Заказы, платежи в приложении для электронной коммерции+ Платформы для торговли акциями Если вам нужна целостность и согласованность данных, выбирайте SQL БД, потому что она поддерживаетсвойство ACID.Пример:+ Финансовые транзакции, операции с остатками на счетах в банковском приложении+ Заказы, платежи в приложении для электронной коммерции+ Платформы для торговли акциями Если вам нужна высокая доступность, масштабируемость (то есть хранение больших объёмов данных, которые не помещаются на одном сервере) и низкая задержка, выбирайте NoSQL из-за горизонтальной масштабируемости и сегментирования.Пример:+ Публикации, лайки, комментарии, сообщения в приложении для социальных сетей+ Храните большие объёмы данных в реальном времени, например, местоположение водителя в приложении для доставки Если вам нужна высокая доступность, масштабируемость (то есть хранение больших объёмов данных, которые не помещаются на одном сервере) и низкая задержка, выбирайте NoSQL из-за горизонтальной масштабируемости и сегментирования.Пример:+ Публикации, лайки, комментарии, сообщения в приложении для социальных сетей+ Храните большие объёмы данных в реальном времени, например, местоположение водителя в приложении для доставки Если вам нужно выполнять сложные запросы, объединения и агрегацию данных, используйте SQL. Как правило, при анализе данных нам приходится выполнять сложные запросы, объединения и т. д. Храните необходимые для этого данные в SQL. Если вам нужно выполнять сложные запросы, объединения и агрегацию данных, используйте SQL. Как правило, при анализе данных нам приходится выполнять сложные запросы, объединения и т. д. Храните необходимые для этого данные в SQL. На этом вторая часть перевода подошла к концу. Позже постараюсь сделать новый подход. Изучим разделыМикросервисы, Load Balancer с алгоритмами балансировки нагрузки, Кэширование с примером Redis'a. Меня зовут Невзоров Владимир. Работаю старшим backend разработчиком на HighLoad проекте с порядком пиковой нагрузки в миллион rps. Приветствую) Веду телеграмм канал поАрхитектуре, System Design, Highload бэкэнду. На канале провожу архитектурные каты, публикую полезные материалы, делюсь опытом. Сейчас с участниками канала разбираем книгу Мартина Клеппмана \"Высоконагруженные приложения\"на стримах(youtube запись). Скоро встретимся на разборе 3ей главы - событие наtimepad. Для пополнения багажа знаний по темезаходите на мой канал<= Успехов в дальнейшем изучение темы System Design! "
  },
  {
    "article_id": "https://habr.com/ru/companies/otus/articles/877712/",
    "title": "Руководство по интерпретации данных",
    "category": "Дизайн",
    "tags": "data science, data analytics, метрики, анализ данных",
    "text": "Уроки 10-летнего опыта в Uber, Meta и быстрорастущих стартапах Данные помогают принимать более обоснованные решения. К сожалению, большинство компаний лучше справляются с их сбором, нежели чем с интерпретацией. Они утверждают, что используют подход, основанный на данных, но на практике для принятия решений полагаются на свой опыт. Ваша задача, как аналитика данных (или Data Scientist-а) — помочь заинтересованным сторонам бизнеса понять и интерпретировать данные, чтобы они могли принимать более взвешенные решения. Ваше влияние определяется не самим анализом или моделями, которые вы создаёте, а тем, каких бизнес‑результатов вы помогаете достичь. Это главный фактор, который отличает старших Data Scientist‑ов от младших. Чтобы помочь вам в этом вопросе, я подготовил пошаговое руководство, основанное на моём опыте превращения данных в полезные инсайты, полученном во время работы в Rippling, Meta и Uber. Я расскажу о следующем: Какие метрики отслеживать:Как определить уравнение дохода и дерево драйверов для вашего бизнеса. Какие метрики отслеживать:Как определить уравнение дохода и дерево драйверов для вашего бизнеса. Как отслеживать:Как настроить мониторинг и избежать распространённых ошибок. Мы разберём выбор правильного временного горизонта, учёт сезонности, работу с когортными данными и многое другое. Как отслеживать:Как настроить мониторинг и избежать распространённых ошибок. Мы разберём выбор правильного временного горизонта, учёт сезонности, работу с когортными данными и многое другое. Извлечение инсайтов:Как структурированно и повторяемо выявлять проблемы и возможности. Мы обсудим основные типы тенденций, с которыми вы столкнётесь, и как их интерпретировать. Извлечение инсайтов:Как структурированно и повторяемо выявлять проблемы и возможности. Мы обсудим основные типы тенденций, с которыми вы столкнётесь, и как их интерпретировать. Звучит достаточно просто, но дьявол кроется в деталях, поэтому давайте разбираться по пунктам. Часть 1: Какие метрики отслеживать В первую очередь вам нужно определить, какие метрики следует отслеживать и анализировать. Чтобы максимизировать влияние, вы должны сосредоточиться на тех метриках, которые действительно влияют на доход. Начните с уравнения дохода на высоком уровне (например, «Доход = Показатели * CPM / 1000» для бизнеса, основанного на рекламе), а затем разбейте каждую часть, чтобы добраться до базовых драйверов. Конкретное уравнение дохода зависит от типа бизнеса;здесьможно найти самые распространённые примеры. Полученное дерево драйверов, где на вершине находится результат, а внизу — входные параметры, показывает, что определяет результаты в бизнесе и какие дашборды вам нужно создать для проведения комплексных расследований. Пример: Ниже представлено (частичное) дерево драйверов для B2C‑продукта, основанного на рекламе: Понимание ведущих и запаздывающих метрик Уравнение дохода может создать впечатление, что входные данные немедленно преобразуются в выходные результаты, но в реальности это не так. Самый очевидный пример — воронка маркетинга и продаж: вы генерируете лиды, они превращаются в квалифицированные возможности, и, наконец, сделка закрывается. В зависимости от вашего бизнеса и типа клиента этот процесс может занять несколько месяцев. Иными словами, если вы анализируете метрику результата, такую как доход, вы часто смотрите на последствия действий, совершённых за недели или месяцы до этого. Согласно общему правилу, чем ниже вы спускаетесь по дереву драйверов, тем больше метрика влияет на конечный результат; чем выше вы поднимаетесь, тем более запаздывающей она становится. Измерение задержки Стоит изучить исторические окна конверсии, чтобы понять степень задержки, с которой вы имеете дело. Это позволит вам лучше проводить ретроспективный анализ (например, если вы видите колебания дохода, вы будете понимать, насколько глубоко в прошлое нужно заглянуть, чтобы найти причину) и прогнозировать будущее (сможете понять, сколько времени потребуется, чтобы увидеть эффект от новых инициатив). По моему опыту, разработка общих правил (например, сколько требуется времени, чтобы новый пользователь стал активным) позволяет получить 80–90% ценности, поэтому излишняя детализация здесь не обязательна. Часть 2: Настройка мониторинга и избегание распространённых ошибок Итак, у вас есть дерево драйверов. Как использовать его для мониторинга эффективности бизнеса и получения инсайтов для заинтересованных сторон? Первый шаг — создание дашборда для отслеживания ключевых метрик. Я не буду углубляться в сравнение различных BI‑инструментов (возможно, я сделаю это в другой статье). Всё, о чём я говорю в этой статье, легко реализуется в Google таблицах или другом подобном инструменте, поэтому выбор BI‑программного обеспечения не станет ограничивающим фактором. Вместо этого я хочу сосредоточиться на нескольких лучших практиках, которые помогут вам разобраться в данных и избежать типичных ошибок. 1. Выбор подходящих временных рамок для каждой метрики Хотя важно улавливать тенденции как можно раньше, нужно быть осторожным, чтобы не попасть в ловушку анализа слишком детализированных данных и попыток извлечь инсайты из того, что в основном является шумом. Учитывайтевременные интервалы измеряемых вами действийи возможностьреагировать на полученные данные: Данные в реальном времени полезны для B2C‑маркетплейсов (например, Uber), поскольку:Транзакции имеют короткий жизненный цикл (поездка Uber обычно запрашивается, принимается и завершается менее чем за час).Uber обладает инструментами для мгновенной реакции (например, динамическое ценообразование, стимулирование водителей, коммуникация с ними). Данные в реальном времени полезны для B2C‑маркетплейсов (например, Uber), поскольку: Транзакции имеют короткий жизненный цикл (поездка Uber обычно запрашивается, принимается и завершается менее чем за час). Транзакции имеют короткий жизненный цикл (поездка Uber обычно запрашивается, принимается и завершается менее чем за час). Uber обладает инструментами для мгновенной реакции (например, динамическое ценообразование, стимулирование водителей, коммуникация с ними). Uber обладает инструментами для мгновенной реакции (например, динамическое ценообразование, стимулирование водителей, коммуникация с ними). В отличие от этого, ежедневные данные о продажах в B2B SaaS‑бизнесе будут «шумными» и менее полезными из‑за длительных циклов сделок. В отличие от этого, ежедневные данные о продажах в B2B SaaS‑бизнесе будут «шумными» и менее полезными из‑за длительных циклов сделок. Вам также следует учитыватьвременные интервалы целей, которые вы устанавливаете в отношении метрики. Если у ваших команд‑партнёров цели рассчитаны на месяц, то по умолчанию отображение этих метрик должно быть помесячным. НО:Основная проблема месячных метрик (или данных за более длительные периоды) заключается в том, что у вас мало точек данных для анализа, и вам приходится долго ждать обновления информации о производительности. Хорошим решением может быть использование скользящего среднего для отображения метрик: в этом случае вы сможете уловить актуальные тренды, устраняя при этом значительную часть шума за счёт сглаживания данных. Пример:Смотря на ежемесячные данные (слева), можно сделать вывод, что мы мы находимся в выгодном положении для достижения цели на апрель. Однако, глядя на 30-дневное скользящее среднее, становится очевидно, что доход резко снизился и нужно срочно разбираться в причинах. 2. Установление бенчмарков Чтобы извлечь инсайты из метрик, необходимо интерпретировать их в контексте. Самый простой способ — отслеживать метрикуво времени: показатель улучшается или ухудшается? Конечно, ещё лучше, если вы точно знаете, какого показателя хотите достичь. Самый простой способ — отслеживать метрикуво времени: показатель улучшается или ухудшается? Конечно, ещё лучше, если вы точно знаете, какого показателя хотите достичь. Если у вас естьофициальная цельдля метрики — отлично. Но даже если цели нет, можно понять, идёте ли вы в нужном направлении, выведяпредполагаемые ориентиры. Если у вас естьофициальная цельдля метрики — отлично. Но даже если цели нет, можно понять, идёте ли вы в нужном направлении, выведяпредполагаемые ориентиры. Пример:Допустим, у команды продаж есть месячная квота, но нет официальной цели по объёму «входящего пайплайна», который им нужно создать для её выполнения. В этом случае можно рассмотреть историческое соотношение открытого пайплайна к квоте («Pipeline Coverage») и использовать его как эталон.Однако имейте в виду: таким образом вы предполагаете, что производительность останется на одном уровне (в данном случае, что команда конвертирует пайплайн в доход с неизменной скоростью). 3. Учёт сезонности Практически в любом бизнесе для корректной интерпретации данных нужно учитывать сезонность. Другими словами, имеет ли метрика повторяющиеся закономерности в зависимости от времени суток, дня недели, времени месяца или календарного месяца? Пример:Рассмотрите этот месячный тренд нового ARR в B2B SaaS‑бизнесе: Если взглянуть на снижение нового ARR в июле и августе на простой столбчатой диаграмме, можно запаниковать и начать масштабное расследование. Однако, если наложить данные за разные годы друг на друга, можно выявить сезонный паттерн и обнаружить, что это всего лишь ежегодный летний спад, после которого в сентябре бизнес восстанавливается: Сезонность может проявляться не только на уровне месяцев, но и в более коротких периодах времени — например, в зависимости от дня недели, который влияет на результаты, или же в том, что бизнес обычно активизируется ближе к концу месяца. Пример:Предположим, вы хотите оценить, как команда продаж работает в текущем месяце (в нашем примере это апрель). Сегодня 15-й рабочий день месяца, и вы уже достигли $26,000 из цели в $50,000. Если игнорировать сезонность,может показаться, что команда не достигнет цели, так как осталось всего 6 рабочих дней. Однако вы знаете, что команда, как правило, закрывает большое количество сделок в последние дни месяца. В этом случае можно построить график накопительных продаж и сравнить его с предыдущими месяцами, чтобы выявить закономерности. С его помощью мы обнаружим, что для текущего времени месяца мы находимся в хорошей позиции, так как динамика продаж не является линейной. 4. Работа с «недозревшими» метриками Одна из самых распространённых ошибок при анализе метрик — это использование данных, которые ещё не «созрели», то есть не достигли окончательного значения. Вот несколько распространённых примеров: Воронка привлечения пользователей:Вы измеряете конверсию от трафика к регистрации и покупки, но не знаете, сколько из недавних регистраций ещё конвертируется в покупку. Воронка привлечения пользователей:Вы измеряете конверсию от трафика к регистрации и покупки, но не знаете, сколько из недавних регистраций ещё конвертируется в покупку. Воронка продаж:Средний цикл сделки длится несколько месяцев, и вы не знаете, сколько из недавних открытых сделок будет закрыто. Воронка продаж:Средний цикл сделки длится несколько месяцев, и вы не знаете, сколько из недавних открытых сделок будет закрыто. Удержание:Вы хотите понять, насколько хорошо определённая когорта пользователей сохраняет свою активность. Удержание:Вы хотите понять, насколько хорошо определённая когорта пользователей сохраняет свою активность. Во всех этих случаях показатели недавних когорт кажутся хуже, чем они есть на самом деле, потому что данные ещё не являются окончательными. Если вы не хотите ждать, у вас есть три основных варианта решения этой проблемы: Вариант 1: Разбить метрику по периодам времени Самый простой способ — разделить совокупные метрики на периоды времени (например, конверсия за первую неделю, за вторую неделю и т. д.). Это позволяет получить ранние данные, сохраняя корректность сравнения (по принципу «сравнивать сопоставимое») и избегая смещения в пользу более старых когорт. Затем можно отобразить результат в виде когортной тепловой карты. Вот пример для воронки привлечения пользователей, в котором прослеживается динамика конверсии от регистрации до первой транзакции: Таким образом, можно увидеть, что при сравнении сопоставимого конверсия действительно ухудшается (конверсия за первую неделю снизилась с >20% до ~15% в недавних когортах). Анализируя только совокупную конверсию (последний столбец), мы бы не смогли отличить реальное снижение от неполных данных. Вариант 2: Изменить определение метрики В некоторых случаях можно изменить определение метрики, чтобы избежать работы с неполными данными. Например: вместо того чтобы анализировать, сколько сделок, добавленных в пайплайн в марте, было закрыто к настоящему моменту, можно рассматривать, сколько из закрытых в марте сделок были выиграно или проиграно. Это число не изменится со временем, тогда как для получения окончательных результатов по когорте сделок за март может потребоваться несколько месяцев. Вариант 3: Прогнозирование На основе предыдущих данных можно спрогнозировать, каковы будут финальные результаты когорты. Чем больше времени проходит и чем больше данных собирается, тем ближе прогноз будет к реальному значению. Однако будьте внимательны:прогнозирование показателей когорты требует тщательного подхода, здесь легко допустить ошибку. Например, в B2B‑бизнесе с низкими коэффициентами выигрыша одна сделка может значительно изменить показатели когорты. Точно спрогнозировать это крайне сложно. Часть 3: Извлечение инсайтов из данных Все эти данные полезны, но как получить из них инсайты? У вас наверняка не будет времени регулярно анализировать каждую метрику, поэтому начните с приоритизации самых больших разрывов и изменений: Где команды не достигают своих целей? Где вы видите неожиданные успехи? Где команды не достигают своих целей? Где вы видите неожиданные успехи? Какие метрики резко снижаются? Какие тренды разворачиваются? Какие метрики резко снижаются? Какие тренды разворачиваются? После выбора интересующей вас тенденции нужно будет углубиться в анализ и выявить основную причину, чтобы бизнес‑партнёры смогли разработать точечные решения. Чтобы придать углублённым исследованиям структуру, я рассмотрю ключевые типы тенденций метрик, с которыми вы вероятнее всего столкнётесь, и приведу конкретные примеры для каждого из них, основанные на реальном опыте. 1. Нейтральные движения метрик Когда вы замечаете резкое изменение метрики, сначала поднимитесьвверхпо дереву драйверов, прежде чем спускаться вниз. Вы поймёте, влияет ли данное изменение на то, что действительно важно для вас и вашей команды. Если нет, то поиск первопричины становится менее приоритетным. Пример:На изображении выше видно, что конверсия с визитов в регистрации на сайте резко упала. Вместо того чтобы паниковать, вы смотрите на общее количество регистраций и видите, что оно остаётся стабильным. Выясняется, что снижение средней конверсии вызвано скачком низкокачественного трафика на сайт; эффективность вашего «основного» трафика осталась неизменной. 2. Числитель против знаменателя При изменении метрик‑отношений (ratio metrics) — например, показы на активного пользователя, поездки на водителя и т. д. — сначала проверьте, изменился числитель или знаменатель. Люди часто предполагают, что изменился числитель, так как в краткосрочной перспективе мы обычно стараемся увеличить метрику вовлечённости или продуктивности. Однако нередко это оказывается неверным. Примеры: Вы видите снижение количества лидов на одного продавца, потому что к команде только присоединилась новая группа сотрудников, а не из‑за проблемы с генерацией спроса. Вы видите снижение количества лидов на одного продавца, потому что к команде только присоединилась новая группа сотрудников, а не из‑за проблемы с генерацией спроса. Количество поездок на одного водителя Uber в час снизилось не потому, что уменьшилось количество запросов от пассажиров, а потому что команда увеличила стимулы, и больше водителей вышли на смену. Количество поездок на одного водителя Uber в час снизилось не потому, что уменьшилось количество запросов от пассажиров, а потому что команда увеличила стимулы, и больше водителей вышли на смену. 3. Изолированные / концентрированные тренды Многие изменения метрик обусловлены событиями, происходящими только в определённой части продукта или бизнеса, и агрегированные данные не дают полной картины. Общий процесс диагностики для выявления первопричины выглядит следующим образом: Шаг 1:Продолжайте декомпозировать метрики, пока не сможете изолировать тренд или пока не достигнете точки, в которой метрики больше нельзя разложить. Так же, как в математике любое число можно разложить на простые множители, любую метрику можно разбить вплоть до фундаментальных входных данных. Сделав это, вы сможете изолировать проблему в определённой части дерева драйверов, что значительно упростит понимание происходящего и выбор правильного ответа. Шаг 2:Сегментируйте данные, чтобы изолировать соответствующий тренд Сегментация помогает определить, является ли конкретная область бизнеса причиной проблемы. Сегментируя данные по следующим параметрам, можно выявить более 90% проблем: География (регион / страна / город) География (регион / страна / город) Время (период месяца, день недели и т. д.) Время (период месяца, день недели и т. д.) Продукт (разные SKU или разделы продукта, например, лента Instagram vs. Reels) Продукт (разные SKU или разделы продукта, например, лента Instagram vs. Reels) Демография пользователей или клиентов (возраст, пол и т. д.) Демография пользователей или клиентов (возраст, пол и т. д.) Отдельные субъекты / участники (например, сотрудник отдела продаж, продавец, пользователь) Отдельные субъекты / участники (например, сотрудник отдела продаж, продавец, пользователь) Рассмотрим конкретный пример: Предположим, вы работаете в DoorDash и видите, что количество завершённых доставок в Бостоне сократилось по сравнению с предыдущей неделей. Вместо того чтобы разрабатывать идеи для увеличения спроса или повышения показателя завершения заказов, попробуем изолировать проблему, чтобы разработать более целевые решения. Первый шаг — разложить метрику «Завершённые доставки»: На основе дерева драйверов мы можем исключить проблемы со стороны спроса. Вместо этого становится очевидным, что в последнее время мы испытываем сложности с привлечением водителей для выполнения заказов (на этапе же передачи заказа от ресторана курьеру или доставки еды клиенту проблем нет). Последний шаг — проверить, носит ли эта проблема массовый характер. В этом случае одними из самых перспективных разрезов для анализа будут география, время и поставщик (ресторан). Анализ данных по поставщикам показывает, что проблема широко распространена и затрагивает множество ресторанов, что не помогает нам сузить круг. Однако, создав тепловую карту времени и географии для метрики «заказы на доставку, для которых не удалось найти курьеров», мы обнаруживаем, что проблема в основном затрагивает окраины Бостона в ночное время: Что делать с этой информацией? Умение точно выявить проблему позволяет направить усилия на целевое привлечение курьеров и предоставление стимулов именно в эти промежутки времени и в этих локациях, а не распределять ресурсы равномерно по всему Бостону. Другими словами, изоляция первопричины позволяет использовать ресурсы более эффективно. Другие примеры концентрированных трендов, с которыми вы можете столкнуться: Большая часть внутриигровых покупок в онлайн‑игре совершается небольшим числом так называемых«китов»— поэтому команда сосредотачивает усилия на удержании и вовлечении этих пользователей. Большая часть внутриигровых покупок в онлайн‑игре совершается небольшим числом так называемых«китов»— поэтому команда сосредотачивает усилия на удержании и вовлечении этих пользователей. Большинство эскалаций заявок в техподдержке до инженерного отдела вызвано действиями небольшого числа сотрудников поддержки — что предоставляет компании возможность освободить время инженеров путём обучения этих сотрудников. Большинство эскалаций заявок в техподдержке до инженерного отдела вызвано действиями небольшого числа сотрудников поддержки — что предоставляет компании возможность освободить время инженеров путём обучения этих сотрудников. Одной из наиболее распространённых причин путаницы при анализе производительности являются сдвиги в структуре ипарадокс Симпсона. Под сдвигами в структуре подразумеваются изменения в структуре общей совокупности.Парадокс Симпсона описывает контринтуитивный эффект, при котором тренд, наблюдаемый в общей совокупности, исчезает или меняется на противоположный при анализе её составляющих (и наоборот). Как это выглядит на практике? Допустим, вы работаете в YouTube или любой другой компании, размещающей рекламу. Вы замечаете, что доходы падают, и, углубляясь в данные, видите, что CPM (затраты на тысячу показов) уже некоторое время снижается. CPM как метрика не может быть дальше декомпозирована, поэтому вы начинаете сегментировать данные, но не можете обнаружить первопричину. Например, CPM во всех географических регионах остаётся стабильным: Здесь и вступают в игру сдвиги в структуре и парадокс Симпсона.CPM в каждом отдельном регионе остаётся неизменным, но если вы посмотрите на состав показов по регионам, то обнаружите, что их доля смещается от США к регионуAPAC. Так как CPM в APAC ниже, чем в США, общий CPM уменьшается. Знание точной первопричины позволяет разрабатывать более целевые решения. Основываясь на этих данных, команда может: Попробовать восстановить рост в регионах с высоким CPM. Попробовать восстановить рост в регионах с высоким CPM. Рассмотреть дополнительные возможности монетизации для APAC. Рассмотреть дополнительные возможности монетизации для APAC. Сосредоточиться на компенсировании более низкой ценности отдельных показов за счёт значительного увеличения объёма показов на крупном рынке APAC. Сосредоточиться на компенсировании более низкой ценности отдельных показов за счёт значительного увеличения объёма показов на крупном рынке APAC. Заключительные мысли Помните, данные сами по себе не имеют ценности. Они становятся полезными, когда используются для генерации инсайтов или рекомендаций для пользователей или внутренних заинтересованных сторон. Следуя структурированному подходу, вы сможете надёжно выявлять значимые тренды в данных, а также, используя приведённые советы, выделять полезную информацию из шума и избегать неверных выводов. В завершение темы приглашаем всех желающих на открытые уроки: 6 февраля: «Цифры решают все: как внедрение метрик и KPI ускоряет достижение целей».Подробнее 6 февраля: «Цифры решают все: как внедрение метрик и KPI ускоряет достижение целей».Подробнее 11 февраля: «От запроса к решению: как разобраться в потребностях заказчика и не упустить главное».Подробнее 11 февраля: «От запроса к решению: как разобраться в потребностях заказчика и не упустить главное».Подробнее Посмотреть полный список бесплатных уроков по аналитике и анализу, а также по другим ИТ-направлениям можнов календаре. "
  },
  {
    "article_id": "https://habr.com/ru/companies/banki/articles/877562/",
    "title": "DIP, SLAP, Coupling — База",
    "category": "Дизайн",
    "tags": "solid, принципы solid, solid принципы, solid принципы программирования, инверсия зависимостей, принцип инверсии зависимостей",
    "text": "Всем привет! Я Борис Зырянов, разработчик в команде Платформы. В этой статье хочу рассказать про Dependency Inversion Principle, потому что это, пожалуй, один из самых важных принципов SOLID, понимание которого дает ключи к архитектуре программного обеспечения. Задача, стоящая перед настоящим текстом — придать объем DIP, продемонстрировав, как из лаконичных дефиниций следуют строгие правила по организации кода. В статье будут очерчены критерии корректного использования принципа, разобраны его определения и рассмотрен пример применения. Это даст понятную модель организации зависимостей кода, которую вы сможете применить на практике. Предисловие Это вторая версия статьи переработанная и улучшенная. Первая версия публиковалась в моемблоге. Декомпозиция формулировок и классификация кода Известно два каноничных определения принципа от его автора — Роберта Мартина. К другим определениям DIP мы будем обращаться при необходимости по ходу статьи (здесь и далее все определения будут приведены «as is» без перевода, во-первых, потому что приводятся цитаты, во-вторых, потому что еще один вариант перевода абсолютно излишен). Приведу определения в хронологическом порядке. Определение первое от 1996 года: A. High level modules should not depend upon low level modules. Both should depend upon abstractions. B. Abstractions should not depend upon details. Details should depend upon abstractions. Martin R. The Dependency Inversion Principle Определение второе от 2020 года: Depend in the direction of abstraction. High level modules should not depend upon low level details. Martin R. Solid Relevance Сразу оговоримся, что понимать слово«module»следует как некий структурный элемент кода, обособленную часть программы, отвечающую за конкретный функционал. В зависимости от вашего языка программирования это может быть пространство имен, пакет или действительно модуль (привет, Java). На самом деле, независимо от того, какие возможности предлагает ваш язык для объединения кода в модули, DIP сохраняет применимость для организации зависимостей между ними. И начнем разбор формулировок DIP с той части, где упоминается про модули и детали —«High level modules should not depend upon low level modules (details)». Представим, что у нас уже есть код некоторого приложения, к которому мы и будем применять DIP. Если мы попытаемся применить эту часть определений DIP к коду, то обнаружим, что нам навязывается «пространственная» лексика — появляется некоторая «вертикальность»:«high level modules»и«low level modules (details)». И, шире, иерархичность («should not depend upon»). Это приводит к тому, что отношения между модулями получают дополнительное измерение — вертикальную ось, относительно которой измеряется «уровень» модуля, определяется «низкоуровневость» деталей и выстраивается иерархия модулей (по нюансам различий между модулями и деталями пройдемся позднее). Далее, чтобы продолжить применять DIP, нам необходимо выделить в коде приложения модули, а затем расположить модули относительно вертикальной оси (далее эту ось мы будем называтьось уровня абстракции), выстроив иерархию между ними. Выделять модули в коде и строить зависимости между ними начнем с решения более общей задачи, а именно с распределения кода относительно оси уровня абстракции. Для этого нам необходимо классифицировать код, который можно встретить в любом приложении. А так как уровень кода определяется назначением (характером решаемых задач) кода (Мартин Р. Чистая архитектура. Искусство разработки программного обеспечения. СПб.: Питер, 2020. С. 187), то и классификацию будем проводить по этому основанию. По назначению код приложения можно разделить на 3 категории: Бизнес-правила (домен, бизнес-логика, high level policy, etc.). Код, который описывает логику предметной области и обслуживает интересы заказчика. Собственно, это тот код, ради которого вы начали писать приложение. Бизнес-правила (домен, бизнес-логика, high level policy, etc.). Код, который описывает логику предметной области и обслуживает интересы заказчика. Собственно, это тот код, ради которого вы начали писать приложение. Код приложения (application). Код, появившийся потому, что мы пишем наше приложение. Код, который обслуживает инстанцирование (подготавливает данные для и осуществляет вызовы) бизнес-правил и передает «выхлоп» от работы логики этих правил куда-то еще. Код приложения (application). Код, появившийся потому, что мы пишем наше приложение. Код, который обслуживает инстанцирование (подготавливает данные для и осуществляет вызовы) бизнес-правил и передает «выхлоп» от работы логики этих правил куда-то еще. Инфраструктурный код (infrastructure). Код, который обеспечивает взаимодействие с различными устройствами ввода/вывода (I/O) и потоками — драйверами различного ПО, устройствами, файловой системой, web-сервером, standard streams, etc. Инфраструктурный код (infrastructure). Код, который обеспечивает взаимодействие с различными устройствами ввода/вывода (I/O) и потоками — драйверами различного ПО, устройствами, файловой системой, web-сервером, standard streams, etc. Наложив выделенные категории на ось уровня абстракции, получим следующее распределение (рис. 1). Выше остальных категорий по оси располагаются высокоуровневые бизнес-правила («high level»), а внизу устройства ввода/вывода и код, который обеспечивает с ними взаимодействие («low level»). Распределение на рис. 1 показывает, насколько та или иная категория кода отстоит от решения бизнес-задач и обслуживает задачи ввода и вывода или наоборот — насколько далек код от взаимодействия с вводом/выводом и в какой степени решает бизнес-задачи. Именно удаленностью от ввода/вывода характеризуется уровень абстракции: чем дальше код от I/O, тем выше его уровень абстракции. Модули объединяют в более крупные структуры — слои. Задачи одной категории кода могут решать несколько слоев (в зависимости от общей архитектуры приложения). Чтобы не уходить в архитектурные дебри, будем считать, что количество слоев нашего приложения равно количеству категорий кода, а «уровень абстракции» и «слой приложения» — сходные до определенной степени понятия в контексте статьи. Особенностью большинства увиденных мной подходов к написанию кода является то, что классификация по назначению не всегда соответствует (почти никогда) реальной декомпозиции кода на структурные элементы. Тем не менее, в любом коде независимо от языка программирования, парадигмы и «чистоты» (даже если это процедура на несколько сотен строк) можно найти код соответствующий этим категориям — прежде чем что-то сделать с данными необходимо их каким-то образом получить, ну а то что ваш код делает с данными, как правило, кому-то нужно. Итак, код нашего гипотетического приложения мы сгруппировали в модули, модули у нас специализированные, то есть решают задачи, находящиеся на разных уровнях абстракции: кто-то в базу данных ходит (low level), а кто-то считает скидку на заказ в интернет-магазине у конкретного пользователя (high level). Это значит, что мы разобрались с «вертикальностью» и самое время идти дальше. А дальше нам необходимо выстроить взаимодействие (построить иерархию) между этими модулями. Поможет нам в этом дальнейшее прочтение определений DIP:«depend in the direction of abstraction»(определение №2). Или более подробно в определении №1 :«Both (high and low level modules) should depend upon abstractions». Здесь абстракция это контракт, выделенный в абстрактный класс или интерфейс. Необходимость проводить зависимости через абстракции проистекает из того, что контракт более стабилен, чем его конкретная реализация и не будет меняться со временем (либо будет это делать очень редко) (Мартин Р. Чистая архитектура. Искусство разработки программного обеспечения. СПб.: Питер, 2020. С. 101). Вообще,«depend in the direction of abstraction»это самая популярная часть определения, в которой зачастую игнорируют«in the direction», получая«dependin the directionof abstraction», чем и ограничивают весь DIP. Это приводит к его наивному пониманию: казалось бы, определяй зависимости через интерфейсы и вот он DIP. Однако, без учета уровня абстракции, на котором расположен код, и того места где расположена сама абстракция, определять зависимости через интерфейсы не то, чтобы было в целом бессмысленно, но такой подход соответствует DIP не в полной мере. Учитывать уровень абстракции (задачи, которые решает код) необходимо для выбора правильного направления зависимостей. И здесь самое время обратиться к еще одной формулировке DIP, которая прямо об этом говорит (определение №3): Abstractions should not depend on details Abstractions should not depend on details Code should depend on things that are at the same or higher level of abstraction Code should depend on things that are at the same or higher level of abstraction High level policy should not depend on low level details High level policy should not depend on low level details Capture low-level dependencies in domain-relevant abstractions Capture low-level dependencies in domain-relevant abstractions Schuchert B. L. DIP in the Wild Цитата выше дополняет нашу картину самым важным постулатом, который следует из каноничных формулировок —«Code should depend on things that are at the same or higher level of abstraction»: абстракция, от которой зависит код, должна находиться либо на одном уровне с кодом, либо на более высоком. Таким образом, общее направление зависимостей в приложении должно стремиться к наиболее удаленному от устройств ввода/вывода коду. Еще одно следствие из постулата выше: на одном уровне может быть сколько угодно зависимых друг от друга модулей и это не будет нарушением DIP (рис. 2). На зависимости между одноуровневыми модулями могут накладываться явные и неявные ограничения архитектурными парадигмами (hexagonal architecture, clean architecture, etc.) и подходами к декомпозиции (DDD, здравый смысл, конвенции на уровне команды, etc.), которые не являются предметом данной статьи. Необходимо оговориться, что в полной мере следование DIP невозможно, ввиду наличия конкретных деталей, предоставляемых языком программирования и используемых в коде. Конечно, можно считать условный класс String в условном языке программирования низкоуровневым механизмом и провести взаимодействие между ним и бизнес-логикой через все уровни абстракции, будто это какой-нибудь драйвер для базы данных. Но смысла в этом исчезающе мало, так как классы, которые предоставляет язык программирования, обычно достаточно стабильны и нам не нужно опасаться их изменчивости (Мартин Р. Чистая архитектура. Искусство разработки программного обеспечения. СПб.: Питер, 2020. С. 101). Поэтому следует определить область действия DIP: принцип применяется к тому коду, который пишет непосредственно разработчик. Также DIP не определяет взаимоотношения между типами внутри модуля (зависимость ApplicationClass1 от конкретного класса ApplicationClass2 на рис. 2 внутри модуля). О low level modules и low level details В первом и втором определении модули и детали упомянуты в равнозначном контексте и думаю, что это требует отдельного пояснения. Обычно деталями называют реализацию абстракции, поэтому модуль и деталь это пересекающиеся до определенного предела понятия. Так любой код, становится деталью по отношению к уровням выше. Так как является прямой реализацией высокоуровневого контракта, либо выполняет задачи специфичные для его уровня (и в конечном счете также помогая реализовать функционал модулям верхнего уровня), либо является связующим звеном между модулями верхнего уровня и внешним миром. В последнем случае можно говорить о переиспользовании кода модулей верхнего уровня модулями нижнего уровня — модули верхнего уровня дают одинаковые для любых нижних модулей правила («high level policy»определение №3). Инверсия А где, собственно, инверсия? Почему и зачем инверсия? Чтобы ответить на эти вопросы рассмотрим отношения между модулями с явным нарушением DIP. Для примера возьмем условный класс ApplicationUseCase, который зависит от классов Repository и Authorization, а вызывается в WebController (рис. 3). Зависимости ApplicationUseCase в модуле верхнего уровня, необходимые ему для решения своих задач, направлены против оси уровня абстракции сверху вниз: ApplicationUseCase напрямую зависит от конкретных классов Authorization и Repository, расположенных в модулях низкого уровня. Изменения, которые произойдут в этих классах (и в их модулях), будут влиять на все, что от них зависит, что сделает ApplicationUseCase потенциально нестабильным. Ну а прямая зависимость WebController от ApplicationUseCase нарушает часть«Details should depend upon abstractions»определения №1. Попробуем исправить это, применив DIP к текущим модулям (рис. 4). Поменяв прямую зависимость от классов Authorization и Repository в классе ApplicationUseCase на зависимость от интерфейсов (расположенных в границах модуля верхнего уровня) AuthorizationInterface и RepositoryInterface, в которых определили требования к необходимому поведению (далее для краткости просто «требования») для модулей нижнего уровня, мы инвертировали общее направление зависимостей (относительно рис. 3). Теперь зависимости направлены в сторону модуля верхнего уровня и более высокого уровня абстракции. Там, где зависимости были направлены «естестественным» образом в сторону модуля верхнего уровня, взаимодействие с ними теперь происходит через API  модуля — UseCaseInterface. UseCaseInterface может быть вызван там, где необходимо — консольная команда, обработчик сообщений очереди или, как здесь, веб-контроллер. Конечно, и без интерфейса можно было осуществлять вызовы методов ApplicationUseCase откуда угодно снизу, однако, контракт здесь создает стабильное API. И теперь связи между модулями на рис. 4 соответствуют DIP —  модули нижнего уровня зависят от модулей верхнего уровня, все модули зависят от абстракций. Инвертировав направление зависимостей, мы привели код в состояние, в котором: А. модули верхнего уровня «управляют» низкоуровневым кодом посредством определения требований в интерфейсах; B. низкоуровневый код реализует эти требования и обращается к высокоуровневому коду через абстракции, расположенные и реализуемых в модулях верхнего уровня. Определение требований к деталям в абстракциях и направление зависимостей в сторону верхнего уровня приводит к тому, что любой нижележащий модуль превращается в своего рода плагин, реализацию которого можно заменить. Это помогает упорядочить разработку: можно начинать с написания логики предметной области (то есть самого важного в вашем приложении), необходимых ей контрактов и интерфейсов (API) для ее запуска, проходя уровень за уровнем вниз. Несомненные (для меня, как разработчика, которому часто приходится зарываться в несколько проектов одновременно) плюсы такого подхода заключается в двух моментах. Первый, точно известно «business value» такого кода, то есть понятно как именно он «зарабатывает деньги» — достаточно взглянуть на модули верхнего уровня (хорошо, просто «взглянуть», очевидно, недостаточно, но также, очевидно, что выделенная предметная логика потратит гораздо меньше вашего времени на свое освоение). Второй, как правило, отсутствует или сведена до необходимого «техническая интоксикация» в модулях верхних уровней. Под «технической интоксикацией» я понимаю ситуацию, когда в коде, который как раз и «зарабатывает деньги» присутствует низкоуровневая семантика, например, методы именуются в стиле handleRequest, refreshCache, transformData, etc. Прямо скажем, вряд ли вашему бизнесу приносит деньги непосредственно кэширование, вероятно, без него ваш продукт не сможет обработать большое число запросов, возможно оно необходимо, но оно не главное. Задачи предметной области служат отправной точкой для семантики наименования поведения и контрактов на любом уровне абстракции. Семантика предметной области «проникает» с верхнего уровня на модули уровнями ниже через реализуемые абстракции, модули нижних уровней как бы «говорят» на языке модулей верхнего уровня —«Capture low-level dependencies in domain-relevant abstractions»из определения №3. Однако, что именно приносит деньги зависит от предметной области и если ваша бизнес-логика достаточно низкоуровневая (например, вы действительно разрабатываете решение для кэширования данных), то, очевидно, что «техническая интоксикация» это неизбежность. Но и в этом случае выстроенные по DIP зависимости помогут быстрей понять суть вашего программного продукта пришедшему со стороны человеку (здесь, вероятно, стоит поднять вопрос о необходимости такого подхода в применении к низкоуровневым решениям, но статьей предполагается, что все требования собраны, необходимые изыскания проведены и вы решили, что вам нужен DIP; исследование границ применимости остаются за рамками этого текста). DIP и Single Level of Abstraction Principle Здесь я хотел бы заострить внимание на уровнях абстракции и провести некие линии между DIP и Single Level of Abstraction Principle (SLAP), думаю, что это даст отличный ориентир при написании кода. SLAP упоминается в книге «Clean Code: A Handbook of Agile Software Craftsmanship» (Martin R. C. Clean code: a handbook of agile software craftsmanship. – Pearson Education, 2009. С. 36), выделенного определения в книге нет, а сам принцип сформулирован как требование для функций, которое впоследствии стало именоваться SLA принципом. Это требование выглядит так: In order to make sure our functions are doing “one thing,” we need to make sure that the statements within our function are all at the same level of abstraction. Martin R. Clean Code: A Handbook of Agile Software Craftsmanship Приведу пример функции на golang, которая нарушает этот принцип: Функция читает из файла, преобразует прочитанное в слайс со структурами, рассчитывает скидку и записывает результат в некое хранилище. Такой набор операций согласно SLAP слишком обширен, так как в функции пересекаются обязанности кода разных уровней абстракции. Если DIP требует установить иерархические отношения между уровнями и модулями, то SLAP, по сути, накладывает требование на код модуля выполнять обязанности соответствующие конкретному уровню абстракции.Поэтому SLAP является хорошим маркером, который может подсказать, что что-то идет не так при выстраивании зависимостей. Coupling, интерфейсы и еще раз инверсия Уровень абстракции по определению включает в себя множество модулей и выступает в качестве ориентира для выстраивания направления зависимостей, но для описания и упорядочения отношений между модулями существуют более точные критерии, определяемые черезметрики пакетов. Применимость метрик достаточно широка — метрики можно снимать и с класса, и с модуля, пакета и т.д., в общем с почти любого интересующего вас контекста. Метрики модуля afferent coupling («Ca», количество типов в других модулях, которые знают о типах в данном модуле) и efferent coupling («Ce», количество типов в других модулях, о которых знают типы в данном модуле) показывают отношения модуля с окружающим его кодом. На основе afferent coupling и efferent coupling рассчитывается еще одна метрика — instability, отражающая устойчивость модуля, это отношениеCeк общему числу связей модуля (Сa+Ce). Высокое число использований кода модуля другими модулями и низкое количество использования кода других модулей внутри данного модуля приводит к тому, что модуль становится устойчивым и менее подверженным влиянию извне. Такой модуль сложнее сломать, сломав что-то, от чего он зависит. Что и логично — чем меньше код знает об окружающем мире, тем меньше на него этот мир влияет и чем больше этот код используется извне, то тем больше он имеет ответственности. Почему нам вообще нужно считать связность (coupling) между модулями? Потому что идея лежащая в основе DIP это переиспользование модулей верхнего уровня за счет их абстрагирования от низкоуровневого кода и уменьшения связности между модулями. Логично предположить, что при организации зависимостей согласно DIP afferent coupling высокоуровневых модулей должна расти, instability модулей в направлении зависимостей должна уменьшаться (Мартин Р. Чистая архитектура. Искусство разработки программного обеспечения. СПб.: Питер, 2020. С. 133), а efferent coupling от низкоуровневых модулей не должна возникать (отсутствующая efferent coupling от модулей низкого уровня как раз свидетельствует об абстрагированности от них). Однако, при выстраивании зависимостей и попытке посчитать связность между модулями мы можем столкнуться с интересной коллизией компоновки, которая нарушает наше предположение о том как должны вести себя метрики. Если расположить интерфейс за пределами модуля верхнего уровня, то его метрика efferent coupling увеличится на число вынесенных за модуль контрактов, а afferent coupling, вместо ожидаемого увеличения, не изменится или уменьшится (рис. 5). Как следствие instability (на рис. 5 «I») верхнеуровневого модуля будет расти. Более того, в текущей компоновке модулей проявилась такая же проблема как на рис. 3 — модуль верхнего уровня стал зависеть от модулей нижнего уровня. Так как модуль скомпонован так, что интерфейсы, необходимые ему, находятся вне его границ и вне границ его уровня абстракции, но рядом с реализацией в модулях нижнего уровня. Идея скомпоновать абстракцию и её реализацию рядом друг с другом в модуле нижнего уровня может и выглядит логично, но в нашем случае появляются следующие проблемы. Во-первых, верхнеуровневый модуль, которому эти контракты необходимы, начинает знать о чем-то, что находится за пределами его уровня абстракции и такая компоновка нарушает инкапсуляцию верхнеуровневого модуля (так как что-то, что модулю необходимо для решения его задач, находится за его границами). Во-вторых, увеличивается его efferent coupling и, как следствие, instability. Мы помним что DIP не накладывает ограничения на одноуровневые зависимости и efferent coupling от одноуровневых зависимостей вполне приемлем. Но в данном случае у нас меняется направление зависимостей, которое определяется DIP, и мы получаем efferent coupling, возникающий из-за того, что интерфейсы находятся в модуле, расположенном ниже по оси уровня абстракции. Располагая интерфейс в конкретном модуле, мы подразумеваем, что интерфейс будет выражать требования модуля или служить его API. И расположив интерфейс вне модуля верхнего уровня, мы создали ситуацию, когда из описания необходимого для модуля верхнего уровня поведения контракт превратился в API низкоуровневого модуля, которое вызывается модулем верхнего уровня. Такая компоновка предполагает, что рано или поздно где-то появится (или уже есть) еще один модуль, который воспользуется поведением, описанным в интерфейсе модуля нижнего уровня. Рано или поздно возникнет пересечение требований по этому интерфейсу. Как должен развиваться этот контракт? Требования какого модуля будут приоритетней? Будет ли такой контракт«domain-relevant»? Посмотрим на Low Level Module с RepositoryInterface внутри. Это устойчивый модуль нижнего уровня, оказывающий влияние на модуль верхнего уровня (High Level ModuleI=0 рис. 5). Модулям, зависящим от RepositoryInterface, либо придется использовать универсальный контракт, семантически никак не связанный с решаемыми задачами предметной области (именование методов в духе findBy, findOneBy, findAll, etc., для того чтобы быть пригодным к использованию примерно везде), либо RepositoryInterface нарушит Interface Segregation Principle и станет включать множество методов, которые нужны разным модулям. Либо же, чтобы не нарушать ISP, модуль с RepositoryInterface будет содержать несколько интерфейсов, делящие между собой методы по их модулям-потребителям. Однако, тактические успехи в части следования ISP и каким угодно еще принципам не спасут нас от стратегических просчетов. Из такого положения вещей мы получаем два следствия: первое — фокус с решения задач предметной области смещается на поддержку и развитие модуля нижнего уровня и его контрактов. Второе (и главное) — логика предметной области в модулях верхнего уровня становится ограниченно переиспользуемой или не переиспользуемой вовсе, так как зависит от поведения модулей нижнего уровня (это поведение нужно будет учитывать при вызовах модуля верхнего уровня). Также использование методов RepositoryInterface в ApplicationUseCase нарушает SLAP: модуль верхнего уровня использует что-то внутри метода своего класса, что расположено не на его уровне абстракции, а уровнем ниже. Таким образом, вместо переиспользуемого модуля верхнего уровня мы получаем переиспользуемый модуль нижнего уровня и это явно не тот результат применения DIP, который мы хотим видеть. В случае с инвертированными зависимостями такая ситуация просто не может возникнуть. Каждый уровень абстрагирован от более низкого не просто за счет того, что зависимости между разноуровневыми модулями проходят по абстракциям. А за счет того, что эти абстракции описывают необходимое поведение для модулей верхнего уровня (и от этих же абстракций зависят модули и на верхнем, и на нижнем уровне). Это и обеспечивает общее направление зависимостей в сторону верхних уровней абстракции. Низкоуровневое поведение не должно появляться в модулях верхнего уровня. В контрактах должно описываться то поведение, которое соответствует уровню решаемых модулем задач. Попробуем исправить сложившуюся ситуацию — поменяем компоновку модулей и снова посчитаем метрики (рис. 6). Если вернуть ответственность за формирование контрактов в модуль верхнего уровня (переместив интерфейс, содержащий необходимое модулю поведение, внутрь его границ), то метрики High Level Module будут демонстрировать ожидаемое поведение — afferent coupling увеличится, instability уменьшится, а efferent coupling от низкоуровневых модулей пропадет. При такой компоновке логика расположенная в High Level Module станет более устойчива (и абстрагирована), а зависимости инвертированы — модули нижнего уровня зависят от модуля верхнего уровня; все три модуля зависят от абстракции; зависимости направлены в сторону высшего уровня абстракции. Модуль верхнего уровня сохраняет инкапсуляцию: он не зависит от поведения модулей нижнего уровня — требуемое ему поведение описано в его контракте и реализуется в модулях нижнего уровня. Инверсия зависимостей буквально освобождает модули на любом уровне абстракции от знания о чем-то, что находится на уровнях ниже. Метрики, описывая связи модуля, как и SLAP, являются отличным подспорьем для того, чтобы понимать, что происходит с вашим кодом. В коде Вернемся к прошлому листингу и представим, будто бы это фрагмент гипотетического приложения из начала статьи до того, как мы декомпозировали его на специализированные модули. Попробуем выделить смешанные в одной функции уровни, обрисовав широкими мазками как мог бы выглядеть этот код, соответствуй он и DIP и SLAP. Для это взглянем на функцию еще раз и найдем в ней код, принадлежащий разным уровням абстракций: Определив, какой код решает задачи какого уровня абстракции, можно двигаться дальше и компоновать код в модули. Начнем делать это с выделения модулей верхнего уровня: Теперь логика предметной области сконцентрирована вpackage product, также она выдвигает некие требования в виде интерфейсаIdGeneratorи предоставляет методы для ее запуска через интерфейсProduct. Пройдем на уровень ниже и выделим модули уровня приложения: Для расчета скидки на товар нам необходимо получить данные, после их обработать и передать результат в хранилище. Определяем требования для модулей нижнего уровня в видеRepositoryиLoaderинтерфейсов и рассчитываем скидку с помощью интерфейсаProductс уровня выше. Так же, как и интерфейсProduct, интерфейсProductServiceпредоставляет методCalculateDiscountв качестве API. Также в этом пакете реализуется интерфейсIdGenerator. Идем дальше и смотрим, что происходит в самом низу: А в самом низу на инфраструктурном уровне происходит реализация интерфейсов уровня приложенияRepositoryиLoaderвpackage repositoryиpackage loader, соответственно. Также вpackage loaderиспользуется интерфейсFactoryдля того, чтобы инстанцировать модельProductпредметной области. Здесь мы не ограничены ни конвенциями, ни архитектурными правилами и поэтому можем использовать высокоуровневый интерфейс напрямую, минуя application уровень, избегая излишней для примера конвертации типов данных. Также где-то на этом уровне должен существовать пакет с обработчиками сигналов к приложению извне, в который делается инъекция с реализациейapplication.ProductService, у которой вызываетсяCalculateDiscount, чтобы запустить всю цепочку вызовов. Что же, кажется, мы декомпозировали код в модули, и теперь код соответствует и DIP, и SLAP. Посмотрим как выглядят связи между модулями в UML, где интересующий нас контекст модуля ограничен пакетом (рис. 7). На схеме видно, что зависимости между пакетами направлены в сторону верхних уровней, а сами зависимости проходят через абстракции. В тоже время, абстракции или описывают необходимое для модулей верхнего уровня поведение, или служат в качестве API для таких модулей. Здесь я отображал связи между реализацией интерфейса с помощью «утиной типизации» и самим интерфейсом: пусть и неявная, но связь между абстракцией и её имплементацией все равно существует (строго говоря, такой тип связи можно не учитывать в расчетах, но в примере дальше я буду принимать их в расчет; в целом же вопрос «как считать связность утиной типизации?» мне кажется вопросом конвенций). Если посчитать coupling и instability для пакетов, то получим следующее: package afferent coupling efferent coupling instability = Ce / (Ce + Ca) product 4 0 0 application 2 2 0.5 repository 0 1 1 loader 0 3 1 Метрика instability уменьшается в направлении зависимостей: на самом верху оказались очень устойчивые модули, но чем ниже к I/O, тем выше instability. Кажется, что именно этого мы и добивались — на пакет с логикой, реализующей правила предметной области, не влияет ничего, кроме того, что расположено в этом же пакете (что, кстати говоря, кажется отличным вариантом с учетом того, что мы не хотим случайно повлиять на правила расчета скидок) и требований предметной области. При малой связности между модулями упрощается оперирование кодом, например, появляется простота в тестировании. ПриCe=0 у type product не требуется никаких моков, testcontainers и тому подобных техник и технологий — старые добрые (и дешевые) unit-тесты вполне надежно (и быстро) проверят все кейсы расчета скидок на товар, а значит, самая важная часть вашего приложения будет еще и надежна. Изолированная на верхнем уровне предметная логика (Ce=0 у package product) также обещает беспроблемное написание тестов. А в целом такой подход это хорошая заявка на построение пирамиды тестирования: на модули сверху легко писать unit-тесты (более того, модулям верхнего уровня комплементарен подход Test-Driven Development), а модули ниже вполне удобно закрываются интеграционными тестами. Впрочем, про тестирование и организацию модулей верхнего уровня с их high level policies поговорим как-нибудь в другой раз. Вероятно, читатель справедливо заметит, что маленькое и понятное превратилось в большое и развесистое и спросит про плюсы и минусы такого подхода. На что я отвечу, что маленькое и понятное оно только потому, что это синтетический пример, придуманный мной для этой статьи. В нашей печальной реальности это будет метод на несколько экранов с side-эффектами из-за неожиданной конфигурации условного хранилища или еще какого-нибудь ПО вокруг. Тем не менее, вопрос — что вам даст такой подход и что потребует взамен — имеет место быть, поэтому отмечу сначала преимущества. Преимущества такого подхода происходят из декомпозиции кода в модули, а выстраивание связей между модулями согласно DIP придает такой модульности следующий смысл: Снижается когнитивная нагрузка: программный продукт, в частности, его цели, становится прозрачным для разработчика. Код легко понимать, если держать в голове, что главное – это то, какой код «зарабатывает деньги». Человеку, впервые пришедшему на проект, достаточно изучить модули верхнего уровня, содержащие в себе дистиллированные правила предметной области, а во все остальное погружаться при необходимости. Не нужно тратить время на утомительный поиск и изучения логики-которая-приносит-деньги, размазанной по всему проекту и перемешанной с низкоуровневым кодом с имплицитной семантикой. Снижается когнитивная нагрузка: программный продукт, в частности, его цели, становится прозрачным для разработчика. Код легко понимать, если держать в голове, что главное – это то, какой код «зарабатывает деньги». Человеку, впервые пришедшему на проект, достаточно изучить модули верхнего уровня, содержащие в себе дистиллированные правила предметной области, а во все остальное погружаться при необходимости. Не нужно тратить время на утомительный поиск и изучения логики-которая-приносит-деньги, размазанной по всему проекту и перемешанной с низкоуровневым кодом с имплицитной семантикой. Равномерность поставки изменений: при разработке такого кода можно легко определить «ось изменений» (модули, которые будут разрабатываться или меняться) — ваш код прозрачный и предсказуемый, нет проблем с определением точки приложения усилий. Планировать разработку такого кода и модифицировать его значительно проще, так как модификация и разработка проходят через ряд изолированных друг от друга модулей с изменением и разработкой только необходимого. Равномерность поставки изменений: при разработке такого кода можно легко определить «ось изменений» (модули, которые будут разрабатываться или меняться) — ваш код прозрачный и предсказуемый, нет проблем с определением точки приложения усилий. Планировать разработку такого кода и модифицировать его значительно проще, так как модификация и разработка проходят через ряд изолированных друг от друга модулей с изменением и разработкой только необходимого. Большая тестируемость: ваш код не отправит в доставку две единицы товара, вместо одной и 100% скидки на оплату ваш код тоже не посчитает (это ли не счастье спать спокойно?). Как минимум, вам легко получить минимальный набор гарантий работоспособности того, ради чего вы пишите ваше приложение, как максимум, вы можете построить классическую пирамиду тестирования. Большая тестируемость: ваш код не отправит в доставку две единицы товара, вместо одной и 100% скидки на оплату ваш код тоже не посчитает (это ли не счастье спать спокойно?). Как минимум, вам легко получить минимальный набор гарантий работоспособности того, ради чего вы пишите ваше приложение, как максимум, вы можете построить классическую пирамиду тестирования. DIP это framework agnostic by design: в примере наш код, начиная с application уровня, является framework agnostic — модули верхнего уровня свободны от знаний о деталях, они решают свои задачи, а все, что им необходимо находится в абстракциях и реализуется где-то внизу. Что-то конкретное про окружающий мир знают только модули инфраструктурного уровня — какой фреймворк и что за хранилище используется. Поэтому, например, переезд с файловой системы на S3 для модулей верхнего уровня пройдет незамеченным. Да и вообще любые изменения в любом модуле нижнего уровня не будут влиять на модули верхнего уровня. Модули нижнего уровня это просто плагины для вашей логики, которые можно легко менять. DIP это framework agnostic by design: в примере наш код, начиная с application уровня, является framework agnostic — модули верхнего уровня свободны от знаний о деталях, они решают свои задачи, а все, что им необходимо находится в абстракциях и реализуется где-то внизу. Что-то конкретное про окружающий мир знают только модули инфраструктурного уровня — какой фреймворк и что за хранилище используется. Поэтому, например, переезд с файловой системы на S3 для модулей верхнего уровня пройдет незамеченным. Да и вообще любые изменения в любом модуле нижнего уровня не будут влиять на модули верхнего уровня. Модули нижнего уровня это просто плагины для вашей логики, которые можно легко менять. Изолированные проблемы: как минимум, у вас сократится время на поиск места где что-то идет не так, как максимум, вы сможете легко реализовать graceful degradation («это ли не счастье спать спокойно?»^2). Изолированные проблемы: как минимум, у вас сократится время на поиск места где что-то идет не так, как максимум, вы сможете легко реализовать graceful degradation («это ли не счастье спать спокойно?»^2). Переиспользуемая бизнес-логика: зачем нам модульность без переиспользования кода? При таком подходе логика предметной области, инкапсулированная в модулях верхнего уровня, переиспользуется «естественным» образом (что, в общем-то, и является одной из главных целей DIP). Переиспользуемая бизнес-логика: зачем нам модульность без переиспользования кода? При таком подходе логика предметной области, инкапсулированная в модулях верхнего уровня, переиспользуется «естественным» образом (что, в общем-то, и является одной из главных целей DIP). Преимущества мы осветили, теперь можно переходить и к перечислению платы за них: Инвестиции в разработку: как ни странно, но, чтобы заработать на этом деньги, вам придется сначала инвестировать время — в людей, в процессы, в существующий код, а вероятнее всего, во все это сразу. Нельзя просто так взять и внедрить best practices (да и серебряных пуль на рынке не осталось, увы). Инвестиции в разработку: как ни странно, но, чтобы заработать на этом деньги, вам придется сначала инвестировать время — в людей, в процессы, в существующий код, а вероятнее всего, во все это сразу. Нельзя просто так взять и внедрить best practices (да и серебряных пуль на рынке не осталось, увы). Архитектура: сам по себе DIP налагает известные ограничения на зависимости между модулями для уменьшения связности и только. Чтобы извлечь из модульности максимальную пользу необходимы дополнительные правила взаимодействия для модулей. А дополнительные правила ведут к дополнительным затратам, особенно на первых этапах, и приносят видимые дивиденды не сразу. Архитектура: сам по себе DIP налагает известные ограничения на зависимости между модулями для уменьшения связности и только. Чтобы извлечь из модульности максимальную пользу необходимы дополнительные правила взаимодействия для модулей. А дополнительные правила ведут к дополнительным затратам, особенно на первых этапах, и приносят видимые дивиденды не сразу. Существующие стандарты: явные или неявные стандарты, договоренности и конвенции существуют всегда и на разных уровнях и, как бы то ни было, но код мы обычно пишем так, чтобы он был понятен большинству разработчиков в вашей компании (или команде). Сложившиеся практики по управлению зависимостями сулят еще одну статью затрат («нельзя просто так взять и внедрить best practices» x2). Существующие стандарты: явные или неявные стандарты, договоренности и конвенции существуют всегда и на разных уровнях и, как бы то ни было, но код мы обычно пишем так, чтобы он был понятен большинству разработчиков в вашей компании (или команде). Сложившиеся практики по управлению зависимостями сулят еще одну статью затрат («нельзя просто так взять и внедрить best practices» x2). Границы применимости: в любом случае, прежде чем применять какой-либо подход, необходимо убедиться, что он принесет пользу. Например, при написании маленьких (действительно маленьких — несколько экранов кода, пара-тройка ручек и т.п.) приложений вы возможно придете к выводу, что ни DIP, ни модульность вам не нужна. А, возможно, нужна, если у маленького приложения планируется большое будущее (увы, но иногда такое будущее наступает очень внезапно). Если же у вас присутствует необходимость экономить процессорные такты, то тоже стоит получше взвесить все за и против. Границы применимости: в любом случае, прежде чем применять какой-либо подход, необходимо убедиться, что он принесет пользу. Например, при написании маленьких (действительно маленьких — несколько экранов кода, пара-тройка ручек и т.п.) приложений вы возможно придете к выводу, что ни DIP, ни модульность вам не нужна. А, возможно, нужна, если у маленького приложения планируется большое будущее (увы, но иногда такое будущее наступает очень внезапно). Если же у вас присутствует необходимость экономить процессорные такты, то тоже стоит получше взвесить все за и против. Вам придется писать больше кода: модули, абстракции, уровни, архитектура — все это на самом деле раздует вашу кодовую базу. Но ничего не поделаешь: если разделять большое и сложное на много маленьких и относительно простых элементов, то правила и ограничения, при которых это разделение будет иметь смысл, потребуют плату в виде строчек кода. А еще всем этим нужно будет управлять для того, чтобы в один прекрасный момент ваша жизнь не превратилась в ад (впрочем, это справедливо для любой кодовой базы и любого IT-ландшафта). Вам придется писать больше кода: модули, абстракции, уровни, архитектура — все это на самом деле раздует вашу кодовую базу. Но ничего не поделаешь: если разделять большое и сложное на много маленьких и относительно простых элементов, то правила и ограничения, при которых это разделение будет иметь смысл, потребуют плату в виде строчек кода. А еще всем этим нужно будет управлять для того, чтобы в один прекрасный момент ваша жизнь не превратилась в ад (впрочем, это справедливо для любой кодовой базы и любого IT-ландшафта). Заключение Во время написания кода перед разработчиком встает ряд задач, решить которые возможно только с помощью правильной компоновки кодовой базы. Это и задачи, связанные с объединением изменчивых аспектов в отдельные модули для более легкой поддержки, и обеспечение стабильности высокоуровневых абстракций, создание расширяемой архитектуры, и многие другие. Рассмотрев DIP и ограничения, накладываемые на наш код при его применении, мы подошли к тому, что можем просуммировать некоторые принципы, которые будут полезными при декомпозиции кода приложения в модули и организации зависимостей между ними: «Главный» код— модули, находящиеся на верхнем уровне, это те модули, ради которых пишется ваше приложение; модули, находящиеся ниже, подчинены целям модулей верхнего уровня и их появление обусловлено необходимостью решать специфические классы задач не характерные для предметной области. «Главный» код— модули, находящиеся на верхнем уровне, это те модули, ради которых пишется ваше приложение; модули, находящиеся ниже, подчинены целям модулей верхнего уровня и их появление обусловлено необходимостью решать специфические классы задач не характерные для предметной области. DIP это про level и coupling— логика организации зависимостей между модулями подчинена иерархическим отношениям между уровнями — зависимости должны быть направлены в сторону верхнего уровня абстракции. Просто зависеть от абстракции недостаточно, важен её уровень абстракции, её расположение и то поведение, которое она описывает. DIP это про level и coupling— логика организации зависимостей между модулями подчинена иерархическим отношениям между уровнями — зависимости должны быть направлены в сторону верхнего уровня абстракции. Просто зависеть от абстракции недостаточно, важен её уровень абстракции, её расположение и то поведение, которое она описывает. Инверсия— инверсия зависимостей заключается в формировании абстракций на верхних, и в реализации этих абстракций на нижних уровнях таким образом, чтобы общее направление зависимостей между модулями было направлено в сторону верхнего уровня абстракции. Необходимо, чтобы абстракция находилась в границах модуля и описывала необходимое модулю поведение. Инверсия— инверсия зависимостей заключается в формировании абстракций на верхних, и в реализации этих абстракций на нижних уровнях таким образом, чтобы общее направление зависимостей между модулями было направлено в сторону верхнего уровня абстракции. Необходимо, чтобы абстракция находилась в границах модуля и описывала необходимое модулю поведение. API модуля— абстракция это не только требования, которые реализуются где-то ниже, но и «безопасный» контракт для обращения к высокоуровневым модулям снизу. Благодаря своей стабильности подобное API может использоваться в скольких угодно модулях нижнего уровня. API модуля— абстракция это не только требования, которые реализуются где-то ниже, но и «безопасный» контракт для обращения к высокоуровневым модулям снизу. Благодаря своей стабильности подобное API может использоваться в скольких угодно модулях нижнего уровня. Метрики модуляи framework agnostic— модули верхних уровней должны иметь метрику efferent coupling, вызванную связями с кодом на своем уровне и выше и метрику afferent coupling, вызванную связями с кодом на своем уровне и ниже. Efferent coupling модуля не должна возникать от связи с кодом, расположенным на уровнях ниже — модули не должны знать ничего о чем-то, что находится ниже их уровня абстракции. Уменьшение метрики instability у модулей должно совпадать с общим направлением зависимостей. Это облегчает их тестирование и делает модули нижнего уровня легко заменяемыми плагинами для логики верхних уровней. Метрики модуляи framework agnostic— модули верхних уровней должны иметь метрику efferent coupling, вызванную связями с кодом на своем уровне и выше и метрику afferent coupling, вызванную связями с кодом на своем уровне и ниже. Efferent coupling модуля не должна возникать от связи с кодом, расположенным на уровнях ниже — модули не должны знать ничего о чем-то, что находится ниже их уровня абстракции. Уменьшение метрики instability у модулей должно совпадать с общим направлением зависимостей. Это облегчает их тестирование и делает модули нижнего уровня легко заменяемыми плагинами для логики верхних уровней. SLAP и модули— на каждом уровне абстракции модули должны решать характерные для уровня задачи. Необходимое модулю поведение, определяемое интерфейсами, также должно соответствовать уровню абстракции. SLAP и модули— на каждом уровне абстракции модули должны решать характерные для уровня задачи. Необходимое модулю поведение, определяемое интерфейсами, также должно соответствовать уровню абстракции. «From top to bottom»— лучший подход к организации зависимостей это начинать с определения модулей, находящихся на самом высоком уровне, а после определять их контракты, проходя уровень за уровнем вниз. Так мы сможем лучше контролировать связность модулей и получим более устойчивый дизайн. Это и контроль связности между модулями позволит вам построить любую желаемую архитектуру, в фокусе которой будет выполнение задач предметной области. «From top to bottom»— лучший подход к организации зависимостей это начинать с определения модулей, находящихся на самом высоком уровне, а после определять их контракты, проходя уровень за уровнем вниз. Так мы сможем лучше контролировать связность модулей и получим более устойчивый дизайн. Это и контроль связности между модулями позволит вам построить любую желаемую архитектуру, в фокусе которой будет выполнение задач предметной области. Источники Martin R. The Dependency Inversion Principle Martin R. The Dependency Inversion Principle Martin R. Solid Relevance Martin R. Solid Relevance Мартин Р. Чистая архитектура. Искусство разработки программного обеспечения. СПб.: Питер, 2020 Мартин Р. Чистая архитектура. Искусство разработки программного обеспечения. СПб.: Питер, 2020 Martin R. Clean code: a handbook of agile software craftsmanship. – Pearson Education, 2009 Martin R. Clean code: a handbook of agile software craftsmanship. – Pearson Education, 2009 Schuchert B. L. DIP in the Wild Schuchert B. L. DIP in the Wild Wikipedia Software package metrics Wikipedia Software package metrics банки "
  },
  {
    "article_id": "https://habr.com/ru/articles/877558/",
    "title": "Как документация помогает выйти в прод быстрее. Бонус — шаблоны, которые выручают в работе аналитика",
    "category": "Дизайн",
    "tags": "business analysis, system analysis, системный анализ, бизнес-анализ",
    "text": "Всем привет! Я Ангелина Набатчикова, BA/SA в QIC digital hub. Я работала аналитиком в нескольких продуктовых командах с различными подходами к документированию и постановке задач. Этот опыт показал мне, что даже в условиях гибких методологий Agile нельзя недооценивать важность детальной и структурированной документации.Хотя манифест утверждает, что «работающий продукт важнее исчерпывающей документации», качественная документация на самом деле поддерживает порядок и слаженность работы команды, а главное — ускоряет поставку, а не замедляет ee, как иногда ошибочно считают. Давай разберем чуть более убедительные аргументы на эту спорную тему. Очевидно, что ошибки, выявленные на этапе анализа, обходятся дешевле и решаются быстрее, чем те, которые обнаруживаются во время разработки или тестирования. Детальная проработка и документирование задачи — это не формальность, а возможность тщательно пересмотреть требования,найти все несоответствия заранее и избежать багов в будущем. Когда проблемы всплывают на этапе тестирования, их исправление требует больше времени и усилий. Иногда это решается быстро, а в иных случаях задача возвращается обратно к аналитику для доработки и проходит весь цикл разработки снова, что еще больше оттягивает поставку. Когда требования описаны четко и прозрачно, нет необходимости постоянно устраивать созвоны, чтобы прояснить детали. Это не только экономит время всех участников, но и ускоряет процесс разработки. Аналитику не нужно «вести за ручку» разработчика, что освобождает время для проработки следующих функций. Подробно описанные задачи и документация делают процесс онбординга нового сотрудника гораздо проще. Новый член команды сможет быстро вникнуть в проект и эффективно работать с ним дальше. В QIC благодаря отличной документации я смогла вникнуть во все детали проекта в первый рабочий день и начать активно перформить сразу же, что еще раз подтвердило  ценности документации. Когда информация подробно документирована, команда становится менее зависимой от источника знаний. Если кто-то из ключевых сотрудников временно отсутствует или уходит из компании, работа не останавливается. Документация позволяет любому участнике команды быстро получить доступ к необходимым знаниям и продолжить выполнение задач. Качественная документация укрепляет доверие со стороны стейкхолдеров. Она делает процесс разработки более прозрачным: все заинтересованные стороны могут видеть, какие решения были приняты, какие изменения внесены, и как команда планирует достигать поставленных целей. Это помогает снизить напряжение и недоразумения, улучшает сотрудничество и в целом укрепляет доверие. Это лишь несколько аргументов, почему я считаю, что нельзя пренебрегать детальным и структурированным описанием задач и документации в целом. Даже если вы достигли дзена и разработка понимает, что нужно сделать с полуслова. С чего начать? Если спросить аналитиков, 8 из 10 скажут, что документация — это самое ненавистное в нашей работе, несмотря на осознание ее ценности. Один из способов упростить этот процесс — это формализовать все документы, которые аналитик создает при работе над новой функциональностью. Проще говоря, подготовить шаблоны, которые можно будет использовать из раза в раз. Например, шаблоны для описания API, новой фичи, экранной формы, постановки задачи на FRONT и BACK и т. д.. Чтобы это стало еще более убедительным и принесло реальную пользу, предлагаю рассмотреть пример шаблона для описания API. Такой шаблон включает достаточное количество полей, чтобы задача была успешно разработана и понятна всем участникам команды — от аналитиков до разработчиков и тестировщиков. Этот шаблон охватывает: Описание метода и его назначения Описание метода и его назначения Детализацию входных и выходных параметров Детализацию входных и выходных параметров Описание структуры запросов и ответов Описание структуры запросов и ответов Примеры запросов и ответы Примеры запросов и ответы И другие опциональные пункты И другие опциональные пункты Полный шаблон можноскачать по ссылке.Даже если проект не нуждается в полной спецификации методов или экранных форм, наличие таск-трекера с четко прописанными задачами может оказаться не менее важным. Рассмотрим, как может выглядеть задача на Change Request для фронтенд-разработки. В идеальном мире команда должна стремиться к формализации всех типов задач — от дизайна и дискавери до фронтенда и бэкенда. Yверяю, что такой арсенал заготовленных шаблонов ускорит процесс для аналитика в два, а то и три раза. Однако важно избегать крайности. Документация не должна требовать бесконечных согласований и ревью от всех участников проекта. Все должно быть в меру: документация должна помогать разработке, а не мешать ей. Её цель — ускорить процессы, сделать их прозрачнее и предсказуемее, а не тормозить работу команды. Найти этот баланс — и есть ключик к выходу на прод быстрее. А как вы считаете, стоит ли уделять больше внимания документации в условиях Agile, несмотря на стремление к гибкости? И не забудьте поделиться статьей с тем, кто еще не видит в этом ценности."
  },
  {
    "article_id": "https://habr.com/ru/articles/877454/",
    "title": "Digital Twin. Часть 1. Цифровой двойник vs цифровой самозванец",
    "category": "Дизайн",
    "tags": "digital twin, bpm, business activity monitoring, scada, digital transformation, process mining",
    "text": "Сегодня распространённым (модным) явлением стало называть «старое» моделирование новым термином «Производство цифровых двойников» (Digital Twin), тем самым создавая туман-интригу, при котором соблазн монетизации пафосного бренда и хайп вокруг Цифрового двойника побеждают здравый смысл. За редким исключением все, что сейчас называют Цифровой трансформацией и Цифровым двойником – по сути ими не являются, отсюда и желание разобраться в термине «цифровой двойник». Первый шаг на пути «от путаницы к ясности» - это определиться с терминологий. Так необходимым условием идентификации Digital Twin предлагается считать наличие трех раздельных компонентов (два близнеца и связь между ними) и адекватность модели (точность, т.е. «as-really-is», и требуемая детализация). По тексту приведены в основном цитаты из [DTatom20] / [DTatom19min] и [Dozortsev22] / [Dozortsev23]. Однако эти и подобные исследования (вкл. [Pavlov23]) носят описательный (обзорный) характер, а хороших (внятных) классификаций, концепций и framework для DT не встречал. Вообще, по DT информации – тьма, но после прочтения хотя бы указанных книжек и ссылок становится вопросов только больше. Пока что красивая метафора «Цифровой двойник» \\ Digital Twin (ЦД \\ DT) имеет скупую концептуальную проработку: требуется даже не уточнение концепции, а скорее ее формирование, т.к. это пока только аморфная и противоречивая парадигма, точнее множество парадигм DT/ Pseudo DT, в которых называют одним именем (DT) разные вещи (размытие концепции). По большому счету сегодня имеем только концепт Майкла Гривса и массу «маркетинговых оберток», которые не соответствуют даже его концепту 2002 года. Попытки концептуализации, классификации, разработки таксономий хоть и проводятся, например, [Archetypes], но пока они не очень результативны. По тексту приведены адрес-страничные ссылки на pdf-документы (файлы) с окончанием url «#page=х», где х равен номеру страницы в pdf-файле (не в книге). Номера страниц книги и их pdf-файла иногда немного не совпадают, как правило, на единицу из-за дополнительного листа обложки. Пара критических цитат о DT: [DTatom20#page=15]:термин «цифровой двойник» (ЦД, Digital Twin, DT) стал крайне популярным в последнее время. Увы, разные авторы подразумевают под этим термином разные вещи, и человека, который хочет уточнить для себя понятие ЦД, разнообразие формулировок подчас приводит ко все более размытому представлению. [DTatom20#page=165]:Рекламирование старых технологий под новой «вывеской» дискредитирует технологию ЦД. Чтобы соответствовать новым веяниям на рынке цифровых технологий, некоторые компании стремятся придать своим продуктам новые маркетинговые названия, используя словосочетание «цифровой двойник» в том числе там, где это не совсем корректно. По словам Хорста Гроссера, управляющего директора Communications, Media And Technology, Industry X.0, «некоторые непреднамеренно создают Цифровых Самозванцев (Digital Impostors) – фрагментированных или неполных цифровых двойников В Приложении 2. «Ажиотаж (хайп) Digital Twin» более подробно показана эта проблема. Большое число критических суждений, включая крайне низкий уровень концептуализации DT и очень высокий «градус маркетинга» и выдача за DT прежних продуктов, говорит, что DT еще далеко незрелая концепция, но активно завоевывающая рынок (и умы). 1 Базовый Трехкомпонентный состав DT Начнем … со здравого смысла, который подразумевает, что любые близнецы \\ двойники – это «по определению» парная сущность: нельзя, например, сказать «пара чисел из одного числа»: - Если нет реального объекта \"первака\", то нет и \"двойника\" - существование двойников подразумевает наличие двух сущностей(8 мифов DT). «Two be or not to be. Вот в чем вопрос». Если в системе отсутствует реальный объект, то это не может быть двойником \\ близнецом. Близнецом чего, если второй объект двойственности отсутствует? Классическое Имитационное моделирование (simulation) без взаимодействия с физическим близнецом не укладывается в определение DT, т.к. реализована лишь имитация \\ симуляция, а не виртуальный \\ имитационный двойник реально существующего физического объекта (вещи или процесса). Имитационное моделирование может включаться только как дополнение (например, в составе подсистемы прогнозирования) в составе DT и только при наличии физического двойника \\ близнеца. [Pavlov23#page=3]… реализованная имитационная модель чаще всего не является полноценным цифровым двойником из-за отсутствия других необходимых компонентов. [DTatom20#page=18]:Большинство авторов сходится во мнении [5], что первоначально концепция ЦД была озвучена Майклом Гривсом на PLM (Product Lifecycle Management) курсе в Мичиганском университете в начале 2002 г. и позднее, в 2003 г., была представлена на конференции по PLM. Типовые определения Digital Twin (DT) приведены в Приложении 1 «Типовые определения Digital Twin (DT, ЦД)». Согласно ним, Digital Twin - как концепция, т.е. DT в «широком смысле» (Digital Twin Technology / Digital Twin System, DTS) включает три основные сущности – компоненты (Трехкомпонентный состав DT, три кита DT): - Физический продукт (физический объект, ФО) в реальном пространстве (табуретка или процесс), физический двойник (Physical Twin); - Виртуальный продукт в виртуальном пространстве (модель объекта, МО), собственно сам - цифровой двойник (DT в «узком смысле»), модель ФО (Digital Model, DM); - Связь обоих продуктов, т.е. данные и информация, которые объединяют виртуальный и физический продукт. Передачу от физического двойника в модель (обратная связь) иногда называют «цифровой тенью»Digital Shadow. Обычно указывают, чтообязательно связь должна быть двухсторонняя, но иногда формализуют только обратную (данные из реальности в модель). Двухсторонняя связь – явно указана в [57700.37] и является «красной нитью» в [Pavlov23#page=3]: Таким образом, ключевым отличием цифрового двойника от систем мониторинга, сбора и обработки данных заключается в наличии двусторонней обратной связи между виртуальным компонентом системы и реальным объектом. Однако это видимо все же не обязательное условие, т.к. возможны исключения (односторонняя связь). Обычно требование к обратной связи on-line, однако бывают DT, которые наоборот внедряются, чтобы минимизировать связь физического близнеца, например,Device Twins для LPWAN IoT. Часто характеристика информационной связи (информационного взаимодействия близнецов) во многом определяет «неподдельность» DT (качество реализации DT). Например, на Рис. 2. Дерево решений ЦД [Dozortsev23#page=5] предлагается условие: Если поток данных к оригиналу неавтоматизированный, то это не двойник, а цифровая тень (спорное утверждение). Комплект чертежей, по которому изготовили изделие, например, по которым плотник сколотил табуретку, не может считаться Цифровым двойником, т.к. связь табуретки с чертежами  «неавтоматизированная». Природа (и требования) связности физического и цифрового двойника – тема отдельного исследования. Пример связности Digital Twin приведен на рис. 4 «Интеллектуальное взаимодействие Интеллектуальных близнецов huawei» (см. ниже) на примере технологической эталонной архитектуры Intelligent Twins [IDhuawei]. «Три в одном»: модель, натурное изделие, их взаимодействие: передача информации от «физика» (physical object) в модель и связь (обратная или двухсторонняя связь). Консорциум OGC подчеркивает свойство «раздельности» и наличие двухсторонней связи термином «Автономный цифровой двойник» (п. 4 Классификация DT, Городские цифровые близнецы). Кроме наличия трехкомпонентного состава DT, добавим четверное условие: положительная оценка адекватности модели, т.е. звание DT нужно подтвердить через оценку адекватности модели. Рассмотренные требования к Трехкомпонентному составу DT (три облака \\ «три кита DT») показаны на рис. 1. Три облака наложены на слайдConceptual Ideal for PLMпрезентации 2002 года (Dr. Michael Grieves «Product Lifecycle Management» Digital Twin’s, Models, And Systems Engineering: Integrating 21st Century Product Development Concepts Рис. 1 через VS1, VS2 … отражает (см.8 мифов DT):У каждого ФД может быть сколько угодно ЦД, созданных для различных целей. Ключевой момент – это четкое разделение обоих двойников. В DTS должны быть четко разделены оба близнеца, отчуждены друг от друга и связаны только информационно. Например, «усыпили» Digital Twin, но Physical Object (PT, material object) продолжит функционирование в штатном режиме. Ситуация «цифровые сиамские близнецы» (совмещение составных частей близнецов) - противоречит концепции DT (DTS): PT и DT – должны быть структурно полностью отчуждаемые сущности, при необходимости DT может содержать копии составных частей своего близнеца. Например, в состав DT могут входить «копии» составных частей вполне физические, например, когда нужно в точности воспроизвести нужный функционал и поставить (продублировать) аппаратный генератор случайных чисел, идентичный установленному на физическом близнеце. 2 Pseudo Digital Twin, PDT Все системы, позиционирующиеся как DT, но не содержащие какую-либо часть из трехкомпонентного состава DT будем называть псевдо DT, включаяметавселенные(DTvs metaverse,Industrial Metaverse от Nvidia, Siemens). К PDT также относимпродукты дизайна, «используемые для создания виртуальных прототипов новых вещей и их тестирования перед тем как на их основе создаются реальные объекты», т.е. это просто макеты и виртуальные прототипы, модели для создания объектов в которых нет реальной связи с физическим близнецом ввиду его отсутствия. Цифровые модели для виртуальных испытаний на цифровых полигонах. Разработка модели, по которой разработка физического близнеца вообще не предполагается, - это всего лишь цифровая модель, даже если по ней проводятся «серьезные и очень реалистичные» виртуальные испытания на цифровых полигонах (читайметавселенных - цифровых мирах). Даже если после цифровых испытай и предполагается по модели создание физического образца, то на этапе испытаний это всего лишь Pre-Digital Twin (Digital Twin Prototype). Показательный пример Pseudo Digital Twin от «Сухого» (инструменты Логос, SimInTech и т.п.): Цифровые двойники \\ Цифровые полигоны \\ Цифровые испытания (последний слайд). Однако под «классическим Digital Twin» в «Сухом» понимают компьютерную модель, «которая адекватно описывает не семейство изделий, а поведение конкретного образца (экземпляра изделия) со своей историей» (см.последние секунды выступления), но все равно без наличия соответствующего конкретного физического образца – как спарки «модель – реальная вещь». Поэтому продукты дизайна под цифровые испытания можно назвать PDT \"Сухой\" (см. рис. 2). Этот  тип Pseudo Digital Twin - имитационное \\ математическое моделирование для реализации «цифровых испытаний» моделей на цифровых полигонах, что фактически является вариантом метавселенных (промышленных) – «цифровых испытательных» миров. Если по разработанной модели будет создан (запланирован) физический близнец, то ее называют Pre-Digital Twin (см. рис. 1). Однако если в дальнейшем никакой связности между цифровым и физическим объектом не будет (см. Трехкомпонентный состав DT), то модель так и останется в статусе Pre-Digital Twin (не путать с PDT), а Digital Twin так и не возникнет. К PDT относим все тренажёры и симуляторы – какими бы натуралистическими они не были. При этом негатива в понятии Pseudo Digital Twin (PDT) – может и не быть, и его можно рассматривать как класс систем, удовлетворяющих определенным требованиям, но отличным от Digital Twin. Есть еще [Dozortsev22]: Киберфизическая эквивалентность и киберфизические системы (cyber-physical system, есть дажеВысшая школа их управления), аватары, цифровая тень (есть дажеSmart Digital Shadow), цифровой след,цифровая нитьи т.п. На рис.2 показаны лишь примеры некоторых PDT (поэтому v0.1): предлагайте и добавляйте новые типы, их будет много, во всяком случае больше чем типов DT, учитывая «современные тренды» и пафос (сознательное преувеличение) «цифровой трансформации» в «обличии DT». Иногда считают, что добавление в систему модуля предсказания \\ прогнозирования или автопилотирования позволяет присвоить статус «DT». Это не так: современный автомобиль с наличием, как простейшего маршрутного компьютера, прогнозирующего пробег (километраж) до выработки топлива, так и автопилота даже класса SAE 4 не станет DT. Такой автомобиль, напичканный «под завязку» всевозможными IoT, включая датчики давления в камерах (TPMS c акселерометром), и дажеAIoT,все равно останется Physical, а не Digital. Иногда за DT пытаются выдать («продать») классическую SCADA – систему или иную систему мониторинга (например, BAM) как без, так и с дополнительным модулем симуляции или прогнозирования. Даже что-то типа «ЦД пассивного мониторинга» (Passive monitoring DT) должен иметь отдельную систему (отдельный инстанс, копию ПО SCADA), чтобы удовлетворить принципу трех-компонентности. Некоторая классификация DT (за основу там сложно что-то взять) приведена в Приложении 3. Некоторые примеры DT отThingworx. Описание примера DT автомобиля(Thingworx): Например, хозяин автомобиля сможет просто навести мобильный телефон или планшет на него и узнать, когда потребуется очередное техническое обслуживание. Цифровой двойник покажет потребителю важные данные: уровень масла в двигателе, информацию о работе систем автомобиля и т.д. С другой стороны, инженер или техник смогут увидеть «рентгеновский снимок» двигателя и определить, какие части нуждаются в ремонте, сколько часов проработал двигатель, либо получить информацию совершенно другого рода. Подобные описания не дают ответ, что это действительно DT, т.к. непонятно что у DT (DTS) «под капотом» (какая реализация, насколько разделены оба близнеца). 3 Digital Twin checklist Наличие трехкомпонентного состава – это недостаточное условие. Ключевая проблема: действительно ли цифровая модель (цифровой близнец) адекватна реальному физическому близнецу? Может быть перед нами не близнец, а самозванец? Как распознать DT и отличить его от Digital Impostors? Вообще оценка адекватности – это во многом определяемая величина для конкретного случая. Это как критерий отказа при оценке надежности системы: с одной формулировкой получаем одно значение показателя надежности, а с другой – совсем иное, причем для одного и тоже объекта. Если требование «Трехкомпонентный состава + Адекватность модели» являются обязательными (базовыми), то другие, такие как хранение исторических данных, агрегация информации, возможность прогнозирования (предсказание поведения), наличие AI\\ML, поддержка со стороны DT всего жизненного цикла физического двойника, 3D-модель и др. – являются опциональными. Для выявления цифровых самозванцев необходим аналогтеста Тьюринга, например: эта «имитационная игра» сводится к тому, чтобы двойник был неотличим от своего физического аналога при осмотре с использованием различных сенсорных модальностей, и обеспечивать в контролируемых условиях ответы на входные стимулы и наблюдения за мгновенным состоянием, которые также были бы неотличимы от ответов, исходящих от физического продукта, который он представляет. Например,Цифровой двойник линии конвертинга на основе ПО от SIEMENS- цифро-близнец или цифровой самозваниец? ИлиПлатформа SmartTwin– она точно для построения цифрового двойника? Вообще поражает обилие в интернете предложений по созданию DT, а также то, что некоторые отечественные «DT-платформы» позиционируются как замещение западных совсем «не DT-продуктов». Получается некий такой отечественный «Универсальный солдат» «Цифровой двойник» (универсальный замещатель), что конечно подрывает доверие как к такому продукту, так и к самой концепции DT. Применительно к BPMS(Business Process Management System): какие будут доказательства, что предлагаемые сегодня «BPM-образные» решения – это действительно DT компании? В начале 2000-х наблюдал масштабный ребрендинг ARIS и BPwin. В то время в РФ «стремительно ворвался» новомодный термин «BPM» и достаточно быстро эти и подобные системы, ранее называемые «CASE-средства», получили новое имя: «BPM-системы» (чуть позже Gartner их обозвал BPA). Современный ребрендинг ARIS-подобных систем (с дополнительным «бантиком» конечно, например, в виде майнинга или BAM) будет более конструктивен, чем четверть века назад? Верхняя часть рисунка 3 –презентация банка ПСБ, где «DT» построен наBusiness Studio +Proceset(BPMS + Process\\Task Mining), нижняя часть - скрины с сайтаSila Union.Ниже аналогичный «фокус» увидим от финской QPR, да и другие «не отстают от моды». Это точно примеры настоящих DT? Или маркетинговая обертка из комбинации «старых» BPMS \\ BAM-систем (Business Activity Monitoring), «разбавленных» «свежими» технологиями майнинга Process \\ Task Mining? Из «Эволюция процессного управления в ПАО «Промсвязьбанк»: от внедрения стандартов к построению цифрового двойника Банка»: Перед нами стояла задача адаптировать свою деятельность под требования цифровой трансформации, и мы сделали это в формате преображения инструментов процессного управления в рамках инициативы «Создание цифрового двойника Банка». При позиционировании систем класса DT нужно явно указывать все три составляющие, их границы, а для третьего компонента – хотя бы топ10 связей между близнецами. Где тут «Цифровой двойник банка»? Кстати, самProcesetсебя не позиционирует платформой создания DT. Да и «Цифровой трансформации» в показанном примере что-то не разглядел: BPM – явно не трансформация, так неужели Process \\ Task Mining – это «она самая» Цифровая трансформация? Хочется хоть один раз увидеть DT класса DTO (нет, не Digital Transformation Office, а Digital Twin of Organisation), а не 100 раз про него услышать, поэтому если кто-то увидел в пример – покажите где тут DT (части DTS), но желательно «близко к тексту» Майкла Гривса» \\ Приложения 1 к настоящей статьи. Впрочем, также интересен пример «цифрового двойника банки консервной» (методологически даже правильнее начинать с более простых примеров). Получается, что под девизом «Даешь Цифровую трансформацию!» ставим рядом с ARIS-подобной системой (BPMS) систему класса Process\\Task Mining и называем это «Цифровой двойник организации»? \"А что, так можно было?!\" С таким подходом «Процессный офис компании» можно смело переименовывать «Инкубатор \\ ферма Цифровых двойников» во главе с Директором цифровых двойников, Chief Digital Twin Officer. Где то недалеко должна быть и «Академия / Высшая школа цифровых двойников». При этом некоторые уверяют, что именно они единственные и неповторимые «маги DTO»:QPR являетсяединственным поставщиком на рынке, предлагающим комплексный пакет программного обеспечения DTO, а также непревзойденный сервис и поддержку. В случае с ARIS-подобными Business Studio и Sila Union (а также QPR ProcessDesigner и т.п.)–как якобы «платформами DT» – смущает то, что это системы для построения «as is», а не гарантированного «as-really-is», но для DT нужен именно второй подход, т.е. «как в реальности есть на самом деле». В ARIS-подобных системах схема процесса «выходит из-под пера художника» (рисовальщика процесса или согласователя – не важно), т.к. «он художник - он так видит» фиксируемый им процесс. При этом получаемый «цифровой близнец» (как схема процесса) может совсем не соответствовать своему прототипу (физическому процессу): «Гладко было на бумаге …» Для движения в сторону DT в связке «BPM + mining tech» вместо ARIS-подобных систем видимо нужно использовать WFE-системы [WFE24], имеющие «врожденное» свойство «as-really-is» (за счет workflow engine, исполняющей ранее смоделированный процесс, например, в BPMN),см. пример, но интеграция должна быть куда «более тесной», а возможности обоих систем куда «шире». Существуют разновидности (разные виды) «настоящих DT», т.е. которые однозначно соответствуют типовым определениям DT из Приложения 1. Во второй части рассмотрим пару таких классов DT, у которых принцип «as-really-is» является «врожденным». Для каждого типа Digital Twin видимо понадобится специальный (свой) DT-checklist, у кого есть идеи как составить такой чек-лист для BPM-систем (ARIS, WFE) в связке с Process\\Task Mining – поделитесь. Заключение Во многом новое «дыхание» тема DT получила благодаря взрывному развитию технологий ИИ / машинного обучения. Рассказы про DT редко встречаются без упоминания AI/ML. Например, DT от IBM (Cognitive Digital Twin) часто позиционируется как комплекс из линейки продуктов IBM:Maximo Asset Management; Predictive Maintenance and Quality (IBM PMQ); Business Process Manager (IBM BPM); Watson Explorer (AI); IBM Internet of Things (IoT); Cognitive Digital Thread (когнитивная цифровая нить). DT от IBM также рассмотрен в[DTatom20#page=213] Упоминание использования в модели слов AI\\ML обычно дает «однозначный» ответ: «Ну, тогда это точно Digital Twin, а не просто модель». Более того, если прогнозирование «естественное» (классические математические модели прогнозирования), то это «просто DT», а если с использованием искусственного интеллекта (ИИ), то это уже Intelligent Digital Twin [DTatom20] или когнитивный цифровой двойник. Уже в моде новый термин Intelligent Twins [IDhuawei], в качестве иллюстрации приведем пару рисунков этой технологической эталонной архитектуры от huawei из ее «Белой книга архитектуры Intelligent Twins» (рис. 4). Digital Twin, Cognitive \\ Intelligent Twin, кто следующий? В целом удивляет существующие сегодня неразборчивость и беспринципность к термину Digital Twin, идея которого была озвучена в далеком 2002 году. Можно дать отдельные определения DT для каждого типа (класса) DT, для каждой отрасли \\ области, например, «DT в строительстве, DT компании, DT вещи (табуретки или ракеты), но все они должны быть обоснованы, терминологически формализованы и иметь «принятые экспертным Инженерным сообществом» критерии идентификации. При этом нужно «без церемоний» отделять «мух от котлет»: лже-двойников от Digital Twin, но для этого вначале придется формализовать требования к DT, критерии оценки (верификации), стандартизировать классификацию. Или иначе придется признать, что DT - это просто красивая метафора, очередная «IT-фишка» и очаровывающая многих передовая технология маркетинга. Ссылки [DTatom20] А. Боровков и др. Цифровой двойник. Анализ, тренды, мировой опыт [DTatom19min] А. Боровков и др. Цифровые двойники в высокотехнологичной промышленности (& Краткий доклад) [DTatom22min] [IDhuawei] Официальный документ: Intelligent Twins.Совместное создание интеллектуальных двойников и построение мира интеллектуальных технологий (Белая книга архитектуры Intelligent Twins) Huawei представила в России «Белую книгу» по архитектуре «Интеллектуальных двойников» Цифровые двойники Artificial intelligence с эталонным интеллектом Цифровые двойники Учебное пособие A.Y.C. Nee, S.K. OngDigital Twins in Industry(прямаяссылка на pdf) Жиляев Методы и средства построения «цифровых двойников» процессов управления предприятиями на основе онтологий и мультиагентных технологий (диссертация) Маклаков Н.А. Разработка цифрового двойника экспериментального высокоавтоматизированного транспортного средства (диплом) Open Source and Open Standards for Digital Twins [DTmedic] Цифровые двойники в здравоохранении: оценка технологических и практических перспектив - PDF (Русский) Опыт и перспективы применения цифровых двойников в общественном здравоохранении Цифровой двойник Digital Twin of Organization, DTO Онтологии кибер-физических систем национального цифрового двойника Великобритании и BIM на примерах умных городов, железных дорог и других проектов [Dozortsev22] В.М. Дозорцев Цифровые двойники в промышленной автоматизации – на пике моды или наступившее будущее? [Dozortsev23] В.М. Дозорцев Цифровые двойники в промышленности: жизнь после Хайпа [Pavlov23] Лычкина Н.Н., Павлов В.В. Концепция цифрового двойника и роль имитационных моделей в архитектуре цифрового двойника (файлpdf) [Archetypes] Hendrik van der Valk et al. Archetypes of Digital Twins // Business & Information Systems Engineering. 2022 Copy[Archetypes] (pdf) «digital twin» на simulation.su [Klezkova23] Клецкова Ю.С. Цифровой двойник как следующий этап ведения информационной модели Цифровые двойники на протяжении жизненного цикла продукта: систематический обзор литературы по применению в производстве Перспективы применения цифровых двойников в строительной отрасли A Virtual Commissioning based Methodology to integrate Digital Twins into Manufacturing Systems Онтология как основа для создания цифровых двойников объектов управления интеллектуальной распределенной энергетики [WFE24]Исполняемый BPMN в Open Source Runa WFE (WfMS). Hello Calculator и немного классификации [57700.21] ГОСТ Р 57700.21-2020. Компьютерное моделирование в процессах разработки, производства и обеспечения эксплуатации изделий. Термины и определения. 3.37цифровой двойник изделия:Связанная совокупность компьютерных моделей различных видов, описывающих с требуемым уровнем адекватности свойства и поведение экземпляра изделия, изменение его характеристик и внутренние процессы в зависимости от состояния внешней среды (управляющих воздействий), решаемых задач и условий их выполнения. [57700.37]ГОСТ Р 57700.37-2021. Компьютерные модели и моделирование. Цифровые двойники изделий. Общие положения. «цифровой двойник изделия»: это «система, состоящая из цифровой модели изделия и двусторонних информационных связей с изделием (при наличии изделия) и (или) его составными частями. ISO 23247-1:2021.Automation systems and integration. Digital twin framework for manufacturing. Part 1: Overview and general principles. Системы автоматизации и интеграция. Структура цифрового двойника производства. Часть 1. Обзор и общие принципы См. обсуждение. ISO/IEC 30173:2023(en) Digital twin — Concepts and terminology DT – цифровое представление целевого объекта через информационный обмен (data connections), обеспечивающий схождение (конвергенция, наложение) цифрового и физического представления (состояния) на подходящем (досочном) уровне синхронизации. IDTA– стандарт цифрового двойника Приложение 1. Типовые определения Digital Twin (DT, ЦД) и история 1 Цифровой двойник — это цифровая (виртуальная) модель любых объектов, систем, процессов или людей. Она точно воспроизводит форму и действия оригинала и синхронизирована с ним.https://trends.rbc.ru/trends/industry/6107e5339a79478125166eeb 2 Реалистичное цифровое представление чего-либо физического. Отличиецифрового двойникаот любой другой цифровой модели заключается в его связи с физическимдвойником. https://www.designingbuildings.co.uk/wiki/Digital_twin 3 неразрывно связанные между собой виртуальные представления физического объекта (продукта или процесса) в каждый момент времени. Цифровые двойники используются на протяжении всего жизненного цикла объекта для моделирования, прогнозирования и оптимизации продукта или системы производства. https://hes.mephi.ru/?page_id=18708 4 Digital Twin по сути является цифровым представлением физического объекта (asset), которое обеспечивает лучшее понимание его динамики путем объединенияаприорныхзнаний (знание, полученное до опыта и независимо от него) о системе с помощью математических моделей с онлайн-данными, полученными от датчиков и инструментов, размещенных в или на физическом объекте (активе). https://www.sciencedirect.com/science/article/pii/S016816992400067X 5 ЦД - это цифровая копия конкретного физического объекта, которая отражает структуру, производительность, техническое состояние и характер рабочей миссии физического объекта, включая такие параметры, как, например, пройденные километры, возникшие неисправности, а также историю технического обслуживания и ремонта реального изделия (физического двойника). https://cyberleninka.ru/article/n/analiz-tendentsiy-razvitiya-tsifrovyh-dvoynikov-novogo-pokoleniya Прим. В контексте DT иногда используют термин не объект (object), а актив (asset), а вместо двойника упоминают близнец. 6 Пара определений и правил из «Разоблачение восьми мифов о цифровых двойниках: вот вам реальность» (Джо Уолш): Определение от CIMdata:Цифровой двойник – это виртуальное представление некоторой физической сущности или набора таких сущностей (физического двойника), которое основано на использовании двустороннего информационного обмена с ассоциированным физическим двойником. Определение от Рабочей группы по системному моделированию:Цифровой двойник – это цифровой заменитель (суррогат), являющийся описанием физической сущности, такой как продукты, процессы, системы, люди и устройства, который может быть использован с разными целями. Цифровой двойник использует данные и информацию от объекта реального мира и обеспечивает обратную связь с этим реальным объектом. Правила для ЦД: 1 Для ЦД должен существовать соответствующий физический двойник (ФД). 2 Между ЦД и ФД поддерживается двунаправленный информационный обмен, обуславливающий взаимное использование двойников. 7 [DTatom22min#page=27]1.4. Цифровой двойник. Определение Центра НТИ СПбПУ: Цифровой двойник – это семейство сложных мультидисциплинарных математических моделей с высоким уровнем адекватности реальным материалам, реальным объектам / конструкциям / машинам / приборам … / техническим и киберфизическим системам, физико-механическим процессам (включая технологические и производственные процессы), описываемых 3D нестационарными нелинейными дифференциальными уравнениями в частных производных. Цифровой двойник в основе своей содержит многоуровневую матрицу целевых показателей и ресурсных ограничений (временных, финансовых, технологических, производственных, интеллектуальных, экологических и т. д.). Высокий уровень адекватности означает, что цифровой двойник должен обеспечивать отличие между результатами виртуальных и физических / натурных испытаний в пределах ± 5% по сотням датчиков. Именно в этом случае он имеет право называться цифровым двойником, в противном случае, это электронная модель, цифровой макет, цифровой прототип и т. д., предполагающий использование традиционного подхода – «доводка продуктов / изделий до требуемых характеристик на основе многочисленных дорогостоящих испытаний и итерационного перепроектирования». См. также Приложение [DTatom20#page=358] ключевые компоненты цифрового двойника в концепции Центра НТИ СПбПУ 8Национальные лаборатории Айдахо(Idaho): Digital Twin - это объединение интегрированных и связанных данных, датчиков и приборов, искусственного интеллекта и онлайн-мониторинга в единое целое. 9Цифровой двойник— это динамическое виртуальное представление физического объекта, процесса или системы, которое развивается вместе со своим реальным аналогом посредством непрерывного обмена данными. Эта технология, играющая ключевую роль в Четвертой промышленной революции, воплощает основные принципы оцифровки, децентрализации, модульности и работы в реальном времени 10 Подборка других определений DT см. Таблица 1 «Перечень определений понятия цифрового двойника» [Pavlov23#page=2] 11 [DTatom20#page=20]Восемь отличительных признаков DT:Объединяя разные источники, можно привести следующий перечень: … 12 Таблица 1. Определения «Метавселенная» и «Цифровой двойник» из «Цифровые платформы метавселенных как цифровой двойник» https://artsoc.jes.su/s207751800022959-3-1 (13) Рис. 1.76. Распределение ответов на вопрос: «Какие компоненты, на ваш взгляд, должны входить в понятие «цифровой двойник»?»[DTatom20#page=152] наибольшее число голосов получила формулировка «виртуальная копия физического объекта, которая используется для мониторинга работы этого объекта и оценки его производительности»(90% голосов) [DTatom20#page=154] 14 Определения ЦД и цифровой модели приведены в ГОСТ Р 57700.21-2020, ГОСТ Р 57700.37-2021, ISO 23247-1:2021, ISO/IEC 30173:2023 см. разделСсылки.III Стандарты (и определения из них)первой части статьи. NASA 2010(первое упоминание термина): Термин «цифровой двойник» был впервые представлен широкой публике в 2010 году в дорожной карте интегрированных технологий НАСА в области технологий: моделирование, информационные технологии и обработка. Он был описан так: «Цифровой двойник - это интегрированная мультифизическая, многомасштабная симуляция транспортного средства или системы, в которой используются наилучшие доступные физические модели, обновления датчиков, история парка и т. д., чтобы отразить жизнь соответствующего летающего близнеца» [14].  М. Гривс 2014 (стр. 12),[DTatom19min#page=13]: В 2003 году эта концепция была включена в курс по управлению жизненным циклом продукта (PLM), который Гривс читал в Мичиганском университете; для ее обозначения использовался термин «модель зеркальных пространств» (Mirrored Spaces Model), который к 2006 году трансформировался в «модель зеркального отображения информации» (Information Mirroring Model). Название «цифровой двойник» Майкл Гривс первый раз использовал в книге «Virtually Perfect: Driving Innovative and Lean Products through Product Lifecycle Management» (2011 год). При этом Гривс отметил, что этот термин он впервые услышал от эксперта NASA Джона Викерса (John Vickers), во время совместной работы в рамках проектов космического агентства. В 2014 году Майкл Гривс опубликовал статью, полностью посвященную цифровым двойникам в производстве – «Digital Twin: Manufacturing Excellence through Virtual Factory Replication». Спонсором публикации выступила компания Dassault Systèmes [7]. В этой публикации профессор Гривс отметил, что основа концепции – три базовых составляющих цифрового двойника – осталась неизменной: - реальный продукт в его реальном окружении, - виртуальный продукт в его виртуальном окружении, - информация и данные, связывающие физический и виртуальный продукт. В интернете масса отсылок, включаякрупные издания типа РБК, на странную книжку: Впервые концепцию цифрового двойника описал в 2002 году Майкл Гривс, профессор Мичиганского университета. В своей книге «Происхождение цифровых двойников» он разложил их на три основные части: …https://trends.rbc.ru/trends/industry/6107e5339a79478125166eeb Что это за книга? Нет такой уDr. Michael Grieves(см. 2002), нет и наwww.researchgate.net Видимо речь про его книгу 2002 года «SME Management Forum Completing the Cycle: Using PLM Information in the Sales and Service Functions» https://www.researchgate.net/publication/356192963_SME_Management_Forum_Completing_the_Cycle_Using_PLM_Information_in_the_Sales_and_Service_Functions но в ней вместо DT употребляется «Conceptual Ideal for PLM»: Предполагается, что каждая система является подмножеством двух других систем: физической системы в физическом мире и виртуальной системы, существующей в виртуальном пространстве, содержащей всю необходимую информацию о физической системе. Идея оттуда, но самого термина «Digital Twin» там еще нет. Краткая, начиная с Apollo 13 [DTatom20#page=38] «Рис. 1.7. Эволюция ЦД и сопутствующих технологий в концепции Microsoft» Apollo 13 считается первым цифровым двойником1970 года выпуска: Центр управления полетами обратился к «цифровым двойникам» на Земле, чтобы выяснить, как вернуть астронавтов домой. Однако построено было два идентичныхфизическихкорабля-близнеца: один летающий, а второй на земле, которого и назвали «близнецом» и использовали для тренировок при подготовке к полету и проверке гипотез параллельно полету первого аппарата. Поэтому назвать физическую копию «цифровым двойником» – не верно и выходит, что это не первый DT. Приложение 2. Ажиотаж (хайп) Digital Twin Модный термин современного времени («цифровой герой нашего времени») – «Цифровой двойник» (Digital Twin, DT, он же Цифровой близнец) какого-либо объекта – это «очень непонятная, но очень интересная» тема почти для всех сфер деятельности и отраслей (универсальная), т.к. объект может быть предметом, например, табуреткой, или целымпредприятием, отраслью,правительством и даже государством - национальным цифровым двойником (NDT). Этой темой «больна» медицина [DTmedic], есть даже «Цифровой двойник человеческого тела» на платформе3DEXPERIENCE. При этом «Что такое DT» - не совсем понятно, ясно только, что речь идет о модели объекта и как минимум об обратной связи (в общем случае, двухсторонней) с физическим близнецом в режиме «on-line». Современный термин «digital twin» во многом маркетинговый и часто обычную цифровую модель продают под новым «модным» именем («гренка vs крутон»). Создается впечатление, что если идет падение спроса и цены на продукцию, то вендору достаточно создать новую «IT-фишку»: провести «цифровой ребрендинг» с заменой надписи на «Цифровой двойник – как инструмент Цифровой трансформации» и «дело обязательно пойдет в гору». Четких критериев идентификации DT нет и в «схватке» Digital Twin vs Digital Impostors + Pseudo Digital Twin первый всегда проигрывает. Текущее состояние с Digital Twin (ЦД)хорошо отражает фраза: «Цифровой двойник» стал симпатичным и гламурным понятием, которое каждый вендор в той или иной степени использует для рекламы своей деятельности. Поэтому не удивляйтесь, что одни вендоры назовут цифровыми двойниками свои 3D- и CAE модели, что вендоры, фокусирующиеся на PLM, объяснят, насколько важны для ЦД данные периода жизненного цикла, а другие подчеркнут, что без IoT нельзя построить ЦД. В результате Цифровой Двойник стал крутым средством рекламы уже существующих продуктов. Каждый из упомянутых аспектов важен и ценен, однако, называя всё это цифровыми двойниками, мы мало помогаем пользователям понять важность стоящей за всем этим технологии. В конечном счете получается, что на уровне отрасли мы создаем новый хайп. Среди новинок от Gartner (Gartner Hype Cycle2022,а также2024, Цикл ажиотажа вокруг новых технологий) - Цифровой двойник клиента (DToC). Подборка «Gartner Hype Cycle for IoT» и «Gartner Hype Cycle for Emerging Technologies» [DTatom22min#page=31] аналогично [DTatom20#page=159] Когда приводят оценки глобального рынка ЦД (более 17 млрд долл. в 2023 г) [DTatom20#page=166], то остается загадкой: какая из них часть рынка действительно DT, а какая часть продаж «цифровых самозванцев» под вывеской DT? Западные игроки рынка ЦД [DTatom20#page=178]. ПО для DT: CAD/CAM/CAE/PLM [DTatom20#page=189]. Комплексные решения класса ЦД (стек технологий, вендоры и их клиенты) [DTatom20#page=204]. Российские поставщики ПО для построения ЦД [DTatom20#page=230], например: В ФГУП РФЯЦ-ВНИИЭФ создается также система поддержки жизненного цикла (СПЖЦ) «Цифровое предприятие». BPMS Не разобравшись со «старыми» цифровыми двойниками уже обсуждаются «цифровые двойники нового поколения»: умные, интеллектуальные, когнитивные, а также параллельные и промышленные метавселенные. Интернет «усыпан» заголовками: «Next generation Digital Twin». Под вывеской «Digital Twin» сегодня «продаются» проекты: Live Enterprise \\ Model-Based Enterprise, Enterprise Architecture, BPM (DTO), автоматизацию процессов на BPMN WFE (WfMS) / noBPMN LCNC, SCADA, AutoCAD (3D-модель), BAM, симулятор самолета или процесса (имитационную модель банка) и вообще любое моделирование (включая, MBSE, Model Based System Engineering [DTatom22#page=108]) любого предмета или явления, технологического или бизнес процесса. Практически любой проект с упоминанием IoT / IIoT (industrial) / AIoT, Индустрия 4.0, Системная инженерия 4.0 (см. ссылку выше), BIM (Building Information Modeling, информационное моделирование зданий), PLM/CAD/CAM/CAE/PDM (Product Data Management)/SPDM (Simulation Process and Data Management), VR/AR/XR/MR и т.п. может иметь «высокотехнологичную» вывеску «Цифровой двойник» из-под которой просвечивается «Digital Impostor». [Dozortsev22#page=7] При этом, есть … мнение, что двойники не предлагают ничего принципиально нового, что, по сути, они просто продолжение идеи использования модели в контуре обратной связи, но на новом технологическом витке и в присутствии ранее недоступных технических возможностей (глобальные измерения, мощные сети передачи данных, ИИ, пр.). Полагаю, что таких «двойников» нужно выявлять и не стесняясь назвать «Digital Impostors» - как советовал Хорст Гроссер. И вообще … использование термина Digital Transformation и Digital Twin (DT) в 99% случаев ничего нового не обозначает, т.к. это просто «свежеиспеченный маркетинговый термин» для обозначения уже давно существующих вещей и технологий, но под новомодной вывеской, исключительно чтобы (см. «Цифровая трансформация, инженерия, модель, тень, нить и так далее»): … «попадать в правильные строчки бюджетов, если будете следовать моде». Для подобных презентаций подойдет Дежурная универсальная фраза: Цифровые двойники (например, в контексте BPMS + Process mining) представляют собой смену парадигмы в Управлении бизнес-процессами (можно подставить любую другую отрасль, например, строительство в контексте BIM) и являются примером беспрецедентной точности и адаптивности цифровой трансформации предприятия (tag «humor \\ marketing»). Приложение 3. Некоторые подходы к классификации DT 1 Таблица. Архетипы цифровых двойников [Dozortsev23#page=6],оригинал «Архетипы цифровых двойников» см. в [Archetypes]. 2 [DTatom20] 2.1 Типы ЦД и их классификация: по уровню сложности, зрелости и др. Свод - таблица со ссылками на источники [DTatom20#page=148] 2.2 Отдельные упоминания: цифровые двойники объекта/продукта (Digital Twin, DT-1) и цифровые двойники производства (Digital Twin, DT-2) [DTatom20#page=50]. Полагаю, что речь идет про противопоставление: статический объект (вещь) vs динамический (процесс). Вообще, определение DT для статического объекта (табуретка) будет иным по сравнению с динамическим объектом (бизнес-процессом) при соблюдении базовых условий трехкомпонентности и адекватности. 3 Pre-Digital Twin, Digital Twin, Adaptive Digital Twin, Intelligent Digital Twin[Klezkova23]; 4 Подкатегории цифровых двойников на протяжении жизненного цикла продукта (Классификация по типу Гривса): Pre-Digital Twin, Digital Model (DM), Digital Shadow (DS), Digital Twin (DT) 5 по типу двойника:Digital Twin Prototype (DTP), Digital Twin Instance (DTI) и Digital Twin Aggregate (DTA); 6 Технологии для построения ЦД согласно пятиэлементной модели [Dozortsev23#page=100]. Отраслевые «места обитания» DT см. [Dozortsev22#page=10] Таблица 4. ЦД по отраслям промышленности и областям экономики. Хотел размещать (дублировать) на нем все ссылочные источники для своих статей. Как сделать в нем ссылку на файл с переходом на нужную страницу pdf? Например, тут работает переход на страницу 15: https://dfnc.ru/wp-content/uploads/2020/09/Kniga-TSfirovoj-dvojnik.pdf#page=15 Но, в dropbox.com переход не работает (#page=15), например (при переходе он сам подставляет «?e=1» и переходит на первую страницу документа): https://www.dropbox.com/scl/fi/ikhv0y2ieoxj3blezarjw/digital_twin_book.pdf#page=15 В каком бесплатном облаке лучше организовать библиотеку книг в pdf с переходом на нужную страницу. Если контекстно-страничные ссылки на pdf, приведенные по тексту настоящей статьи, не сработают (например, будет удален источник, закроют сайт и т.п.), то к новой ссылке (на новом сайте) нужно добавлять в окончание url «#page=х», где х равен номеру страницы в файле pdf. Удивительно, но даже такие именитые компании как Software AG занимаются подобным мелким пакостничеством:PS2 Software AG удалил прежние ссылки на ARIS - документацию  PS1 Предложения в формат паспорта DT Для каждого упоминания где-либо «примера DT» предлагаю требовать от автора платформы или конечного решения ссылочку (on-line) на стандартизованный паспорт этого DT. Форму паспорта нужно придумать и закрепить, например, в составе «Анти Маркетингового Манифеста DT». Основная часть паспорта DT 1 Базовый состав DTS («три кита» DT). 1.1 Состав Физического двойника-близнеца (ФД) 1.2 Состав Цифрового двойника-близнеца (ЦД) 1.3 Перечень связей «ФД-ЦД» (направление, технологический стек - уровни, дисциплина обмена, целевые данные), хотя бы для топ 10 связей 2 Адекватность модели 2.1 Критерий адекватности (допущения по адекватности, т.е. подобие «критерия отказа» при расчете надежности). Доказательства достижения заданного уровня адекватности. 3 Дополнительные модули DT 3.1 Модуль прогнозирования. Характеристика, технология реализация (имитационная модель, AI\\ML) 3.2 Модуль агрегирования информации (обобщение) и исторических данных (память \\ воспоминания DT). Тип и номенклатура информации, период истории и т.п. 4 Типовые сценарии использования DT. Возможности и пользовательские сценарии. Далее будут идти дополнительные части паспорта, которые будут специфичны для каждого класса DT, например, DTO. Нужны разделы Цели и Назначения, только нужно как-то ограничить их формой, чтобы туда разный маркетинговый спам не попал типа «Назначение: реализация Цифровой трансформации» "
  },
  {
    "article_id": "https://habr.com/ru/articles/877052/",
    "title": "Уйти от ORM",
    "category": "Дизайн",
    "tags": "orm, sql, java, python, субд",
    "text": "Сегодняшние проблемы ORM Проблемы ORM хорошо известны - именно поэтому вы и читаете эту статью. Подробно о проблемах конкретной реализаций ORM на Java мы писалиранее. Все тезисы этой статьи распространяются и на Python с SQLAlchemy и другие ЯП. Начнем с определения ORM в контексте, например, Java. ORM - СУБД, написанная на Java в качестве бекэнда которой является другая СУБД. Почему так? свой язык декларации схемы данных свой язык декларации схемы данных свой язык запросов (Criteria Api, HQL, JPQL) свой язык запросов (Criteria Api, HQL, JPQL) свой оптимизатор (anti N+1) свой оптимизатор (anti N+1) свой pl/SQL - API вокруг Entity Manager, Sessions со скриптингом на Java свой pl/SQL - API вокруг Entity Manager, Sessions со скриптингом на Java Буква “О” в названии нужна по двум причинам. Во-первых потому что Java (как и многие другие) - это объектно ориентированный язык (что бы это не значило). Во-вторых – подчеркнуть размеры дыры в абстракции ØRM. В такой постановке сразу виден корень зла ORM – вы используете сразуДВЕСУБД вместо одной. Например, Hibernate и PostgreSQL. Всеобъемлющий мануал прикладного программиста (включая вопросы производительности и сам язык запросов, pl/SQL) для PostgreSQL, составляет около 700 страниц. Middle Vulgaris уверенно знает страниц 200 из них. Hibernate, как минимум, удваивает необходимый объем знаний, добавляя еще 600 страниц. При этом, широко известные проблемы с композицией ставят в тупик даже стреляных синьоров. Чего мы на самом деле хотим? Сейчас мы опустим разные вопросы, типа того, что жаль что ORM фреймворки так и не научились самостоятельно решать вопросы оптимизации, композиции и сдвинули все эти проблемы на Петровича. Самое важное это нейро-моторесурс разработчика и на что его потратить. Его можно потратить на то чтобы научиться делать нормальный софт (например, научиться обрабатывать ошибки), либозапрыгнутьв паровозик Spring Data JPА. Давайте, наконец, признаемся, что нам нужна просто хорошая библиотека дляподключенияк базе данных, легко решающая типовые задачи backend разработки. Правильный API для взаимодействия с базой. Весь нужный нам API: И это не шутка. Немного развернем идею: Query пишется на нативном языке базы (например, SQL) Query пишется на нативном языке базы (например, SQL) Query проверяется на этапе компиляции, типы аргументов тоже Query проверяется на этапе компиляции, типы аргументов тоже Typeof(result) выводится автоматически, что избавляет от ручного написания DTØ. Это приводит к образованию единого типового пространства между бэкэндом и схемой базы Typeof(result) выводится автоматически, что избавляет от ручного написания DTØ. Это приводит к образованию единого типового пространства между бэкэндом и схемой базы С помощью SQL выпо меступолучаете нужные вам проекции с базы. Это позволит отказаться от некоторых сомнительных “слоев” вроде сущностей, репозиториев и “мэпперов”. Ситуацию с сервисами мы описывалиранее. С помощью SQL выпо меступолучаете нужные вам проекции с базы. Это позволит отказаться от некоторых сомнительных “слоев” вроде сущностей, репозиториев и “мэпперов”. Ситуацию с сервисами мы описывалиранее. SQL базы имеют некоторую специфику, вроде результата в табличной (строковой) форме и некоторых дополнительных усложнений, вроде разницы между “просто запросом”  и вызовом хранимых процедур, всяких дополнительных вещей типа generated keys и update count, доступ к которым не всегда есть на уровне запроса. Все это тоже немного влияет на итоговый  API. Не торопитесь писать, что это очередная статья, которая к чему-то призывает, но не дает никаких решений! Новое безальтернативное решение появится совсем скоро. Доктор едет-едет сквозь снежную равнину. Порошок целебный людям он везёт. Человек и кошка порошок тот примут, И печаль отступит, и тоска пройдёт. Ноль - Человек и кошка "
  },
  {
    "article_id": "https://habr.com/ru/articles/877130/",
    "title": "Кого и что будет проверять Роскомнадзор в 2025 году",
    "category": "Веб-разработка",
    "tags": "роскомнадзор, ркн, сервер, база данных предприятия, базы данных, проверка, бизнес, штрафы, персональные данные",
    "text": "Надеюсь, 3 недели назад все благополучно вышли из «салатной комы» и телепортировались в суровые рабочие будни. Теперь наша общая действительность – не безудержное веселье и тазы оливье, а подготовка к внеплановой проверке Роскомнадзора, если она внезапно случится, так как плановых проверок в 2025 году не будет, с чем я сердечно поздравляю каждого. Зачем РКН устраивает проверки по персональным данным Утечки ПД случаются все чаще, а штрафы за них становятся все выше. Если вы ведете бизнес, значит, вы точно работаете с информацией о клиентах и сотрудниках, то есть являетесь оператором персональных данных (ПД). Проверка операторов – одна из основных задач Роскомнадзора. Соответственно, шанс удостоиться повышенного внимания регулятора есть у каждого. О том, как пережить интерес РКН к своему бизнесу с наименьшими потерями, читайте дальше. Виды проверок Роскомнадзора Есть два вида проверок – плановые и внеплановые. Плановые и внеплановые проверки подразделяются на документарные и выездные. Но есть особый вид работы РКН – проведение контрольных мероприятий без взаимодействия: они могут коснуться всех, у кого есть сайты. Плановых проверок в 2025 году, как я писала выше, не будет.Внеплановые проверки будут проводить в форме: Документарных проверок Во время таких проверок бизнес должен предоставить копии документов. РКН может запросить: документ, определяющий политику в отношении обработки персональных данных, правила обработки и защиты ПД; документ, определяющий политику в отношении обработки персональных данных, правила обработки и защиты ПД; перечень лиц, имеющих доступ и непосредственно допущенных к обработке ПД; перечень лиц, имеющих доступ и непосредственно допущенных к обработке ПД; согласия субъектов  на  обработку ПД; согласия субъектов  на  обработку ПД; приказ о назначении ответственного за организацию обработки ПД (перечень не исчерпывающий). приказ о назначении ответственного за организацию обработки ПД (перечень не исчерпывающий). Выездных проверок Сотрудники РКН приезжают на предприятие, чтобы лично изучить документацию и удостовериться, что правила работы с персональными данными соблюдаются. В ходе выездной проверки могут совершаться такие контрольные (надзорные) действия, как: осмотр; осмотр; опрос; опрос; получение письменных объяснений; получение письменных объяснений; истребование документов; истребование документов; инструментальное обследование; инструментальное обследование; экспертиза. экспертиза. Внеплановые проверки будут проводиться только в чрезвычайных случаях: для их проведения требуется согласование прокуратуры. Из РКН могут прийти без предупреждения из-за того, что: На вашу компанию неоднократно (более 10 раз в течение календарного года) пожаловались в Роскомнадзор. Причем неважно, кто: сотрудники, клиенты или конкуренты – вас в любом случае возьмут на карандаш. На вашу компанию неоднократно (более 10 раз в течение календарного года) пожаловались в Роскомнадзор. Причем неважно, кто: сотрудники, клиенты или конкуренты – вас в любом случае возьмут на карандаш. Вы направили в РКН неправдивую информацию в ответ на запросы. Вы направили в РКН неправдивую информацию в ответ на запросы. На ваших ресурсах произошла утечка данных. На ваших ресурсах произошла утечка данных. Вы не до конца устранили нарушения, которые Роскомнадзор обнаружил ранее. Вы не до конца устранили нарушения, которые Роскомнадзор обнаружил ранее. Контрольные мероприятия без взаимодействия Контрольное мероприятие без взаимодействия – это, в том числе, проверка сайта, о которой оператор ПД может и не узнать. В рамках контрольных мероприятий без взаимодействия проверяют: локализацию баз данных, где хранятся данные, собранные с помощью сервисов сайта. Сервера, хранящие сведения о россиянах (в том числе данные, собираемые метрическими программами), должны находиться в РФ; локализацию баз данных, где хранятся данные, собранные с помощью сервисов сайта. Сервера, хранящие сведения о россиянах (в том числе данные, собираемые метрическими программами), должны находиться в РФ; размещение политики конфиденциальности на сайте; размещение политики конфиденциальности на сайте; наличие правовых оснований обработки ПД (пользовательское соглашение, согласие на обработку ПД и т.д.); наличие правовых оснований обработки ПД (пользовательское соглашение, согласие на обработку ПД и т.д.); соответствие объема обрабатываемых ПД целям обработки (не нужно собирать фамилию и отчество, если нужно связаться по заявке). соответствие объема обрабатываемых ПД целям обработки (не нужно собирать фамилию и отчество, если нужно связаться по заявке). Если РКН выявил нарушения, то он отправляет владельцу сайта письмо с перечнем нарушений и требованием дать информацию о соблюдении закона. Проверки Роскомнадзора по персональным данным При проверке компании Роскомнадзор может провести экспертизу, потребовать предоставить документы и письменные объяснения. Ведомство не проверяет абсолютно все, что касается работы компании: его интересует лишь часть, где было допущено нарушение. Подготовка к проверке Роскомнадзора по персональным данным Залог успешного прохождения проверки РКН – изначальное выделение работы с персональными данными в отдельную систему. Я не устану повторять, что работа с ПД – это не приложение к основной деятельности, а фактическиеще одно полноценное направление работы вашего бизнеса. Относиться к нему надо ответственно: подход «как-нибудь что-нибудь сделаю» РКН не оценит. Заблаговременно проведите внутренний аудит перед проверкой Роскомнадзора, если сомневаетесь в своих силах, или пригласите напомощь компетентного специалиста– ничего зазорного в этом нет. Всегда читайте уведомление о проверке организации Роскомнадзором: регулятор часто сам прописывает, что он хочет выяснить. Теперь давайте по порядку: Соблюдайте требования к хранению данных не только перед проверкой, а на постоянной основе, то есть будьте готовы к проверке заранее, ведь вас могут не уведомить о том, что она состоится. Соблюдайте требования к хранению данных не только перед проверкой, а на постоянной основе, то есть будьте готовы к проверке заранее, ведь вас могут не уведомить о том, что она состоится. Разберитесь с физическими носителями, содержащими персональные данные. Убедитесь, что они хорошо защищены. И бумажные, и электронные носители сведений не должны быть хаотично разбросаны по офису. Пара копий паспортов сотрудников на стеллаже с почетными грамотами, пара флэшек на тумбочке, куда уборщица аккуратно складывает тряпки… За это гарантированно прилетит штраф: безалаберность нынче стоит дорого. Носители с информацией должныхраниться в сейфах и специальных защищенных шкафах, а те, в свою очередь, должны стоять в помещениях, доступ к которым есть только у ответственных сотрудников. Вот такая матрешка. Проверьте, соблюдается ли этот принцип на вашем предприятии. Разберитесь с физическими носителями, содержащими персональные данные. Убедитесь, что они хорошо защищены. И бумажные, и электронные носители сведений не должны быть хаотично разбросаны по офису. Пара копий паспортов сотрудников на стеллаже с почетными грамотами, пара флэшек на тумбочке, куда уборщица аккуратно складывает тряпки… За это гарантированно прилетит штраф: безалаберность нынче стоит дорого. Носители с информацией должныхраниться в сейфах и специальных защищенных шкафах, а те, в свою очередь, должны стоять в помещениях, доступ к которым есть только у ответственных сотрудников. Вот такая матрешка. Проверьте, соблюдается ли этот принцип на вашем предприятии. Если вы знаете, что к вам направляют выездную проверку, то постарайтесьвыделить инспекторам отдельный кабинет(либо часть кабинета) с компьютером, принтером и сканером. Если вы знаете, что к вам направляют выездную проверку, то постарайтесьвыделить инспекторам отдельный кабинет(либо часть кабинета) с компьютером, принтером и сканером. Разберитесь с документами и обратите внимание на их актуальность. Выше я писала  о том, какие именно нужны. Все должно быть на своих местах и в правильном порядке. Во время проверки ответственному сотруднику следует быстро находить бумаги, которые требуют работники РКН, и вести учет документов, которые он передал проверяющим. Убедитесь, что ответственные сотрудникивели и ведут журналы, в частности, журнал проверок юридического лица контролирующими органами (сотрудники РКН поставят в нем отметки в начале и в конце своего визита). Разберитесь с документами и обратите внимание на их актуальность. Выше я писала  о том, какие именно нужны. Все должно быть на своих местах и в правильном порядке. Во время проверки ответственному сотруднику следует быстро находить бумаги, которые требуют работники РКН, и вести учет документов, которые он передал проверяющим. Убедитесь, что ответственные сотрудникивели и ведут журналы, в частности, журнал проверок юридического лица контролирующими органами (сотрудники РКН поставят в нем отметки в начале и в конце своего визита). Проверьте, во всех ли локальных актах, регулирующих работу с ПД, стоятподписи сотрудников. Проверьте, во всех ли локальных актах, регулирующих работу с ПД, стоятподписи сотрудников. Разберитесь суведомлением о начале обработки персональных данных. За редким исключением этот документ обязаны подавать все операторы ПД, тем более с 30 мая 2025 года штраф за его отсутствие составит 300 000 рублей. Разберитесь суведомлением о начале обработки персональных данных. За редким исключением этот документ обязаны подавать все операторы ПД, тем более с 30 мая 2025 года штраф за его отсутствие составит 300 000 рублей. Если вы не подавали уведомление в Роскомнадзор, но ведете обработку персональных данных, то немедленно исправляйте ситуацию! Крайне рекомендую подать уведомление до 30 мая 2025 года, так как потом будет действовать штраф от 100 000 до 300 000 рублей. Если на момент проверки уведомление не будет подано, то вынесут предписание, в нем укажут на выявленное нарушение: его нужно будет устранить в срок, указанный в предписании и, вероятнее всего, заплатить штраф. Если вы не подавали уведомление в Роскомнадзор, но ведете обработку персональных данных, то немедленно исправляйте ситуацию! Крайне рекомендую подать уведомление до 30 мая 2025 года, так как потом будет действовать штраф от 100 000 до 300 000 рублей. Если на момент проверки уведомление не будет подано, то вынесут предписание, в нем укажут на выявленное нарушение: его нужно будет устранить в срок, указанный в предписании и, вероятнее всего, заплатить штраф. Заявленные в уведомлении цели должны соответствовать фактическим. Если вы продаете онлайн-продукты, но требуете от потенциальных покупателей указывать адреса проживания, РКН обратит на это внимание и потребует пояснить, зачем вам такая информация. Актуализируйте данные для текущей ситуации. Например, на момент подачи сведений, вы реализовали только онлайн-продукты, а недавно стали продавать и доставлять офлайн-товары, следовательно, собирать адреса, а дописать этот пункт в уведомление о работе с персональными данными, забыли. Если поняли, что чего-то не хватает, заполните форму о внесении изменений, а затем уведомите об этом РКН удобным способом: Заявленные в уведомлении цели должны соответствовать фактическим. Если вы продаете онлайн-продукты, но требуете от потенциальных покупателей указывать адреса проживания, РКН обратит на это внимание и потребует пояснить, зачем вам такая информация. Актуализируйте данные для текущей ситуации. Например, на момент подачи сведений, вы реализовали только онлайн-продукты, а недавно стали продавать и доставлять офлайн-товары, следовательно, собирать адреса, а дописать этот пункт в уведомление о работе с персональными данными, забыли. Если поняли, что чего-то не хватает, заполните форму о внесении изменений, а затем уведомите об этом РКН удобным способом: распечатайте и направьте в территориальный орган РКН в бумажном виде; распечатайте и направьте в территориальный орган РКН в бумажном виде; направьте в электронном виде с использованием усиленной квалифицированной электронной подписи; направьте в электронном виде с использованием усиленной квалифицированной электронной подписи; направьте в электронном виде с использованием средств аутентификации ЕСИА. направьте в электронном виде с использованием средств аутентификации ЕСИА. 3 рекомендации для прохождения проверки Роскомнадзора Вот что стоит запомнить: Не конфликтуйте с проверяющими, даже если на ваш взгляд они неправы в выводах, а их претензии необоснованны. Лучше спокойно попросите ссылки на законодательство. Не конфликтуйте с проверяющими, даже если на ваш взгляд они неправы в выводах, а их претензии необоснованны. Лучше спокойно попросите ссылки на законодательство. Если вы сомневаетесь,что сможете пройти проверку своими силами, пригласите специалиста, который будет представлять ваши интересы или обучите для этих целей штатного сотрудника. Если вы сомневаетесь,что сможете пройти проверку своими силами, пригласите специалиста, который будет представлять ваши интересы или обучите для этих целей штатного сотрудника. Важно не то, что говорится, а то, что прописывается в итоговом акте проверки. По каждому аспекту вам могут указать на десятки недочетов, но в соответствующих графах итогового акта пропишут: «Нарушений не выявлено». Важно не то, что говорится, а то, что прописывается в итоговом акте проверки. По каждому аспекту вам могут указать на десятки недочетов, но в соответствующих графах итогового акта пропишут: «Нарушений не выявлено». Акт проверки Роскомнадзора По результатам проверки сотрудники РКН должны составить итоговый акт. В него включаются сведения о результатах проверки, в том числе о выявленных нарушениях с указанием требований законодательства, которые были нарушены, и лицах, их допустивших. На основании итогового акта проверки Роскомнадзора выносится предписание, которое должно содержать: дату вынесения предписания; дату вынесения предписания; наименование организации, фамилию, имя, отчество (при наличии) индивидуального предпринимателя, ИНН, ОГРН, адрес в пределах места нахождения (адрес регистрации по месту жительства) контролируемого лица; наименование организации, фамилию, имя, отчество (при наличии) индивидуального предпринимателя, ИНН, ОГРН, адрес в пределах места нахождения (адрес регистрации по месту жительства) контролируемого лица; номер и дату акта проверки, по результатам которой принято решение о вынесении предписания; номер и дату акта проверки, по результатам которой принято решение о вынесении предписания; информацию о выявленных нарушениях с указанием требований законодательства, которые были нарушены; информацию о выявленных нарушениях с указанием требований законодательства, которые были нарушены; сроки устранения выявленных нарушений; сроки устранения выявленных нарушений; информацию о способах подтверждения устранения выявленных нарушений; информацию о способах подтверждения устранения выявленных нарушений; фамилию, имя, отчество (при наличии), должности лица (лиц), проводившего (проводивших) проверку и вынесшего предписание. фамилию, имя, отчество (при наличии), должности лица (лиц), проводившего (проводивших) проверку и вынесшего предписание. Срок, устанавливаемый в предписании для устранения выявленных нарушений, не может составлять менее десяти рабочих дней и более тридцати рабочих дней со дня получения контролируемым лицом предписания. Если вы не согласны с результатами проверки, можете оспорить их. В срок до 10 рабочих дней с даты получения предписания подавайте письменное возражение в управление РКН своего региона. Разумеется, вам придется аргументировать доводы – предоставить доказательства правоты (документы). Жалобу рассмотрят, а затем частично или полностью пересмотрят выводы либо откажутся менять мнение. В крайнем случае вы сможете обратиться в суд: придется потрудиться над составлением жалобы и, скорее всего, воспользоваться помощью грамотного специалиста. Имейте в виду, что в ходе проверкисотрудники РКН имеют право фиксировать детали на видео и аудио: это не будет являться нарушением. Это основное, что касается проверок оператора персональных данных Роскомнадзором. Чаще и больше о нихговорим здесь. И еще. Имейте в виду, что на проверке от РКН история вашего бизнеса не заканчивается. Даже если допустили нарушения и получили штрафы – ничего страшного. Зато будет повод изучить все, что связано с ошибкой и не допустить ее впредь. Если материал был полезен для вас, но вы не знаете, что написать в комментариях, скопируйте и вставьте: «Уже иду готовиться к внеплановой проверке Роскомнадзора». Так я пойму, что тема актуальна, отложу дела и пойду думать над новой статьей. "
  },
  {
    "article_id": "https://habr.com/ru/articles/874846/",
    "title": "Как ChatGPT может помочь джуну: практический гайд",
    "category": "Веб-разработка",
    "tags": "Джуну, советы начинающим, лайфхаки, советы новичкам, chatgpt, нейросети, программирование, разработка",
    "text": "Статья будет полезна новичкам и тем, кто только начинает знакомиться с нейронками. Не является руководством к бездумному делегированию всех своих задач чату. Нейросети как и любой инструмент который когда либо был изобретен — всего лишь инструмент и без умелых рук бесполезен. Ситуация:Ты джун, и тебе дают задачу на разработку. Часто это может быть фикс багов или минорная доработка. Ты еле развернул проект, чуть-чуть разбираешься в синтаксисе, но что значат все эти многобукаф и строчки кода – пока понятия не имеешь. Что пишем в GPT: Во все начальные промты желательно добавлять роль и контекст для чата. Например:«Ты профессиональный разработчик на языке [твой язык и его версия], работаешь с проектом [краткое описание для чего проект]» Стартер для кода:Просишь GPT сгенерировать начальный каркас кода на заданном языке. Например:«Напиши функцию на Python, которая ищет дубликаты в массиве и возвращает их индексы.»GPT не только выдаст решение, но и предложит оптимизации. Стартер для кода:Просишь GPT сгенерировать начальный каркас кода на заданном языке. Например:«Напиши функцию на Python, которая ищет дубликаты в массиве и возвращает их индексы.»GPT не только выдаст решение, но и предложит оптимизации. Исправляем ошибки:У тебя есть код ошибки и шаги, которые к ней привели, тут добавляешь следующее«Исправь ошибку которая возникает. Ошибка [шаги до ошибки, код и описание], код проекта [скопированный код]» Исправляем ошибки:У тебя есть код ошибки и шаги, которые к ней привели, тут добавляешь следующее«Исправь ошибку которая возникает. Ошибка [шаги до ошибки, код и описание], код проекта [скопированный код]» Объяснения сложного кода:Ты нашел пример решения, но не понимаешь его до конца. Попроси GPT объяснить строку за строкой:«Тебе необходимо изучить код, шаг за шагом описывая, что делает каждая строчка кода. Объясни, как работает этот код с использованиемset().» Объяснения сложного кода:Ты нашел пример решения, но не понимаешь его до конца. Попроси GPT объяснить строку за строкой:«Тебе необходимо изучить код, шаг за шагом описывая, что делает каждая строчка кода. Объясни, как работает этот код с использованиемset().» Динамическое улучшение:Можешь задавать вопросы:«Как оптимизировать этот код для больших массивов?» Динамическое улучшение:Можешь задавать вопросы:«Как оптимизировать этот код для больших массивов?» Реальный пример:Задача: Реализовать функцию, которая принимает список строк и возвращает новый список, содержащий только строки, длина которых больше заданного значения. Ситуация:Написать документацию к функции или проекту — скучно и сложно, особенно если постоянно ничего не успеваешь. Как помогает GPT: Генератор документации:Передаешь GPT код и просишь:«Напиши документацию к этому коду,шаг за шагомописывая алгоритм его работы, используемые переменные, структуру БД».GPT создаст структуру с описанием параметров, возвращаемых значений и примерами. Также можно скопировать пример документации и попросить чат описать по аналогии. Генератор документации:Передаешь GPT код и просишь:«Напиши документацию к этому коду,шаг за шагомописывая алгоритм его работы, используемые переменные, структуру БД».GPT создаст структуру с описанием параметров, возвращаемых значений и примерами. Также можно скопировать пример документации и попросить чат описать по аналогии. Перевод технического в человеческое:Попроси объяснить сложные моменты простыми словами. Например:«Перепиши это описание для не-программиста» Перевод технического в человеческое:Попроси объяснить сложные моменты простыми словами. Например:«Перепиши это описание для не-программиста» GPT напишет тебе документацию с минимальными затратами. Например, нужно описать что делает код функции для управления корзиной товаров: GPT быстро выдаст нужное описание: Ситуация:Написал код, не уверен в нём и не хочешь отправлять на ревью плохой код. Как помогает GPT: Ревью кода:Передаешь GPT свой код и спрашиваешь:«Какие проблемы ты видишь в этом коде и как их можно исправить?»GPT укажет на ошибки, предложит улучшения и объяснит, почему так лучше. Ревью кода:Передаешь GPT свой код и спрашиваешь:«Какие проблемы ты видишь в этом коде и как их можно исправить?»GPT укажет на ошибки, предложит улучшения и объяснит, почему так лучше. Рефакторинг:Можно попросить переписать код с учетом лучших практик:«Сделай этот код более читаемым и используй паттерн Singleton» Рефакторинг:Можно попросить переписать код с учетом лучших практик:«Сделай этот код более читаемым и используй паттерн Singleton» Ситуация:На планировании дали задачу, но её формулировка размыта, и ты не понимаешь, с чего начать. Как помогает GPT: Разбор задачи:Пишешь:«Задача: реализовать поиск по базе данных. Как лучше подойти к её решению? Какие вопросы следует задать, чтобы лучше понять требования?»GPT предложит шаги: уточняющие вопросы, выбор алгоритма, построение структуры данных, интеграция с API. Разбор задачи:Пишешь:«Задача: реализовать поиск по базе данных. Как лучше подойти к её решению? Какие вопросы следует задать, чтобы лучше понять требования?»GPT предложит шаги: уточняющие вопросы, выбор алгоритма, построение структуры данных, интеграция с API. Генератор вопросов:Если задача неполная, GPT подскажет, какие уточняющие вопросы задать заказчику или тимлиду:«Какие поля в базе должны участвовать в поиске?» Генератор вопросов:Если задача неполная, GPT подскажет, какие уточняющие вопросы задать заказчику или тимлиду:«Какие поля в базе должны участвовать в поиске?» Ситуация:Коллега или стажёр просит тебя объяснить сложный концепт. Как помогает GPT: Пошаговые объяснения:Пишешь:«Объясни, что такое микросервисы для начинающего разработчика»GPT предложит аналогии и упростит концепцию до базового уровня. Пошаговые объяснения:Пишешь:«Объясни, что такое микросервисы для начинающего разработчика»GPT предложит аналогии и упростит концепцию до базового уровня. Демонстрации:Просишь GPT написать короткий пример кода:«Покажи, как работает паттерн Observer на Python» Демонстрации:Просишь GPT написать короткий пример кода:«Покажи, как работает паттерн Observer на Python» Ситуация:Надо повышать грейд, хочешь развиваться, но не знаешь, какие вопросы задают на уровне мидла/сеньора. Как помогает GPT: Собеседования:Просишь составить список вопросов:«Какие вопросы могут задать на собеседовании для мидл/сеньор-разработчика?»GPT предложит темы по алгоритмам, архитектуре, популярным паттернам проектирования. Собеседования:Просишь составить список вопросов:«Какие вопросы могут задать на собеседовании для мидл/сеньор-разработчика?»GPT предложит темы по алгоритмам, архитектуре, популярным паттернам проектирования. Обучение:Просишь советы по обучению джунов:«Как проводить ревью кода, чтобы было полезно и не токсично?» Обучение:Просишь советы по обучению джунов:«Как проводить ревью кода, чтобы было полезно и не токсично?» Автор:Dmitrii Tunguskov "
  },
  {
    "article_id": "https://habr.com/ru/companies/ru_mts/articles/869740/",
    "title": "Remote Config и A/B-эксперименты: история разработки и основные возможности",
    "category": "Веб-разработка",
    "tags": "аналитика, разработка, python, веб-аналитика, remote config, ab-тестирование",
    "text": "Привет, Хабр! Меня зовут Леша Жиряков, я техлид backend-команды витрины онлайн-кинотеатра KION: путь от разработчика до этой должности прошел за три года. Сейчас я продолжаю готовить свою серию статей и докладов про Python — всегда топлю за него, хочу, чтобы этот ЯП использовали в проде как можно чаще и больше. Сегодня расскажу про наш сервис Remote Config и A/B-эксперименты — это переработка одного из моих докладов. Если что, задавайте вопросы в комментариях — постараюсь на все ответить. Что такое Remote Config и зачем он нам нужен В приложении KION при запуске происходит запрос на сервер для получения конфигурации. На основе этих данных приложение определяет, в каком эксперименте оно участвует, какие витрины и полки должны отображаться, и применяет соответствующие настройки. Если серверная конфигурация недоступна или возникает ошибка, используется дефолтный Config, встроенный в приложение. Но Remote Config позволяет применять и то, чего нет в коде. Как выглядит Config Конфигурация в KION представлена в формате JSON, содержащем множество параметров. Среди передаваемых параметров есть два ключевых — hash и список экспериментов: Hashиспользуется для проверки актуальности конфигурации на устройстве. Это позволяет определить, нужно обновить конфигурацию или оставить текущую. Hashиспользуется для проверки актуальности конфигурации на устройстве. Это позволяет определить, нужно обновить конфигурацию или оставить текущую. Список экспериментовпередается устройству для идентификации, где именно оно участвует. Эта информация включается в каждое продуктовое событие, которое отправляется на сервер. Список экспериментовпередается устройству для идентификации, где именно оно участвует. Эта информация включается в каждое продуктовое событие, которое отправляется на сервер. В аналитической базе данных сохраняются все события. Это позволяет оценивать ключевые показатели и учитывать участие устройства в экспериментах. Например, помогает определить, как повлияла на пользователя та или иная витрина или полка. В конечном счете все это важно для принятия продуктовых решений. Вот тестируем мы новую модель построения персональной витрины и видим результаты A/B-эксперимента. Если группа, у которой работала эта витрина, смотрела больше, чаще и дольше, мы оставляем модель в проде. Для чего нужен Remote Config Получать конфигурацию важно для обновлений без деплоя — так можно обойтись без yuml-файлов и перевыкатки. Это удобно, чтобы управлять фича-флагами — например, оперативно выключить что-то в пятницу. На бэке при поломке мы просто выключаем фичу, нам не нужно ничего выкатывать. Результат — в выходные ничего не поломается, и команда будет отдыхать, а не работать. Конфигурация нужна и для проведения A/B-экспериментов и включения и выключения функций в реальном времени. Еще один плюс — настройка контуров окружений: prod, stage, dev. Общий Config наследуется, а отличия задаются только для отдельных параметров, таких как IP-адреса или хосты. Почему свой продукт Мы начали разрабатывать Remote Config из-за санкционных рисков. Была вероятность, что в какой-то момент все может перестать работать. К тому же создание собственного решения позволило вносить доработки под наши нужды. Раньше использовался известный Remote Config от ИТ-гиганта, связанного с поиском, но он имел ряд ограничений. Например, сложности с таргетингом по версиям, проблемы с black- и white-листами, из-за чего некоторые пользователи просто не попадали в нужные списки. Собственный продукт избавляет от этих ограничений и позволяет нам развиваться на наших условиях. Плюс свое решение можно сильно кастомизировать для себя, например применять списки топ-менеджеров, чтобы они не попадали в группы активных экспериментов. Потому что часто топ-менеджмент бывает не готов к авангардным решениям, которые тестируются. К примеру, вырезать просмотренный тайтл из баннерной карусели. Если это Originals или эксклюзив, в который много проинвестировали, и его не будет на главной полке витрины, могут не понять и сначала принять оргмеры, а потом разбираться (шутка). Анализ готовых решений показал, что большинство из них ориентировано на веб. Среди их преимуществ — визуализация, интеграция с метриками, построение графиков и даже прогнозирование результатов A/B-экспериментов. Но они не подходят для нашего случая, так как мы работаем с шестью платформами, включая множество приложений, а не только веб. Среди минусов — сложность доработки open-source-решений и ограниченность в проведении параллельных A/B-экспериментов, что важно для KION. Учитывая эти лимиты и относительную простоту реализации, мы решили разработать собственный Remote Config. Главные фичи нашего Remote Config Теперь расскажу подробнее об основных возможностях продукта. У системы изначально были такие ключевые функции: Изменение конфигурации устройств в реальном времени. Позволяет оперативно вносить изменения без обновления приложения. Изменение конфигурации устройств в реальном времени. Позволяет оперативно вносить изменения без обновления приложения. Система нотификации в Telegram. Уведомления о том, какой конкретный пользователь изменил параметр конфигурации. Система нотификации в Telegram. Уведомления о том, какой конкретный пользователь изменил параметр конфигурации. Ролевая модель в рамках одного неймспейса. Поддержка администраторов с полными правами и юзеров с ограничениями на чтение или запись. Ролевая модель в рамках одного неймспейса. Поддержка администраторов с полными правами и юзеров с ограничениями на чтение или запись. Независимые namespaces. Возможность создавать отдельные пространства конфигураций. Например, бэковые системы KION, продукты внутри МТС и все приложения KION используют собственные пространства. Независимые namespaces. Возможность создавать отдельные пространства конфигураций. Например, бэковые системы KION, продукты внутри МТС и все приложения KION используют собственные пространства. A/B-эксперименты. Поддержка проведения тестов для анализа влияния изменений. A/B-эксперименты. Поддержка проведения тестов для анализа влияния изменений. Наследование конфигураций. Позволяет не переписывать полностью конфигурацию для каждого окружения, а вносить изменения только в отдельных строках. Это полезно как для настройки production-конфигурации, так и для работы над новыми фичами. Можно создавать отдельные конфиги для тестирования, а потом интегрировать изменения в основной при релизе. Наследование конфигураций. Позволяет не переписывать полностью конфигурацию для каждого окружения, а вносить изменения только в отдельных строках. Это полезно как для настройки production-конфигурации, так и для работы над новыми фичами. Можно создавать отдельные конфиги для тестирования, а потом интегрировать изменения в основной при релизе. Есть и другие фичи: полное кэширование данных, высокая скорость ответа; полное кэширование данных, высокая скорость ответа; вариативность конфигов по версии, модели, ОС; вариативность конфигов по версии, модели, ОС; создание своих вариаций; создание своих вариаций; разделение параметров по продуктовым командам; разделение параметров по продуктовым командам; поэтапная раскатка; поэтапная раскатка; белые и черные списки. белые и черные списки. Кэширование данных и вариативность конфигурации по версии и модели операционной системы обеспечивают гибкость и точность работы. Можно учитывать особенности разных платформ, где смарты получают индивидуальные конфигурации. Например, если новая фича недоступна для устаревших устройств, конфиг применяется только к актуальным моделям. Система также поддерживаетпоэтапную раскатку. Это значит, что новая версия сначала применяется на 10% устройств, и это позволяет мониторить продуктовые метрики. А потом — убедиться, что приложение работает стабильно и количество сбоев не увеличивается. После успешного тестирования раскатка продолжается на остальные устройства. Механизмбелых и черных списков— обязателен. Онуправляет доступом к экспериментам. Ключевые пользователи (топы, которые хотят тестировать, и продакт-менеджеры) совершенно точно включаются в эксперименты. Причем даже в том случае, если проводится долгосрочное тестирование — например, фиксированное поведение для 1% юзеров без новых фич. На иллюстрации выше — стандартная упрощенная схема работы. Приложения отправляют запросы на сервер. Все это построено на Django, за которым — Cache, PostgreSQL и Celery. События отправляются в Kafka и логируются через GrayLog. Еще на схеме показан юзер — это человек, который управляет конфигурацией. Изначально использовалась Django-админка. Но со временем ее возможностей стало недостаточно, нам был нужен удобный поиск. Поэтому начали разрабатывать собственный GUI. Как работает Remote Config Дальше поговорим именно про удаленный Config. Реализация включает общие настройки Core. От них наследуются Config. Например, можно создавать продовый или стейджевый конфиг, указывая: «Наследуйся от такого Core». Есть эксперименты и специальные параметры: эксперименты — это когда задаются параметры для конкретного устройства с определенной версией; эксперименты — это когда задаются параметры для конкретного устройства с определенной версией; специальные параметры — то же самое, но для фиксирования настроек после экспериментов. Когда новая фича с какой-то версией становится постоянной, ее нужно отдавать без разделения. Так что параметры собираются и передаются устройствам. специальные параметры — то же самое, но для фиксирования настроек после экспериментов. Когда новая фича с какой-то версией становится постоянной, ее нужно отдавать без разделения. Так что параметры собираются и передаются устройствам. Здесь представлена детальная схема. Видно, чтоCoreвыступает базовой конфигурацией, от которой наследуется, например, продовый конфиг. Основные параметры включают модель устройства, операционную систему, версию и client id — он используется как ключ для экспериментов. При создании необходимо указать, на что ориентироваться: client id, device id или другие параметры. Эти данные задаются для корректной работы экспериментов и настройки конфигураций. Как собирается конфиг Все достаточно просто. Например, телевизор делает запрос. У него есть client id. SDK указывает модель, версию, в ответ ему возвращаются параметры. Список экспериментов и параметр hash нужны для понимания, применять этот параметр или нет, есть ли там что-то новое. Вот более подробно про механизм наследования. Есть дефолтная конфигурация с двумя ключами. Видно, что потом создается еще один Config, где перезаписывается второй ключ. У каждого эксперимента есть Core и параметры — это работает так же, как с обычным Config. Например, при запуске эксперимента с двумя группами нужно задать 20 параметров, из которых 19 — общие. Чтобы не прописывать их дважды, создается общий Config, от которого наследуются группы. К каждой указывается: «Наследуйся от этого, тут такой атрибут, а здесь — другой». Дальше применяются специальные атрибуты, которые перезаписываются в зависимости от наследования. Последовательность настроек соблюдается в этом порядке. A/B-эксперименты С Remote Config они вполне обычные. Попадает ли человек в покрытие эксперимента?Например, если эксперимент раскатывается на 10%, то при поступлении запроса система должна определить, входит ли человек в эти 10%. Для этого проверяется, соответствует ли запрос установленным критериям покрытия. Какой Config ему вернуть и в какой эксперимент он попал?После проверки покрытия определяется, в каком из экспериментов участвует пользователь. Дальше возвращается соответствующий Config, включающий параметры для его группы. Это достаточно просто вычислить. При создании каждого эксперимента формируются соль покрытия и соль Config, которые представляют собой хэши. Механизм работает так: приходит client id; приходит client id; client id складывается с солью покрытия, затем берется их хэш; client id складывается с солью покрытия, затем берется их хэш; полученное число делится на 100. Берется остаток от деления, который дает значение от 0 до 100; полученное число делится на 100. Берется остаток от деления, который дает значение от 0 до 100; если это число меньше процента покрытия, например 10%, пользователь не попадает в эксперимент. Если больше, то да. если это число меньше процента покрытия, например 10%, пользователь не попадает в эксперимент. Если больше, то да. Какой Config возвращается клиенту? Здесь уже роль играет не соль разбиения, а соль Config, которая тоже сгенерирована хэшем. Берется ключ разбиения, который зависит от контекста. Например, если эксперимент в авторизованной зоне, используется profile id, если нет — device id. Складываем ключ разбиения с солью Config, считаем хэш, остаток от деления и определяем, в какой чанк попал клиент. Под капотом система работает не с процентами, а именно с чанками, и в зависимости от результата возвращается нужный Config. Благодаря соли Config можно проводить независимые эксперименты. Например, на витрине могут одновременно идти 2–3 из них. Для нового генерируется своя соль, что гарантирует равномерное распределение групп. В итоге влияние предыдущих исключается, и результаты текущего эксперимента становятся максимально точными. Работа с чанками позволяет делить аудиторию на равные доли. Например, когда нужно разделить на три одинаковые группы, это делается легко благодаря механизму чанков. В одном из случаев мы запускали два эксперимента, но по важным причинам потребовалось срочно разделить один из них на два дополнительных. Используя соль Config, это удалось реализовать быстро и без проблем. Группы не перемешиваются и продолжают участвовать в эксперименте. Ну и, соответственно, задаем белые и черные списки. Это все применяется на этапе получения конфигурации. Заведение параметров Заведение параметров тоже реализовано удобно. Часто нужно загрузить какой-нибудь сложный JSON, и важно, чтобы он не ломался, если вдруг возникает ошибка. Это позволяет минимизировать риски и обеспечить стабильность системы. Для удобства работы с параметрами реализована подсветка синтаксиса. Это дает возможность сразу видеть ошибки, например отсутствие запятой. Еще можно задавать тип данных — число или строку. Тогда на клиенте не будет ошибок при обработке данных. Удобство заведения параметров дополнено историчностью. Обязательно фиксируется, кто и когда изменил атрибуты. Мы добавили поле для указания причины, чтобы не нужно было искать по чатам, кто и почему принял такое решение. Теперь при изменении параметра это поле обязательно заполняется и отображается. Так что становится понятно, кто и что именно изменил — и почему он это сделал. Этих Config действительно много, как и продуктовых вертикалей и команд. Раньше были проблемы с документацией, но мы решили их, добавив описание к каждому параметру и указание соответствующей вертикали. Нотификация Теперь при открытии Config сразу видно, какая команда за него отвечает и для чего он нужен. Еще настроена нотификация: если кто-то вносит изменения, уведомление приходит сразу. Это удобно, так как команды следят, чтобы ничего не изменялось после 17:00, особенно в пятницу — выше уже писал, что на выходных сотрудникам нужно отдыхать. Про производительность: при работе с кэшем система выдерживает на одно ядро 1 000 RPS с задержкой в 30 миллисекунд. Такая конфигурация полностью удовлетворяет нашим требованиям. Настроен автоскейлинг. Вот и все: поделился всем, чем хотел. Вопросы оставляйте в комментариях! Еще мои посты: Machine Learning в онлайн-кинотеатрах: как повысить время смотрения и понять, что одного ML мало. Часть 1 Machine Learning в онлайн-кинотеатрах: как повысить время смотрения и понять, что одного ML мало. Часть 1 Machine Learning в онлайн-кинотеатрах: как повысить время смотрения и понять, что одного ML мало. Часть 2 Machine Learning в онлайн-кинотеатрах: как повысить время смотрения и понять, что одного ML мало. Часть 2 Доверяй, но проверяй: как мы в KION идем по пути продуктовых событий Доверяй, но проверяй: как мы в KION идем по пути продуктовых событий "
  },
  {
    "article_id": "https://habr.com/ru/articles/869696/",
    "title": "Узнать всё о конкурентах за 5 шагов + бесплатный фреймворк внутри",
    "category": "Веб-разработка",
    "tags": "Фреймворк анализа конкурентов, конкурентный анализ, исследование конкурентов, анализ рынка, бизнес-анализ, позиционирование на рынке, маркетинговый анализ, инструменты для анализа рынка, сравнение конкурентов, анализ конкурентов",
    "text": "В мире бизнеса нельзя игнорировать конкуренцию. Даже если ваш продукт уникален сегодня, завтра на рынке может появиться аналог, который завоюет внимание ваших клиентов. Конкурентный анализ — это ваш способ предвидеть такие сценарии и оставаться на шаг впереди. Знаете ли вы, что компании, регулярно исследующие своих конкурентов, увеличивают шансы на успех своего продукта на 30%? Это не просто цифры — это результат, подтвержденный практикой многих и моим личным опытом. Конкурентный анализ позволяет: Понять, что работает у других, и использовать эти идеи для улучшения своего продукта. Понять, что работает у других, и использовать эти идеи для улучшения своего продукта. Выявить слабости конкурентов и предложить клиентам то, чего они не получают. Выявить слабости конкурентов и предложить клиентам то, чего они не получают. Увидеть новые возможности для роста, которые остаются незаметными без глубокого исследования. Увидеть новые возможности для роста, которые остаются незаметными без глубокого исследования. Что вы получите из этой статьи? Если вы хотите запустить новый, или вывести существующий бизнес на новый уровень, читайте дальше. В статье вы узнаете: Как найти и классифицировать конкурентов. Как найти и классифицировать конкурентов. Какие данные собирать и как это делать эффективно. Какие данные собирать и как это делать эффективно. Как использовать полученную информацию для создания уникального предложения. Как использовать полученную информацию для создания уникального предложения. А в конце вас ждет бонус — готовыйфреймворк для анализа конкурентов, который можно скачать и сразу применить. Классические способы поиска конкурентов Поисковые системы и аналитические инструменты:Использование Google, Яндекс, SimilarWeb, Ahrefs помогает выявить компании, которые занимают лидирующие позиции по ключевым запросам. Поисковые системы и аналитические инструменты:Использование Google, Яндекс, SimilarWeb, Ahrefs помогает выявить компании, которые занимают лидирующие позиции по ключевым запросам. Социальные сети:Многие бренды активно продвигаются через соцсети. Анализ их активности может дать представление об их стратегии. Социальные сети:Многие бренды активно продвигаются через соцсети. Анализ их активности может дать представление об их стратегии. Отзывы и рейтинги:Платформы вроде Otzovik, Trustpilot или отзывы на маркетплейсах помогают понять, что нравится и не нравится клиентам конкурентов. Отзывы и рейтинги:Платформы вроде Otzovik, Trustpilot или отзывы на маркетплейсах помогают понять, что нравится и не нравится клиентам конкурентов. Прямое тестирование продуктов:Покупка товаров или услуг конкурентов позволяет на собственном опыте понять их сильные и слабые стороны. Прямое тестирование продуктов:Покупка товаров или услуг конкурентов позволяет на собственном опыте понять их сильные и слабые стороны. Анализ открытых данных:Отчёты, пресс-релизы, статьи в СМИ часто содержат полезные детали о финансовых результатах и планах компаний. Анализ открытых данных:Отчёты, пресс-релизы, статьи в СМИ часто содержат полезные детали о финансовых результатах и планах компаний. Как обычно проводится анализ конкурентов Обычно процесс выглядит так: Сбор информации:Используются различные источники для получения данных о продуктах, стратегиях и репутации конкурентов. Сбор информации:Используются различные источники для получения данных о продуктах, стратегиях и репутации конкурентов. Сравнение предложений:Продукты и услуги анализируются по ключевым параметрам: цена, качество, дополнительные опции. Сравнение предложений:Продукты и услуги анализируются по ключевым параметрам: цена, качество, дополнительные опции. Оценка маркетинга:Изучаются рекламные кампании, каналы продвижения и работа с клиентами. Оценка маркетинга:Изучаются рекламные кампании, каналы продвижения и работа с клиентами. Выявление сильных и слабых сторон:Анализируются успехи и ошибки конкурентов, чтобы понять, что можно перенять или улучшить. Выявление сильных и слабых сторон:Анализируются успехи и ошибки конкурентов, чтобы понять, что можно перенять или улучшить. Такой подход эффективен, но часто приводит к перегрузке информацией. Данные разрознены, требуют длительной обработки и могут терять ценность без четкой структуры. Мой новый подход: фреймворк анализа конкурентов Чтобы избежать этих проблем, я разработалфреймворк анализа конкурентов— структурированный инструмент, который позволяет эффективно упорядочить процесс. Что делает фреймворк уникальным? Вы работаете по четким шагам, не пропуская важных деталей. Вы работаете по четким шагам, не пропуская важных деталей. Данные систематизируются в единой структуре, что облегчает анализ. Данные систематизируются в единой структуре, что облегчает анализ. Результаты превращаются в конкретные шаги и действия для бизнеса. Результаты превращаются в конкретные шаги и действия для бизнеса. Фреймворк позволяет команде не только собирать информацию, но и быстро переходить к решениям, которые дадут реальные результаты. В следующем разделе я расскажу, что внутри этого инструмента и как с ним работать. Что такое фреймворк анализа конкурентов Фреймворк — это готовая структура или шаблон, который помогает решать задачи последовательно и эффективно. В контексте анализа конкурентов фреймворк упрощает процесс: вы следуете четким шагам, собираете нужные данные, анализируете их и формируете выводы. Фреймворк структурирует процесс, делает его понятным и позволяет избежать хаоса. Благодаря логическому порядку шагов вы можете последовательно двигаться к цели, не пропуская важные аспекты анализа. Что внутри фреймворка Цели анализаОпределяем, зачем нужен анализ: изучить рынок, найти сильные и слабые стороны конкурентов, оценить свои возможности. Это помогает сосредоточиться на нужных аспектах, создать уникальное предложение и избежать ошибок, которые допустили другие компании. Источники и методыВыбираем, где искать информацию (сайты, соцсети, интервью) и как её собирать (анализ, тестирование, опросы), что позволяет собрать точные данные о рынке, клиентах и конкурентах для дальнейшего анализа. Поиск конкурентовНаходим прямых, косвенных и потенциальных конкурентов. Они влияют на рынок и определяют ваши возможности. Учет всех типов конкурентов помогает понять общую картину. Сбор информацииСобираем данные о конкурентах: их продукте, стратегии, ценах и финансах, что дает полное представление о том, как они работают и на чем основывается их успех. Анализ данныхИзучаем, что делают конкуренты хорошо и какие у них слабости, что помогает найти точки роста, уникальные преимущества и избежать ошибок. ДействияНа основе анализа разрабатываем план: улучшаем продукт, меняем маркетинг, усиливаем отстройку, что позволяет выделиться на рынке и повысить конкурентоспособность. Как работать с фреймворком Этот фреймворк можно использовать как инструмент для командной работы или самостоятельного анализа. Если вы работаете с командой, распечатайте шаблон на большом листе формата A1 и повесьте его в офисе или на доске. Каждый раздел фреймворка станет отдельным блоком, который можно заполнять стикерами или записывать результаты совместных обсуждений. Фреймворк отлично подойдет для мозгового штурма, визуализации идей и обмена мнениями в реальном времени. Вы можете заполнить его самостоятельно или подключить коллег, чтобы совместно работать над каждым шагом, что позволит быстро делиться прогрессом, отслеживать изменения и создавать единую базу знаний о конкурентах. Я решил поделиться этим фреймворком совершенно бесплатно, однако если он вам понравится и вы захотите отблагодарить меня, то можно найти QR-код в подвале фреймворка или поддержать звездами в моем ТГ-канале. Скачать фреймворк в формате PDF можнопо этой ссылке.Пишу о стартапах и психологии предпринимателейу себя в ТГ. "
  },
  {
    "article_id": "https://habr.com/ru/articles/869184/",
    "title": "7 Дыр Я.Директ. Новогодний обзор фишек за которые платишь ты",
    "category": "Веб-разработка",
    "tags": "Янедкс Директ, скликивание, защита от ботов, защита сайта, прокси фермы",
    "text": "Ранее я уже писал омотивированном трафикеи о том, как пользователи кликают за деньги. Кто хочет прочитать, вот ссылка на тотпост. Однако, те клики, которые генерирует рекламная сеть, нельзя просто так создать через биржу микро-задач. Видно, что кликают профессионалы — а потоково делать это возможно только с одного рабочего места. «Отключай рекламу» Это «угроза», которую вводили одному из моих клиентов в вебвизоре. Видно, что клики целенаправленные, и исходят из одного источника. Я, Григорий Мельников, автор сервиса защиты сайтаKillBotрешил повторить этот кейс и понять как они скликивают нашу рекламу. Пост основан не на моем опыте Основы я получил из телеграм каналов тех, кто занимается этим профессионально. Ссылки на телеграм каналы и специализированный софт есть в моем другомпостена Хабре. Ввод e-mail с разных устройств — это основа защиты Пользователи бот-программ давно поняли это, да и Яндекс подробноописывает, как работает их антифрод-система. Основная идея — пользователь настоящий, если он владеет несколькими устройствами. Если с разных устройств на сайтах вводятся одни и те же триплексыимя+mail+телефонто, Яндекс может предполагать, что эти разныеустройства принадлежат одному пользователю. Таким образом, клик может быть учтён если ты: возьмешь имя, маил и телефон друга (нужно чтобы эти данные были засвечены в метрике — т. е. друг ранее вводил эти данные где‑то в интернете) возьмешь имя, маил и телефон друга (нужно чтобы эти данные были засвечены в метрике — т. е. друг ранее вводил эти данные где‑то в интернете) откроешь браузер в режиме инкогнито откроешь браузер в режиме инкогнито отправишь форму с именем, маил и телефоном — на любом сайте где есть такая форма и счетчик метрикис этого момента алгоритмы могут посчитать, что это новое устройство реального владельца отправишь форму с именем, маил и телефоном — на любом сайте где есть такая форма и счетчик метрикис этого момента алгоритмы могут посчитать, что это новое устройство реального владельца зайдёшь в поисковик и кликнешь. зайдёшь в поисковик и кликнешь. Поздравляю! Твой «друг» сделал клик. Это основа того, как могут действовать профессиональные кликеры. Боты и реальные мотивированные пользователи используют реальные контакты, вводя их с разных реальных устройств или эмуляторов. В итоге создается впечатление, что это реальные заявки. 10% моих грязных кликов были учтены Это был грубый завод с толкача. Если поставить это на поток, то процент можно поднять до 30–50%. У 10-ти своих друзей я попросил почту и телефон для проведения эксперимента.Я не контролировал качество этих данных и не знаю засветились ли эти пары (телефон+маил) в Метрике или нет. При этом половина почт были корпоративные. Для подмены IP я приобрёл индивидуальные проксиздесь— по запросу для каждой сессии можно менять ГЕО и оператора. Для создания разнообразных сессий я использовал мультиаккаунтовый браузерMoreLogin. Схожих антидетект браузеров много, поэтому, можно использовать и другие. Особенность именно MoreLogin в том, что он даёт доступ к реальным мобильным устройствам. То есть, я со своего рабочего компьютера могу выбрать настоящий удаленный телефон и воспользоваться его браузером. Схема работы Начинаем сессию в MoreLogin (создаем профиль пользователя). Начинаем сессию в MoreLogin (создаем профиль пользователя). Подключаем прокси для смены IP. Подключаем прокси для смены IP. Устанавливаемрусский язык и подходящий часовой пояс (по другому я не пробовал, есть подозрение, что это не обязательно). Устанавливаемрусский язык и подходящий часовой пояс (по другому я не пробовал, есть подозрение, что это не обязательно). Зайдем на новостной сайт (например,lenta.ru— почитаем пару новостей — реальный пользователь может начать день за чтением. Без новостного сайта не пробовал — может и не обязательно). Зайдем на новостной сайт (например,lenta.ru— почитаем пару новостей — реальный пользователь может начать день за чтением. Без новостного сайта не пробовал — может и не обязательно). Переходим на сайт с формой, заполняем реальные контактные данные (номер телефона, имя и email). Я в гугле вбивал коммерческий запрос типа «купить хомяка в СПБ», переходил на сайт интернет магазина, убеждался что на нем установлена метрика, создавал заказ и в форму заказа вводил Имя, телефон и маил. Переходим на сайт с формой, заполняем реальные контактные данные (номер телефона, имя и email). Я в гугле вбивал коммерческий запрос типа «купить хомяка в СПБ», переходил на сайт интернет магазина, убеждался что на нем установлена метрика, создавал заказ и в форму заказа вводил Имя, телефон и маил. Далее я ждал 20–30 минут чтобы все данные дошли и синхронизировались — возможно это и не нужно или период должен быть существенно меньше. Далее я ждал 20–30 минут чтобы все данные дошли и синхронизировались — возможно это и не нужно или период должен быть существенно меньше. Открывал поиск и кликал по своему сайту. Открывал поиск и кликал по своему сайту. ВАЖНОНужно использовать различные сайты для каждой пользовательской сессии — если разные пользователи посещают одни и те же сайты, то это один и тот же пользователь. Результат из10-ти контактов сработали точно 3, возможно 4-ре. из10-ти контактов сработали точно 3, возможно 4-ре. из 44-ти кликов — 5 были учтены — это примерно 10% — это очень много с учетом того, что я кликал «в лоб». из 44-ти кликов — 5 были учтены — это примерно 10% — это очень много с учетом того, что я кликал «в лоб». 13 показов было учтено — это почти в 3 раза больше чем кликов. 13 показов было учтено — это почти в 3 раза больше чем кликов. На кейс я потратил одно утро и один вечер. На кейс я потратил одно утро и один вечер. 3 фактора как эти клики можно было отфильтровать Поисковик мог вернуть средства на баланс вторым проходом антифрода - но этого не произошло. Что грязного я делал, если первый клик был не учтен?: менял ГЕО IP — прямо страну (РФ,KZ), и кликал примерно в тот же момент. менял ГЕО IP — прямо страну (РФ,KZ), и кликал примерно в тот же момент. менял статичные браузерные параметры — чтобы разрешение экрана отличалось от моего оригинального, шрифты, и т. п. менял статичные браузерные параметры — чтобы разрешение экрана отличалось от моего оригинального, шрифты, и т. п. В результате некоторыеклики были учтены со второй или третьей попытки. Смена статических браузерных параметров + разнообразные ГЕО IP, + новое ранее нигде не засвеченное устройство —вот три фактора почему клики этих пользователей нельзя было учитывать. т. е. офлайн антифрода просто нет. Как могут работать клик-фермы, наблюдаемые в РСЯ? Мне видятся только 2 значимых фактора, которые повлияли на учитываемость кликов: качество связки имя+маил+телефон- они должны быть доверительными (как минимум, попасть ранее в Яндекс Метрику) качество связки имя+маил+телефон- они должны быть доверительными (как минимум, попасть ранее в Яндекс Метрику) качество IP- некоторые клики были учтены за счет простой смены покупного прокси-IP на \"чистый\" IP моего компьютера или мобильного телефона. качество IP- некоторые клики были учтены за счет простой смены покупного прокси-IP на \"чистый\" IP моего компьютера или мобильного телефона. Клик фермы могут: Купить слитую базу пользователейкакого либо крупного интернет магазина, МФО и т.п. Вот естьтелеграм-каналв котором отслеживаются утечки данных. Купить слитую базу пользователейкакого либо крупного интернет магазина, МФО и т.п. Вот естьтелеграм-каналв котором отслеживаются утечки данных. Возможно, доверительных пользователей можно «нагулять» — выходишь в интернет как бы с разных устройств, спамишь в формы — если грамотно сделать — возможно так тоже заработает. Возможно, доверительных пользователей можно «нагулять» — выходишь в интернет как бы с разных устройств, спамишь в формы — если грамотно сделать — возможно так тоже заработает. Использовать собственные прокси‑IP,не покупные(!) — через покупные прокси ходят все подряд — там очень грязные диапазоны. Нужно купить свою мобильную прокси ферму. Прокси ферма — это множество мобильных модемов со своими сим картами. Стоит 50000–100000р, вгуглесразу есть несколько предложений для покупки. Использовать собственные прокси‑IP,не покупные(!) — через покупные прокси ходят все подряд — там очень грязные диапазоны. Нужно купить свою мобильную прокси ферму. Прокси ферма — это множество мобильных модемов со своими сим картами. Стоит 50000–100000р, вгуглесразу есть несколько предложений для покупки. В качестве софта можно использовать как мультиаккаунтовые браузеры с управлением по АПИ, так и фреймворки браузерной автоматизации типа BAS. т.е. клики могут быть как программные (БОТЫ), так и ручные (мотивированный клик). В качестве софта можно использовать как мультиаккаунтовые браузеры с управлением по АПИ, так и фреймворки браузерной автоматизации типа BAS. т.е. клики могут быть как программные (БОТЫ), так и ручные (мотивированный клик). 6 ДЫР, которые нужно закрыть Учет кликов от новых, нигде не засвеченных ранее, устройств. Учет кликов от новых, нигде не засвеченных ранее, устройств. Нечувствительность к изменению (подмене) браузерных параметров. Нечувствительность к изменению (подмене) браузерных параметров. Отсутствие двухпроходного антифрода или его не чувствительность. Было 3 причины отклонить мои клики именно на 2-м проходе и вернуть деньги на баланс. Отсутствие двухпроходного антифрода или его не чувствительность. Было 3 причины отклонить мои клики именно на 2-м проходе и вернуть деньги на баланс. Отсутствие анализа поведения пользователей в разрезе одного сайта. Все клики с новых устройств — это статистически не вероятное событие. Отсутствие анализа поведения пользователей в разрезе одного сайта. Все клики с новых устройств — это статистически не вероятное событие. Стимулирование спроса на торговлю цифровыми следами интернет пользователей. Стимулирование спроса на торговлю цифровыми следами интернет пользователей. Стимулирование СПАМа. Стимулирование СПАМа. 7 ДЫРИЩА — случайные клики НЕТ возможности ЗАПРЕТИТЬ пихать мою рекламу на кликбейт места. Даже если в кампании Яндекс Директ чистый трафик — нет ботов и мотива, то эти мусорные клики всё портят. Мне НЕ надо чтобы моя реклама: всплывала на весь экран всплывала на весь экран выплывала при скролинге выплывала при скролинге всплывала блоками поверх контента всплывала блоками поверх контента была бы засунута в любое другое место где возможен случайный клик была бы засунута в любое другое место где возможен случайный клик Т.е. сам рекламный подход ориентированна нагон кликов без оглядки на то, целевые они или нети антифрод здесь не причем. Из достоинств Общая схема, чтобы клик был учтен Яндекс Директ не элементарна, но те, кото этим занимаются они или сами администрируют свой софт или инструктируют как именно это делать.НО, кейса учета кликов с одного рабочего места быть не должно. НЕ грамотное обоснование почему клики реальные Ниже пример ответа на запрос проверки трафика на роботность: Медленный мобильный интернет?В Томске он быстрый, да и в Москве тоже. Как нужно обосновывать, что клики реальные. Пример: Мы проверили ваш трафик — все ваши 10 000 кликов реальные и вот почему: С устройств с которых был клик: 5% пользователей имеют активную подписку Яндекс Плюс 5% пользователей имеют активную подписку Яндекс Плюс 15% пользователей используют Яндекс Такси 15% пользователей используют Яндекс Такси 30% осуществляли оплату в интернет 30% осуществляли оплату в интернет 35% пользователей проходили двухфакторную аутентификацию 35% пользователей проходили двухфакторную аутентификацию 80% активны в интернет больше года 80% активны в интернет больше года 10% кликов были с кликбейт мест 10% кликов были с кликбейт мест Вот, если бы я почитал такое обоснование — то да, есть весомые аргументы для признания трафика реальным. А предоставление ссылки на стандартный отчет, что эти 20% кликов были сделаны с Windows, а 80% c iOS — это не серьезно. Кейс по снижению стоимость клика в РСЯ в 2 раза Это последний пост в моем телеграмм канале. Повторить можно для любой кампании в которой есть боты. А предпоследнийпост о том, как «подвесить» бота.Переходите в мойтелеграм канали изучайте этот интересный материал. "
  },
  {
    "article_id": "https://habr.com/ru/articles/869452/",
    "title": "Их будут искать с особым пристрастием: 7 документов, которые должны быть на сайте каждой российской компании",
    "category": "Веб-разработка",
    "tags": "сайт, сайт компании, документы, бизнес, штрафы, закон, роскомнадзор, оферта, персональные данные, cookies",
    "text": "Сайт – это лицо компании. Даже если в вашем случае сайт — не площадка для продаж, а дань «тренду», оформлять его в соответствии с требованиями законодательства вы обязаны. Если прогуляться по подвалам онлайн-площадок, можно увидеть настоящие жемчужины околоправовой мысли. Один из таких променадов натолкнул меня на идею написать эту статью. Есть ощущение, что даже крупные игроки рынка не всегда понимают, какие документы обязательны для размещения на сайте. Я могу и хочу ответить на этот вопрос. В 2025 году на вашем сайте должно быть минимум семь правильно оформленных документов. И точка. За несогласие — штрафы. Какие документы нужны для сайта Прежде чем я расскажу про пакет документов для сайта, напомню, что все они должны быть общедоступны. Посетители и контрольные органы вправе получать информацию здесь и сейчас. Если им придется долго бродить по разделам сайта, у вас появятся проблемы. Имейте это в виду. Перечень документов для размещения на сайте: 1. Пользовательское соглашение В нем юристы прописывают правила пользования платформой, условия регистрации, порядок работы с личным кабинетом, фиксируют права и обязанности компании и посетителя сайта. Например, пользовательским соглашением может быть запрещено копирование контента с сайта. Отдельный раздел пользовательского соглашения выделяют под описаниеответственности сторон: здесь можно указать, что вы не возместите ущерб в случае технического сбоя на сервере. Этот документ на сайте компании убережет вас от ответственности за неполадки, к которым вы не имеете отношения. 2. Политика конфиденциальности или политика обработки персональных данных Это название одного и того же документа.Его содержание должно соответствовать требованиям закона «О персональных данных». Документ включает сведения о том, какие данные пользователей собираются, как применяются, какую их защиту вы обеспечиваете 3. Согласие на обработку персональных данных Его нельзя объединить с политикой конфиденциальности или политикой обработки персональных данных. Это разные документы: такими они и должны быть на сайте вашей компании. Политика конфиденциальности (политика обработки ПД) объясняет пользователям, как вы будете работать со сведениями о них. Документ вы размещаете для ознакомления, пользователь может быть с ним согласен или не согласен. Для подтверждения согласия на обработку ПД владелец сайта разрабатывает отдельныйнормативный документ для сайта. «Подписывая» его, пользователь указывает, что готов передать информацию о себе и знает, как ваш сайт будет с нею работать. 4. Согласие на обработку данных метрическими программами Такое согласие нужно, если вы используете, например, «Яндекс Метрику». Объединять этот документ с Согласием на обработку персональных данных не рекомендую. 5. Уведомление об использовании cookie, содержащее ссылка на политику обработки ПД и пользовательское соглашение У уведомлений о cookie есть две важные функции: они сразу предупреждают пользователя о сборе сведений, пока он не начал пользоваться сервисами сайта, и являются способом получения согласия пользователя на обработку персональных данных файлами cookie и метрическими программами. 6. Документ об использовании рекомендательных технологий Тех, что анализируют поведение пользователя и предлагают интересный ему контент. 7. Согласие на получение рекламной рассылки Еще один документ, подлежащий размещению на сайте. Он необходим, если сведения, которые собирает сайт, вы используете для рекламных рассылок. Рекламный контент – это любые уведомления о продаже товаров и услуг: СМС, пуш-уведомления, сообщения на электронную почту, в социальных сетях, мессенджерах и т.д. Напоминание о том, что товар доставлен, рекламной рассылкой не считается. В контексте согласия на получение рекламной рассылки не могу не сказать о чек-боксах для заполнения галочками. Споры о них велись весь 2024 и за несколько лет до него: помните? Если нет, поясняю суть. В отечественной «сайтовой культуре» сложилась традиция размещать блоки с текстом, наподобие: «Соглашаюсь на получение рекламной рассылки» на странице, где пользователю нужно выполнить важное действие – зарегистрироваться, подтвердить заказ либо что-то подобное. В эти слова или поблизости вмонтированы ссылки на тексты Политики конфиденциальности (обработки ПД) и Согласия на обработку ПД, а рядом стоят окошки, которые нужно заполнить галочками для продолжения пользования функционалом. Разумеется, осознают, что происходит, проходят по ссылкам и вчитываются в тексты не все пользователи. Многие расставляют галочки не задумываясь, на что именно они соглашаются. Некоторые владельцы сайтов практикуют размещение чек-боксов с предустановленными галочками. Здесь и не требуется никуда нажимать: все сделано за вас – остается только кликнуть по кнопке «Продолжить». Так поступать не нужно, чтобы не тратить деньги на выплату штрафов. Требования к выражению согласия на получение рекламной рассылки жесткие. Отделаться безликими галочками нельзя: подтверждать намерение видеть рекламный контент нужно через отправку СМС или другим интерактивным со стороны пользователя образом. Такая система, например, работает на сайте DNS. Чтобы получать рекламу, нужно найти письмо от DNS на почте и намеренно нажать на кнопку«Подтвердить». Документы на продающий веб-сайт Реквизиты Пропишите наименование компании, адрес и место нахождения, адрес электронной почты и (или) номер телефона, местоположение главного офиса, ОГРН, ИНН, если продаете через сайт. Для реквизитов выделите отдельную страницу либо место в подвале сайта. Публичная оферта Это документ, который должен быть на сайте, если вы продаете с него товары или услуги всем желающим.Здесь я объяснила, как составить его самостоятельно. Рекомендую освежить в памяти, поскольку некорректные формулировки нередко становятся причиной конфликтов с клиентами и вызывают повышенный интерес контролирующих органов к бизнесу, а этого в наступающем 2025 году, я уверена, вы не хотите. Это базовый набор необходимых документов для сайта. Разумеется, по своему усмотрению вы можете дополнить его. Правовых нюансов и юридических тонкостей достаточно: если сомневаетесь в правильности составления документа, тообращайтесь за помощью. Это проще и дешевле, чем платить штрафы, а затем пытаться все исправить самостоятельно. И снова платить штрафы. Останавливаться подробно на размерах штрафов не буду, но тем, кто не верит, что их выписывают, напомню: удостоиться интереса контролирующих органов со всеми вытекающими проще, чем кажется. Такой участи, например, не избежала автономная некоммерческая организация \"Поддержка связей с еврейской диаспорой \"Еврейское Агентство «Сохнут»\"\". На своем сайте она собирала персональные данные пользователей, но не разместила документ, определяющий политику их обработки. Выяснилось также, что «Сохнут» хранит собранные сведения с использованием баз данных, находящихся за пределами РФ. Итог: штраф 500 000 рублей. Номер дела: N 16-1661/2023. Нарушения выявились в ходе прокурорского надзора, а под него ежегодно попадает много организаций. Проверкой сайтов также занимается Роскомнадзор: как я говорила выше, ему судьба персональных данных россиян особенно небезразлична. Соблюдайте законодательство, и вашему бизнесу будет счастье. Следите за тем, что происходит на ваших сайтах, чтобы сберечь нервы и бюджет. В 2025 они точно пригодятся. "
  },
  {
    "article_id": "https://habr.com/ru/articles/868794/",
    "title": "JavaScript: Удобство или Угроза? Размышления о Приватности и Вебе",
    "category": "Веб-разработка",
    "tags": "Анонимность, приватность, безопасность, деанонимизация",
    "text": "Интернет проник во все сферы жизни, вклад этой технологии в прогресс невозможно переоценить. Интернет-браузеры (Chrome, Firefox, Safari, Opera и т.д.) занимают топ среди инструментов \"использования интернета\", а сайты, которые посещают через эти браузеры — самый распространенный способ для обмена информацией. Читатель, вероятно, знаком с основными технологиями для создания сайтов: HTML, CSS и JavaScript (он же — ECMAScript). Первые две технологии — статичный текст со специальным синтаксисом, который браузер использует для формирования визуальной части сайта. JavaScript, в свою очередь, является полноценным языком программирования, который оживляет сайты, превращая их в веб-приложения. Код JavaScript чаще всего приходит с сервера в виде текста, как и HTML/CSS, но это не статичный макет страницы, а код, который исполняется интерпретатором, встроенным в браузер. Сайт с использованием JS может быть чем угодно: от полноценного мессенджера и приложения для видеоконференций до онлайн-шутера с 3D-графикой. Браузеры исполняют код JavaScript в изолированной среде с очень ограниченным доступом, поэтому сайты не имеют доступа, например, к терминалу операционной системы, файловой системе и другим приложениям. Несмотря на прогрессивность, в кругу людей, которые заботятся о своей приватности и что-то смыслят в технологиях, JS пользуется дурной славой. Как правило, его отключают через специальные браузерные расширения и негодуют на онлайн-ресурсы, которые вынуждают включать JavaScript для их использования. Сложно переоценить пользу JS, который делает сайты интерактивными, удобными и современными. В то же время опасность, которая в нем таится, обычно ускользает от внимания. Эта статья — попытка заглянуть монстру в глаза, попутно открывая глаза читателя. HTTP Сайты и все, что с ними связано—не конь в вакууме. Поэтому кратко опишу каким образом сайты открываются у нас в браузере. Знающей публике просьба перепрыгнуть на следующий заголовок. Для связи с сервером браузеры используют HTTP — протокол передачи текста со ссылочками и кнопочками, т.е. передачи документов в формате HTML, красоту которому придает CSS (цвета, рамки, отступы между элементами и прочее). Сообщения HTTP состоят из двух частей: заголовков и тела. В заголовках передаются служебные данные, а в теле — полезная нагрузка вроде файлов сайта для отображения в браузере или картинку, которую пользователя загружает на сайт со своего устройства. При запросе на сервер, заголовки сообщают базовую информацию о клиенте: предпочитаемый язык, версию браузера, версию протокола HTTP и прочее, что сервер использует для отображения пользователю версии сайта, которая подойдет ему лучше всего. Например, будет содержать текст на родном языке. HTTP поддерживает сохранение состояния через куки (cookie). Это специальный заголовок, который браузер принимает от сервера, сохраняет его данные, и затем подставляет их в свой каждый запрос. Это обеспечивает простую реализацию авторизации и прочего, когда сервер при получении запроса имеет информацию о том, что именно сейчас пользователь должен увидеть и есть ли у него для этого права доступа. Также куки могут быть использованы без очевидной для пользователя причины: для идентификации случайного гостя сайта, чтобы в дальнейшем работать с ним как с уже известным, закрепляя за ним историю посещенных страниц и выдавая персонализированный контент. В общем, куки могут использоваться не только в кристалльно-понятных и полезных для юзера целях, но в целом механизм необходимый и вполне понятный. В цивилизованном мире стало нормальной практикой хотя бы предупреждать о использовании куков, а факт их использования легко увидеть через инструменты разработчика. HTTP разрабатывался до появления JavaScript и позволяет делать весьма сложные и функциональные сайты, но для обновления содержимого на странице такого сайта, страница должна быть полностью обновлена, например, по клику на кнопку. Пока этого не случится, страница неизменна с момента ее получения от сервера. О современном чате, который работает в реальном времени, или всплывающих уведомлениях, а также о многом другом, что современный пользователь привык видеть, без JS говорить не приходится. JavaScript предоставляет разработчикам возможность создавать динамическое поведение на страницах, взаимодействовать с пользователем и изменять содержимое в реальном времени. Это делает его мощным инструментом, но также и источником потенциальных угроз. Поскольку JS-код это всегда \"кот в мешке\", существует риск выполнения вредоносного кода, который может украсть данные пользователя, отслеживать его действия и что еще похуже. Слежка обыкновенная Помимо простых куков, JavaScript предлагает такие механизмы, как LocalStorage, SessionStorage и IndexedDB, которые позволяют хранить данные в браузере. LocalStorage и SessionStorage схожи, отличие только в том, что LocalStorage сохраняет информацию на неограниченный срок, а SessionStorage очищается при закрытии вкладки. Оба метода при грамотном использовании улучшают общее впечатление от сайта, который запоминает выбранную тему оформления, товары в корзине и т.п. Третий тип хранилища отличается. IndexedDB — это уже не баловство с мелкими файликами, а база данных, которая позволяет упорядоченно хранить большие объемы информации, например, для возможности полноценного использования веб-приложения в режиме оффлайн, а также для явного кеширования большого объема данных, которые редко изменяются на сервере. Шизофренией будет сказать, что все это придумано злодеями. Конечно нет, упомянутые технологии делают интернет таким, каким его любят миллионы людей. Но есть нюанс: пользователь, который понимает, что такое куки, которые открыто передаются в хедерах (заголовках) при каждом запросе, может не иметь представления о каком-то еще хранилище данных, которые записываются и читаются кодом JS. Следовательно, использовать эти данные и передавать их на сервер, JS-приложение, исполняемое в браузере, может более скрытно, неочевидно. Чтобы веб-разработка не превратилась в очевидный бандитизм, существуют ограничения на то, чтобы использовать хранилища, включая куки, могли только те сайты, которые их установили. То есть хранилища в браузере работают с привязкой к домену сайта (есть Third-Party Cookies, но погоду это не делает). Таким образом, случайный визит на сайт Пети и Васи не может привести к краже токена авторизации от Хабра. Познания в вопросах приватности при веб-сёрфинге на этом заканчиваются у 90% респондентов моего вымышленного опроса: историю надо чистить, куки удалять, хранилища освобождать. Все это на виду в настройках браузера. Когда появилась уверенность в полном понимании куков, самое время начать говорить проEverCookie— вечные и никак не удаляемые. Однако, грубо говоря, это просто специальный подход к комплексному использованию обычных куков и хранилищ, чтобы затруднить пользователю попытки их полного удаления. Сделаем вид, что в целом механизм куков обсудили и как ни крути — победить куки возможно. Истоки недоверия Кому-то может показаться, что эта статья и подобные материалы — глупая истерия фанатиков, которые либо занимаются криминалом, либо старадают паранойей. Чтобы контекст недоверия к JavaScript был более понятным, надо перечислить векторы злоупотребления. Технологии слежки в веб-разработке обеспечивают фантастическую точность психологического портрета пользователя. Благодаря этому контекстная реклама в социальных сетях и на рандомных сайтах показывает не что-то случайное, а нечто очень интересное, на что пользователь наверняка кликнет. Очевидные участники процесса — бизнес рекламодателей и рекламных сетей, где они размещаются. Отсюда и бурное развитие технологий обеспечения такой слежки. Наивно полагать, что слежка за пользователем ограничивается экосистемой сервисов одного владельца вроде Google, Yandex и прочих. Самый посредственный сайт из провинции подключает внешнюю аналитику, чтобы наблюдать за посещаемостью, заодно сливая хозяевам мониторинга информацию о своих пользователях, которые загружают вредоносный JS-код, открывая страницу. Хозяевами таких мониторингов, как правило, являются те самые гигантские корпорации, обещающие, что их отслеживающие скрипты на странице стороннего сайта помогают индексировать его содержимое для выдачи при поисковых запросах — заманчиво! Помимо отслеживания пользователей, на которое явно соглашается веб-мастер, обслуживающий сайт, существуют другие уловки. Например, внешний CDN. Просто говоря, это сервисы, которые дают возможность загружать JS-библиотеки, наборы готовых стилей CSS или красивые шрифты с их серверов. Почему-то у разработчиков не принято скачивать эти файлы к себе, а затем явно добавлять их в проект, загружая на свой сервер. Делается иначе: в страницу своего сайта они вставляют лишь ссылку на нужный файл на чужом сервере. В таком случае при загрузке страницы условно моего сайта, пользовательский браузер увидит, что какой-то файл надо скачать с другого адреса и обратится к нему. Если ранее сервер с ресурсами встречался на других сайтах и успел выдать вам идентификтор, браузер автоматически подставит его в запрос. Хитрый CDN отдаст файлы, а также запишет информацию с какого сайта к нему пришел пользователь (заголовок Referrer) и сделает все возможные выводы о его поведении. В конце концов любой сайт может подключить отслеживание своих пользователей с полным осознанием — на основании специальной партнерской программы. Собранные данные о пользователях продаются абсолютно бесконтрольно, проходя десятки и сотни рук больших корпораций и незначительных компаний. В лицензионном соглашении популярных социальных сетей без прикрас описывается факт наличия передачи информации о пользователях третьим лица: безымянным партнерам, партнерам партнеров и далее по цепочке, пока сама площадка случайно не купит свою же информацию о вас через сотню рукопожатий. Это конечно шутка, однако мировые социальные сети многократно были замечены в скандалах, когда информации о пользователях собиралось больше, чем они могли подумать, и передавалась она куда-то, куда вроде бы была не должна. Не бэкдор, а фича Теперь самое интересное: технологии, которые при использовании не по прямому назначению могут быть сравнимы с тяжелой артиллерией по личной жизни и интересам пользователя. Fingerprint (отпечаток) — это общее название для любого уникального идентификатора пользователя. В отличие от традиционных методов отслеживания, таких как куки, фингерпринтинг не может быть легко заблокирован пользователями, а его создание может быть адаптированным под возможные изменения источников. Это создает дополнительные сложности для тех, кто хочет сохранить свою анонимность в сети. Удалили куки в один клик, но сайт при входе здоровается с вами по имени? Поздравляю, это фингерпринт. Откуда он берется узнаете дальше. Фингерпринты WebGL(Web Graphics Library) — это мощный инструмент для рендеринга 2D и 3D графики в веб-браузерах. Самое первое, что приходит на ум — майнинг условной криптовлюты в браузере, но более практичное нецелевое использование — фингерпринтинг. WebGL дает обширный набор параметров для их уникальной привязки к пользователю: аппаратное оснащение для параллельных вычислений (видеокарты), разрешение экрана(-ов) и прочие более специфичные аспекты. Canvas— это HTML-элемент, который позволяет динамически рисовать графику с помощью JavaScript. Он предоставляет мощный инструмент для создания 2D и 3D графики, а также для работы с изображениями, анимацией и интерактивными приложениями. Canvas стал важной частью веб-разработки, особенно в контексте игр, визуализаций данных и других графически насыщенных приложений. Разные устройства и браузеры могут обрабатывать графику по-разному, что приводит к различиям в получаемых данных. Например, если на одном устройстве используется определенный шрифт, а на другом — другой, это может создать уникальный отпечаток, который можно использовать для идентификации пользователя. Отпечаток шрифтов— это метод, который использует информацию о шрифтах, установленных на устройстве пользователя, для создания уникального профиля. Разные устройства могут иметь разные наборы шрифтов, и это может быть использовано для идентификации пользователя. Это может быть сделано с помощью создания текстовых элементов с различными шрифтами и измерения их ширины или высоты. Если браузер отображает текст с определенным шрифтом, это может быть использовано для определения, установлен ли этот шрифт на устройстве. Сравнение размеров текста с различными шрифтами позволяет определить, какие шрифты доступны, и создать уникальный отпечаток. Отпечаток расширений— информация о расширениях и плагинах, установленных в браузере пользователя. Каждое расширение может добавлять свои уникальные функции и изменять поведение браузера, что делает его частью уникального профиля пользователя. Веб-сайты могут использовать JavaScript для проверки наличия определенных расширений. Например, они могут пытаться получить доступ к объектам, которые создаются расширениями, или использовать специфические методы, которые доступны только при установке определенных плагинов. Audio API— одна из многих полезных штук браузеров, которая в конкретном случае отвечает за воспроизведение звуков. Веб-приложение может генерировать звук и собирать данные о различных характеристиках вывода, таких как: частота дискретизации, задержки и анализ частот (искажения или изменения в амплитуде). Фингерпринтинг часто комбинирует в себе несколько уникальных значений в одно. Это позволяет создать более точный и уникальный отпечаток конкретного браузера, который может не меняться годами несмотря на очистку истории и прочих успокоительных мер. Приведенный здесь список источников для фингерпринта не полный. WebRTC WebRTC (Web Real-Time Communication) — мощная технология, разработанная для обеспечения прямой передачи аудио, видео и данных между браузерами без необходимости в промежуточных серверах. Она особенно привлекательна для создания сервисов видеозвонков, онлайн-игр и других интерактивных приложений. Несмотря на свои достоинства, WebRTC представляет прямую угрозу для конфиденциальности пользователей. Одним из основных рисков является возможность утечки IP-адресов. Когда пользователь подключается к WebRTC-приложению, его реальный IP-адрес может быть раскрыт, даже если он использует VPN или прокси-сервер. Это происходит потому, что WebRTC устанавливает прямое соединение между участниками соединения, что может привести к утечке информации о местоположении пользователя. Технически говоря, WebRTC работает по UDP. Запросы этого типа вовсе не отображаются во вкладке сети в инструментах разработчика. В ряде сценариев, они минуют настройки прокси и в лучших традициях UDP такие запросы разлетаются со всех сетевых интерфейсов устройства, имея потенциал даже для обхода VPN-подключения. Никаких внешних признаков — идеальный деанон. Простой юзкейс: пользователь настроил в браузере HTTP-прокси, чтобы ходить на сайты с подмененным IP-адресом. Он заходит на сайт, который инициирует соединение по WebRTC и, если оно удалось, сервер тут же узнаёт реальный IP-адрес гостя. Этот прием часто используется антифрод-системами, чтобы определить пытается ли пользователь прятаться и где он на самом деле находится. Более фатально это выглядит в ситуации, когда речь идет о пользователе анонимной сети вроде I2P, который использует HTTP-прокси. При включенном JavaScript, любой сайт в анонимной сети может инициировать прямое соединение вызовом одного метода. Есть пара \"но\", но есть и успешные тесты. Аудио-куки Лично мне этот вектор деанонимизации кажется самым жутким по своей эффективности, простоте и неочевидности для зеваки-маминого-молодца. Веб-приложения могут использовать динамики для воспроизведения звуковых сигналов с частотой выше 20 кГц, которые не воспринимаются человеческим ухом, но которые могут быть улавливаемы другими устройствами, такими как смартфоны, планшеты и другие устройства с микрофонами в радиусе квартиры или кафе (горячий привет всем \"умным\" колонкам в вашем доме!). Прямых ссылок на факт того, что все голосовые помощники в смартфонах умеют слышать такие аудио-маячки и отправлять информацию о них куда следует у меня нет. Для большего погружения в тему просто рекомендую ознакомиться состатьейи посмотретьэто. Из личного опыта могу сказать, что на криптовалютных обменниках я неоднократно слышал писк, который пропадал после закрытия вкладки браузера. Как быть? В лес нам наверняка уже не уйти, в землянке не жить. Чтобы статья не превратилась в околофилософскую воду, дам краткий и во многом очевидный набор советов. Насколько возможно откажитесь от использования продукции корпораций — это самый большой гвоздь в крышку вашей приватности. В выборе между свободными технологиями и закрытыми, выбирайте свободные, даже если за это придется заплатить частью вашего комфорта: открытые протоколы для обмена сообщениями, свободные ОС для вашего компьютера и т.п. Настройте веб-браузер. Пусть это будет Firefox или его форк — никакой привязки к корпорации со своим поисковиком из коробки, как это сделано в Google Chrome и прочих хромиумных клонах. Смените поисковик на DuckDuckGo - это меньшее из зол; Смените поисковик на DuckDuckGo - это меньшее из зол; Установите плагины для отключения WebRTC, затруднения фингерпринтинга и блокировки рекламы. Также можете установить NoScript, который полностью отключает JS и для каждого домена его надо будет разрешать вручную. Установите плагины для отключения WebRTC, затруднения фингерпринтинга и блокировки рекламы. Также можете установить NoScript, который полностью отключает JS и для каждого домена его надо будет разрешать вручную. </> В триллерах и детективах часто фигурируют «жучки», «трекеры» и прочие схожие термины, цель и суть которых в слежке. Если в фильмах и книгах такое встречаетсяиногдаи граничит с применением силы, то в современном интернете слежка встречаетсяоченьчасто и зачастую красиво обставлена. Акцент на слове «очень» можно было бы отразить как «ООООЧЕНЬ», но и этого было бы мало. JavaScript: удобство или угроза? Простого ответа нет. Ясно лишь, что технология не безобидная и использовать ее надо осторожно, ответственно и с теоретической подготовкой. Приватность - право каждого пользователя, равно как и его личная ответственность. "
  },
  {
    "article_id": "https://habr.com/ru/articles/868358/",
    "title": "Переход на новую архитектуру проекта: как это влияет на надежность стриминга web-данных",
    "category": "Веб-разработка",
    "tags": "аналитика, kafka, веб-аналитика, мобильная аналитика, маркетинг, интернет-маркетинг, цифровая аналитика, bigdata",
    "text": "Предположим, что перед вашей командой стоит задача по поиску надежного стриминга web и app данных, который бы соответствовал требованиям службы безопасности, ожиданиям отделов маркетинга и аналитики, а также был бы полезен для управляющей команды. Не менее важно удобство и прозрачность работы стриминга, а внесение изменений в ожидаемый результат, желательно, без привлечения дополнительного ресурса аналитиков и разработчиков. Этот материал будет полезен проектам, которые: Выстраивают глубинную сквозную аналитику; Выстраивают глубинную сквозную аналитику; Рассматривают возможность интеграции аналитических решений; Рассматривают возможность интеграции аналитических решений; Выстраивают аналитическую инфраструктуру инхаус. Выстраивают аналитическую инфраструктуру инхаус. Чем поделимся? Новая архитектура и переезд на apache Kafka.Почему мы пошли в разработку новой архитектуры сервиса, и как это влияет на развитие наших решений; Новая архитектура и переезд на apache Kafka.Почему мы пошли в разработку новой архитектуры сервиса, и как это влияет на развитие наших решений; Сравним прошлую и новую версии сервисов; Сравним прошлую и новую версии сервисов; Система мониторинга и алертинга инцидентов. Как мы выстроили систему уведомлений и быстрого реагирования на них, и почему на это важно обращать внимание при выборе стриминга; Система мониторинга и алертинга инцидентов. Как мы выстроили систему уведомлений и быстрого реагирования на них, и почему на это важно обращать внимание при выборе стриминга; Как мы работаем с кейсами потери данных. Как мы работаем с кейсами потери данных. Про DataGo! Команда DataGo! (ex. OWOX в РФ) предоставляет прозрачные и удобные инструменты для построения маркетингового DWH. Наша платформа помогает аналитикам получать качественные данные, а маркетологам — строить прикладную отчетность. Как мы обеспечиваем надежность маркетинговых и продуктовых данных? Данные являются бесценным активом для большинства компаний. На их основе компании принимают стратегические и управленческие решения, исследуют поведение и предпочтения целевой аудитории, разрабатывают маркетинговую и продуктовую стратегию. Надежность данных, которые попадут в маркетинговые и продуктовые отчеты, — важнейший аспект для бизнеса. Что такое надежные данные? Надежные данные — это объективные и своевременные данные, соответствующие целям и задачам бизнеса, переданные без ошибок и ограничений. Если ваш бизнес принимает решения, основанные на данных, то вы должны быть уверены в их надежности, полноте и актуальности. С чего все началось? Предпосылки В портфеле DataGo! более 80 крупных клиентов из e-com, retail, finance и других сегментов. Это создает высокую нагрузку на нашу внутреннюю инфраструктуру: почти на каждом клиентском проекте используется мобильное приложение и сайт, на которые поступает трафик из нескольких источников (таргетированная и контекстная реклама, СРА-площадки, баннерная и медийная реклама и др.) В начале 2024 года мы пришли к выводу, что при повышении нагрузки на наш сервис, возникнет риск невозможности принимать весь объем входящих данных. Это значит, что новые клиенты, подключенные к стримингу, и текущие будут рисковать частью своих данных и могут потерять их безвозвратно. Мы просто не смогли бы принимать входящие запросы и обрабатывать необходимый объем информации. С целью защиты данных текущих клиентов и обеспечения качественного сервиса при подключении новых проектов, мы приняли решение о переезде на новую архитектуру аналитического проекта apache Kafka. Как было? Старая архитектура Компания DataGo! была основана в 2022 году. Создание DataGo! стало ответной реакцией на уход зарубежных аналитических инструментов из РФ: многие проекты вынужденно мигрировали на альтернативный аналитический стек, спасая свои данные от высокого риска безвозвратной потери. Устройство старого стриминга: Все сервисы необходимые для работы стриминга разворачивались на каждой виртуальной машине, входящей в состав инфраструктуры приложения, создавая монолитную структуру; Все сервисы необходимые для работы стриминга разворачивались на каждой виртуальной машине, входящей в состав инфраструктуры приложения, создавая монолитную структуру; На каждую ВМ поступали сырые данные; На каждую ВМ поступали сырые данные; Запросы проходили через все модули стриминга в рамках одной ВМ без сохранения их в сыром виде и промежуточных результатов обработки, что создавало риск потери данных на любом этапе обработки. Запросы проходили через все модули стриминга в рамках одной ВМ без сохранения их в сыром виде и промежуточных результатов обработки, что создавало риск потери данных на любом этапе обработки. Почему была реализована такая архитектура? Нехватка физического ресурса на реализацию более сложной логики; Нехватка физического ресурса на реализацию более сложной логики; Сжатые сроки для реализации: аналитическим проектам необходимо было решение, позволяющее быстро мигрировать на альтернативный стек. Сжатые сроки для реализации: аналитическим проектам необходимо было решение, позволяющее быстро мигрировать на альтернативный стек. Сложности поддержки старой архитектуры: Масштабирование. Каждая новая ВМ стриминга увеличивала нагрузку на ClickHouse - формировались мелкие батчи, приводящие к высокому rps; Масштабирование. Каждая новая ВМ стриминга увеличивала нагрузку на ClickHouse - формировались мелкие батчи, приводящие к высокому rps; Релиз. Обновление требовало разворачивать новую версию сервиса на всех машинах, вне зависимости от масштаба изменений; Релиз. Обновление требовало разворачивать новую версию сервиса на всех машинах, вне зависимости от масштаба изменений; Надежность. Любая ошибка в процессе обработки запроса могла привести к его потере, что создавало риск потери данных. Надежность. Любая ошибка в процессе обработки запроса могла привести к его потере, что создавало риск потери данных. Эти ограничения потребовали радикального пересмотра подходов к обработке входящего потока Переход на новую архитектуру Устройство нового стриминга Архитектура проекта была разбита на несколько микросервисов, выстроенных в единую цепочку, где каждое звено выполняет небольшую часть работы: Прием входящих запросов и их сохранение в очередь на обработку; Прием входящих запросов и их сохранение в очередь на обработку; Разбор хитов и извлечение значений из параметров; Разбор хитов и извлечение значений из параметров; Обогащение данных; Обогащение данных; Трансформация в конечную структуру; Трансформация в конечную структуру; Загрузка в БД. Загрузка в БД. Изолированные процессы: Повышение стабильности системы; Повышение стабильности системы; Возможность внесения правок в отдельные компоненты системы; Возможность внесения правок в отдельные компоненты системы; Упрощение локализации ошибок при обработке потока данных. Упрощение локализации ошибок при обработке потока данных. Переход на новую архитектуру обеспечил DataGo! решение сразу нескольких кейсов: Все данные сохраняются в “сыром” виде.Стриминг это не изолированный процесс, на результат напрямую влияет полнота и корректность входящих запросов. Наличие “сырых” данных помогает определять причины и источник ошибок при их возникновении, а также значительно ускоряет процесс их исправления; Все данные сохраняются в “сыром” виде.Стриминг это не изолированный процесс, на результат напрямую влияет полнота и корректность входящих запросов. Наличие “сырых” данных помогает определять причины и источник ошибок при их возникновении, а также значительно ускоряет процесс их исправления; Исключена обработка данных до их сохранения.Это практически исключает вероятность того, что данные от клиента не дойдут или не сохранятся из-за программных ошибок; Исключена обработка данных до их сохранения.Это практически исключает вероятность того, что данные от клиента не дойдут или не сохранятся из-за программных ошибок; Подключено резервное хранилище S3 при сохранении входящего потока.Получение и обработка входящих запросов наиболее критичный слой сервиса, для повышения стабильности этого процесса, помимо достаточно стабильной Kafka, используется в качестве резерва S3 от YC. Хранилище данных S3 имеет динамически расширяемый условно безграничный ресурс, репликацию и высокую пропускную способность сети, что решает потенциальные сложности с “перегрузкой” Кафки или ее недоступностью по различным причинами Подключено резервное хранилище S3 при сохранении входящего потока.Получение и обработка входящих запросов наиболее критичный слой сервиса, для повышения стабильности этого процесса, помимо достаточно стабильной Kafka, используется в качестве резерва S3 от YC. Хранилище данных S3 имеет динамически расширяемый условно безграничный ресурс, репликацию и высокую пропускную способность сети, что решает потенциальные сложности с “перегрузкой” Кафки или ее недоступностью по различным причинами Важно отметить! Мы часто сталкиваемся с кейсами, когда у клиентов возникают сложности с текущим хранилищем, и по какой-то причине данные до него не могут быть доставлены. В таких случаях наша система сохраняет всю хитовую информацию до тех пор, пока хранилище клиента снова не станет доступным. Это гарантирует, что данные клиента не потеряются и дойдут в полном объеме. Система мониторинга и алертинга инцидентов В случае возникновения сложностей с [некорректной] доставкой данных до клиента, мы оперативно реагируем для выяснения причины возникшей ситуации и стараемся ее исправить. Например, сложности могут возникнуть из-за увеличения нагрузки на сервис, из-за проблем у хостинг провайдера и пр. Для чего мы используем мониторинг инцидентов? Обнаружение проблем.Мониторинг позволяет быстро выявить проблему и уведомить команду о необходимости вмешаться в процесс, минимизируя риски потери данных; Обнаружение проблем.Мониторинг позволяет быстро выявить проблему и уведомить команду о необходимости вмешаться в процесс, минимизируя риски потери данных; Оптимизация производительности.С помощью мониторинга инцидентов можно анализировать производительность систем и находить bottle necks: нагрузка процессоров, использование памяти и других параметров; Оптимизация производительности.С помощью мониторинга инцидентов можно анализировать производительность систем и находить bottle necks: нагрузка процессоров, использование памяти и других параметров; Прогнозирование.Мониторинг текущего состояния позволяет собирать данные для прогнозирования будущих нагрузок и планирования ресурсов; Прогнозирование.Мониторинг текущего состояния позволяет собирать данные для прогнозирования будущих нагрузок и планирования ресурсов; SLA.Мониторинг позволяет контролировать выполнение соглашений об уровне обслуживания и обеспечивать соответствие требованиям. SLA.Мониторинг позволяет контролировать выполнение соглашений об уровне обслуживания и обеспечивать соответствие требованиям. Мониторинг помогает в принятии обоснованных решений на основе данных.Например, если система работает стабильно, но производительность начинает снижаться, можно использовать данные мониторинга для определения причин и принятия мер по их устранению. Это позволяет не только поддерживать стабильную работу системы, но и улучшать ее производительность. Как мы замечаем, что с данными происходят аномалии? Мониторинг.Сформированные внутренние процессы позволяют отслеживать объем хитов и трафика по каждому клиенту. В текущей версии валидация расхождений происходит в полуавтоматическом режиме. Мониторинг.Сформированные внутренние процессы позволяют отслеживать объем хитов и трафика по каждому клиенту. В текущей версии валидация расхождений происходит в полуавтоматическом режиме. Настроили автоматический алертинг,позволяющий отслеживать стабильность работы всех ВМ, находящихся в архитектуре проекта. Алертинг настроен на список метрик, отражающих стабильность системы (базовые показатели ВМ: CPU, Memory usage, Free disk size, Network), а также метрики, учитывающие специфику продукта (RPS и его динамику, коды ответов, скорость обработки запросов и др.) Настроили автоматический алертинг,позволяющий отслеживать стабильность работы всех ВМ, находящихся в архитектуре проекта. Алертинг настроен на список метрик, отражающих стабильность системы (базовые показатели ВМ: CPU, Memory usage, Free disk size, Network), а также метрики, учитывающие специфику продукта (RPS и его динамику, коды ответов, скорость обработки запросов и др.) Данные не остаются без присмотра:ответственные дежурныеследят за работой всего процесса и, в случае необходимости, исправляют возникающие инциденты. Данные не остаются без присмотра:ответственные дежурныеследят за работой всего процесса и, в случае необходимости, исправляют возникающие инциденты. Своевременный переход на новую архитектуру позволил нам не только снизить риски потери данных и повысить контроль над возникновением ошибок, но и способствовал повышению надежности хранения и обработки данных для клиентских проектов. В компаниях, где управленческие и стратегические решения принимаются на основе данных, особое внимание уделяют как полноте этих данных, так и их качеству, своевременности, согласованности и другим аспектам. Осознавая важность в получении данных, соответствующих задачам бизнеса, мы постоянно совершенствуем и развиваем наши продукты "
  },
  {
    "article_id": "https://habr.com/ru/articles/866172/",
    "title": "17 запретов для бизнеса в соцсетях в 2025 году: объясняю, как не спустить выручку на штрафы",
    "category": "Веб-разработка",
    "tags": "Соцсети, контент, как вести блог, штрафы 2025, бизнес, telegram, telegram bot, вконтакте, бот, блогерство",
    "text": "«Зашли в соцсети, чтобы делиться экспертностью и привлекать новых клиентов, а по итогу погрязли в штрафах и разбирательствах...» – итоги  2024 для бизнеса, который очень-очень хотел, но не знал, как работать в социальных сетях, не нарушая законов. Полагаю, десятки юрлиц могут поделиться такими же результатами уходящего года, и это грустно. В умелых руках социальные сети – хороший способ продвижения услуг и товаров. «Умелые руки» – это не про способность убедить руководителя выделить гигантские рекламные бюджеты. Периодически даже крупные игроки рынка просто сливают их, не получая конверсии. Моя формула «умелых рук»: ведение страниц без нарушений законодательства + немного креатива. По вопросам креатива не проконсультирую, а вот о том, как не попасть под штрафы и не получить пачки претензий в наступающем году расскажу, так как в юридических аспектах ведения социальных сетей разбираюсь. Что запрещено законом размещать в социальных сетях Сначала об общих правилах ведения соцсетей по российскому законодательству. Я знаю, что специфика презентации бизнеса на разных площадках различается, но есть моменты, которые важно учитывать, в какой бы форме вы ни рассказывали о своем проекте. 1. Не размещайте экстремистскую информацию А также информацию террористической направленности, тексты или картинки с  оскорблением чувств верующих, пропаганду нетрадиционных сексуальных отношений и пр. Не стоит постить или репостить на странице компании что-то, хотя бы косвенно намекающее на поддержание радикальных идей и взглядов. Даже если вам кажется, что высказывание – шутка, и вы уверены, что все поймут ее как надо. Ваша аудитория и государственные органы все же отличаются чувством юмора. Яркий пример: осенью в СМИ гремелановость о двух девушках, которые снимали себя на камеру на фоне «Крокуса»и говорили, что \"приехали в «Крокус Сити Холл» и сейчас будут устраивать теракт\". Факт того, что изначально они снимали это видео для своих собственных ресурсов, ничего не изменил — теперь в деталях дела разбирается Следственный комитет. Не допускайте публикаций, которые могут расценить как возбуждение ненависти, вражды, унижение достоинства человека или группы лиц по признакам пола, расы, национальности, языка, отношения к религии, принадлежности к какой-либо социальной группе. Какие штрафы предусмотрены законом: публичные призывы к осуществлению террористической деятельности и распространение материалов, оправдывающих терроризм или экстремистскую деятельность — до 1 000 000 рублей; публичные призывы к осуществлению террористической деятельности и распространение материалов, оправдывающих терроризм или экстремистскую деятельность — до 1 000 000 рублей; акты неуважения к обществу и оскорбление религиозных чувств верующих — до 300 000 рублей; акты неуважения к обществу и оскорбление религиозных чувств верующих — до 300 000 рублей; за пропаганду нетрадиционных сексуальных отношений и (или) предпочтений, смену пола либо отказ от деторождения — до 4 000 000 рублей; за пропаганду нетрадиционных сексуальных отношений и (или) предпочтений, смену пола либо отказ от деторождения — до 4 000 000 рублей; возбуждение ненависти либо вражды, унижение достоинства человека либо группы влечет штраф суммой до 500 000 рублей. возбуждение ненависти либо вражды, унижение достоинства человека либо группы влечет штраф суммой до 500 000 рублей. Один из резонансных примеров к первым двум пунктам —возбуждение уголовного дела в отношении жительницы Самары, снявшей циничное, противоречащее нормам морали и нравственности видео возле монумента «Родина-мать зовет!» в Волгограде. 2. Не демонстрируйте символику экстремистских организаций Законом не преследуется продвижение через запрещенные на территории РФ соцсети: если вы решили рассказывать о своем проекте в них, то помните о соблюдении законодательства России и не забывайте об этике ведения бизнеса в Интернете. Да, вы поняли все верно: речь о Instagram* и Facebook*. Вы можете размещать там контент, однако упоминать публикации из этих социальных сетей в разрешенных на территории РФ без специальной сноски нельзя. Сноска может выглядеть, например,  вот так: «*Принадлежат Meta Platforms Inc., деятельность которой запрещена на территории Российской Федерации по основаниям осуществления экстремистской деятельности» Ставить сноску нужно в каждом материале, где упоминаются эти ресурсы. Забудете один раз – заплатите 40 000 – 50 000 рублей за распространение сведений об экстремистской организации. Не используйте также символику запрещенных социальных сетей. Поставите значок Instagram* или Facebook*, чтобы не делать сносок – будете должны заплатить штраф до 100 000 рублей за демонстрацию символики экстремистских организаций с конфискацией предмета административного правонарушения. 3. Не дискредитируйте армию Посягательство на авторитет ВС РФ влечет штраф 300 000 – 500 000 рублей. Любые призывы помешать действиям военных тоже расцениваются как дискредитация. Пожалуйста, никаких шуточных, обличающих и обвиняющих роликов, подкастов, иллюстраций, коротких заметок и лонгридов. 4. Не публикуйте чужие материалы Нашли подходящую фотографию для текста и опубликовали на странице? Помните, что нарушение авторских прав «стоит» от 10 000 до 5 000 000 рублей. Сколько именно придется выплатить автору фотографии, решит суд. Если обращаетесь к сервисам стоковых фото, то внимательно читайте пользовательские соглашения. Где-то можно воспользоваться фото бесплатно, но обязательно определенным образом указать автора. Где-то нужно заплатить деньги за использование изображения. Если считаете, что найти материал, который вы позаимствовали в Сети без спроса, нереально, то разочарую. Реально. Но пусть лучше вы расстроитесь сейчас, чем в момент, когда станете должны выплатить правообладателю несколько сотен тысяч рублей, как, например, ООО «ФОРПОСТ 47». Компании пришлось платить ООО «Восьмая заповедь» 208 000 рублей за использование фото без разрешения в 2022 году (номер дела N А56-48179/2022). Про работу с нейросетями без нарушения законодательства я рассказывалаздесь. 5. Не публикуйте чужие фотографии Даже если вы получили согласие человека на размещение его фото, помните: у него есть право отозвать согласие в любой момент и потребовать удалить фотографию. Поэтому, если используете снимки, например, моделей, то с ними лучше заключать договор на услуги по позированию: в этом случае они не смогут запретить публиковать изображения. Согласие получать не нужно, если съемка велась в общественном месте, и если   конкретный человек не является основным объектом на фото. Если размещаете в соцсетях фото сотрудников, их имена, должности, то получите письменное согласие на обработку персональных данных, разрешенных для распространения по установленной Роскомнадзором форме. Если публикуете данные клиентов, например оставивших отзыв, то либо заблюрьте фото, имена и другие данные, либо попросите от клиентов согласие на распространение персональных данных. Полученное согласие выручит компанию во время проверки Роскомнадзором. Штрафы за распространение персональных данных (публикации на сайте и в социальных сетях) доходят до 700 000 рублей. Я рассказывала, когда и как можно публиковать отзывы,в этом материале. 6. Не порочьте честь, достоинство физлиц и репутацию компаний За это предусмотрена административная ответственность: штраф до 1 000 000 рублей. 7. Не призывайте принимать психотропные и наркотические вещества Иначе можете расстаться с суммой до 1 500 000 рублей. 8. Не оскорбляйте представителей власти Штраф за это нынче до 40 000 рублей. Простых людей тоже оскорблять не стоит, даже если не адресуете высказывание кому-то конкретно: удовольствия на 5 минут, а оформлять кредит на оплату штрафа до 700 000 рублей — день. Пример —история петербурженки: она, конечно, выплатила сумму поменьше, но и творческого подхода к формулированию мысли в переписке не применила. 9. Не рассказывайте публично о чужой личной жизни И не делитесь другой конфиденциальной информацией: штраф за «посплетничать» о личной или семейной тайне — до 200 000 рублей. 10. Не нарушайте коммерческую тайну Представьте ситуацию: вам хочется поделиться с аудиторией социальных сетей удачным кейсом, вы делаете это и подпадаете под штраф, предусмотренный в разделе договора о конфиденциальности. Рекомендую прописать условия о публикации тех или иных сведений о совместной работе в договоре. В крайнем случае, можно обратиться к клиенту после завершения проекта и попросить дать письменное согласие на размещение материалов в Сети. 11. Не материтесь За мат в социальных сетях тоже есть ответственность по закону — до 100 000 рублей. Особенно велики риски подпасть под максимальное значение, если нецензурная брань адресована конкретному человеку: в таком случае придется нести ответственность и за оскорбления (как петербурженке, о которой я писала выше). И да, варианты сглаживания мата «маскировкой», наподобие «ко*фе*а», «конфефефета», «кОООн8еtа», «konffett@@» и т.п., от штрафа не уберегут. У вас получилось распознать словоконфета? Выйдет и у контролирующих органов. 12. Маркируйте рекламу Даже если на аккаунт компании подписано 15 человек, и все они ее сотрудники. Маркировка – это обязанность, а не волеизъявление.Штраф за отсутствие маркировкиможет доходить до 500 000 рублей. На запрещенных на территории РФ и некрупных площадках маркировать рекламу тоже нужно, как и в популярных соцсетях. Не забывайте об опасности немаркированной саморекламы: штрафы здесь такие же. Самореклама отличается от рассказа о работе компании явным призывом воспользоваться какой-либо конкретной услугой, подробным списком ее преимуществ. Маркировать в качестве рекламы абсолютно все публикации «на всякий случай» я также не рекомендую, даже если бюджет компании позволяет. Как минимум потому, что тогда алгоритмы социальных сетей могут хуже продвигать такие посты, даже если они действительно интересные и полезные. 13. Не размещайте рекламу на площадках иноагентов Штраф за это может доходить до 500 000 рублей. 14. Не обрабатывайте персональные данные без согласия владельца В большей степени это касается сайтов, где потенциальные клиенты отправляют свою контактную информацию через формы сбора данных, но также обработка данных может осуществляться с помощью ботов или в личных сообщениях. Имейте в виду, что штраф за обработку персональных данных без согласия их владельца может составить 100 000 руб.  За распространение ПД придется заплатить до 700 000 рублей (писали выше). В целом надзор за защитой данных в соцсетях будет только усиливаться, поэтому рекомендую бытькрайне внимательными. Закрытые каналы в Telegram и не только Несколько слов о каналах, куда подписчики попадают по одобрению админов. Если вы решили вести закрытый канал для освещения деятельности в Telegram, ВКонтакте, Одноклассниках или где-то еще, рекомендую соблюдать требования законодательства так же, как при ведении открытого сообщества. Почему? Как минимум потому, что вы наверняка не знаете о мотивах своей аудитории.  Через 15 минут после одобрения заявки на вступление новоиспеченный подписчик может сообщить об увиденном и услышанном в компетентные органы или лицу, чьи права нарушены публикацией. Подтвердить противоправные действия можно даже если настройками вы запретили делать скриншоты. Всегда можно взять второй телефон и записать все, что нужно, с экрана первого телефона. Это не проблема для того, кто задался целью. И да,прецеденты есть: летом 2024 года пятнадцатый арбитражный апелляционный суд разбирал ситуацию с закрытым чатом Telegram. Здесь, конечно, не совсем про бизнес, но тоже интересно. В общем, женщина написала в закрытом чате Telegram, что она думает об управляющей компании. Управляющая компания обратилась в суд с иском о защите деловой репутации. Иск удовлетворили. Женщина посчитала, что размещение информации в закрытой группе Telegram не может быть расценено как порочащее деловую репутацию УК, но мнение суда оказалось другим. Вывод: нельзя публиковать непроверенные сведения даже в закрытых чатах (номер дела: N А32-33539/2023) Как пользоваться ботами Вконтакте и Telegram Если вам удобно принимать заявки и общаться с клиентами через ботов, то не забывайте в приветственном сообщении давать ссылку наПоложение об обработке персональных данныхиСогласие на обработку персональных данных. Укажите, что продолжение использования бота клиентом приравнивается к согласию со всеми описанными в этих документах условиями. Кстати, вот как я решила эту проблему дляботаLegal Up Что будет в 2025 году Легче и проще, чем сейчас, не будет – предупреждаю сразу. Законодательство продолжает дорабатываться и обновляться. Чтобы не подпадать под штрафы, читайте юридические новости, корректируйте контент-планы и удаляйте контент (даже старый), если он не соответствует текущей повестке. Из самого актуального: с 1 января 2025 года владельцы аккаунтов, каналов, групп, сообществ с числом подписчиков более 10 000 человек, не смогут размещать рекламу и просить донаты, если они не заявили о себе в Роскомнадзор. Сейчас в реестре социальных сетей РКН числятся Twitch, LiveJournal, Yappy, YouTube, Telegram, likee, Pinterest, Rutube, TikTok, Пикабу, Одноклассники, ВКонтакте, Дзен. Владельцы каналов с аудиторией более 10 000 в указанных соцсетях должны подать уведомление в РКН: в нем необходимо сообщить сведения о себе. Юридическим лицам-владельцам каналов также желательно это сделать, иначе их каналы могут быть заблокированы. Напомню, чтособлюдать законодательство РФследует абсолютно на всех существующих площадках, даже если их деятельность запрещена на территории Российской Федерации. Предостережение: с 1 января 2025 года не покупайте рекламу у блогеров , которые не уведомили Роскомнадзор о том, что число их подписчиков превысило 10 000 человек. В этом случае вы как рекламодатель нарушите законодательство «О рекламе»: штраф за размещение постов в аккаунтах соцсетей, которые не уведомили РКН о своем статусе – до 500 000 рублей. Узнать, зарегистрирован ли блогер в реестре РКН, можноздесь. Поздравляю! Вы дочитали до конца: теперь дело за малым – вести социальные сети, не нарушая законодательство. Желаю, чтобы у вас это получалось. Имейте в виду, что при продвижении бизнеса через соцсети нужно учитывать множество правовых нюансов: сомневаетесь в какой-либо детали – лучше уточните у юриста. Если нужна консультация о безопасности онлайн-бизнеса, то вы всегда можете написать вбот. ________ *Принадлежат Meta Platforms Inc., деятельность которой запрещена на территории Российской Федерации по основаниям осуществления экстремистской деятельности"
  },
  {
    "article_id": "https://habr.com/ru/articles/878638/",
    "title": "Как из каши импортов сделать сортированный список Frontend",
    "category": "Веб-разработка",
    "tags": "Frontend, react, nextjs, оптимизация, prettier, форматирование кода, веб-разработка, фронтенд",
    "text": "Всем привет! Меня зовут Владимир и работаю джуниор фронтенд разработчиком в одной из лучших компаний :-) Сегодня хочу вам рассказать, как можно немного причесать ваш проект чтобы он выглядел более читабельным.И так рецепт нашего блюда Конечно же наш Prettier Конечно же наш Prettier Редактор кода Редактор кода Плагин @trivago/prettier-plugin-sort-imports\": \"^5.2.2 Плагин @trivago/prettier-plugin-sort-imports\": \"^5.2.2 И конечно же ваш проект(в моем случае это будет NextJs) с файлом .prettierrc для конфига нашего Prettier форматера) И конечно же ваш проект(в моем случае это будет NextJs) с файлом .prettierrc для конфига нашего Prettier форматера) 15 минут вашего времени) 15 минут вашего времени) 1 - Для начала устанавливаем плагин в ваш редактор кода(в моем случае это vsCode). Если вы пишите в блокноте - значит дальше вам читать нет смысла. Далее открываем вкладку с расширениями и находим данный плагин Prettier - Code formatter. Устанавливаем 2 - Далее после установки надо зайти в конфиг данного плагина. Это можно сделать нажав на кнопку шестеренку при открытие плагина в магазине и затем выбрать Параметры.После ищем поле Config Path и добавляем название файла конфига, которые лежит в корне проекта. У меня он называется .prettierrc . 3 - затем устанавливаем в дев зависимости проекта наш пакет для сортировки и пишемnpm i prettier@trivago/prettier-plugin-sort-imports -D 4 - затем открываем наш файл .prettierrc и закидываем туда конфиги, которые вам нужны. Помимо сортировки импортов вы можете туда добавить добавление одинарных или двойных кавычек, отступы, перенос строк. Думаю вы знакомы с этим плагином, так как каждый день он помогает навести порядок в вашем проекте. 5 - начинаем прописывать туда конфиги. В проекте использую FSD архитектуру и импортирую по значимости для меня в работе (порядок вы всегда можете поменять). И теперь заходим в любой файл нашего проекта и нажимаем  cmd + s и получаем сортированные импорты. Спасибо всем, кто дочитал до конца! Буду очень рад, если посоветуете, как это можно улучшить с вашей точки зрения.По вопросам и предложениям работы можнописать мне в телеграм."
  },
  {
    "article_id": "https://habr.com/ru/articles/878628/",
    "title": "Разбираемся в конструкции советского резистора ПЭВ-7,5",
    "category": "Электроника",
    "tags": "резистор, резистор ПЭВ, радиодетали, советские радиодетали, электроника, схемотехника, радиоэлектроника, радиотехника",
    "text": "Всем привет. Друзья, сегодня мне бы хотелось обсудить с Вами конструкцию советского резистора ПЭВ-7,5. Внешний вид данного резистора представлен на рисунке ниже. Маркировка данного резистора расшифровывается следующим образом: П – проволочный, Э – эмалированный, В – влагостойкий. Значение 7,5 означает значение номинальной мощности, на которую рассчитан данный резистор. В данном случае номинал данного резистора составляет 390 Ом, отклонение фактического значения сопротивления от номинала составляет не более 5%. Винт с гайками и пластиковыми шайбами выступает только в качестве крепежной конструкции данного резистора внутри изделия и не влияет на параметры данного резистора. Если в крепеже нет потребности, его можно просто снять. Откручиваете гайки по бокам резистора и извлекаете винт из керамического корпуса резистора. Конструктивно данный резистор выполнен в виде трубчатого основания из керамики (высокопрочный ультрафарфор или талько-шамотная огнеупорная керамика), на которое намотана константановая проволока (для низкоомных моделей) или нихромовая проволока (для высокоомных моделей). Обратите внимание: данная проволока не изолированная. Сверху данная обмотка покрывается стекловидной теплостойкой эмалью зеленого цвета. Она очень прочная, когда готовил данную статью пытался соскрести ее ножом, из этого ничего не получилось. Сколоть небольшой кусочек удалось при помощи нескольких средней силы ударов молотком. Заметьте, при этом основание из керамики никак не пострадало, а лишь откололся кусочек краски с верхней части резистора. Самая конструкция резистора очень мощная и очень прочная, повредить его весьма и весьма не просто. К концам проволоки, находящейся под лаком, припаяны выводы выполненные из латуни. Выводы залужены. Данные выводы служат для подключения резистора к схеме устройства, в которой он будет использоваться. Номинал данного резистора определяется материалом использованной проволоки, ее сечением и количеством ее витков. Бонусом, вся эта информация в формате видео: На сегодня у меня все. Если у Вас остались вопросы — пишите их в комментариях, буду рад на них ответить."
  },
  {
    "article_id": "https://habr.com/ru/articles/878452/",
    "title": "Рассказываю про конструкцию советских резисторов МЛТ-2",
    "category": "Электроника",
    "tags": "резистор, электроника, схемотехника, радиотехника, радиодетали, резистор МЛТ-2, МЛТ",
    "text": "Всем привет. Сегодня мне бы хотелось поговорить с вами о советских радиодеталях. А конкретно мы поговорим сегодня о резисторах МЛТ-2. Как все вы знаете, резисторы в электрических цепях необходимы для решениях различных задач: ограничения тока, поглощения энергии и др.. Их основными характеристиками являются: значение их сопротивления и мощность, на которую они рассчитаны. Резисторы МЛТ-2 являются классическим образцом советского резистора и их можно встретить практически в любой технике того времени. Аббревиатура МЛТ расшифровывается как Металлопленочные Лакированные Теплостойкие. Цифра 2 означает мощность резистора в Ваттах, которую он может рассеять. То есть МЛТ-2 рассчитан на 2Вт. Также на корпусе резистора нанесена маркировка, указывающая на его номинал по сопротивлению. В целом маркировка резистора, представленного на фото, расшифровывается следующим образом: Металлопленочный лакированный теплостойкий резистор номиналом 33 кОм, способный рассеять мощность до 2Вт. Давайте разберемся в конструкции данного резистора. В общем случае она представлена на фото ниже. Весь резистор покрыт сверху лакокраской, которая обладает электроизоляционными свойствами. Обычна красно-коричневого цвета. Обратите внимание, если резистор в процессе своей работы длительное время нагревается, данная краска может изменить свой цвет. Если поскрести данную краску острым предметом, то она легко сходит с резистора и обнажает части, расположенные под ней. Так, по бокам резистора расположены медные чашки с никелированным покрытием, к которым припаяны медные луженые выводы. В зависимости от года выпуска данные выводы могут иметь посеребрение. Какого-то смысла собирать данные выводы ради извлечения серебра не имеет смысла, больше потратите денег на реактивы, так как содержание серебра в одном резисторе МЛТ-2 составляет всего порядка 0,006г. Но этот факт нужно иметь ввиду материально ответственным лицам, которые отвечают в организации за старую аппаратуру, которая состоит на бухгалтерском учете и в которой могут оказаться данные резисторы. При списании такой аппаратуры все данные резисторы придется или извлекать и сдавать отдельно или же передавать в специализированную организацию. О наличии содержания серебра в данные резисторах можно уточнить в формуляре на аппаратуру, в которой они содержатся. Визуально определить наличие серебра в выводах можно по их почернению со временем, а еще такие контакты будут хуже паяться, чем обычные медные луженые выводы. Если продолжить соскабливать краску с баков резистора, то можно увидеть, что между двумя медными чашками зажата керамическая трубка, на которую по ее поверхности нанесена резистивная пленка. Именно эта пленка отвечает за номинал сопротивления резистора. Те спирали, что вы видите на фото, нарезаны на керамической основе. Изменяя материал данной пленки и число витков данных нарезанных на керамики спиралей, можно регулировать итоговое сопротивление данного резистора. Если расколоть резистор пополам, то можно увидеть, что основной объем резистора занимает именно керамика, а резистивная пленка очень-очень тонкая. Бонусом вся данная информация в формате видео: На сегодня это все, что я хотел вам рассказать про резисторы МЛТ-2. Если у вас остались вопросы— пишите, буду рад на них вам ответить."
  },
  {
    "article_id": "https://habr.com/ru/companies/cleverence/articles/877598/",
    "title": "В чем разница между китайским ТСД и «сделанным в Китае»?",
    "category": "Электроника",
    "tags": "ТСД, Китай, Софт, клеверенс, сборка, железо",
    "text": "ТСД, он же терминал сбора данных — это мобильный компьютер со встроенным сканером штрихкодов, RFID-меток и прочего. Используется на складах для учета товаров. Бывает двух видов: сделанный в Китае и китайский. Вот здесь вы, наверняка, споткнулись и перечитали. Простите, что? Это как сделанный в Китае и китайский? А в чем разница? Сейчас все объясним. Для людей, которые не очень понимают, как работает склад и какую роль там играет ТСД, рекомендуемпосмотретьнаши обучающие видео. Они совсем короткие и наглядно поясняют, что как работает. А мы продолжим. Сделано в Китае Как формирует поставку ТСД для себя какой-нибудь крупный игрок? Прям совсем крупный, уровня Амазона или Федекса? Когда ТСД может поставляться пять лет, общей партией в 100 тысяч штук? Или даже больше. Условный Амазон/Федекс приходит к надежному производителю и ставит свои требования. Такой-то размер, такие-то функции, такой-то вес, такой-то срок жизни от аккумуляторов. Это начальное ТЗ, которое в процессе еще будет корректироваться. Далее Амазон/Федекс и дизайн-хаус проходят несколько итераций пилотных образцов, получают нужный продукт. И, наконец, подписывается контракт. Понятно, что история сильно упрощена, но идея в целом такая. Итак, Амазон/Федекс стал для производителя якорным заказчиком. Производить ТСД будут в Китае на заводе. Но это именно “сделано в Китае”. В каких странах куплены комплектующие или прошли пилоты — не суть важно. Дизайн-хаус — это такое конструкторское бюро полного цикла. Те, кто могут разработать электронный продукт под ключ, включая начинку, дизайн, тестирование, отладку и прочее. Все крупнейшие производители по своей сути — дизайн-хаусы на максималках. Дальше они передают сборку на завод, а вот завод — это совсем не то, что мы привыкли вкладывать в это понятие. Завод скорее похож на шарагу со сборочной линией. Но к этому мы еще вернемся. Что делает хороший производитель, готовясь к своему контракту на пять лет со своим “якорем”? Производитель закупает чипы. И закупает их сразу на все будущие 100 тысяч штук с небольшим запасом на брак, ремонт и безалаберность кладовщика. Плюс еще купит часть важных компонентов сразу на все. Зачем он так поступает? Ведь срок поставки пять лет! Комплектухой придется забить огромный склад и хранить ее там. Следить, чтобы не отсырела, чтоб мыши не погрызли, чтоб на соседний завод не утащили. А зачастую производитель реально хранит у себя комплектующие, чтобы сборка (условный завод, где происходит монтаж) не пускала их налево. Все очень просто. Амазон/Федекс четко оговаривает в контракте ЧТО он хочет получать все эти пять лет. Там прописано все до мелочей. И “якорь” не станет слушать объяснения про “сняли с производства”, “кризис микрочипов” и “застряли в Суэцком канале”. Ему нужно сто тысяч, пять лет и именно такой состав комплектующих, без замен и обновлений. “Якоря” тоже можно понять. Он не хочет бесконечно адаптировать драйвера в своей системе, если у ТСД вдруг сменилась ревизия или прошивка. Не хочет заниматься редизайном и снова проходить этапы пилотирования. Это дорого и долго. “Якорь” сразу оговорил это условиями контракта, он за это платит. Так что завод вынужден сразу втянуть в себя все ключевые запчасти и следующие пять лет заниматься сборочным процессом. Вот с производством он может не спешить. Китайский ТСД “Якорь” не дурак и не первый день в бизнесе. Платить-то он платит, да только он наверняка цену контракта в пол продавил. Потому, производитель делает следующее. Задает себе (и своим коммерсантам) вопрос. А нужен ли такой продукт на рынке еще кому? Давайте быстро поспрашиваем, походим, послушаем? У нас отличный и оттестированный ТСД уровня топ-заказчика. Наверняка, еще кому-то надо. Ну а мы доппартию выпустим, заказчикам поменьше продадим, но уже по рыночной цене. Явно маржа подрастет. Есть мелочи, типа NDA с “якорем” или потенциального брендирования продукции под “якоря”. Но из-под полы можно сделать что угодно. Например, найти страну, где “якоря” нет и не будет в ближайшее время. Ну или цвет кнопок поменять. Надо понимать, что вот такой производитель не будет продавать ТСД дешево. У него качественный продукт (за деньги “якоря”). Он хочет на нем навариться по полной. Есть тут и другой риск. В какой-то момент он может перестать отгружать товар. Это значит, что кончилась комплектуха, а весь склад ушел “якорю”. Или модель просто кончилась навсегда. Читайте — контракт с “якорем” закрыт, мы пошли на следующий цикл, поддерживать текущую производственную цепочку смысла нет. Бывает, есть такие риски, да. Проблема в том, что система может дать сбой. Могут поменяться вводные со стороны “якоря”. Например, законодательно его заставят покупать только американское (или российское). Критически сменится софт/бизнес-процесс/любимый цвет директора. Ну или просто пройдут эти самые пять лет и пойдет следующий технологический цикл. Причин масса. И “якорь” пойдет искать новые девайсы, снова объявит тендер. А раз “якорь” крупный, то вокруг него постоянно вьются какие-то представители, компании и конторки. И не всегда ясно кто из них кто. Реально перед тобой серьезный производитель с заводом и дизайн-хаусом? Или аферисты? А может вообще дизайн-хаусы кончились, а аферисты уже все? Короче, могут выбрать “не тех”. В смысле какую-нибудь мутную конторку, у которой нет ничего, кроме энтузиазма. Помним, что “якорь” конкретно давит вниз цену и условия на этапе тендера. И что делать крошечной фирме, которая выиграть-то выиграла, но отсрочка платежа год, а делать надо сейчас? Мы думаем, вы уже догадались. Нет, они не сбегут с деньгами “якоря”, это подсудное дело. Они просто купят комплектухи на десять тысяч устройств, а не на сто. И будут клепать небольшими партиями по мере поступления денег и ходу времени. Попутно молясь Китайскому Дракону, что после первой поставки комплектуха не выйдет из серии. Спойлер — обязательно выйдет и хрен они где ее купят снова. Так в чем же разница Собственно, этопервое отличиесобранного в Китае ТСД от китайского ТСД. Собранная в Китае — это та, что сделана для крупного “якоря” и сверхпартией пущена налево. У нее могут поменять цвет кнопок или считыватель, чтобы совсем не палиться. Но она сделана хорошо, у нее все протестировано и, что очень важно, у нее не часто выходят ревизии. А вот настоящее китайское — это продукт мелких фирмочек или завода. Они урвали крупный заказ и сейчас везде его распихивают, помимо якоря, чтобы нарастить оборотку. Но за ними нет склада комплектухи. А значит в их пятой ревизии от первой уже мало что останется. Это могут выдавать за “улучшения”, но по факту — это дичайшая головная боль. Второе отличие— в софте. Настоящий производитель получит от “якоря” и требования по софту. Там все как с железом, “якорь” каждый год драйвера писать не хочет, софт не меняется, все работает, все счастливы. Ну может баги где пофиксят или новую фичу внедрят. Даже если такие устройства сливаются налево, там максимум интерфейс чуть изменят. Оно и так работает по лучшим корпоративным стандартам, пользуйтесь! Если вы закажете такой ТСД спустя два года, то там вряд ли что-то критично поменяется. Зачем? Рабочий софт, постоянная элементная база. Мелкая фирмочка или завод старается ТСД распихать по максимальному числу клиентов. И раз ей надо выживать, она свяжется и с теми, кто закажет тысячу и с теми, кто закажет пять тысяч. И еще кастомизирует прошивку силами самого дешевого китайского программиста. Вот у нее и получается дичайший микс из кастомных прошивок и меняющегося железа. Какие там могут вылезти сюрпризы знает только Китайский Дракон. Для примера: меняются требования к освещенности для сканера. Или надо руку чуть повернуть. Привычное обращение софта со старым сканером работает, а в новой ревизии вызывает непонятную залоченную менюшку на корейском (которую для корейцев сделали и убирать не стали). По идее, те, кто ввозят и продают ТСД в России могли бы их локализовать. Но это их страшный сон. При средних затратах на андроид-девелопера 400 тысяч в месяц, он благополучно съест всю маржу. И есть ее будет всегда, т.к. работы у него будет много. Когда с каждой новой ревизией у ТСД все новые и новые сюрпризы, то их надо как-то адаптировать в софт. Как мы с этим боремся У нас в Клеверенсе такая боль тоже есть, но мы предлагаем комплексное решение для склада. Когда клиенты приходят к нам, у них, как правило, есть ТСДшки. Чтобы они работали корректно — мы ставим на них свой софт. И очень любим, когда клиент не сэкономил, а купил продукт настоящего производителя. На другой чаше весов у нас есть 30-40, может быть, 50 “китайских” брендов, которые пытаются составить конкуренцию нормальным производителям. Их в страну ввозят по нескольку десятков, может, сотен штук. Каждый раз с новой начинкой, с новыми прошивками, и могут даже не указать в документации, что есть изменения. Мы должны  обеспечить точно такую же работу и потратить в 50-100 раз больше усилий. Ведь придет клиент и скажет — ребята, у вас модель указана в списке поддерживаемых, но чет не запускается. Ага, конечно. Ее в мешке с Алиэкспресса ввезли, там от модели только коробка, остальное все другое. Мы не жалуемся, сами в этот рынок вписались, все понятно. Это точно не забота клиентов. Но хочется подсветить, чем отличаются разные ТСД при, казалось бы, одинаковом функционале и сильно разной цене. Собственно, это работает не только с ТСД. Вечными сменами ревизий железа при глюках софта страдают многие устройства. В случае продукта от хорошего производителя можно один раз написать драйвер и забыть. До следующей ревизии точно. А ревизия это будет лет через пять, в лучшем случае."
  },
  {
    "article_id": "https://habr.com/ru/companies/first/articles/850640/",
    "title": "Радиоуправление игрушками в диапазоне 27 МГц: двухкомандные системы",
    "category": "Электроника",
    "tags": "радиоуправление, радиоуправляемые модели, радиоуправляемые, микросхемы, радиоуправляемые модели автомобилей, декодер, игрушки своими руками",
    "text": "Законно ли это? Всё лучшее — детям 26.995 МГц; 27.045 МГц; 27.095 МГц; 27.145 МГц; 27.195 МГц. Двухканальная система GS-683 Что можно запитать от полутора вольт? Многокомандное радиоуправление Декодер команд BACK = 0, FORWARD = 0: двигатель игрушки выключен; BACK = 0, FORWARD = 1: движение вперёд; BACK = 1, FORWARD = 0: движение назад. Приступаем к испытаниям"
  },
  {
    "article_id": "https://habr.com/ru/companies/timeweb/articles/874786/",
    "title": "Автоматизируем отопление и горячее водоснабжение в отдельно взятой квартире",
    "category": "Электроника",
    "tags": "timeweb_статьи, микроконтроллеры, отопление, водоснабжение, diy-проекты",
    "text": "На кухне синим цветком горит газ До сих пор во многих квартирах, в т. ч. и на территории бывшего СССР, для приготовления горячей воды часто используются индивидуальные проточные газовые водонагреватели, в обиходе – газовые колонки. Данные устройства имеют несколько неприятных недостатков по сравнению с системами централизованного горячего водоснабжения. Эти недостатки существенно снижают комфорт проживания современного человека, привыкшего к удобствам 21 века. Во-первых, при открытии крана с горячей водой, последняя появляется не сразу, а только через некоторое время. Это время включает в себя как задержку на включение самой колонки, тепловую инерцию нагревателя, так и время, необходимое на то, чтобы нагретая вода просто дошла по трубам до потребителя, и при большой длине труб это время может достигать нескольких десятков секунд. При этом в канализацию бесполезно сливается ненагретая вода и бесполезно тратится время на ожидание. Во вторых, температура самой воды зависит от большого количества факторов. От погоды на улице (температуры приходящей холодной воды), от расхода самой воды, от давления газа. Если первый и третий факторы изменяются во времени довольно медленно и их можно скорректировать ручкой регулировки подачи газа на самой колонке, то второй фактор может изменяться в реальном времени, прямо здесь и сейчас. Прежде всего, температура воды зависит от степени открытия крана. Меньше расход воды – выше температура, и наоборот. Но и открытие горячего крана в другой точке водоразбора также изменяет расход воды – температура падает. Даже открытие холодного крана в другой точке водоразбора влияет на температуру – падает давление в квартирной разводке, уменьшается расход воды – температура растет. Приходится все время манипулировать расходом чтобы поддерживать комфортную температуру при принятии водных процедур. При этом еще и эффект от этих манипуляций проявляется с большой задержкой, опять же из-за тепловой инерции и длины труб. Также некоторой проблемой является каждый раз объяснять гостям, привыкшим к централизованному снабжению, вышеописанные эффекты и учить их пользоваться элементарными, с их точки зрения, бытовыми вещами. Некоторые модели газовых котлов (имеющих функцию как отопления, так и горячего водоснабжения) допускают использование дополнительного накопительного бака с теплообменником. Такая схема, по идее, должна устранить вышеописанные недостатки. Но, по правде говоря, такую систему нигде в квартирах я не встречал. Вероятно, потому что покупать к и так недешевому котлу еще одну вундервафлю не всем по карману. Кроме того, этот бак еще нужно где-то размещать. Такой вариант пригоден скорее только для частного дома, где с местом для размещения нет таких острых проблем Поэтому, делая ремонт в своей квартире, я решил реализовать автоматизированную систему горячего водоснабжения, свободную от этих недостатков. Также дополнительно к ней реализовать и автономную систему отопления. Поскольку в моем регионе отсутствуют длительный отопительный сезон, и температура на улице лишь изредка и ненадолго принимает отрицательные значения, к основному источнику тепла не предъявляется особых требований по тепловой мощности. Поэтому основой системы была выбрана недорогая газовая колонка фирмы «ARICTON», которая шла в комплекте вместе с квартирой. Название колонки написано верно, лол, а еще я встречал колонку «BOSH». Чудо-юдо фирмы \"Вошь\" Это ноунейм колонки производства неизвестного турецкого или иранского завода. Они достаточно просты, даже можно сказать — примитивны. Их внутреннее устройство почти полностью идентично и вплоть до мелочей повторяет устройство газовой колонки, показанное в замечательноманимационном ролике. Единственное отличие – эти колонки оборудованы принудительной вытяжкой и закрытой камерой сгорания и поэтому работают уже не от батареек, а от сети. Принудительная вытяжка включается перед зажиганием и выключается через несколько секунд после погасания, обеспечивая необходимую тягу и очищение топки от продуктов сгорания. Такая система полностью исключает попадание продуктов сгорания в помещение и при этом отсутствуют какие-либо особые требования к длине дымохода. Минусом такой системы является дополнительный шум, издаваемый вытяжным вентилятором при работе. Несмотря на то, что подобные колонки просты и относительно недороги, они оборудованы всеми видами защит – от погасания пламени, перегрева, остановки движения теплоносителя и пропадании тяги. Микропереключатель газовой колонки Для включения нагрева необходимо сделать две вещи — обеспечить проток теплоносителя через теплообменник (при этом открывается первый, механический клапан подачи газа) и щелкнуть микропереключателем (электронная схема колонки открывает второй, электрический, газовый клапан и подает искру для воспламенения газа). Это будет делать новая система управления, которую подробнее рассмотрим далее. Для того, чтобы обеспечить постоянное и стабильное по температуре получение горячей воды была выбрана схема с постоянной циркуляцией теплоносителя. Аналогично тому, как это делается в современных многоквартирных домах с местным тепловым узлом, но только в масштабах одной квартиры. Поскольку нагретая вода постоянно циркулирует по трубам, она уже готова к потреблению сразу же после открытия крана. Также ее температура не зависит от величины и продолжительности открытия крана, поскольку за температурой следит система управления. Рассмотрим подробнее схему водоснабжения. Схема водоснабжения Холодная вода из общедомового стояка холодного водоснабжения (ХВС) поступает через главный запорный вентиль, фильтр-грязевик, счетчик воды и обратный клапан на магистральный фильтр тонкой очистки. Фильтр необходим для очищения воды от загрязняющих примесей и продления срока службы сантехнических приборов. В квартире имеется второй стояк водоснабжения, он подключается аналогично и играет роль резервного, на случай если что-то случится с основным. Обратные клапаны препятствуют перетоку воды из одного стояка в другой при одновременном открытии обоих запорных вентилей. Основная и резервная подачи воды после фильтров объединяются в единую точку водоразбора холодной воды. Из нее запитываются краны ХВС кухни, умывальника, ванной, бачок унитаза и подключается стиральная машина – автомат (СМА) через фильтр-умягчитель для жесткой воды. Из этой же точки подпитывается контур горячего водоснабжения (ГВС) через запорный вентиль, обратный клапан и фильтр-умягчитель. Фильтр-умягчитель содержит кристаллы полифосфатов, которые кроме функции умягчения воды, несут еще и антикоррозионную функцию, являясь ингибиторами коррозии. Циркуляцию воды в контуре обеспечивает циркуляционный насос (помпа). Перед помпой стоит дополнительный фильтр тонкой очистки, улавливающий накипь, ржавчину и прочие вещи, образующиеся при нагревании воды. Нагретая вода поступает в бак-накопитель, в качестве которого используется накопительный электрический нагреватель (в просторечии — бойлер) на 100 л. Как показала практика, такой объем несколько избыточен. Вполне достаточно было бы бака и на 50-80 л. Такие нагреватели доступны с более плоской формой бака, которая намного лучше бы вписалась в интерьер ванной комнаты. Но, уж как получилось, так получилось. Кроме функции аккумулирования нагретой воды, бойлер также выступает в качестве резервного нагревателя на случай проблем с газом. Правда, мощность такого нагревателя всего 2 кВт и при включенных ветках отопления этой мощности будет недостаточно для полноценного отопления квартиры. Бойлер и дополнительная батарея отопления Циркулирующая вода также постоянно проходит через полотенцесушитель и дополнительную небольшую батарею отопления ванной комнаты. Постоянная работа полотенцесушителя необходима для создания в ванной комнате повышенной температуры для комфортного принятия водных процедур и быстрого высыхания полотенец и влажных поверхностей для предотвращения появления плесени. Дополнительная батарея добавлена из-за большой площади ванной комнаты и предположения того, что в холодное время года одного полотенцесушителя может оказаться недостаточно. Батарея выбрана самого наименьшего размера из доступных и снабжена байпасом и вентилем для возможности ее отключения в теплое время года. Полотенцесушитель После полотенцесушителя осуществляется водоразбор горячего водоснабжения. Краны горячей воды подключены через клапаны сброса давления на 1-2 бара. Клапаны необходимы на случай отключения холодной воды, когда отсутствует давление сетевой воды. При этом контур циркуляции остается под избыточным давлением и продолжает функционировать. Несмотря на то, что такие клапаны широко представлены в интернете, приобрести их не удалось. В качестве них был использован обратный клапан, у которого прижимная пружина была заменена на более сильную. Разводка труб. Фильтр и помпа Контур циркуляции замыкается через байпас и обратный клапан. В точку после обратного клапана через вентиль можно подать сжатый воздух для продувки всей системы перед обслуживанием или ремонтом, например. Слив воды осуществляется из точки до обратного клапана. Расширительный бачок в системе не предусмотрен, в качестве него работает воздушная подушка, естественным образом образующаяся в баке бойлера. Параллельно байпасу проложены два независимых контура отопления, состоящих из двух и трех радиаторов соответственно, включенных последовательно. Первые радиаторы в каждой цепочке располагаются в двух жилых комнатах и оборудованы автоматическими регулирующими клапанами с термоголовками. Таким образом, температура в жилых комнатах поддерживается стабильной. Остальные радиаторы расположены в нежилых комнатах и работают по остаточному принципу, там температура будет — какая получится и, очевидно, меньше. Для некоторой компенсации этого, радиаторы в этих комнатах выбраны большего размера. Таким образом, получилось существенно упростить и удешевить разводку труб отопления по квартире, отказаться от коллекторного узла, сохранив при этом комфорт проживания. Клапаны с термоголовками имеют достаточно высокую цену и вполне могут быть заменены на обычные, не автоматические. Никаких их особых преимуществ автоматических клапанов при эксплуатации замечено не было. Один клапан (в длинной ветке) всегда открыт на максимум, вторым клапаном (в короткой ветке) проток воды вручную регулируется таким образом, чтобы температура теплоносителя в точке соединения обоих веток была минимальная и примерно одинаковая. Этим достигается самое полное использование запасенной теплоты теплоносителя и равномерный нагрев всего помещения. Фильтр-умягчитель заменен на фильтр большего объема. Также, фильтр такой конструкции проще обслуживать Позже на входе в контур циркуляции был установлен редуктор давления на 3 Бара. Это опциональный элемент, он оказался необходим потому, что давление холодной воды в моем доме могло доходить до 5 Бар. А предохранительный клапан, шедший вместе с бойлером рассчитан всего на 6,5 Бар. Запас на увеличение давления от расширения воды при нагреве всего 1,5 Бара и объема воздушной подушки оказывалось недостаточно чтобы скомпенсировать это расширение. Один из вариантов решения проблемы – установка дополнительного расширительного бака, второй вариант – уменьшить рабочее давление в контуре. Редуктор давления стоит значительно дешевле и занимает места значительно меньше расширительного бака, поэтому был выбран вариант с редуктором давления. По итогу, работу редуктора нужно признать неудовлетворительной. Редуктор создает большое сопротивление протоку воды, которое и так ограничено различными обратными клапанами и тонкими трубками водонагревателя. Из-за чего горячая вода течет существенно слабее холодной. Также он очень неточно поддерживает целевое давление — при закрытом кране давление достигает 3,5 Бар, при открытом — падает до 2 Бар. В дальнейшем редуктор предполагается исключить, есть идея заменить его на электроклапан, управляемый умной электроникой. В этом случае, на электронику можно будет также повесить функцию обратного клапана, запорного вентиля и защиты от протечек, реализовать так сказать, сразу 4 функции в одном приборе. Давление в контуре отопления В системе установлены два контрольных манометра — в линии холодной воды и в циркуляционном контуре. Основные операции при эксплуатации системы: Заполнение системы водой. Для этого необходимо открыть все запорные вентили, кроме 7.7 и 7.16. Система заполняется сама под действием давления сетевой воды. Заполнение водой происходит небыстро, поскольку вначале должен заполнится бойлер на 100 л. По мере заполнения периодически необходимо открывать вентиль 7.16 для выпуска вытесняемого воздуха. После того как из открытого вентиля 7.16 пойдет вода, его необходимо перекрыть и зафиксировать чем-нибудь в перекрытом состоянии во избежание его случайного открытия. Далее нужно стравить воздух из всех радиаторов отопления и полотенцесушителя с помощью кранов Маевского (на схеме не показаны). Во время заполнения можно включить газовую колонку, чтобы заполнение шло уже подогретой водой. Заполнение системы водой. Для этого необходимо открыть все запорные вентили, кроме 7.7 и 7.16. Система заполняется сама под действием давления сетевой воды. Заполнение водой происходит небыстро, поскольку вначале должен заполнится бойлер на 100 л. По мере заполнения периодически необходимо открывать вентиль 7.16 для выпуска вытесняемого воздуха. После того как из открытого вентиля 7.16 пойдет вода, его необходимо перекрыть и зафиксировать чем-нибудь в перекрытом состоянии во избежание его случайного открытия. Далее нужно стравить воздух из всех радиаторов отопления и полотенцесушителя с помощью кранов Маевского (на схеме не показаны). Во время заполнения можно включить газовую колонку, чтобы заполнение шло уже подогретой водой. Слив воды из системы. Для слива воды из контура ГВС и отопления нужно перекрыть вентиль 7.6, открыть вентиль 7.16 и 7.7 (остальные вентили в контуре тоже должны быть открыты). К вентилям 7.16 и 7.7 удобно подсоединить подходящего диаметра резиновый шланг. Шланг из вентиля 7.16 нужно опустить в подходящую емкость для воды или в канализацию. К шлангу из вентиля 7.7 можно подключить воздушный насос. Я использую автомобильный электрический насос для подкачки шин. Давлением воздуха, создаваемым этим насосом система «продувается» от воды. Вода удаляется отовсюду кроме бойлера. Для его опустошения необходимо выкрутить обратный клапан, совмещенный с предохранительным, установленный на входе в бойлер, и слить воду отдельно из бойлера. Слив воды из системы. Для слива воды из контура ГВС и отопления нужно перекрыть вентиль 7.6, открыть вентиль 7.16 и 7.7 (остальные вентили в контуре тоже должны быть открыты). К вентилям 7.16 и 7.7 удобно подсоединить подходящего диаметра резиновый шланг. Шланг из вентиля 7.16 нужно опустить в подходящую емкость для воды или в канализацию. К шлангу из вентиля 7.7 можно подключить воздушный насос. Я использую автомобильный электрический насос для подкачки шин. Давлением воздуха, создаваемым этим насосом система «продувается» от воды. Вода удаляется отовсюду кроме бойлера. Для его опустошения необходимо выкрутить обратный клапан, совмещенный с предохранительным, установленный на входе в бойлер, и слить воду отдельно из бойлера. Обслуживание и ремонт. Для возможности обслуживания отдельных элементов системы предусмотрено большое количество запорной арматуры. Для обслуживания контура ХВС необходимо перекрыть главные запорные вентили, вентиль 7.6 и стравить давление, открыв любой кран водоразбора ХВС. При этом, контур отопления продолжает функционировать независимо. Для обслуживания водонагревателей нужно перекрыть вентили 7.6, 7.9, 7.13, 7.14, 7.15 и стравить давление через вентиль 7.7. При этом изолируются ветки отопления и водоразбора ГВС, нет необходимости их потом перезаправлять и стравливать воздух. Каждую ветку отопления можно индивидуально изолировать перекрыв вентили 15.1 и 7.14 или 15.2 и 7.15 без остановки работы всей остальной системы. Обслуживание и ремонт. Для возможности обслуживания отдельных элементов системы предусмотрено большое количество запорной арматуры. Для обслуживания контура ХВС необходимо перекрыть главные запорные вентили, вентиль 7.6 и стравить давление, открыв любой кран водоразбора ХВС. При этом, контур отопления продолжает функционировать независимо. Для обслуживания водонагревателей нужно перекрыть вентили 7.6, 7.9, 7.13, 7.14, 7.15 и стравить давление через вентиль 7.7. При этом изолируются ветки отопления и водоразбора ГВС, нет необходимости их потом перезаправлять и стравливать воздух. Каждую ветку отопления можно индивидуально изолировать перекрыв вентили 15.1 и 7.14 или 15.2 и 7.15 без остановки работы всей остальной системы. Промывка. Для промывки радиаторов, например, первой ветки необходимо перекрыть вторую ветку и байпас и открыть кран 7.16. Шланг из вентиля 7.16 нужно опустить в подходящую емкость для воды или в канализацию. Перекрыв первую ветку и открыв вторую аналогичным образом промывается вторая ветка радиаторов. Эту процедуру не стоит делать слишком часто, так как на это расходуется нагретая вода. Промывка. Для промывки радиаторов, например, первой ветки необходимо перекрыть вторую ветку и байпас и открыть кран 7.16. Шланг из вентиля 7.16 нужно опустить в подходящую емкость для воды или в канализацию. Перекрыв первую ветку и открыв вторую аналогичным образом промывается вторая ветка радиаторов. Эту процедуру не стоит делать слишком часто, так как на это расходуется нагретая вода. Восстановление воздушной подушки. Как выше уже было указано, в системе отсутствует расширительный бачок, призванный сгладить колебания давления при расширении нагреваемой воды. Его роль выполняет пустое пространство над поверхностью воды в бойлере. Выходная трубка бойлера сделана таким образом, чтобы уровень воды не доходил до самого верха. Как известно, в воде растворен воздух. При нагреве он высвобождается из воды. Возможен и обратный процесс – при охлаждении воздух растворяется в воде и уносится ее потоком. Также, большая часть воздуха над поверхностью воды представляет собой насыщенный водяной пар, который при охлаждении конденсируется. Если допустить несколько циклов расхолаживания системы, воздушная подушка в бойлере может существенно уменьшится. Это приведет к тому, что при каждом нагреве воды будет сильно возрастать давление, излишек которого будет стравливаться через специально предусмотренный для этого предохранительный клапан. Чтобы вода из клапана не лилась на пол, на выходное отверстие клапана необходимо надеть силиконовую трубочку, конец которой можно вывести в подходящую емкость или в канализацию. Но предпочтительнее, конечно, восстановить воздушную подушку в бойлере. Для этого нужно перекрыть вентили 7.6, 7.8, 7.13, 7.14, 7.15, открыть любой из кранов водоразбора ГВС и продуть эту часть контура сжатым воздухом через вентиль 7.7 с помощью автомобильного насоса как при полной продувке системы. После продувки перекрыть вентиль 7.7 и открыть 7.6 чтобы обратно заполнить все водой. После проделанной процедуры вернуть все вентили в рабочее состояние и запустить систему. Восстановление воздушной подушки. Как выше уже было указано, в системе отсутствует расширительный бачок, призванный сгладить колебания давления при расширении нагреваемой воды. Его роль выполняет пустое пространство над поверхностью воды в бойлере. Выходная трубка бойлера сделана таким образом, чтобы уровень воды не доходил до самого верха. Как известно, в воде растворен воздух. При нагреве он высвобождается из воды. Возможен и обратный процесс – при охлаждении воздух растворяется в воде и уносится ее потоком. Также, большая часть воздуха над поверхностью воды представляет собой насыщенный водяной пар, который при охлаждении конденсируется. Если допустить несколько циклов расхолаживания системы, воздушная подушка в бойлере может существенно уменьшится. Это приведет к тому, что при каждом нагреве воды будет сильно возрастать давление, излишек которого будет стравливаться через специально предусмотренный для этого предохранительный клапан. Чтобы вода из клапана не лилась на пол, на выходное отверстие клапана необходимо надеть силиконовую трубочку, конец которой можно вывести в подходящую емкость или в канализацию. Но предпочтительнее, конечно, восстановить воздушную подушку в бойлере. Для этого нужно перекрыть вентили 7.6, 7.8, 7.13, 7.14, 7.15, открыть любой из кранов водоразбора ГВС и продуть эту часть контура сжатым воздухом через вентиль 7.7 с помощью автомобильного насоса как при полной продувке системы. После продувки перекрыть вентиль 7.7 и открыть 7.6 чтобы обратно заполнить все водой. После проделанной процедуры вернуть все вентили в рабочее состояние и запустить систему. Замена фильтрующих элементов. Чтобы заменить фильтрующие элементы входных фильтров холодной воды, нужно перекрыть оба входных вентиля 1.1 и 1.2 и стравить давление, открыв любой кран ХВС. То же самое необходимо сделать для добавления кристаллов полифосфатов в фильтры-умягчители для СМА и контура с горячей водой. Для замены фильтрующего элемента в контуре ГВС необходимо остановить циркуляцию, выключив систему, перекрыть вентили 7.6, 7.9, 7.13, 7.14, 7.15 и стравить давление вентилем 7.7. После замены фильтра можно дополнительно промыть систему согласно п. 4. После проделанной процедуры вернуть все вентили в рабочее состояние и запустить систему. Замена фильтрующих элементов. Чтобы заменить фильтрующие элементы входных фильтров холодной воды, нужно перекрыть оба входных вентиля 1.1 и 1.2 и стравить давление, открыв любой кран ХВС. То же самое необходимо сделать для добавления кристаллов полифосфатов в фильтры-умягчители для СМА и контура с горячей водой. Для замены фильтрующего элемента в контуре ГВС необходимо остановить циркуляцию, выключив систему, перекрыть вентили 7.6, 7.9, 7.13, 7.14, 7.15 и стравить давление вентилем 7.7. После замены фильтра можно дополнительно промыть систему согласно п. 4. После проделанной процедуры вернуть все вентили в рабочее состояние и запустить систему. Теперь подробнее рассмотрим систему управления. Вообще, для включения колонки, как я уже писал, достаточно обеспечить циркуляцию воды и щелкнуть микропереключателем. Для этого достаточно простого термостата за 1,5-2 $ или даже термовыключателя на 60 градусов, например типа такого: Термовыключатель. Фото из интернета Все они будут включать нагрев если температура меньше установленной и выключать нагрев при ее превышении. Бак-накопитель (бойлер) обеспечит аккумулирование этой воды, сглаживание колебаний ее температуры и некоторую тепловую инерцию чтобы колонка не включалась-выключалась слишком часто (хотя это, по сути, ее нормальный, штатный режим работы). Таким образом, в циркуляционном контуре температура воды всегда будет примерно одинаковой. Однако современные газовые колонки могут оснащаться дополнительными системами защиты и одна из них – реле времени. Колонка сама отключает подачу газа и нагрев если включена более определенного количества времени (примерно 20-30 минут). Это защита от перерасхода газа если, например, забыли выключить воду или оставили открытым кран при отключении воды в доме. В этом случае при возобновлении подачи воды колонка проработает полчаса и сама отключится. Для ее повторного включения необходимо выключить и снова включить микропереключатель. Эта защита может помешать работе, если, например, колонка не успевает за отведенный временной интервал нагреть воду до заданной температуры (например, в холодное время года). Можно, конечно, поискать и более старую колонку без этих наворотов, но, поскольку раз мы все равно делаем автоматизацию, эту задачу можно легко повесить на систему управления. Кроме обхода защиты колонки, система управления будет заниматься и другими вещами. Можно, например, реализовать различные режимы экономии – отключать или уменьшать нагрев ночью, когда все спят и днем, когда все на работе. Также можно добавить зависимость температуры нагрева от температуры наружного воздуха, чтобы не регулировать это каждый раз вручную. В результате получилась следующая схема. Схема электрическая принципиальная В качестве управляющего микроконтроллера использована микросхема ATmega328P, установленная на плате китайского аналога Arduino Nano. К микроконтроллеру подключены: 4 аналоговых датчика температуры LM335 (температура входящей воды, температура уходящей воды, температура наружного воздуха (опционально), температура воздуха в помещении (опционально)); OLED дисплей для индикации параметров и меню настройки; клавиатура с 4 кнопками для навигации по меню настройки; часы реального времени на микросхеме DS1307Z; звуковой излучатель; реле управления. LM335. Краткое описание из интернета Аналоговые датчики температуры LM335 выбраны из-за их доступности и простоты программирования. Датчики имеют линейную зависимость от температуры, что существенно упрощает пересчет значений АЦП в температуру по сравнению с NTC датчиками. Подключаются они достаточно просто – всего по двум проводам, по типовой схеме. Рядом с датчиками смонтирован подстроечный резистор для точной подгонки их показаний. Термодатчики LM335 выпускаются в пластиковом корпусе ТО-92. Этот корпус не очень хорошо приспособлен для того, чтобы обеспечивать хороший тепловой контакт с трубками нагревателя. Я его прикрепил пластиковыми стяжками плоской частью корпуса датчика прямо к медной части трубки (входной и выходной) радиатора. На медную трубку в точку контакта я предварительно нанес каплю термопасты КПТ-8. Термоизоляция датчика Место установки также необходимо термоизолировать от окружающей среды куском трубчатого вспененного материала как на фото. Это нужно для того, чтобы датчик не охлаждался наружным воздухом и его показания не уплывали. После этого показания термодатчиков стали примерно совпадать с показаниями встроенного в колонку термометра. Пробный запуск системы Датчик температуры наружного воздуха смонтирован на конце длинного провода и упрятан в термоусадку. Его следует смонтировать за окном таким образом, чтобы на него, по возможности, не попадали осадки и прямые солнечные лучи. Я его закрепил пластиковыми стяжками под водоотливным козырьком. Наружный датчик температуры Реле включения колонки запитано через термопредохранитель и штатный микропереключатель газовой колонки. Таким образом, реле аппаратно отключается при прекращении протока теплоносителя через колонку или при перегреве этого теплоносителя свыше 70 градусов. Состояние микропереключателя и термопредохранителя считывается микроконтроллером через вход PD5 (PROTECT, Protect Circuit). Термопредохранитель смонтирован на выходной трубе радиатора рядом с таким же штатным термопредохранителем самой колонки. Место установки штатного термопредохранителя. Дополнительный крепим рядом Кроме реле включения колонки опционально предусмотрены еще 2 реле управления помпой. Приобретенная мной помпа имеет переключатель скорости циркуляции на 3 положения – малая, средняя и большая. После сборки системы и ее опробации оказалось, что малой скорости циркуляции недостаточно для включении колонки, средней хватает едва-едва, а вот на большой — колонка стабильно включается. Таким образом, при включении колонки необходима работа помпы на максимальной скорости. При выключенной колонке можно переключить помпу в режим малой или средней скорости циркуляции, для поддержания температуры в контуре этого будет достаточно. Также помпу можно будет вообще периодически выключать, например, в режиме дневной экономии, поскольку днем обычно температура выше и нет необходимости циркуляции теплоносителя через батареи отопления. Вообще же, достаточно с помощью реле управлять только помпой, переключая ее с максимальной скорости на минимальную, без вмешательства в электрическую схему колонки. От изменения скорости протока колонка будет сама включаться и выключаться как в штатном режиме. Однако, для безопасности было решено еще дополнительно размыкать цепь микропереключателем газовой колонки, поэтому и были предусмотрены отдельные реле. Это поможет, например, если механический газовый клапан залипает в открытом положении, в этом случае система прекратит подачу газа электрическим клапаном. Схема подключения трехскоростной помпы Помпа имеет следующую схему. Ее двигатель имеет 3 обмотки. Цифрами около обмоток обозначены номера контактов внутреннего разъема. Переключателем меняется соотношение между количеством витков в рабочей обмотке L1 и пусковой (L2 + L3). Первое реле просто включает помпу. Второе реле переключает ее производительность – с маленькой на максимальную. Второе реле включено параллельно реле включения колонки. Доработка схемы помпы. Подпаиваем провода прямо к контактам Переключение между маленькой скоростью вращения и средней производится клавишным переключателем S1 (на схеме помпы), смонтированном на самой помпе вместо удаленного поворотного. Контакты реле зашунтированы снабберными цепями для продления их срока службы. Сами реле необходимо выбирать с запасом, на коммутируемый ток 6-10 А. Часы реального времени собраны на микросхеме DS1307 с отдельным источником питания в виде литиевой батарейки на 3 вольта. Часы подключены к микроконтроллеру по шине I2C. В этой же микросхеме имеется некоторое количество ячеек ОЗУ для хранения параметров настройки, например, таких как уставки температуры и гистерезиса срабатывания. Плата часов была немного доработана. Удалены диод D1, резисторы R4, R5. Вместо R6 установлен удаленный R5 на 200 Ом. Удалена микросхема памяти U3 за ненадобностью. Конденсаторы С1 и С2 заменены на 100 нФ. Примерная схема модуля Tiny RTC (схема из интернета) Почти вся схема собрана на основе готовых покупных модулей – OLED экран 0,96” 128x64 с интерфейсом SPI, модуль Arduino Nano с микроконтроллером ATmega328P и необходимой обвязкой, модуль часов реального времени Tiny RTC с батарейкой резервного питания, пленочная клавиатура на 4 кнопки с клеевым слоем. Все упомянутые модули стоят недорого и легко доступны на Aliexpress. Указанные модули необходимо соединить проводами, добавив лишь только цепи управления и реле, несколько резисторов, конденсаторов и звуковой излучатель (использован со встроенным генератором – для упрощения схемы и ПО). Забегая вперед, хочу предупредить, что использованный модуль с микроконтроллером ATmega328P обладает просто потрясающей чувствительностью к помехам. В чем причина этого – я так до конца не понял. Может быть, виновата разводка печатной платы модуля, может быть применена плохая китайская копия микроконтроллера ATmega328P. Микроконтроллер поначалу даже отказался нормально работать с блоком питания, постоянно глючил и зависал, пока я не поставил в блок питания дополнительные конденсаторы по питанию. Также микроконтроллер глючит от наводок, создаваемых системой электроподжига колонки. Поэтому при повторении конструкции не стоит экономить на фильтрующих и блокирующих конденсаторах, а всю конструкцию желательно поместить в металлический кожух, соединенный с общим проводом и землей. Вся схема питается от стабилизированного импульсного источника питания 12 В, в качестве которого использован переделанный (снижено выходное напряжение с 24 до 12 В) блок питания от вышедшего из строя принтера. Плата сетевого блока питания и реле управления помпой смонтированы в отдельной пластиковой коробке, крепящейся сбоку колонки саморезами. Сетевой источник питания и реле управления помпой Низковольтная часть смонтирована на небольшой макетной плате. Экранчик и модуль RTC крепятся на стандартных монтажных стойках. Монтаж платы, экранчика и модуля RTC Вся сборка целиком помещена в подходящий пластиковый корпус и закреплена внизу колонки. Самоклеящаяся клавиатура наклеена на корпус колонки в подходящем месте. Установка модуля управления Окончательный вид в собранном состоянии: ПО доступно наGitHub. Первая версия обеспечивает базовую функцию управления температурой, всякие режимы экономии будут добавляться позже по мере необходимости. ПО выполняет следующие функции: Инициализация периферии. Обработка аналоговых датчиков температуры. Опрос клавиатуры, вход и выход из режима меню (установки параметров). Чтение информации из часов реального времени и ОЗУ, запись информации в часы и ОЗУ. Подготовка и вывод информации на дисплей. Реализация функции управления реле в зависимости от температуры. Работа пользователя с кнопками сделана аналогично подобным решениям в другой технике. Две кнопки отвечают за изменение выбранного параметра вверх или вниз. Еще одна кнопка (M — menu) служит для входа в режим изменения параметров (длительное нажатие) и для переключения между корректируемыми величинами (короткое нажатие). Последняя кнопка – выход из меню (E — exit). При выходе из меню все измененные параметры сохраняются в оперативной памяти микросхемы RTC. Также кратковременное нажатие любой кнопки приводит к выходу из режима гашения экрана. Дисплей отображает только одну страницу, куда одновременно выводятся как текущие состояния датчиков температуры, так и уставки и настройки даты и времени. OLED дисплеи склонны к выгоранию, поэтому, по истечению некоторого времени и отсутствии нажатий кнопок, экран гаснет для экономии его ресурса. Информация на дисплее В строке состояния программа отображает текущее состояние конечного автомата, процент тепловой нагрузки, код ошибки, дату и время. Состояний у конечного автомата 5: 0 – выключено (OFF), 1 – ожидание (STOP), 2 – розжиг (START), 3 – нагрев (HEAT), 3 – ошибка (ERROR). Состояние 0: все выключено. Чтобы перейти из этого состояния должны быть исправны хотя бы 2 основных датчика температуры (входной и выходной воды) и микросхема RTC, где хранятся уставки температуры. Исправность датчиков температуры контролируется по напряжению, оно должно быть в определенных пределах. Если напряжение близко к 5 вольтам, значит датчик не подключен или в цепи обрыв (на экране отображается как NC – Not Connected). Если напряжение велико, что соответствует температуре выше 100 градусов (такие температуры теоретически возможны, но при нормальной работе недостижимы), то такое состояние обозначается как OH – Over Heat. Если температура находится в диапазоне от минус 40 до 99 градусов, она отображается в числовом виде (со знаком «минус» в случае отрицательных температур). При этом выставляется лог. 1 в соответствующем бите регистра temp_valid. Условием для перехода в следующее состояние является температура на входе не ниже TEMP_MIN, которая выбрана в районе 5 градусов. Если температура ниже этого значения, возможно перемерзание трубопроводов и отсутствие циркуляции. Запуск нагрева в этом состоянии нежелателен. Если температура на датчике меньше минус 40 градусов, такое состояние отображается символами OC – Over Cold. И, наконец, если напряжение с датчика близко к нулю, это может говорить о коротком замыкании датчика (или обрыве подтягивающего резистора). Это состояние отображается как SC – Short Circuit. Исправность микросхемы RTC и отсутствие проблем с обменом по интерфейсу I2C контролируется отсутствием превышения таймаута ожидания приема следующего байта. В случае срабатывания хотя бы одного таймаута, устанавливается соответствующий бит в регистре ошибок. Валидность данных в ОЗУ контролируется простым дублированием. Если обе копии этих данных совпадают, делается вывод что проблем нет. В противном случае выставляется тот же бит ошибки I2C в регистре ошибок. При наличии этого бита в состоянии 0, переход в следующее состояние не происходит. При возникновении неисправности в других состояниях происходит переход в состояние 4 (ошибка). Если эта ошибка возникает кратковременно (например, из-за помехи), в расчетах используется предварительно сохраненное значение уставок, которые были считаны безошибочно ранее. Состояние 1: ожидание. Циркуляция включена на медленной скорости, колонка выключена. При отсутствии ошибок, в этом состоянии осуществляется контроль текущей входной температуры. Если она ниже чем [уставка минус гистерезис] происходит переход в состояние 2 (розжиг). При возникновении ошибок происходит переход либо в состояние 0 либо 4, в зависимости от «тяжести» ошибки. Состояние 2: розжиг. Включены реле колонки и циркуляция на максимальной скорости. В этом состоянии делается до трех попыток розжига. Положительным результатом розжига является нагрев теплоносителя на величину не менее MIN_DIFF_TEMP. Если за 3 попытки разжечь колонку не получается, происходит переход в состояние ошибки 4 и устанавливается бит ошибки FLAME_ERR. Также в этом состоянии контролируется цепь защиты и другие ошибки. Цепь защиты и нагрев начинают контролироваться после выдержки некоторой паузы. Состояние 3: нагрев. Включены и нагрев и циркуляция на максимальной скорости. В этом состоянии контролируется достижение целевой температуры [уставка плюс гистерезис], максимальная температура на выходе, разница температур между входом и выходом (как максимум, так и минимум), срабатывание защиты от перегрева или отсутствия циркуляции и контроль прочих ошибок. Из этого состояния происходит переход либо в состояние 1 либо 4, в зависимости от «тяжести» ошибки. Состояние 4: ошибка. Включается непрерывный звуковой сигнал. Переход в это состояние происходит при возникновении ошибок, требующих ручного разрешения или внимания оператора. Программно выйти из этого состояния невозможно, требуется аппаратная перезагрузка. Для этого используется кнопочка сброса, смонтированная на плате ардуины. В этом режиме выключен нагрев, но оставлена циркуляция. Код возникшей ошибки фиксируется на дисплее. Переход из одного состояния в другое осуществляется с временной задержкой в несколько десятков секунд. Это сделано для того, чтобы предотвратить частые прыжки из режима в режим, а также для обеспечения паузы на устаканивание тепловых параметров после такого перехода. Составные части колонки обладают тепловой инерцией, на время этой инерции и сделана задержка. Процент тепловой нагрузки отображает отношение времени работы к общему времени простоя и работы. Если этот процент становится достаточно высоким, это говорит о том, что колонка не справляется с тепловой нагрузкой и необходимо или увеличить подачу газа или уменьшить заданную целевую температуру. Параметр вычисляется как скользящее среднее значение за последние сутки. Реализовано с помощью двух счетчиков. Один инкрементируется во время работы, второй – во время простоя. Если их сумма превышает заданный порог, из первого счетчика вычитается рассчитанный процент, из второго – [100 минус рассчитанный процент]. В процессе работы устройства могут возникать ошибки или недопустимые состояния. Их правильная обработка – важнейшее условие безопасности и надежной работы. Каждой ошибке или недопустимому состоянию соответствует бит в регистре ошибок. Регистр ошибок имеет следующие биты: Регистр ошибок непосредственно отображается на дисплее двухразрядным шестнадцатиричным кодом. Еще в один разряд выводится содержимое регистра MCUSR. Биты этого регистра указывают причину последней перезагрузки – вотчдог таймер, кнопка сброса или питание. В процессе эксплуатации был обнаружен интересный эффект. Температура воды в системе стабилизируется по температуре «обратки». В теплое время температура обратки равна температуре ГВС благодаря байпасу. При включенных контурах отопления температура обратки равна температуре на выходе батарей отопления, иными словами, в этом случае приоритетнее стабилизируется температура в помещении. После того как стены помещения прогрелись, температура в нем становится практически постоянной из-за высокой теплоемкости бетонной коробки. Поскольку помпа стоит на входе колонки и при работе ощутимо нагревается, она также подогревает на 1-2 градуса и обратку. Температура обратки отопления намного ниже температуры обратки ГВС и, благодаря прогретому помещению, весьма стабильна. Поэтому эти 1-2 градуса оказывают существенное влияние. Это приводит к тому, что колонка может очень долго не включаться на подогрев, постепенно расходуя тепло, запасенное в бойлере. И включается лишь тогда, когда вода в бойлере остынет настолько сильно, чтобы побороть эти 1-2 градуса, добавляемые помпой. Правильное решение данной проблемы состоит в переносе датчика температуры на выход бойлера. Это не проблема, если бойлер стоит рядом с газовой колонкой. Но в моем случае они разнесены достаточно далеко и протянуть провод составляет проблему. Поэтому я перенес датчик температуры на металлический тройник, расположенный перед фильтром до помпы и дополнительно добавил в алгоритм автоматическое включение по таймауту. Колонка включается по истечении примерно 30 минут после выключения, вне зависимости от того, был ли достигнут нижний предел температуры или нет. Окончательное место установки датчика. После фильтра, до помпы Однако после этого вылез другой эффект. При открытии крана горячей воды в контур начинает поступать ледяная вода из водопровода. Температура входящей воды резко падает. При выключении горячей воды, поступление холодной в контур прекращается и температура также быстро повышается до температуры обратки. При этом температура на выходе из колонки не изменяется – ведь ей требуется успеть нагреть весь тот объем воды, что был в фильтре, трубах и самой колонке. Температура входной воды быстро догоняет температуру выходной и контроллер отключает колонку думая, что в той сработал таймаут по времени работы. После этого контроллер запускает колонку, но нагреть воду хотя бы на минимальную разницу колонка снова не успевает. И только со второй попытки, когда помпа успеет прокачать теплую воду через колонку, она запускается. Чтобы побороть этот эффект пришлось снова переносить точку установки термодатчика – после фильтра, но до помпы. Там было не такое удобное место для монтажа датчика, но таки получилось его надежно закрепить. В противном случае пришлось бы использовать еще один отдельный датчик температуры входной воды – по одному из них контролировать наличие нагрева теплоносителя, по другому – целевую температуру. В общем, правильная точка установки датчика — весьма критичный параметр для работы системы. Катодная защита. При выборе радиаторов отопления и полотенцесушителя я решил сэкономить и отдал свое предпочтение недорогим стальным изделиям белорусского производства. Я предполагал, что сталь в контакте с водой и растворенным в ней воздухом будет понемногу ржаветь и заранее предусмотрел фильтр в циркуляционном контуре для фильтрации результатов этого ржавления. Все таки фильтр стоит намного дешевле чем разница в цене между стальными и алюминиевыми радиаторами. Забитый ржавчиной фильтр Однако реальность оказалась куда менее оптимистичной. Темпы образования ржавчины оказались такие, что 10-дюймовый фильтр забивался полностью уже через неделю (при включенных контурах отопления). При этом ухудшается проток воды, увеличивается разница между температурой входной и выходной воды, помпа даже на максимальном режиме не может обеспечить минимальный проток воды, необходимый для стабильного включения колонки. Некоторое время я искал решение. Замена всех (и так недешевых) радиаторов на еще более дорогие алюминиевые с переделкой всех труб подключения выглядело плохой перспективой. Изолирование контура отопления от водоснабжения (как это обычно и делается в подобных системах) исключило бы подпитку контура свежей водой со свежим кислородом. Потратив весь доступный кислород на ржавление, оно потом бы существенно замедлилось (но все равно не остановилось бы полностью, поскольку пластиковые трубы имеют пористость, через которую в циркулирующую воду может попадать кислород из воздуха в небольших количествах). Кроме того, реализация подобной системы потребовала бы замены электрического накопительного бака на бак с косвенным подогревом горячей воды внешним теплоносителем. Такие баки редки и недешевы. Да и сильно переделывать систему не хотелось. Поэтому я решил попробовать катодную (электрохимическую) защиту от ржавления. Катодная защита — это электрохимическая защита от коррозии, основанная на наложении отрицательного потенциала на защищаемую деталь. Сдвиг потенциала защищаемого металлического объекта осуществляется с помощью внешнего источника постоянного тока (станции катодной защиты) или же соединением с протекторным анодом, изготовленным из металла, более электроотрицательного относительно защищаемого объекта. При этом поверхность защищаемой детали становится эквипотенциальной и на всех её участках протекает только катодный процесс. Обусловливающий коррозию анодный процесс перенесён на вспомогательный, жертвенный электрод. В бойлерах, например, широко применяют пассивную электрохимическую защиту — магниевый анод. Магний в ряду электроотрицательности металлов находится левее алюминия, железа, никеля, меди, то есть всех тех металлов, которые преимущественно используют в системах водоснабжения и отопления. Постепенно корродируя, магний защищает от коррозии сам бойлер и соседнюю с ним арматуру. Судя по интенсивности ржавления, мой недорогой бойлер не был оборудован таким защитным анодом или же эта защита просто не работала. Однако, магниевый анод недешев и его замена может обойтись в существенную часть цены нового бойлера. Есть более дешевые и доступные материалы. Я решил остановиться на меди. Медь находится правее почти всех защищаемых металлов, поэтому чтобы защита работала на нее необходимо подать положительный потенциал. Кроме всего прочего, образующиеся при этом ионы и оксиды меди обладают биоцидным эффектом, обеззараживая воду от микроорганизмов. Если, однако, сдвиг защитного потенциала в отрицательную сторону превысит определённое значение, возможна так называемая перезащита, которая может привести к выделению водорода, изменением состава приэлектродного слоя и другим явлениям. Чтобы объединить все подлежащие защите поверхности в одну эквипотенциальную – проще всего соединить их проводом заземления. Заземление есть в каждой розетке и в каждой комнате. По заземлению не протекают большие токи и его потенциал везде примерно одинаков. Проводником электричества от анода будет сама циркулирующая вода. Анод был выполнен в виде несколько раз сложенного одножильного медного провода без изоляции. Медный электрод Провод герметично впаян в стандартную латунную пробку с водопроводной резьбой ½. Пробка вставлена в кусок пластиковой трубы, присоединенной к водяному контуру. Трубу следует расположить так, чтобы она автоматически заполнялась водой и в ней не образовывались воздушные пробки. Также длину трубы и находящегося в ней провода следует выбрать такой, чтобы электрод не касался других металлических деталей, иначе последние тоже начнут работать анодами и будут корродировать. Общий вид инжектора Питание защиты вначале осуществлялось от регулируемого лабораторного источника питания. Это позволяет в первое время подобрать оптимальный режим работы, затем источник питания можно будет заменить на фиксированный. Вначале напряжение источника питания было выставлено около 5 В чтобы заведомо перекрыть электрохимическую разницу потенциалов между медью и сталью, а также чтобы скомпенсировать падение напряжения в наиболее удаленной от анода точке водопровода. Ток в цепи при этом получился порядка нескольких сотен мкА. После этого напряжение было увеличено сначала до 15, а потом и до 30 В. Однако, катодная защита не очень помогла. Какой либо существенный, видимый глазом результат заметить не получилось (кроме окисления кончика электрода). Окислы на электродах Связано это, прежде всего с тем, что водопроводная вода, несмотря на содержание солей, обладает плохой электропроводностью и даже до рядом расположенного радиатора из поданных 30 В доходят лишь единицы вольт. Видимо, чтобы эффект был сильнее, необходимо устанавливать электроды катодной защиты непосредственно около каждого радиатора (например, в один из незадействованных входов). Однако, небольшой эффект все же был получен – благодаря объединению всех компонентов системы в эквипотенциальную поверхность, стальные радиаторы перестали быть для медного теплообменника жертвенными анодами и их ржавление таки немного замедлилось. Фильтра в контуре циркуляции стало хватать не менее чем на месяц. В общем, этот аспект еще требует своего внимания. Температура в наименее холодной комнате — столовой На данный момент, в отопительный сезон, система вполне исправно функционирует. Температура «подачи» находится в районе 55-60 градусов, «обратки» — 30-33 градуса. Дельта температуры составляет 25-27 градусов. В таком режиме колонка работает на небольшой мощности, гораздо меньшей, чем при нагреве сетевой воды в штатном режиме. Если добавить подачу газа, то температура подачи растет, но также появляется неприятный свист или сипение, как при закипании воды. При температуре на улице минус 5 (ночью)...0 (днем) рассчитанная контроллером тепловая нагрузка колонки находится в районе 80%, температура в жилых комнатах — около 20-21 градуса, в нежилых – 19-20 градусов. Низкая температура в помещении отчасти связана с высокими теплопотерями дома (построенного по проекту для южных широт) и низким качеством изготовления и установки окон. Температура в жилой комнате Относительно низкая рабочая температура системы (не более 60 градусов) обеспечивает более высокий коэффициент использования температуры топлива и, соответственно, некоторую его экономию. Несмотря на сифонящие окна и плохое утепление квартиры площадью 85 м2, расход газа на отопление и ГВС составил всего 236 м3 в декабре месяце. Изначально планировалось реализовать различные режимы экономии. Но, как показала практика, в них нет особой необходимости. Они получаются автоматически. Дело в том, что при частом пользовании горячей водой в контур постоянно попадает холодная вода из водопровода, вынуждая колонку чаще включаться. При этом температура «догоняется» ближе к верхнему порогу. В дневное или ночное время, когда водой никто не пользуется, колонка включается значительно реже — только по таймауту или же при остывании воды ниже нижнего порога. При этом, температура воды держится в районе нижнего порога. Также уменьшается и рассчитанный процент тепловой нагрузки. Был только реализован режим коррекции целевой температуры в зависимости от уличной температуры. Режим включается автоматически при подключении датчика наружной температуры. Степень коррекции задается двумя параметрами — пороговой температурой (THRESHOLD_TEMP) и величиной шага коррекции (STEP_CORR_TEMP). При уменьшении наружной температуры ниже пороговой, на каждую величину шага начинает добавляться один дополнительный градус к уставке. Например, при величине пороговой температуры в 10 градусов и величине шага 3 градуса, при наружной температуре ниже 7 градусов, к уставке добавится 1 дополнительный градус, ниже 4 градусов — 2 градуса, ниже 1 градуса — добавится 3 градуса, и так далее. Такая схема позволит уменьшить суточные колебания температуры из-за изменения температуры наружного воздуха, а также снимет необходимость постоянно регулировать температуру в зависимости от погоды на улице. Однако, этот режим снивелирует эффект ночной экономии, поскольку ночью обычно температура ниже чем днем, что заставит колонку сильнее греть воду. Предупреждение. Эксплуатация газового оборудования требует соблюдения определенных мер безопасности, с которыми можно ознакомиться, например, в инструкции к газовой колонке. Главные из них – обязательный и постоянный контроль наличия тяги (если нет принудительной вытяжки) и отсутствия утечек газа. Для постоянного контроля крайне желательно приобрести специальный извещатель, контролирующий содержание молекул СО и газа в воздухе. В статье описана доработка газовой колонки фактически до функции газового котла. Не повторяйте конструкцию, описанную в статье, если вы до конца не понимаете и не осознаете риск, связанный с вмешательством в работу газовых приборов. Не повторяйте конструкцию, описанную в статье, если в вашей стране действуют особые правила или ограничения на самостоятельный ремонт или модернизацию газового оборудования. В этом случае имеет смысл сразу приобрести готовый одноконтурный газовый котел и доверить его установку сертифицированному мастеру. Схема водоснабжения может остаться той же. Из схемы необходимо будет только исключить помпу – она уже есть в составе котла. Или же использовать управление колонкой только включением-выключением помпы. В этом случае лезть внутрь колонки не потребуется (датчики температуры можно будет установить непосредственно на металлических фитингах подачи воды). Заранее предвижу первый комментарий: А не еб@нет? Не должно. Если газовая колонка исправна, а ее монтаж выполнен правильно и по инструкции, проблем не будет. Глубокого вмешательства в устройство колонки и ее штатные цепи защиты – от погасания пламени, перегрева, отсутствия тяги, отсутствия протока воды — не происходит. Наоборот, добавляется еще один дополнительный элемент защиты в цепь микропереключателя включения колонки. Аппаратный датчик перегрева выше 70 градусов и микропереключатель разорвут цепь, даже если контроллер зависнет или его реле управления залипнет или не будет протока теплоносителя. Также контроллер может спокойно отработать другую частую неисправность колонки – залипание механического клапана в открытом положении. В этом случае при достижении целевой температуры контроллер просто отключит нагрев электрическим клапаном. Наличие накопительного бака позволяет колонке работать в значительно более лайтовом режиме, поскольку не требуется нагрев воды сразу до высокой температуры в реалтайме. "
  },
  {
    "article_id": "https://habr.com/ru/articles/876786/",
    "title": "Сумматор, триггер, регистр, почти счётчик, и можно было бы больше и лучше на асинхронной логике, но надо менять IDE",
    "category": "Электроника",
    "tags": "Асинхронная логика, триггер, регистр, полный сумматор, счётчик",
    "text": "С чего стоит начать, с того, что IDE у меня пока Gowin 1.9.9.03 Education. Но если кто-то захочет посмотреть только файл Logisim Evolution, то для него это значения не имеет. Свободное время я как мог отдал этой работе, не забывая при этом отдыхать, без этого вообще никак. Исправил сделал доработку полного сумматора, отладил проект асинхронного счётчика в Logisim Evolution v3.9.0. Плата всё та же - Tang nano 9k. Все проекты в Zip папках. Это всё та-же асинхронная логика, тактирование тут применяется собственного изготовления и действует как замена сигнала согласования. И плюс своя версия языкового описания в структурном виде схем на базовых логических элементах (символы могут быть заменены на более визуабельные, просто на настоящий момент передать смысл описания и для чего оно такое и как это можно использовать). Всё ещё предположительно Fast триггеры и регистры (проект триггера архивирован). Если вам ещё не надоело читать - прошу далее, в конце ждёт видео (и ссылка на видео с рутуба) с работающим асинхронным шестнадцатиразрядным  счётчиком на базе сумматоров и регистров. Если кому нужны доказательства, как оказалось, то все проекты в архивах по предоставленным в публикации ссылкам. Кроме того- публикация ничего не доказывает, это просто описание проделанных мною работ, часть которой является вполне успешной, а часть - не очень, но направление асинхронной логики интересное и мало исследованное. Версия IDE важна, поскольку при смене результаты могут быть почти непредсказуемы. Итак что я смог выжать из версии для обучения - озадачить её непомерно, чтобы хоть как-то заставить её делать что-то похожее на требуемое. Далее сначала в Logisim Evolution - в порядке хронологии. Доработка сумматора — добавление двух буферов, чтобы перекрыть промежуточный сигнал ошибки (в проекте верхний буфер buf я конечно поставил на управление bufif1, а не после него, но переделывать анимацию уже не стал). И вот сразу хотелось-бы сказать о том, как можно было-бы представлять описание подобных схем, с видом на то, что оптимизация на самом деле может, например, не резать логику, считая её неэффективной, а переписать код в чистый - наиболее визуабельный и с отсутствием излишеств в описании, но не в схеме, в логику оптимизация вмешиваться не должна - это моё личное мнение. Вот как это можно было-бы описать, например (символы могут быть заменены на более практичные, они означают номер и роль сигнала, это просто пример) Тут компилятору будет видно что есть что, и что можно упрощать или улучшать, для этого ему выделено три раздела в модуле: описание входов и выходов, промежуточная часть схемы и заключающая. Вот пожалуйста - всё есть для того, чтобы чистить код любителям чистоты, а не резать логику  оптимизацией. Предполагаемый fast триггер (RS). Этот триггер требует запуска перед использованием, осуществляемого через сброс. Тестирование было произведено, да - это действительно быстро и оптимизация не режет триггеры в схеме, но в силу специфичности IDE этому не стоит доверять на все сто, но можно предположить что может быть так и есть, с единственным но - скорость сброса пока не измерена, потому чо в планах маячил уже асихронный счётчик на сумматорах и регистрах, а регистры будут иметь ту-же базу что и триггер. С виду не рабочее, но на самом деле работает. Это иной архитектуры, в большей мере не логической, а архитектуры внутренней синхронизации. В общем не знаю как это назвать точно, но это работает, и вроде как достаточно быстро чтобы сделать из этого позднее регистры, если даже хотя-бы сравнивать с первыми моими триггерами, что на базе защёлок. Проверял на этом проекте. Оптимизация \"не ест\" такую логику, ну и хорошо. https://www.cyberforum.ru/blog_attachment.php?attachmentid=9145&stc=1&d=1735939897 , скриншот Сама первая публикация по этому триггеруhttps://www.cyberforum.ru/blogs/223907/8763.html. Небольшие расчёты: Самое первое, что я смотрел материал, где наносекунда была равна милионной доле секунды, что не верно, оказывается миллирдной. частота сигнала захвата 135 МГц. за 128 тактов почти 11 срабатываний цепи из 100 триггеров. Полный цикл срабатывания цепи проходит почти за 11 тактов. Разделить на 100 получится что один триггер срабатывает за 0,1163636363636364 такта сигнала захвата. Частота тактирования сигнала захвата 135 МГц .Если конечно верить замерам и документации платы. 1000÷135 = 7,407407 наносекунды на такт сигнала захвата. 7,407407×0,1163636363636364 = 0,861953 наносекунды на запись единицы в триггер. Выглядит , уже, не фантастически быстро (с учётом исправленной ошибки), но стоит учесть что сброс такого триггера будет намного медленнее, но и будущий регистр планируется без этого недостатка. В общем это немного лучше будет сетапа предоставляемого разработчиками платы. Хотя если честно, то я не могу найти теперь тот материал, где скорость установки и скорость сбарсывания триггера на плате была. Пояснение к расчётам от 27.01 год публикации. По случаюhttps://habr.com/ru/articles/876786/#comment_27838232- удалено. были не правильно конвертированы единицы времени. Регистр Далее, в рамках проекта, предстояла работа над регистром. Вот анимация пары регистров с их запуском. Всё в Logisim Evolution . Можно было приступать к счётчику. Счётчик в Logisim Evolution https://www.cyberforum.ru/blog_attachment.php?attachmentid=9165&d=1736256071 Проект отлажен в программе Logisim Evolution, на плате возможны некоторые улучшения и сокращения, потому что виртуальный симулятор имеет специфику упрощения своей работы и пришлось усложнять некоторые модули чисто для него.Так как обработка знаков переноса практически не ведятся, и в задачу не входит вычисление числа, счётчик оставлен на этапе ведущего отсчёт до шестнадцати и возобновляющего его по новой. На FPGA планируется что генератором сигнала согласования для следующих по цепи счётчиков будет являться сигнал о том, что предыдущий отсчитал 16 своих рабочих опреаций сложения. Регистры, по замыслу, имеют такую-же скорость работы что и полный сумматор, возможно с небольшими отличиями. То-есть отсчёт происходит за две операции - сложение+ запись в регистр. Отсчёт до 16 происходит за 32 операции. Проект в зиппапке, анимация тут. Для запуска нужно отключить симуляцию CTRL+E , сбросить состояния CTRL+R, произвести нажатий CTRL+I (i заглавная) до тех пор, пока на выходах регистров не установятся нули, и потом на вход нижнего сумматора установить единицу, далее производить симуляцию в пошаговом режиме нажатиями CTRL+I , иначе симулятор обнаруживает цикл и отказывается выполнять его. Счётчик И в конце концов итог, приложил не мало усилий чтобы \"обмануть\" оптимизацию Gowin, на мой взгляд занимающуюся совсем не тем, что ей нужно делать (а именно ранее сказал - привести в порядок текст, вот её назначение, с логикой автор должен справляться самостоятельно и скорее всего это ему по плечу). Архив проекта Для версии 1.9.9.03. Стоит воспользоваться осцилоскопом, а не загрузчиком, но не нажимать кнопку воспроизвдения захвата сигнала, только загрузить битстрим с осцилоскопа, иначе результат будет совсем иной. Только так я смог \"обмануть\" эту оптимизацию. Счётчик шестнадцатиразрядный, диоды подключены к старшим разрядам переноса бита. https://rutube.ru/video/b9de332712e83c385fcc038f75d13e93/?r=wd Мне скинули по просьбе ключи, но я не знаю точно что буду использовать далее, знаю точно - IDE нужно менять.  Публикация сделана второпях немного, поэтому если какие-то несоответствия с ссылками или ошибками - прошу уведомить, я постараюсь исправить в ближайшее время (преимущественно после работы). Спасибо за внимание. Будет интересно услышать не сильно строгое мнение в адрес моих деяний, отнюдь не лестных для традиционной схемотехники, архитектур, да и собственно кода теперь, алгоритмы мы уже проходили. Чего ожидаю я - пространство для продвижения вперёд (и в ракурсе асинхронной логики - оно огромно), и конечно результатов, может и не сильно впечатляющих, но надеюсь не бесполезных. Всего доброго, не судите строго. "
  },
  {
    "article_id": "https://habr.com/ru/companies/timeweb/articles/872618/",
    "title": "Cyfral Intel. Перепрошиваем «говорящий» домофон",
    "category": "Электроника",
    "tags": "timeweb_статьи, домофон, ключи, цифрал, интел, intel, amd, 8051, avr, цап, опс, диспетчеризация, audacity",
    "text": "❯Суть такова ❯При чём тут Интел? ❯Обзор оборудования ❯Внутренности ❯Первый запуск ❯Блок звуковых сообщений ❯Читаем ПЗУ ❯Модифицируем дампы ❯Что ещё примечательного в этих панелях? ❯Заменяем сообщения ❯Так что в итоге? Разбираемся со считыванием и записью магнитных домофонных ключей MagiKey. Магнитный ключ «на максималках» Наследники перфокарт в мире электронных ключей Ключи с динамическим кодом: «Факториал» возвращается Самый суровый магнитный ключ Первый массовый советский домофон Санком. Неизвестный производитель оптических домофонов Оживляем раритетный домофон с магнитным ключом Визит-К. Домофон «Бубум» нового поколения Аналоговый «Цифрал». Оживляем самый простой домофон на дискретной логике Резистивный ключдля оптического домофона Cyfral Intel. Перепрошиваем «говорящий» домофон"
  },
  {
    "article_id": "https://habr.com/ru/companies/grangroup/articles/875508/",
    "title": "Вы могли подумать, что мы покупаем печатные платы в Китае, а перепродаем в России, но все чуточку сложнее",
    "category": "Электроника",
    "tags": "печатная плата, производство электроники, панелизация, спецификация, бланк заказа, ipc, dfm, инженеры, gerber",
    "text": "Привет! На связи Андрей, руководитель проектов компании ГРАН. В комментариях к статьям часто видим слова в духе: «Смысл платить вам как посреднику, если я могу заказать плату напрямую в Китае?». Или: «Почему вы говорите, что производите платы, ведь фактически платы производит китайский завод?». На первый взгляд со стороны правда может показаться, что мы перекупщики: Взяли заказ в России. Взяли заказ в России. Добавили свою наценку. Добавили свою наценку. Отправили заказ в Китай. Отправили заказ в Китай. PROFIT! PROFIT! На деле ситуация сложнее, и такая схема в нашем случае не работает. Мы уже рассказывали, как производим платы и контролируем их качество. Сегодня расскажу как взаимодействуем с людьми от первого знакомства до сопровождения готового продукта. Печатные платы заказывают производители электроники. Масштабы у всех разные: кто-то планирует крупную серию на десятки тысяч штук, а кто-то мелкие серии из десятков и сотен штук. Все проекты пройдут примерно один путь. Сначала образцы и прототипы для отладки, затем массовое серийное производство. На первых стадиях крайне важна качественная инженерная подготовка проекта. На ранней стадии важно найти все ошибки и недочеты, чтобы «отполировать» проект для серии. На практике согласование исправлений и улучшений – самая долгая процедура. Правильная подготовка нужна, чтобы понять, можно ли сделать платы именно так, как задумано. И как сделать это быстро и по возможности дешевле. На всех этапах подготовки главный принцип: Ничего не меняем в проекте сами Только вместе с клиентом и по согласованию. Но всегда предлагаем, как лучше. Это важно соблюдать на всех этапах подготовки по нескольким причинам. Во-первых, разработчик ведет у себя документацию по проекту. Если мы что-то сами исправим, документы станут неактуальными. Во-вторых, мы можем не знать особенностей проекта. В целом, плату можно улучшить определенным образом. Но конкретно в этом проекте способ не подходит. В-третьих, клиент несет ответственность за работу своего устройства. Потому что плата лишь одна из частей готового устройства. Проект попадает в отдел продаж Когда приходит новый заказ, первым делом его нужно понять и оценить. Для этого проект попадает в руки отдела продаж. Обычно заказчик присылает бланк заказа, Gerber-файлы и конструкторскую документацию. Сразу закрепляем за клиентом 1-2 сотрудников, которые будут вести проект. Когда заказчик обратится в следующий раз, он снова будет работать с ними. Не придется объяснять одно и то же разным людям. Задача на первом этапе — отсеять очевидные ошибки, если они есть, определить сложность проекта и производственную площадку для его изготовления. Здесь будут общие технические вопросы, первые согласования и предложения по улучшению. Бланк заказа Это важный документ, приложение к договору. Клиент его заполняет первым делом. Часть информации коммерческая: сколько нужно плат, когда поставить. Остальное — параметры платы: количество слоев; количество слоев; тип материала; тип материала; финишное покрытие контактных площадок; финишное покрытие контактных площадок; вид обработки контура. вид обработки контура. Всего ~40 параметров в зависимости от вида и сложности печатной платы. Платы часто заказывают не по одной, а сразу мультизаготовкой. Мультизаготовка — это большая панель, на которой находятся сразу несколько одинаковых плат. Допустим, у заказчика линия автоматического монтажа компонентов. Тогда он разрабатывает панель специально для своего оборудования. Затем мы возьмем информацию о панели из чертежей или задания на панелизацию в бланке. В бланке заказа для наглядности есть шпаргалка: В бланки заказа иногда проникают ошибки от непонимания или по невнимательности. Задача отдела продаж сразу их отловить и вместе с заказчиком исправить. Ошибки влияют на стоимость и выбор производства, а нам важно сразу верно дать правильную оценку проекта. Немного расскажу о популярных ошибках. Неправильная толщина платы Различают расчетную и финишную толщину платы. Расчетная толщина складывается из базовых материалов – стеклотекстолита и медной фольги. Например, для двусторонней платы это толщина основания 1,5 мм. Финишная толщина включает дополнительные слои, которые появятся в процессе производства: металлизация; металлизация; паяльная маска; паяльная маска; шелкография; шелкография; финишное покрытие. финишное покрытие. В бланке нужно указать финишную толщину платы. Если указать расчетную, плата может потом не войти в устройство по габаритам. Неправильные допуски на габариты Стандартный допуск составляет ±10% для плат толщиной более 1 мм и ±0,1 мм для плат тоньше 1 мм. Тоже самое с допусками на габариты: при фрезеровке не может быть меньше+- 0,1 мм, а при скрайбировании – меньше+- 0,2 мм. Заказчики часто указывают более жесткие допуски, например ±0,05 мм, что технологически невозможно выполнить. В таком случае всегда будем согласовывать смягчение допусков. Неправильная толщина меди При указании толщины меди тоже бывает путаница: с базовой толщиной фольги и с финишной толщиной после металлизации. Базовая толщина медной фольги измеряется в унциях (oz) или микронах. Стандартные значения: 1/2 oz или 18 мкм; 1/2 oz или 18 мкм; 1 oz или 35 мкм. 1 oz или 35 мкм. Коммерческое предложение Анализ документов занимает обычно до 24 часов. До 3 дней может занять в особых случаях, но это бывает редко. В процессе формулируем список вопросов и задач. Параллельно подобрали несколько производственных площадок из нашего пула, которые специализируются на такого типа проектах. На основе всей информации составляем коммерческое предложение — специальный документ, в котором обозначаем стоимость заказа и сроки поставки. Если клиент новый, заключаем договор. В большинстве случаев мы работали раньше, поэтому договор уже есть. После согласования передаем проект в работу инженерам подготовки производства. Проект принимают инженеры подготовки производства На этом этапе могут быть подробные технические вопросы, детальные согласования и корректировки проекта. Сначала проверяем, соответствует ли проект техническим требованиям и отраслевым стандартам. Особое внимание нужно уделить отрасли, для которой заказчик создает электронику. По отраслям бывают свои специфические требования и стандарты. Для автомобилей нужны одни платы, для медицинского оборудования — другие. Бывает так, чтопроект сделали по стандартам ГОСТ. Проблема в том, что мир сейчас делает электронику по стандартам IPC, в том числе и мы. В России же часто проектируют по ГОСТам. Есть старые советские и современные российские стандарты. Старые советские ГОСТы выпускали в 70-80х годах. Сегодня они совсем не актуальны. Новые ГОСТы во многом — переведенные IPC. Если проект сделан по российским стандартам, его нужно адаптировать под международные. Следующая задача — детально проверить топологию платы: проводники, зазоры между ними, контактные площадки. Оцениваются минимальные параметры и узкие места. Возможно, что-то можно улучшить. Далее определяем послойную структуру платы, формируем таблицу сверловки, контроля импеданса, если есть, определяем требования маски и шелкографии, забивки отверстий и остальные параметры. При проектировании тоже случаются ошибки. Немного расскажу о некоторых популярных. Ошибки в топологии Часто встречаются слишком малые значения проводников и зазоров между элементами топологии. Произвести такую плату невозможно. Поэтому обязательно контролировать минимальные размеры: ширину проводников, зазоры между проводниками, между проводниками и контактными площадками, между элементами топологии и полигонами. Частая ошибка — закладывать минимально возможные параметры для простых плат. Это избыточно и удорожает производство, поскольку требует сложных технологий и оборудования. Параметры лучше выбирать исходя из реальных требований к плате. Ошибки для паяльной маски Часто встречаются слишком маленькие или слишком большие вскрытия в маске. При малых вскрытиях маска может попасть на контактные площадки и ухудшить пайку. При больших остаются открытыми соседние проводники. Это ведет к коррозии и возможным замыканиям. Бывают проблемы с защитой переходных отверстий. При закрытии больших отверстий (более 0,55 мм) маской с обеих сторон она истончается по краям и может треснуть при нагреве. Если переходное отверстие расположено близко к контактной площадке и закрыто маской с обратной стороны, маска может протечь внутрь и заблокировать отверстие или испортить площадку для пайки. Неправильная маркировка Основных проблем две: размер и размещение. Первая проблема: символы слишком мелкие. В программе их видно, но на реальной плате надписи будут нечитаемые. Минимальные требования к размерам символов: Высота символов: не менее 0,8 мм Высота символов: не менее 0,8 мм Ширина линий: не менее 0,13 мм Ширина линий: не менее 0,13 мм Соотношение ширины линии к высоте текста: не менее 1:7 Соотношение ширины линии к высоте текста: не менее 1:7 Вторая проблема: маркировка заходит на контактные площадки. В производстве такую маркировку автоматически подрежут, часть текста просто потеряется. Минимальный зазор от текста до вскрытия паяльной маски – 0,1 мм. Документ “вопрос-ответ” Когда инженер исследовал все документы по проекту, он готовит список вопросов для заказчика. В зависимости от проекта, там могут быть все или только некоторые пункты: проблемы проекта; проблемы проекта; предложения, как их можно исправить; предложения, как их можно исправить; вопросы по технологии изготовления; вопросы по технологии изготовления; рекомендации как улучшить конструкцию. рекомендации как улучшить конструкцию. Проект согласован и до производства остаются последние шаги: подготовить спецификации и файлы. Спецификация на печатную плату – документ, который содержит полные и детальные требования к производству печатной платы, отклонения от которых недопустимы или допустимы по согласованию. Вот некоторые из параметров: допуски на размеры элементов; допуски на размеры элементов; требования к металлизации отверстий; требования к металлизации отверстий; параметры паяльной маски; параметры паяльной маски; таблица сервловки; таблица сервловки; структура платы; структура платы; расчеты импедансов. расчеты импедансов. У разных производств могут быть свои дополнительные требования к спецификациям. Инженер переносит данные из проектной документации в спецификацию так, чтобы завод без проблем понял, что от него хотят. После этого проект уходит на производство. Производство вносит свои коррективы После подготовки производства мы отправляем проект на завод. У каждого своя специализация: автомобильное оборудование, телеком оборудование, светодиодная техника. На этом этапе могут быть уточняющие технические вопросы и последующая корректировка проекта. При выборе производства учитываем сложность и назначение платы. Завод часто предлагает изменения для улучшения технологичности. Например, расширить отверстия или передвинуть проводники. Каждое предложение сопровождается пояснением почему так будет лучше. Если нужно увеличить диаметр площадки контактной или расширить проводник, объясняем технологическую причину. DFM: проектирование с учетом возможностей производства Это методика проектирования, чтобы согласование с заводом занимало минимум времени. Идея в том, чтобы сначала изучить возможности завода. Затем сразу проектировать плату так, чтобы ни у кого не возникало лишних вопросов. Как проектировать платы, чтобы тебя сразу все понимали. Тема широкая. Возможно, выпустим об этом отдельный материал. Партия уходит в производство После утверждения правок завод готовит окончательную версию всех файлов. Процесс производства платы подробно показалив этой статье. Готовые платы приезжают до двери Мы выстроили эффективную систему доставки с двумя основными каналами: авиа и железная дорога. Каждый заказ доставляем \"до двери\" заказчика. На таможне у нас \"зеленый коридор\". Оформление занимает всего пару часов благодаря электронному документообороту. Это возможно потому, что мы работаем с одним типом товара и имеем все необходимые сертификаты. При доставке самолетом стандартные сроки получения платы: 7-10 дней с момента выхода платы с завода. Это вместе с таможенным оформлением. Железнодорожная доставка работает по расписанию. Если не успеваем на текущий рейс, следующий пойдет через две недели. В таких случаях, если задержка возникла по нашей вине, переводим заказ на авиадоставку за свой счет, чтобы соблюсти сроки. В Санкт-Петербурге курьерская доставка занимает около двух часов от нашего склада до заказчика. Все грузы застрахованы. Если при транспортировке возникли повреждения, мы фиксируем их в акте. Страховая компания возмещает ущерб, а мы оперативно переделываем заказ без дополнительных вопросов. Такой подход гарантирует сохранность продукции и минимизирует риски для заказчиков. Поддерживаем заказчика после поставки Поддерживаем заказчиков после поставки плат, особенно на этапе монтажа компонентов. Например, если возникают проблемы при пайке, наши специалисты помогут разобраться в причинах. Это может быть связано с финишным покрытием, размерами контактных площадок или другими параметрами платы. А иногда кажется, что проблема в плате. А на самом деле компоненты некачественные или технологию монтажа нарушили. Проводим исследования и помогаем найти истинную причину. Качество тщательно контролируем до, во время и после производства. Подробно об этомрассказали здесь. Бывает, что в процессе сборки первой партии выявляются проблемы с панелизацией. Панель может оказаться слишком маленькой или большой для оборудования заказчика. Или при установке тяжелых компонентов панель прогибается. В таких случаях мы помогаем оптимизировать конструкцию и выпускаем обновленную версию панели. Когда заказчик планирует выпустить новую версию платы, мы делимся опытом и даем рекомендации как улучшить технологичность. Это исключит проблемы первой версии. Также проводим семинары и обучение для сотрудников клиента. Рассказываем о технологиях производства плат, делимся опытом и лучшими практиками. Особенно полезно для разработчиков: начинают лучше понимать возможности производства. Персональные менеджеры остаются за клиентом. Следующий заказ примут люди, которые хорошо знакомы с производством заказчика и знают все нюансы. Выводы Производство плат идет по нашим спецификациям и под нашим контролем. Это называется «интегрированное производство». Не простое посредничество между заказчиком и китайским заводом, а полный производственный цикл. Кроме того, мы постоянно проводим аудиты производств из нашего пула, чтобы быть уверенными в качестве нашего продукта. За каждым клиентом закреплены персональные специалисты, которые знают все особенности его проектов. Команда инженеров готовит каждый проект к производству, находит ошибки, предлагает улучшения. На заводах тоже постоянно присутствуют наши инженеры. Они проверяют качество и оперативно решают производственные вопросы. Мы берем на себя сложную работу: адаптируем проекты под международные стандарты, проверяем их на технологичность, контролируем производство, качество плат и доставляем готовые платы до двери заказчика. Подробно рассказываем о печатных платах в нашем канале Telegram,присоединяйтесь. Если вы заказчик, в закрепе канала для вас чеклист по работе с документами. Приглашаем на офлайн-конференцию ГРАН, которая пройдет 25 февраля 2025 года в Санкт-Петербурге. Обсудим развитие российской электроники, современные технологии производства и перспективные решения в отрасли. В программе: презентации специалистов, обмен опытом с экспертами и интерактивные мероприятия. Конференция бесплатная,принять участие. Не стесняйтесь задавать вопросы в комментариях. "
  },
  {
    "article_id": "https://habr.com/ru/articles/875600/",
    "title": "Touch Aerospace",
    "category": "Электроника",
    "tags": "кт315, ttp223, радиотехника и электроника, компьютерная мышь, 3д-печать, 3д моделирование",
    "text": "Прототип сенсорной мышки с массой 40гр Привет, эта статья -  пример прикладного использования емкостных датчиков вместо физических кнопок. Повторю тезисно, что это и зачемнужно из первой версии. Мышь для людей со спинально мышечной атрофией и другими неврологическими заболеваниями, когда сложно нажать на физическую кнопку. (я не знаю, можно ли тащить в дотан на этой штуке, но клиент вполне может играет в cs go , а с обычной мышкой нет ) По сравнению с 1 версий функционал был увеличен, а масса осталась прежней. Готовый софт с настройкой конфигураций кнопок ( стоило изначально купить модель в которой это есть ) Готовый софт с настройкой конфигураций кнопок ( стоило изначально купить модель в которой это есть ) Корпус напечатан из легкого aerotex Корпус напечатан из легкого aerotex 7 емкостных кнопок 7 емкостных кнопок Сначала покупается мышка ARDOR GAMING Fury (за рекламу мне не платили, к сожалению) Разбирается и  выпаиваются экнкодер (его роль все равно займут 2 кнопки по центру) и все кнопки-омроны.  Потом печатается новая крышка скорлупа В качестве датчиков используются 7 плат ttp223, потому что все прочие варианты просто отказались стабильно работать, например схемы с наводкой на базу транзистора или сопротивление между базой и истоком – начинали глючить как дед на перекрестке - в самый неожиданный момент.  Перемычка А – выход hign/low . Перемычка B – выбор кнопка / триггер Схемотехника первой версии заключалась в том, что вывод микроконтроллера, который считывает замыкание кнопки, просто сажался на землю самой tt223, а в данном случае нужно было замкнуть 2 вывода микроконтроллера между собой. Для этой задачи применились доисторические кт315 (их было удобно паять и их было много)  по току или частоте ограничений нет, вес 0.2гр и заморачиваться с smd - просто создавать головную боль и растягивать время до релиза версии. Конденсатор настройки – 30пФ.   Eсли ловятся помехи, то или нужно увеличить емкость конденсатора, или платы зафиксировать на потолке на более толстый слой чего то ( двойной скотч, термоклей и тд ) Логика такая, что при касании, на выходе платы ttp233 появляется высокий уровень – транзистор открывается и замыкает собой 2 ноги микроконтроллера вместо кнопки.  При отпускании дорожки на корпусе – ttp отключает вывод – отключается транзистор – контакт прерывается Все очень просто Ttp223 были размещены на потолке, разводка сделана эмальпроводом а питание жилой без эмали ( серебристого цвета ) . Отводы сенсоров сделаны так же, как в модели 1 – прошивка проволокой  Я знаю что вы подумали глядя на все эти провода – что это кошмар водопроводчика но оно вполне функционально , разъемы и плоские шлейфы это отдельная головная боль и в единичном экземпляре их смысл будет очень условным. Корпус фиксируется наплавлением пластиком по радиусу и слегка обтачивается. ( потому что места для винтов не осталось и все место занимает плата)    Интерфейс сохранил все функции что были из коробки.   Ответы на возможные вопросы – нужно это все, чтобы легкое касание без нажима приводило к срабатыванию кнопки. нужно это все, чтобы легкое касание без нажима приводило к срабатыванию кнопки. Передняя стенка отсутствует, потому что клиенту был очень важна масса (по этой же причине дно перфорировано) Передняя стенка отсутствует, потому что клиенту был очень важна масса (по этой же причине дно перфорировано) Касательно разборки и ремонта. это прототип и он не предназначен что бы его кто то разбирал или ремонтировал Касательно разборки и ремонта. это прототип и он не предназначен что бы его кто то разбирал или ремонтировал Внутри отсутствуют подвижные компоненты или компоненты, которые могут со временем портиться. Единственное что может сломаться это провод Внутри отсутствуют подвижные компоненты или компоненты, которые могут со временем портиться. Единственное что может сломаться это провод Из того, что можно было модернизироватьэто упростить сборку адаптировав под массовое производство, где транзисторы не россыпью, а 1 сборкойотводы емкостей — это пластины на пластике, а не проволокакорпус собирается на 3-4 винтах.платы без омронов/ энкодера что бы не заниматься выпаиваниемвсе ttp собраны все на 1 плате и там уже есть конденсатор 30pf Из того, что можно было модернизировать это упростить сборку адаптировав под массовое производство, где транзисторы не россыпью, а 1 сборкой это упростить сборку адаптировав под массовое производство, где транзисторы не россыпью, а 1 сборкой отводы емкостей — это пластины на пластике, а не проволока отводы емкостей — это пластины на пластике, а не проволока корпус собирается на 3-4 винтах. корпус собирается на 3-4 винтах. платы без омронов/ энкодера что бы не заниматься выпаиванием платы без омронов/ энкодера что бы не заниматься выпаиванием все ttp собраны все на 1 плате и там уже есть конденсатор 30pf все ttp собраны все на 1 плате и там уже есть конденсатор 30pf (но это все уже совсем другая история, зависящая от мира экономики больше чем от мира электроники) Если вы хотите посотрудничать – напишите в лс. Если вас что-то порадовало – кнопка донатов ниже."
  },
  {
    "article_id": "https://habr.com/ru/companies/timeweb/articles/872966/",
    "title": "Аналого-цифровая автоматика и никаких микроконтроллеров на примере сушилки для рук",
    "category": "Электроника",
    "tags": "timeweb_статьи, схемотехника, diy, электроника, электроника для начинающих, электроника своими руками, электроника шаг за шагом, жесткая логика, стандартная логика, схемотехника для начинающих, Proteus",
    "text": "❯Конструкция макета ❯Описание структурной схемы ❯Описание электрической схемы ❯Заключение 1. Простая схема динамических указателей поворотов, и никаких микроконтроллеров 2. Светодиодная шкала для переменного резистора на «рассыпухе» 3. Светофор на логике со схемотехникой в стиле Beatles. Как электроника вновь стала моим хобби 4. Профессиональные методы прототипирования печатных плат. Распечатать на принтере или фрезеровать, ни слова про утюг 5. Бирдекель или арифметический детектив на операционных усилителях 6. Электронная игра «лабиринт» на сервоприводах. Никаких arduino, только жесткая логика 7. Велосипедный фонарь с динамическими поворотами. Зачем покупать на AliExpress, если можно сделать самому? 8. LPKF ProtoMat S63. Мыши плакали, кололись, но… продолжали фрезеровать печатные платы 9. Звуковой усилитель на драйвере шагового двигателя L298 и таймере 555. Да, 555-й может и спеть 10. Графический спектроанализатор с динамической индикацией на жесткой логике 11. Цифровой термометр на жесткой логике 12. Осциллограф из рассыпухи на светодиодной матрице. Разбор схемы в Proteus 13. SOS-фонарик на жесткой логике с датчиком удара на пьезоэлементе"
  },
  {
    "article_id": "https://habr.com/ru/companies/beeline_cloud/articles/878624/",
    "title": "Базовый минимум: зачем вашей компании WAF",
    "category": "Облачные технологии",
    "tags": "beeline cloud, waf, cloud waf pro, информационная безопасность",
    "text": "В блогеbeeline cloudмы пишем про настройку сетевых сервисов, управление облачной инфраструктурой и информационную безопасность. Сегодня поговорим про корпоративные веб-приложения: почему они так «нравятся» злоумышленникам и что для их защиты предлагают решения класса Web Application Firewall (WAF). Материал подойдет начинающим администраторам и тем, кто делает первые шаги в сфере ИБ. А еще менеджерам, если им нужно оперативно разобраться в вопросе. Веб-приложения — [все еще] любимая мишень В прошлом году количество кибератак на российский бизнесвырослов 2,5 раза. В основном злоумышленников интересовали объекты критической и промышленной инфраструктуры, финансовые организации и телекомы, но под удар нередко попадали и другие компании из самых разных сфер деятельности. При этом чаще атаковали веб-приложения, что ожидаемо, так как подавляющее их большинство содержит простые для обнаружения уязвимости или ошибки в конфигурации. Во всяком случае специалисты из «Лаборатории Касперского», которыепроанализировалистепень защищенности веб-приложений на основе данных за 2021–2023 годы [в выборку попали проекты из России, Китая и стран Ближнего Востока], отметили, что 70% приложений имели недостатки, связанные с контролем доступа, а 61% — были подвержены межсайтовому скриптингу. Наши исследования показывают более внушительные цифры — доля веб-приложений, которые содержат уязвимости, позволяющие успешно атаковать их и пользователей, может доходить до 98%. Так, руководствуясь тем, что новое — (не очень) хорошо забытое старое, возобновил работу крупный ботнет Mozi, который использует известные уязвимости вродеCVE-2017-9841,CVE-2018-15133иCVE-2021-41773, позволяющие атаковать незащищенные сайты. В частности, первая уязвимость даёт возможность удалённо выполнять PHP-код и получать несанкционированный доступ к серверу веб-приложения. Злоумышленникам достаточно направить ресурсу специально подготовленный HTTP POST-запрос. Еще пять лет назад этот ботнет включал порядка 1,5 млн зараженных устройств. Считалось, что с ним было покончено в 2023-м, когда вся сеть перешла в неактивный режим после рассылки специального UDP-сообщения. Но некоторые технические изданияписали, что ботнет может вернуться, а человек, разославший команду, готовит зараженные системы к новой «полезной» нагрузке. Так ипроизошло— судя по всему, элементы инфраструктуры Mozi были интегрированы в новый ботнет Androxgh0st. Хуже всего то, что реализация и проведение подобного рода атак с каждым годом становятся все дешевле. Да, стоимость эксплойтов на черном рынке варьируется от десятков до сотен тысяч долларов, но по оценкам ИБ-специалистов, их приобретение, к сожалению, практически всегда окупается. Чистая прибыль хакеров от реализованной атаки может в пять раз превышать затраты на её подготовку. Ложку дегтя добавляет и растущая популярность систем ИИ. СогласноотчетуНационального центра кибербезопасности Великобритании, развитие нейронок подстегивает рост частоты и мощности кибератак [в том числе на веб-приложения]. Системы ИИ снижают порог входа в сферу киберпреступлений, а также открывают «новые возможности» тем, кто занимается противоправной деятельностью на потоке. Справедливости ради стоит отметить: поданнымIBM, злоумышленники пока не нашли способ эффективно использовать системы ИИ для разработки принципиально нового вредоносного ПО. Но успешно используют AI/ML-инструменты для работы «по классике». Какотмечаютв CERT-подразделении Университета Карнеги — Меллона, ИИ-решения применяют для рекогносцировок: поиска известных уязвимостей в инфраструктуре или «прочесывания» баз в поисках доступов к админкам сайтов. WAF: базовый минимум для выживания Необходимая и (порой) достаточная мера предосторожности для защиты приложения — своевременная установка патчей. Еще в 2018-м компания ServiceNowопросила3 тыс. специалистов по ИБ в организациях, столкнувшихся с утечками данных. Половина из них признала: инцидента можно было избежать, если бы нужное обновление было установлено вовремя. Справедливо и обратное: восстанавливаться после атак дороже и больнее, чем обеспечить профилактику: IBM и Ponemon Instituteутверждают, что на устранение последствий атак на веб-приложения бизнесу требуется в среднем 292 дня. Хорошо известным инструментом профилактики являются решения класса WAF (Web Application Firewall), которые защищают веб-приложения, обнаруживая и блокируя вредоносные HTTP-запросы, направленные на эксплуатацию уязвимостей и ошибок конфигурации. Вообще считается, что первые WAF вышли на рынок в конце 1990-х, когда участились атаки на веб-серверы. В 2002 году проект ModSecurity сделал эту технологию доступной для всех. И уже в 2003 году специалистыOWASPсформировали список самых распространенных уязвимостей для приложений — OWASP Top 10, который стал стандартом для обеспечения веб-безопасности. Можно сказать, что все поставщики технологий класса WAF ориентируются на этот стандарт при разработке своих инструментов защиты. С тех пор рынок WAF продолжил развиваться, особенно в финансовой сфере. С марта 2025 года наличие WAFстанетобязательным для компаний, которые работают с банковскими картами. Это — новое требование стандарта PCI DSS. Но защита финансовых данных — не единственная задача WAF. Помимо банков и финтеха, среди ключевых пользователейчислятся: медицинские и логистические организации, гос. компании, а также телекомы и ритейл. Они применяют WAF для обеспечения защиты веб-приложений на периметре и недопущения проникновения хакеров во внутреннюю сеть за счет эксплуатации их уязвимостей. Современные WAF-инструменты могут проводить: Анализ, основанный на правилах. Современные WAF используют несколько наборов правил. Первый — встроенный, или вендорский — который разрабатывает вендор WAF, основываясь на списках OWASP Top 10, CVE Top 25 и других источниках данных об угрозах (а также собственной экспертизе). Второй — кастомный — формируется непосредственно эксплуатирующими WAF администраторами. Такие правила пишутся адресно под защищаемое веб-приложение. Анализ, основанный на правилах. Современные WAF используют несколько наборов правил. Первый — встроенный, или вендорский — который разрабатывает вендор WAF, основываясь на списках OWASP Top 10, CVE Top 25 и других источниках данных об угрозах (а также собственной экспертизе). Второй — кастомный — формируется непосредственно эксплуатирующими WAF администраторами. Такие правила пишутся адресно под защищаемое веб-приложение. Синтаксический анализ запросов. WAF анализирует запросы на наличие в них определенных комбинаций символов, соответствующих различным векторам атак. Синтаксический анализ запросов. WAF анализирует запросы на наличие в них определенных комбинаций символов, соответствующих различным векторам атак. Поведенческий анализ. Проводится не только анализ синтаксиса запросов, но и корреляционный анализ их количества и частоты. Поведенческий анализ. Проводится не только анализ синтаксиса запросов, но и корреляционный анализ их количества и частоты. Несмотря на то, что принцип работы WAF не нов, а сама технология давно стала де-факто индустриальным стандартом, рынок развивается и трансформируется. Некоторые компании обращаются к открытым WAF-решениям вроде Wafris, который может блокировать атаки, использующие уязвимости во фреймворках, HTTP-серверах или Kubernetes Ingress Controller. Другой пример — SafeLine от китайской компании Chaitin Tech, специализирующейся на кибербезе. Однако файрволы с открытым кодом требуют определенного опыта и экспертизы в плане настройки и обслуживания. Обычно новым ИТ-проектам или не ИТ-ориентированным компаниям проще использовать готовый управляемый сервис, который предоставляет провайдер — в этом случае он берет на себя развертывание и мониторинг решения, снижая нагрузку на внутренние команды. В beeline cloud такой сервис называется Cloud WAF Pro. Что умеет наш WAF РешениеCloud WAF Proанализирует и фильтрует трафик на прикладном уровне. Оно блокирует SQL-инъекции и межсайтовое выполнение сценариев, защищает от атак, направленных на подбор паролей к учетным записям, и других поведенческих атак, характеризующихся большим количеством запросов. В общем случае схема решения выглядит следующим образом: Дополнительно для решения может быть включен модуль сканера уязвимостей в веб-приложениях. Найденные проблемы отображаются в соответствующем разделе модуля сканера уязвимостей. Для каждого события, которое распознает Cloud WAF Pro, формируется карточка атаки. В ней представлены сведения об инциденте: вредоносный пэйлоад, на который среагировала система, домен и путь, на который был отправлен вредоносный запрос, исходные данные про IP-адреса, часть запроса, которая была помечена как нелегитимная. Эти данные важны для анализа инцидента, корректировки правил и адаптации WAF к выявленным угрозам. Кстати, функциональность и возможности WAF-решений на примере Cloud WAF Pro мы разбираем на бесплатномкурсеDeep Security. Он подходит администраторам и начинающим специалистам по ИБ. Например, демонстрируя защиту от инъекций, мы рассмотрели пример с интернет-магазином JuiceShop — специально разработанным приложением от OWASP со «встроенными» уязвимостями. JuiceShop позволяет моделировать атаки и проверять эффективность WAF. В курсе мы показываем, как настроить систему Cloud WAF Pro, чтобы она могла обнаружить и предотвратить подмену изображений и описаний товаров через вредоносный код. beeline cloud— secure cloud provider. Разрабатываем облачные решения, чтобы вы предоставляли клиентам лучшие сервисы. "
  },
  {
    "article_id": "https://habr.com/ru/companies/cloud4y/articles/877988/",
    "title": "Как идёт строительство дата-центра Cloud4Y",
    "category": "Облачные технологии",
    "tags": "дата-центр, строительство, облачные сервисы",
    "text": "Привет! Продолжаем делиться подробностями того, как идёт строительство нашего ЦОД в Марфино. Что именно мы строим и почему решили этим заняться,рассказывали ранее. Сегодня — о том, как продвигаются работы на месте будущего дата-центра. На данный момент на участке почти завершены работы по сносу старых построек и выравниванию земли. Это важный шаг, который определит успех всего проекта, ведь именно от качества подготовки территории зависит долговечность и надёжность будущего сооружения. Но снос — это не просто разрушение. Многие кажущиеся небольшими постройки оказались буквально «зарыты в землю»: у них были большие подвалы и сложные фундаменты. Вот как это выглядит: Ломать, крушить и рвать на части Это не строчка из песни, а краткое описание того, что происходит на нашей стройплощадке.  После экспертизы и многочисленных обсуждений внутриCloud4Yи с экспертами решили, что снесём почти все здания, которые есть на участке. На это есть две главные причины. Первая причина: здания, которые мы решили сносить, находились в разном техническом состоянии. Некоторые выглядели так: А другие — так: Вторая причина: невозможность как‑либо приспособить их под наши нужды. Старые постройки не соответствовали никаким нормам, предъявляемым к современным дата‑центрам. Поэтому вот уже месяц рабочие ломают, сносят, выкапывают и совершают другие разрушительные действия в особо крупном размере. Это выглядит примерно следующим образом:  Одновременно со сносом организованапереработка строительного мусора. После сноса здания образовавшиеся материалы (бетон, кирпич, крупные булыжники) перемалываются  в камнедробилке, образуя курганы мелкой щебёнки. Если вы никогда не видели, как перемалывают строительный мусор, то можете заглянуть под спойлер. Вот так это выглядит со стороны: А вот крупным планом:  Зачем это делать вообще? Тут несколько причин. Экономия ресурсов. Переработка строительных отходов позволяет повторно использовать материалы, что снижает затраты на их утилизацию и покупку новых. Экономия ресурсов. Переработка строительных отходов позволяет повторно использовать материалы, что снижает затраты на их утилизацию и покупку новых. Создание устойчивого основания.Перемолотые в крошево здания используются для отсыпки территории. Они уплотняют грунт, улучшают дренаж и создают прочное основание для будущих построек. Создание устойчивого основания.Перемолотые в крошево здания используются для отсыпки территории. Они уплотняют грунт, улучшают дренаж и создают прочное основание для будущих построек. Экологическая польза. Такая переработка сокращает количество отходов, которые попадают на свалки, и уменьшает нагрузку на окружающую среду. Экологическая польза. Такая переработка сокращает количество отходов, которые попадают на свалки, и уменьшает нагрузку на окружающую среду. Снижение затрат на логистику. Переработка и переиспользование материалов на месте строительства уменьшает необходимость в транспортировке новых материалов, что выгодно в плане бюджета и позволяет уменьшить сроки строительства. Снижение затрат на логистику. Переработка и переиспользование материалов на месте строительства уменьшает необходимость в транспортировке новых материалов, что выгодно в плане бюджета и позволяет уменьшить сроки строительства. Отсыпка дроблёным материалом помогает создать пористую структуру, способствующую хорошему дренажу, предотвращает возможные проблемы с усадкой грунта. Повышается устойчивость, обеспечивается равномерное распределение нагрузки на грунт, соблюдаются другие требования, предъявляемые к такого рода сооружениям. Подготовка территории включает не только расчистку участка от растительности и мусора с последующим уплотнением грунта для повышения его несущей способности, но и прокладку дренажной системы, чтобы избежать затопления во время дождей или паводков. А ещё подведение инженерных коммуникаций, включая электричество, водоснабжение и интернет. С дренажом и коммуникациями работа ещё только начинается, поэтому сейчас показывать фактически нечего. Вот ещё пара кадров с места событий: Помимо строительных работ, мы продолжаем заниматься покупкой начального комплекта оборудования, чтобы уже в этом году запустить первые стойки с серверами. Они будут размещены к модульных контейнерах вроде такого: Позднее, когда закончим постройку зданий и укомплектуем его, основная часть оборудования будет расположена именно в защищённом здании. Хотя контейнеры на 10 стойкомест, скорее всего, останутся. Впрочем, подробнее о контейнерах мы расскажем в следующий раз. Также уже сейчас мы настраиваем системы видеонаблюдения, которые будут обеспечивать контроль территории. Строительство ведётся в соответствии с графиком, и уже в ближайшие месяцы начнётся монтаж ключевых систем. Открытие объекта планируется в следующем году. Вот только знаете, что? Мы вошли во вкус и купили в Мытищах участок под второй дата-центр на пару гектар. На первом объекте будем набивать шишки, а со вторым всё должно получиться попроще. Ну, мы на это рассчитываем. Следите за новостями здесь или в нашемTelegram‑канале! "
  },
  {
    "article_id": "https://habr.com/ru/companies/mws/articles/877898/",
    "title": "Безопасность CI/CD — базовая гигиена и реализация в разработке облака MWS",
    "category": "Облачные технологии",
    "tags": "cicd, MWS, разработка облака, облако",
    "text": "Привет, Хабр! Меня зовут Алексей Федулаев, я руководитель направления Cloud Native Security в МТС Web Services. С этой статьёй мне помогал Андрей Моисеев — DevSecOps из моей команды, и Иван Орлов — Tech Lead из команды Development Platform. Сегодня мы поговорим об основах DevSecOps на примерах GitLab CI/CD. Расскажем о простых методах, которые помогут улучшить безопасность вашего сервиса или продукта. Как CI/CD работает в облаке Разработка собственного облачного решения требует участия огромного количества инженеров. Большая кодовая база, множество инженерных команд накладывают определённые ограничения на разработку, в том числе и на обеспечение безопасности разрабатываемых сервисов и их окружения. Поэтому очень важно подходить с умом к организации процессов разработки и особенно важно уделять внимание вопросам безопасности. В MWS мы используем GitLab, поэтому на нём рассмотрим, какие базовые методы можно применять, чтобы обезопасить ваши продукты, которые могут быть и не связаны с облаками. Почему важно корректно распределить роли Представим ситуацию: разработчик пишет код, определённо кучу крутых фич и без ошибок. Затем он вносит изменения в ветку, в которой работал, допустим: main/release/etc. За такими изменениями не всегда получается уследить — разработчики зачастую выполняют несколько задач параллельно. Кто-то мог попросить пофиксить баг, кто-то — добавить контекст, выполнить минорные правки и так далее. Но в некоторых ветках по умолчанию может быть сделан деплой на Demo-stand или PreProd, и, когда обнаруживается проблема, она затрагивает всех — девопсов, тестировщиков, другие команды разработки, а это бывает больно и дорого. Чтобы избежать ситуаций с непроверенным кодом в важных ветках, нужно правильно настроить права доступа и запретить прямой пуш в дефолтную ветку. Первый шаг — ревизия настроек в GitLab, а именно проверка существующих ролей, их соответствия реальным полномочиям сотрудников. Например, хорошим базовым подходом будет убрать все привилегии и выдать их заново, понимая, какой сотрудник и куда у него есть доступ. Это автоматически запрещает прямой пуш в main-ветку. Давайте отдельно рассмотрим ситуацию с релизной веткой — тут цена ошибки сильно выше, потому что изменения могут пойти сразу на прод. Разработчик, столкнувшись с проблемами на сервере, может попытаться исправить ситуацию через git reset hard с последующим пушем. Это может привести к серьёзным последствиям, вплоть до падения продакшена, неработоспособности приложения и другим неприятным вещам. На практике любая проблема с GitLab становится проблемой девопса и он вынужден сталкиваться с огромным количеством однотипных обращений, а если девопс-культура не развита — это станет ​​monkey’job. Решение включает запрет force push и ограничение работы с определёнными ветками для конкретных групп пользователей. Например, ветка release MVP доступна только мейнтейнерам, с явным запретом force push. Защита может распространяться на несколько веток: main, pre-release, release, release MVP. При этом некоторые ветки, например pre-release, могут быть доступны как девелоперам, так и мейнтейнерам, но с отключённым force push. Дополнительный механизм контроля — работа с защищёнными тегами. Комбинация protected-веток и тегов позволяет сделать ваш релизный флоу идемпотентным. Но при некорректной настройке вышеперечисленных механизмов существуют способы обхода правил. Например, разработчик может создать новую ветку, внести изменения, создать merge request и самостоятельно его подтвердить. Или дождаться, когда кто-то по невнимательности или в спешке внесёт изменения. Реальный пример: сотрудник указал себя тимлидом в профиле, хотя был обычным разработчиком. Он аппрувил собственные коммиты, а другие сотрудники, видя аппрув от предполагаемого тимлида, не проверяли изменения тщательно. Как контролировать доступ к коду с помощью GitLab У GitLab есть настройки — Approval Settings. Если у вас версия дороже, чем Community Edition, их можно кастомизировать. Эти настройки позволяют настроить права на подтверждение конкретных merge requests. Например, позволяют убрать возможность самому аппрувнуть свой MR. В версиях GitlLab от Premium и выше в актуальной версии уже внесены настройки, запрещающие по умолчанию аппрувить свои изменения. Это работает даже в случаях, когда разработчик добавляет коммит в чужую ветку — он всё равно не сможет влить в неё изменения без аппрува. Ещёв GitLab существует система политик безопасности (Security Policies). Эти политики имеют простой интерфейс настройки, похожий на конструктор, что делает их удобными в использовании. Главное преимущество политик безопасности в том, что они имеют приоритет над правилами, установленными на уровне отдельных репозиториев. Это позволяет централизованно управлять правилами безопасности для множества репозиториев с помощью одной политики, что значительно упрощает администрирование и поддержание единых стандартов безопасности. Рассмотрим ситуацию, когда система контроля кода не позволяет просто так внести изменения и есть ответственный человек, проверяющий код. Возникает проблема, когда этот проверяющий может не знать какие-то детали разработки. Например, он может легко заметить очевидную проблему безопасности, такую как хранение ключей доступа непосредственно в коде репозитория. Но если речь идёт о специфической области, например фронтенд-разработке, где у проверяющего недостаточно компетенций, он может не распознать потенциальные проблемы или риски в предлагаемом коде. Для решения этой проблемы нужно настроить правила в системе контроля качества кода (Lint). Эти правила автоматически активируются при создании каждого запроса на слияние (Merge Request). Хотя процесс проверки может быть утомительным и занимать много времени, это необходимая мера обеспечения качества и безопасности кода. В качестве примера можно рассмотреть настройку правил Lint для различных веток — отдельно для бэкенда и фронтенда. При такой конфигурации запрос на слияние не может быть одобрен без проверки и подтверждения от специалистов соответствующего направления. Например, изменения во фронтенде должны быть одобрены фронтенд-разработчиками, а изменения в бэкенде — бэкенд-разработчиками. Эта функциональность доступна в том числе и в Community Edition GitLab. Пользователь с расширенными привилегиями может легко обойти важные механизмы защиты. Например, отключить обязательное прохождение pipeline, проигнорировать failing-тесты или разрешить force push. При этом разработчик может руководствоваться предыдущими результатами локальных тестов, проверкой своей фичи на локальном стенде или высоким приоритетом выполняемой задачи (business). Особую опасность представляет небрежное обращение с токенами доступа. При наличии требуемых полномочий любой пользователь может создать токен с максимальными привилегиями и поделиться им с коллегами через незащищённые каналы связи. При компрометации такого токена злоумышленники могут получить полный контроль над репозиторием. GitLab поддерживает ролевое управление доступом, так, например, роль Owner должна быть только у одного ответственного лица — обычно это тимлид или DevOps-лид, который отвечает за репозиторий. Роль Maintainer подходит для опытных разработчиков и DevOps-инженеров, которые помогают с настройкой и поддержкой CI. Большинство участников команды должны работать с правами Developer — это обеспечивает достаточную функциональность для разработки, но ограничивает доступ к критичным настройкам. При понижении прав с Maintainer до Developer пользователь теряет доступ к настройкам репозитория, что помогает предотвратить случайные или намеренные нарушения процессов безопасности. Такое чёткое разграничение ролей и ответственности является важным элементом безопасной разработки. Часто встречается ситуация неправильного распределения прав доступа к репозиториям. Типичный пример, когда в проекте множество пользователей с максимальными правами — Owners и Maintainers, но практически нет обычных разработчиков. Такой подход создаёт серьёзные риски безопасности. Как не допустить утечки секретов Проблема утечки секретов — одна из самых простых, но в то же время самых критичных угроз безопасности в разработке. Утечка может произойти как случайно, так и намеренно, при этом разобраться в её характере довольно проблематично. Все мы люди и допускаем ошибки, например, при разработке можно случайно запушить свои токены доступа или ключи от тестового окружения, иногда для упрощения тестирования можно вообще хардкодить базовые секреты в конфигурационных файлах. И чаще всего потом уже очень сложно вспомнить — куда какой секрет добавил. Последствия утечки секретов могут быть серьёзными: злоумышленники могут получить доступ к токенам доступа и пройти авторизацию по ним — с правами токена — или получить ключи ssh и развить атаку, например за счёт горизонтального продвижения по сети. Это техника, при которой нарушитель использует начальный доступ для поиска любых данных, которые может использовать для дальнейшей атаки — например, получив доступ к тестовой машине, он может проверить ssh-ключи, которые лежат там, и попробовать их использовать для доступа к инфраструктуре. Особенно опасна ситуация, когда разработчики намеренно оставляют креды и токены в коде «для удобства» или «по привычке». GitLab предлагает встроенное решение для борьбы с утечкой секретов — Secret Detection. Этот инструмент использует сканер GitLeaks, который анализирует историю коммитов и файлы в репозитории на предмет наличия секретов по своим паттернам. При подключении этой джобы к CI/CD-pipeline она автоматически проверяет код и блокирует merge request при обнаружении потенциальных секретов. Для эффективной защиты секретов важно заранее продумать процесс их хранения и ротации. Рекомендуется использовать специализированные хранилища секретов (Secret Management Systems) вместо хранения их в коде или конфигурационных файлах. Также необходимо обеспечить безопасное обращение с секретами, исключив возможность их случайной передачи или вывода в логи. При этом недостаточно использовать только поиск секретов, следует заранее определиться с процессом Secret Management. Подготовить хранилище секретов, процедуры ротации, отзыва и предоставления доступов.Токены доступа требуют особого внимания к их жизненному циклу. Даже после удаления пользователем токен остаётся валидным на сервере, поэтому нужно корректно выполнять отзыв доступов с удалением токенов или ключей, в том числе на серверах. В качестве примера утечки секретов можно привести ситуацию, когда компания X случайно добавила .git в слой публичного контейнера. При этом даже если удалить ключ из репозитория, сделать force push — его всё равно можно извлечь из истории git или, если образ был скачан, — из его слоёв. Чтобы гарантированно удалить секреты из истории гита и не наступить на те же грабли — можно использовать инструмент BFG. Подробнее об этом инструментев Github по ссылке. Отдельная проблема — работа с секретами в GitLab Variables. Существуют разные типы переменных — обычные, protected и masked. Protected-переменные доступны только в protected-ветках и билдах, однако их можно обойти, например конвертировав значение в Base64. Dependency confusion — атака основана на том, что пакетный менеджер может подтянуть вредоносную версию пакета из публичного registry вместо локального. Чтобы защититься от этого, используйте локальный registry с проксированием запросов. Используйте защищённые ветки, раннеры, переменные и теги для критичных операций. Некритичные операции могут выполняться на обычных раннерах. GitLab предлагает альтернативные способы хранения секретов через систему переменных. При создании переменной доступны два основных типа защиты: Masked Variables: определяют информацию внутри секрета как набор символов, которые не допускается выводить в логи сборки. Тем самым предотвращают случайную утечку. Masked Variables можно обойти через вывод значения в base64 encode — в таком случае GitLab не сможет понять, что выводится чувствительная информация, и отобразит строку в логах — дальше просто можно выполнить base64-decode и получить секрет в открытом виде. Masked Variables: определяют информацию внутри секрета как набор символов, которые не допускается выводить в логи сборки. Тем самым предотвращают случайную утечку. Masked Variables можно обойти через вывод значения в base64 encode — в таком случае GitLab не сможет понять, что выводится чувствительная информация, и отобразит строку в логах — дальше просто можно выполнить base64-decode и получить секрет в открытом виде. Protected Variables: доступны только в защищённых ветках и сборках, обеспечивая дополнительный уровень безопасности. Protected Variables: доступны только в защищённых ветках и сборках, обеспечивая дополнительный уровень безопасности. Секреты, определённые как protected, доступны только в protected-ветках репозитория — также дополнительно стоит использовать отдельные protected-раннеры, которые могут запускаться только в рамках пайплайнов protected-веток. Такой уровень сегментации позволяет отделить Dev-секреты, утечка которых может быть не особо важна, от Prod-секретов, которые используются в релизных версиях продуктов. Но даже приведённых методов может быть недостаточно — если у вас есть возможность, рекомендуем применять системы хранения секретов, например HashiCorp Vault. Эти системы позволяют настраивать детальные политики доступа на основе различных параметров: ветки разработки, идентификатора проекта и сервисных аккаунтов. Такой подход значительно снижает риски, связанные с утечкой секретов, и обеспечивает централизованное управление доступом. При внедрении инструментов безопасности они неизбежно повлияют на скорость сборки продукта, скорость деплоя на продакшен. При этом почти всегда есть возможность исключения файлов из сканирования (для снижения false-positive), и эта возможность может быть использована для отключения проверок безопасности. Так, в случае с gitleaks он предоставляет возможность игнорирования файндингов (найденных проблем) через указание их в файле .gitleaksignore — аналогичном флоу с .gitignore. Чтобы контролировать процесс и не допускать таких ситуаций, нужно: Выстраивать правильные процессы code review и merge approve с привлечением      ответственных за флоу тестирования, флоу безопасности, чтобы каждый юнит      проверял корректность своего порядка действий. Выстраивать правильные процессы code review и merge approve с привлечением      ответственных за флоу тестирования, флоу безопасности, чтобы каждый юнит      проверял корректность своего порядка действий. Реализовывать сканеры таким образом, чтобы без наличия определённых прав нельзя было повлиять на флоу их работы. Например, запретить внесение изменений в файл .gitleaksignore всем с ролью Developer — это можно сделать с помощью механизма CodeOwners. Реализовывать сканеры таким образом, чтобы без наличия определённых прав нельзя было повлиять на флоу их работы. Например, запретить внесение изменений в файл .gitleaksignore всем с ролью Developer — это можно сделать с помощью механизма CodeOwners. Важно понимать: правильная организация работы с секретами — не вопрос внедрения пары сканеров или регламентов, а важная часть культуры безопасной разработки, которая требует внимания всей команды. Как мы это внедрили в разработку облачной платформы MWS В процессе начальной разработки облака и развития сопутствующих процессов фокус смещается на реализацию функциональных возможностей, построение рабочего решения, разработку интеграций для сервисов. В разработке принимают участие сотни инженеров, десятки команд, в процессе обрастания функционалом создаётся огромное количество репозиториев с кодом, тестовых репозиториев для проверки гипотез, самописных пайплайнов для автоматизации. Становится всё труднее сосуществовать, а администрирование и исправление проблем CI/CD ложится на разработчиков, потому что невозможно нанять столько DevOps-инженеров. Мы в MWS решили унифицировать разработку с использованием сервисов Development Platform (некоторые используют аббревиатуру IDP), интегрировав часть решений и подходов, описанных выше в статье, в нашу Development Platform. Для контроля за безопасными настройками GitLab в рамках Development Platform мы разработали сервис-помощник Star Maker. Он работает как единая информационная точка входа по всем сервисам и многое знает про устройство облака: как связаны сервисы, из каких компонентов состоят, какой состав команд, роли разработчиков в проекте, кто когда дежурит, где смотреть дашборды. Цель Star Maker применительно к GitLab: — настроить все проекты GitLab в соответствии с требованиями безопасности и compliance; — минимизировать изменения настроек проектов и групп проектов командами разработки; — сообщить о внесённых изменениях команде безопасности. Например, с помощью Star Maker мы делаем ревью кода до попадания в master. Нам важно, чтобы код, попадающий в production, прошёл ревью от группы людей, прошедших обучение по compliance, — это тоже роль, в каждой команде такие люди есть, но это не любой Developer/Maintainer. Благодаря Star Maker мы можем помогать командам настраивать Approval rules и CodeOwners в своих проектах, чтобы approve кода от этих пользователей был обязательным. Также эти люди контролируют изменение конфигурации приложений в production. Новые проекты и группы проектов мы умеем создавать через Star Maker API. Так можно гарантировать, что созданные проекты соответствуют принципам, рассмотренным в статье. Для существующих проектов Star Maker выполняет непрерывное сканирование на предмет отклонения от рекомендуемых настроек — в случае расхождения сервис создаст Jira-тикет с предлагаемыми изменениями и, если команда согласна на изменение, приведёт настройки проектов к рекомендуемым. Помимо Star Maker, в Development Platform интегрировано решение типа Vault, для безопасного хранения секретов. Ещё в платформу интегрированы разные инструменты DevSecOps. Например, инструмент gitleaks, который нам позволяет выявлять наличие секретов в коде на ранней стадии. В следующих статьях мы расскажем подробно про сервисы Dev Platfrom в MWS. А пока можно посмотретьдоклад Сергея Киселёва— руководителя направления Development Platform MWS на DevOops. Простые правила безопасности Чтобы существенно повысить безопасность проекта, нужно соблюдать простые правила «гигиены»: 1. Используйте сетевую изоляцию для ваших окружений DEV TEST PROD. 2. Применяйте методы изоляции GitLab — protected branch/runner/tag/variable. 3. Выстраивайте процесс code review. 4. Настройте CodeOwners и отключите self-approve. 6. Настройте правила аппрувов для подразделений, чей код был затронут коммитом. 7. Используйте RBAC и мониторьте изменения прав. 8. Используйте базовые проверки безопасности: secret management, sast, dast, sca, container security, IAC sast. 9. Обеспечьте неизменный флоу проверок безопасности. 10. Храните все зависимости в локальном registry и запретите на раннерах доступ в интернет. В этом материале рассказали о базовых принципах безопасности CI/CD. В будущих статьях рассмотрим продвинутые атаки на CI/CD и методы защиты от них — не переключайтесь! "
  },
  {
    "article_id": "https://habr.com/ru/companies/timeweb/articles/877972/",
    "title": "Работа с Terraform: эмпирические правила",
    "category": "Облачные технологии",
    "tags": "timeweb_статьи_перевод, terraform, программирования, aws, string, copilot, devops, it-инфраструктура",
    "text": "Примерно с 2018 года я занимаюсь программированием инфраструктуры если не каждый день, то несколько раз в неделю. Я не утверждаю, что это позволяет мне претендовать на какой-то авторитет. Но за это время я определённо успел сформировать конкретные мнения по некоторым вопросам. В этой статье поделюсь некоторыми этими наблюдениями в качестве эмпирических правил, которым пытаюсь следовать при разработке для Terraform, но с тем же успехом вы можете применять их и в других языках для программирования инфраструктуры. 1. Не жертвуйте локальной разработкой Это настоящий флагман, так что, если бы мне потребовалось оставить всего один пункт, то я оставил бы этот! Как подсказывает мой опыт, такое жертвование выражается в двух формах. Во-первых, допускаются жёсткие зависимости от среды, в которой выполняются конвейеры. Например, вместо нормальных переменных Terraform используются инструменты замены токенов, либо общие модули копируются в каталог для развёртывания прямо во время выполнения. Это случается, если Terraform развёртывают с единственной целью — использовать его в конвейере. На самом деле, конвейеры нужны для развёртывания изменений в реальных окружениях. Но для этого вы не должны возводить такую систему, в которой инженер не смог бы прямо у себя на компьютере тестировать и отлаживать создаваемую систему — полностью или частично. Если система запускается локально, то она может запускаться и в конвейере, но обратное верно не всегда. Не допускайте, чтобыgit commit && git push && *вечно дожидаться, пока конвейер сообщит вам об опечатке*остался единственным способом протестировать изменение. Другой вариант полного отказа от локальной разработки — злоупотребление предусмотренными в terraform обобщёнными словарями (maps) применяя их в качестве модульных переменных. Если модуль рассчитан на то, что всё его содержимое будет передаваться вmap(string)—  а мне такое попадалось достаточно часто — то все, кто имеет дело с такими переменными, как разработчики, так и пользователи, работают вслепую, без всяких подсказок типов на уровне IDE или валидации. Такой подход грубо уродует старую добрую привычную разработку на локальной машине. Иногда без динамических словарей никак не обойтись — например, при работе с метками ресурсов. Но, если вы знаете, какие параметры понадобятся вашему модулю — определите их как отдельные переменные или какобъектс известными атрибутами. В первом примере мы летим вслепую. Во втором примере нам в качестве второго пилота помогает terraform-подобная система (нет, это не разновидность copilot). Если вам это кажется настолько очевидным, что даже в пояснении не нуждается, поверьте мне — нуждается. 2. Учитывайте, кто ваша аудитория При разработке модуля terraform держите в уме, для кого он делается. Иногда целевая аудитория – это инженеры платформ или инфраструктурные инженеры, а эти люди зачастую ценят возможность видеть систему в мельчайших деталях и контролировать их конфигурацию. Если скрывать от пользователей эти детали, им это может не понравиться. Программисты, в свою очередь, ценят возможность максимально быстро поднять и запустить инфраструктуру, так, чтобы при этом приходилось принимать как можно меньше решений. Ваш модуль будет пользоваться повышенной популярностью, если вы сможете минимизировать время, в течение которого программисту придётся отвлекаться от повседневной работы. Для таких людей старайтесь, насколько это возможно, ограничить обязательные параметры, а для всех остальных параметров предусмотритеразумные умолчания. 3. Старайтесь при помощи модулей снижать сложность распространённых паттернов «Трамбовка сложности» (compressing complexity) — это термин, подсмотренный всообществе Ruby on Rails. Сам приём — это вариант создания сбалансированных абстракций, которые нескрываютсложность, но всё равно не слишком забивают голову человеку, которому приходится ими управлять. Применительно к модулям terraform такая практика позволяет минимизироватьобязательныепараметры, но в то же время в достаточном количестве предоставитьопциональныепараметры с разумными и осознанно выбранными умолчаниями. Если в модуле хорошо утрамбована сложность, то разработчику легко приступить к работе с ним, поскольку не приходится досконально разбираться в модуле. Тем не менее, сохраняется возможность переопределить умолчания, если понадобится более полный контроль над модулем. . Умение подходить к работе с модулями именно таким образом пригодится, когда вы приступите к компоновке ресурсов и других модулей в более высокоуровневые пакеты. В таких составных модулях могут содержаться полезные абстракции, описывающие паттерны платформ, имеющие смысл именно в той предметной области, в которой вы работаете.Грегор Хопотлично объяснил эту концепциюна следующем примереиз финансового сектора. Пользователь определяет базу данных ledgered-database, в которой компонуется и конфигурируется ряд AWS-сервисов, которые, в свою очередь, хорошо поддаются повторному использованию. Пользователям такого модуля ничего не требуется знать о конкретной базе данных или технологиях маршрутизации событий — и, тем не менее, они могут быстро внедрить такой функционал в своё приложение. В качестве других примеров можно привести feature-storeдля управления признаками при машинном\tобучении и для предоставления их. Это могут быть, в том числе, сервисы для хранения данных, обеспечения работы конвейеров, версионирования и хостинга API. feature-storeдля управления признаками при машинном\tобучении и для предоставления их. Это могут быть, в том числе, сервисы для хранения данных, обеспечения работы конвейеров, версионирования и хостинга API. workflow-orchestratorдля определения потоков бизнес-задач и управления ими. В том числе, это могут быть сервисы для управления очередями, срабатываниями, состоянием и\tнаблюдаемостью. workflow-orchestratorдля определения потоков бизнес-задач и управления ими. В том числе, это могут быть сервисы для управления очередями, срабатываниями, состоянием и\tнаблюдаемостью. policy-enforced-apiдля хостинга защищённых сервисов API, в том числе, шлюза API,\tаутентификации, соответствия политикам\t(например, политикам ограничения частоты\tпередачи), мониторинга, шифрования и соответствия стандартам. policy-enforced-apiдля хостинга защищённых сервисов API, в том числе, шлюза API,\tаутентификации, соответствия политикам\t(например, политикам ограничения частоты\tпередачи), мониторинга, шифрования и соответствия стандартам. Модули такого рода очень ценны, поэтому не упускайте возможностей их создавать. Но учитывайте, что они полезны лишь в случае, если активно используются — так что вот вам ещё одна причина тщательно продумать, как их будет воспринимать пользователь. Вы ведь не хотите попасть вловушку конкурирующих стандартов! 4. Структурируйте код terraform с применением модулей, стеков и единиц развёртывания Здесь я перехожу к настоятельным предписаниям. По мере роста вашей базы кода на terraform приходится задумываться о том, как ее структурировать. Эта структура базируется на трёх каталогах верхнего уровня и, по моему опыту, по-настоящему хорошо работает. Опишу эти каталоги: modules: содержит небольшие модули, описывающие отдельные ресурсы или комбинации отдельных ресурсов, обеспечивающих, например, передачу данных в частной\tсети или управление учётными данными. modules: содержит небольшие модули, описывающие отдельные ресурсы или комбинации отдельных ресурсов, обеспечивающих, например, передачу данных в частной\tсети или управление учётными данными. stacks: содержит относительно крупные модули, в которых из мелких модулей составляются решения (именно здесь вам может встретиться один из тех высокоуровневых\tмодулей, о которых шла речь в предыдущем разделе). В стеках заключена сложность отношений между различными ресурсами, а также часто содержится код, упрощающий взаимодействие между этими ресурсами. Например, это может быть код для создания секретов, присваивания ролей, т.п. Также в стеках контролируется, какие вариации\tдоступны в разных окружениях —\tпредоставляется ровно столько переменных, сколько нужно. stacks: содержит относительно крупные модули, в которых из мелких модулей составляются решения (именно здесь вам может встретиться один из тех высокоуровневых\tмодулей, о которых шла речь в предыдущем разделе). В стеках заключена сложность отношений между различными ресурсами, а также часто содержится код, упрощающий взаимодействие между этими ресурсами. Например, это может быть код для создания секретов, присваивания ролей, т.п. Также в стеках контролируется, какие вариации\tдоступны в разных окружениях —\tпредоставляется ровно столько переменных, сколько нужно. deployments: потребляет стеки и представляет отдельныеинстансыилиокруженияконкретного стека. Именно здесь вы найдете конфигурации backend и provider для\tterraform, а также любые детали, которые могут отличаться от окружения к окружению (например, имена или размеры параметров). deployments: потребляет стеки и представляет отдельныеинстансыилиокруженияконкретного стека. Именно здесь вы найдете конфигурации backend и provider для\tterraform, а также любые детали, которые могут отличаться от окружения к окружению (например, имена или размеры параметров). В итоге у вас получится примерно такая структура: А вот наглядное представление: Обратите внимание:недавно компания Hashicorp а режиме бета-тестирования выкатила для своего облачного варианта Terraform новую фичу под названием “stacks”. Она не имеет никакого отношения к «стекам», о которых я пишу в этом разделе, хотя, концептуально первые и вторые «стеки» очень похожи. Взяв на вооружение такую структуру, вы сможете поддерживать чистую, удобную для масштабирования базу инфраструктурного кода, понятную разработчикам. Она упростит вам управление сложными инфраструктурами и поможет многократно использовать один и тот же код в разных проектах. На этом пока всё. Просто помните: инфраструктурный код может идолженбыть удобен для разработчика! Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud—в нашем Telegram-канале↩ ➤Три юзкейса Terraform, к реализации которых вам пора приступать ➤Три юзкейса Terraform, к реализации которых вам пора приступать ➤Разбиение файлов Terraform на компонуемые слои ➤Разбиение файлов Terraform на компонуемые слои ➤Как парсить данные с Python ➤Как парсить данные с Python ➤Новые возможности ECMAScript — атрибуты импорта и модификаторы шаблона регулярного выражения ➤Новые возможности ECMAScript — атрибуты импорта и модификаторы шаблона регулярного выражения ➤Внутрипроцессная трассировка системных вызовов с использованием цепочного загрузчика ➤Внутрипроцессная трассировка системных вызовов с использованием цепочного загрузчика "
  },
  {
    "article_id": "https://habr.com/ru/companies/selectel/articles/875782/",
    "title": "Как настраивать сети: облачные роутеры и публичные IP",
    "category": "Облачные технологии",
    "tags": "selectel, сети, ip, публичные ip, облачный роутер, it-компании, it-инфраструктура, сетевая связность, маршрутизация, сетевые технологии",
    "text": "Как настраивать сети: определения, типовые схемы, особенности Как настраивать сети: выделенный и облачный серверы Облачный роутер Подготовка сетей Первая сеть будет создана в глобальном роутере. Назначим ей адрес 192.168.0.0/24, а 192.168.0.1/24 — глобальному роутеру. Вторая сеть не будет соединяться с глобальным роутером и получит адрес 192.168.111.0/24. Эти сети будут подключены к облачному роутеру, который работает шлюзом. Обратите внимание, что можно использовать любой сервер, выполняющий его роль — например, виртуальную машинуна RouterOS. Создание и настройка серверов Настройка маршрутизации Публичный IP Предварительные условия Один сетевой порт может использоваться только одним сервером. Например, с определенным адресом связан сервер, работающий на порту 443. Мы не сможем развернуть другой сервер, который использует эти же адрес и порт: правило в таблице маршрутизации будет одно — src port в пределах своего IP‑адреса уникален. Такое принципиальное ограничение заставляет тщательно планировать распределение портов. К примеру, веб‑серверы обычно работают на портах 80, 443 и 8080. Каждый из них может служить точкой связи только для одного экземпляра самой программы. Аналогично, порт назначения может быть тоже только один — dst port в пределах своего IP‑адреса уникален. Проброс портов на примере веб‑сервера Проброс портов на примере SSH Заключение"
  },
  {
    "article_id": "https://habr.com/ru/companies/piter/articles/876674/",
    "title": "Прозрачное программное обеспечение",
    "category": "Облачные технологии",
    "tags": "информационная безопасность, прозрачность по, цепочки поставок, защита данных, облачные сервисы, sbom",
    "text": "Подробнее о книге Вводная часть и традиционные методы защиты Модели зрелости безопасности приложений Global Security Database Опорный принцип прозрачности в разных сферах ПО Актуальные рекомендации для государственного и частного секторов каждый шаг в цепочке поставок должен быть надежным и безопасным; в каждом шаге цепочки поставок должна использоваться автоматизация; среды сборки должны быть грамотно определены и защищены; для всех субъектов в среде цепочек поставок должна применяться взаимная аутентификация. Руководство по реализации S2C2F Практические руководства для поставщиков и потребителей Дальнейшие перспективы Кому пригодится эта книга"
  },
  {
    "article_id": "https://habr.com/ru/companies/documenterra/articles/877304/",
    "title": "Вокруг да около ChatGPT: AI-ассистенты, о которых вы могли не слышать",
    "category": "Облачные технологии",
    "tags": "ии, ai, chatgpt, альтернативы, система управления знаниями, оптимизация контента, создание контента",
    "text": "ChatGPT – лишь один из множества инструментов, меняющих подход к работе с текстом. В этой статье мы разберем его альтернативы, а заодно расскажем, как искусственный интеллект помогает ускорять создание контента и оптимизировать управление знаниями в системе Документерра. Если вам интересны практичные AI-решения и их применение в реальных задачах, эта статья для вас. В последние годыискусственный интеллектвсе активнее внедряется в здравоохранение, образование, промышленность и другие сферы. Такие инструменты, как ChatGPT, помогли компаниям оптимизировать рабочие процессы, улучшить взаимодействие с клиентами и повысить общую эффективность. Это ускорило развитие целых отраслей, включая IT, кибербезопасность и маркетинг, где AI-инструменты используются для автоматизации поддержки, создания контента и множества других задач. Однако с ростом интереса к ChatGPT компании начали сталкиваться с его ограничениями. Среди ключевых проблем — перегрузка сети в часы пиковых нагрузок, ограниченное количество бесплатных запросов и отсутствие узкоспециализированных функций. Например, задержки в доступе к инструменту часто мешают своевременному выполнению задач, а необходимость защищать чувствительные данные вынуждает искать более безопасные альтернативы. Еще одна важная проблема — недостаточная персонализация контента. Ответы, генерируемые ChatGPT, часто ограничиваются общими фразами и не содержат специфики, необходимой для выполнения сложных задач. Кроме того, стиль текста может создавать ощущение «роботизированной» речи. Пользователи замечают повторяющиеся синтаксические конструкции, кальки с английского языка и лексические шаблоны, которые характерны для модели, обученной преимущественно на англоязычных данных. Как признает сам ChatGPT: «Модель обучалась на больших объемах текста на английском языке, что может отражаться на языковых конструкциях и стиле ответов». В результате тексты часто выглядят как переводы, а не как полноценный результат работы на русском языке. Все эти факторы побуждают компании искать альтернативы, которые предлагают расширенные функции, лучше адаптированы под конкретные задачи и обеспечивают большую защиту данных. С развитием технологий появляется все больше решений, способных заменить или дополнить возможности ChatGPT. Чтобы наглядно продемонстрировать особенности работы ChatGPT, мы провели небольшой эксперимент: попросили его сгенерировать мета-описание для статьи на тему «Альтернативы ChatGPT». Вот результат: Узнайте о лучших альтернативах ChatGPT с инструментом искусственного интеллекта Документерры ИИ-корректором для быстрого создания контента и ИИ-помощником для оптимизации управления знаниями. Увеличьте продуктивность и эффективность командной работы вашей группы уже сегодня! На первый взгляд текст выглядит корректно, но при ближайшем рассмотрении становятся заметны характерные недостатки. Фразы вроде «лучшие альтернативы» или «узнайте о лучших альтернативах» — это стандартные шаблоны, которые ChatGPT использует для универсальности, но они часто кажутся искусственными. Живой текст скорее бы звучал так: «расскажем об альтернативах…» или «в этой статье представлены варианты…». Маркетинговые фразы, такие как «увеличьте продуктивность и эффективность», выглядят клишированными, неестественными и, главное, бессодержательными. Они не несут практической пользы для читателя и скорее напоминают роботизированный текст, чем человеческую речь. Шаблонная лексика и синтаксис ChatGPT указывают на недостаточную персонализацию контента. Такие тексты сложно воспринимать как живую, адаптированную речь, что снижает их эффективность в пользовательских или маркетинговых сценариях. Именно поэтому компании всё чаще комбинируют ChatGPT с другими инструментами, которые обеспечивают более точную настройку под задачи, лучшую локализацию и высокую конфиденциальность данных. В статье подробно разберём ключевые проблемы, возникающие при использовании ChatGPT, а также предложим решения, включая обзор альтернативных инструментов, которые помогут улучшить персонализацию, конфиденциальность и производительность работы с текстом. Почему пользователи ищут альтернативы ChatGPT? Искусственный интеллект сегодня помогает решать самые разные задачи, от создания контента до автоматизации клиентской поддержки. Однако, несмотря на универсальность и популярность ChatGPT, у пользователей есть весомые причины задуматься о его альтернативе. Разберём основные из них. С огромной популярностью ChatGPT его серверы периодически перегружаются, особенно в часы пик. Это приводит к замедлению работы, увеличению времени отклика или даже временным сбоям. Такие проблемы могут негативно сказываться на рабочих процессах, особенно в бизнесе, где важна бесперебойная работа. Альтернативные решения часто предлагают более стабильный доступ, минимизируя риски простоя. ChatGPT имеет ограниченное количество бесплатных запросов, что может быть неудобно для пользователей, изучающих его возможности, или для тех, кто выполняет большое количество задач. Многие альтернативы предлагают гибкие тарифы или щедрые лимиты, позволяя дольше работать без ограничений. Как универсальный инструмент, ChatGPT подходит для решения множества задач, но может не справляться с узкопрофильными запросами. Например, для написания технической документации или обработки сложных данных специализированные инструменты окажутся более точными и эффективными. ChatGPT генерирует тексты с базовым уровнем персонализации, что не всегда удовлетворяет пользователей. Тон, стиль и детализация ответа часто не соответствуют требованиям. Альтернативные инструменты предлагают больше возможностей для настройки под индивидуальные нужды. Для многих компаний интеграция ChatGPT в существующую инфраструктуру может стать вызовом. В то время как альтернативы часто предоставляют готовые API, плагины и расширения, упрощающие внедрение в корпоративные системы. Безопасность данных — один из главных факторов для компаний, работающих с конфиденциальной или чувствительной информацией. Многие альтернативные решения предлагают локальное развертывание или расширенные меры защиты данных, что делает их более надёжным выбором. Интерфейс и общая удобство работы с инструментом также играют важную роль. Некоторые альтернативы оказываются более интуитивно понятными, что упрощает обучение и повышает эффективность. Несмотря на свою универсальность, ChatGPT не всегда способен полностью удовлетворить потребности пользователей. Проблемы с перегрузкой, ограниченные бесплатные запросы, недостаток персонализации и вопросы безопасности вынуждают искать инструменты, которые могут предложить больше гибкости, удобства и защиты. Использование альтернативных решений позволяет компаниям улучшить рабочие процессы, повысить производительность и адаптироваться к специфическим задачам. Самые популярные альтернативы ChatGPT: обзор инструментов ИИ С ростом интереса к инструментам искусственного интеллекта, такие как ChatGPT, стали незаменимыми помощниками во многих сферах. Однако существует множество альтернатив, каждая из которых предлагает уникальные возможности. Ниже приведён список самых популярных решений, разделённых по их специализациям. Эти инструменты помогают упрощать управление проектами и автоматизировать рутинные задачи: ClickUp: универсальный инструмент для управления рабочими процессами, автоматизации задач, управления документацией и отслеживания проектов. ClickUp: универсальный инструмент для управления рабочими процессами, автоматизации задач, управления документацией и отслеживания проектов. Microsoft Copilot:  идеально подходит для автоматизации офисных процессов и генерации документов в экосистеме Microsoft. Microsoft Copilot:  идеально подходит для автоматизации офисных процессов и генерации документов в экосистеме Microsoft. Эти решения совмещают обработку текста, изображений и данных, обеспечивая гибкость: Google Gemini: новая мультимодальная ИИ-модель, способная обрабатывать текстовые и визуальные данные, а также генерировать изображения. Google Gemini: новая мультимодальная ИИ-модель, способная обрабатывать текстовые и визуальные данные, а также генерировать изображения. Chatsonic: инструмент для работы с данными в реальном времени. Поддерживает голосовые команды, генерацию изображений и интеграцию с новостными потоками. Chatsonic: инструмент для работы с данными в реальном времени. Поддерживает голосовые команды, генерацию изображений и интеграцию с новостными потоками. YouChat AI:  универсальный инструмент для диалогов, кодирования и генерации текстового контента. YouChat AI:  универсальный инструмент для диалогов, кодирования и генерации текстового контента. Для специалистов по контенту и маркетингу существуют решения, оптимизированные под их задачи: Jasper AI: генерация контента для маркетинга в различных стилях. Jasper AI: генерация контента для маркетинга в различных стилях. Writesonic: создание блогов, статей и чат-ботов. Writesonic: создание блогов, статей и чат-ботов. Semrush ContentShake AI: идеи и оптимизация контента с учетом SEO и анализа конкурентов. Semrush ContentShake AI: идеи и оптимизация контента с учетом SEO и анализа конкурентов. Surfer AI: создание SEO-оптимизированного контента с анализом SERP. Surfer AI: создание SEO-оптимизированного контента с анализом SERP. Undetectable.ai: создание контента, обход детекторов ИИ. Undetectable.ai: создание контента, обход детекторов ИИ. Для разработчиков существуют инструменты, облегчающие процесс написания кода: CodeWhispererот Amazon:  предлагает подсказки по коду в реальном времени. CodeWhispererот Amazon:  предлагает подсказки по коду в реальном времени. GitHub Copilot: помощник для написания кода, предлагающий решения в процессе работы. GitHub Copilot: помощник для написания кода, предлагающий решения в процессе работы. Инструменты, помогающие находить и анализировать информацию: Perplexity AI: поиск информации в реальном времени с цитатами и фактчекингом. Perplexity AI: поиск информации в реальном времени с цитатами и фактчекингом. Elicit:  помогает с аннотациями, написанием научных текстов и резюме. Elicit:  помогает с аннотациями, написанием научных текстов и резюме. DeepSeek: интеллектуальный поиск с поддержкой глубокого анализа данных и автоматического обобщения. Способность к рассуждению, может органически развиваться с помощьюобучения с подкреплением (RL)без необходимости в традиционнойтонкой настройке под контролем (SFT). DeepSeek: интеллектуальный поиск с поддержкой глубокого анализа данных и автоматического обобщения. Способность к рассуждению, может органически развиваться с помощьюобучения с подкреплением (RL)без необходимости в традиционнойтонкой настройке под контролем (SFT). Решения для общения и создания персонализированных диалогов: Claude: инструмент ИИ, способный работать в диалоговом режиме и придерживающийся правил этики; позволяет общаться с людьми в манере, похожей на естественную, благодаря использованию продвинутых языковых моделей. Claude: инструмент ИИ, способный работать в диалоговом режиме и придерживающийся правил этики; позволяет общаться с людьми в манере, похожей на естественную, благодаря использованию продвинутых языковых моделей. HuggingChat: бесплатный инструмент для диалогов с открытым исходным кодом. HuggingChat: бесплатный инструмент для диалогов с открытым исходным кодом. Character.AI: инструмент для создания динамических персонажей, управляемых ИИ, которые реагируют и ведут диалоги, очень похожие на человеческие. Character.AI: инструмент для создания динамических персонажей, управляемых ИИ, которые реагируют и ведут диалоги, очень похожие на человеческие. Для обучения и тестирования возможностей ИИ: OpenAI Playground: экспериментальная площадка для персонализации и тестирования моделей ИИ. OpenAI Playground: экспериментальная площадка для персонализации и тестирования моделей ИИ. Для взаимодействия с изображениями и данными в реальном времени: Meta AI:  Платформа поддерживает функции редактирования изображений, распознавание объектов и совместную работу через сервисы Meta, однако в России недоступна. Meta AI:  Платформа поддерживает функции редактирования изображений, распознавание объектов и совместную работу через сервисы Meta, однако в России недоступна. Перечисленные ресурсы представляют собой целый ряд инструментов и платформ на базе ИИ для различных сфер применения – от создания контента и маркетинга до научных исследований и написания кода. Искусственный интеллект в системе «Документерра»: возможности и перспективы Система управления документациейДокументеррапредлагает инновационный подход к работе с текстами, интегрируя передовые возможности искусственного интеллекта. Два ключевых инструмента —ИИ-корректориИИ-помощник— обеспечивают оптимизацию процессов, сокращают время на выполнение рутинных задач и повышают качество создаваемого контента. Интеллектуальный инструмент для всех, кто ежедневно работает с текстами. Его основные возможности: Автоматизация рутинных операций:С помощьюИИ-корректораможно сэкономить время на таких задачах, как создание, редактирование и форматирование документов. Это освобождает ресурсы для выполнения более значимых задач. Автоматизация рутинных операций:С помощьюИИ-корректораможно сэкономить время на таких задачах, как создание, редактирование и форматирование документов. Это освобождает ресурсы для выполнения более значимых задач. Умные рекомендации:Корректор автоматически улучшает текст, исправляет ошибки, оптимизирует структуру и повышает читаемость. Умные рекомендации:Корректор автоматически улучшает текст, исправляет ошибки, оптимизирует структуру и повышает читаемость. Применяя ИИ-корректор, команды получают не только качественный контент, но и возможность сосредоточиться на ключевых аспектах своей работы. Виртуальный ассистентИИ-помощникупрощает взаимодействие с документами и делает поиск информации интуитивно понятным: Экономия усилий техподдержки:Благодаря пониманию естественного языка, ИИ-помощник способен отвечать на вопросы пользователей, предоставляя релевантные ответы с указанием источников. Это снижает нагрузку на службу поддержки и улучшает восприятие документации. Экономия усилий техподдержки:Благодаря пониманию естественного языка, ИИ-помощник способен отвечать на вопросы пользователей, предоставляя релевантные ответы с указанием источников. Это снижает нагрузку на службу поддержки и улучшает восприятие документации. Мгновенный поиск и структурирование данных:Инструмент находит и анализирует необходимую информацию в считанные секунды. Ответы могут предоставляться как в виде сплошного текста, так и в формате удобных списков. Мгновенный поиск и структурирование данных:Инструмент находит и анализирует необходимую информацию в считанные секунды. Ответы могут предоставляться как в виде сплошного текста, так и в формате удобных списков. Поиск по закрытым публикациям:Для авторизованных пользователей доступна возможность поиска по закрытым документам, что делает использование документации максимально эффективным. Поиск по закрытым публикациям:Для авторизованных пользователей доступна возможность поиска по закрытым документам, что делает использование документации максимально эффективным. Постоянное улучшение:ИИ-помощник обучается и совершенствуется, что минимизирует ответы типа «Я не знаю» и улучшает пользовательский опыт (UX). Постоянное улучшение:ИИ-помощник обучается и совершенствуется, что минимизирует ответы типа «Я не знаю» и улучшает пользовательский опыт (UX). Популярные ИИ-инструменты, такие как ChatGPT, широко применяются в самых разных сферах. Однако их универсальность зачастую сопровождается ограничениями: Перегрузка сети и медленный отклик в часы пик. Перегрузка сети и медленный отклик в часы пик. Лимиты на бесплатные запросы, усложняющие использование. Лимиты на бесплатные запросы, усложняющие использование. Недостаток персонализации и узкоспециализированных функций. Недостаток персонализации и узкоспециализированных функций. Вопросы безопасности данных, особенно для корпоративных пользователей. Вопросы безопасности данных, особенно для корпоративных пользователей. Для компаний, которым необходимы более точные и специализированные инструменты, выбор альтернатив становится не просто опцией, а необходимостью. Документерра— это шаг вперёд в управлении документацией. Благодаря сочетанию ИИ-корректора и ИИ-помощника компании получают мощные инструменты, которые не только повышают эффективность работы, но и обеспечивают высокий уровень персонализации, безопасности и удобства."
  },
  {
    "article_id": "https://habr.com/ru/articles/876750/",
    "title": "Создание образа в Cloud-init",
    "category": "Облачные технологии",
    "tags": "cloud, cloud-init, linux, bash, yaml, облачные технологии, облачные сервисы, виртуализация, virsh, libvirt",
    "text": "Всем привет! Это первая моя статья на Хабре, поэтому судите по всей строгости! Я рыскал по просторам интернетов и не нашел простой информации по созданию своими руками образа cloud-init, поэтому своими глазами изучал официальные маны и методом проб и ошибок теперь имею представление о том, как всё же запустить этот интересный инструмент! Что такое cloud-init и с чем его едят? cloud-init - это стандартный многоплатформенный пакет, который используется для инициализации облачных виртуальных машин при первом их запуске. Он позволяет автоматически настраивать ВМ, применяя конфигурацию, переданную ей через метаданные. Основные задачи, которые решает cloud-init: Настройка сети: Установка статических IP-адресов, DHCP, DNS, маршрутизации. Настройка сети: Установка статических IP-адресов, DHCP, DNS, маршрутизации. Установка hostname: Задание имени хоста для ВМ. Установка hostname: Задание имени хоста для ВМ. Настройка ключей SSH: Добавление открытых ключей SSH для безопасного удаленного доступа. Настройка ключей SSH: Добавление открытых ключей SSH для безопасного удаленного доступа. Установка пакетов: Установка необходимых программ и утилит. Установка пакетов: Установка необходимых программ и утилит. Запуск скриптов: Выполнение произвольных пользовательских скриптов для настройки приложений и сервисов. Запуск скриптов: Выполнение произвольных пользовательских скриптов для настройки приложений и сервисов. Создание пользователей: Добавление новых пользователей и групп. Создание пользователей: Добавление новых пользователей и групп. Настройка часового пояса: Установка правильного часового пояса. Настройка часового пояса: Установка правильного часового пояса. И многое другое: cloud-init обладает большим набором функций, позволяющих гибко настраивать ВМ. И многое другое: cloud-init обладает большим набором функций, позволяющих гибко настраивать ВМ. Как cloud-init работает? Метаданные: При запуске ВМ, cloud-init получает метаданные (конфигурационную информацию) из источника. Обычно это:Meta-data service (API): В облачных средах (AWS, Azure, GCP) метаданные предоставляются через специальный HTTP API.ISO образ: Для локальных ВМ, метаданные часто передаются через файл конфигурации, расположенный на виртуальном CD-ROM (ISO-образе). Метаданные: При запуске ВМ, cloud-init получает метаданные (конфигурационную информацию) из источника. Обычно это: Meta-data service (API): В облачных средах (AWS, Azure, GCP) метаданные предоставляются через специальный HTTP API. Meta-data service (API): В облачных средах (AWS, Azure, GCP) метаданные предоставляются через специальный HTTP API. ISO образ: Для локальных ВМ, метаданные часто передаются через файл конфигурации, расположенный на виртуальном CD-ROM (ISO-образе). ISO образ: Для локальных ВМ, метаданные часто передаются через файл конфигурации, расположенный на виртуальном CD-ROM (ISO-образе). Обработка метаданных: cloud-init анализирует полученные метаданные и выполняет соответствующие действия по настройке ВМ. Обработка метаданных: cloud-init анализирует полученные метаданные и выполняет соответствующие действия по настройке ВМ. Логирование: cloud-init логирует все действия, что помогает при отладке. Логирование: cloud-init логирует все действия, что помогает при отладке. Делаем шаблон виртуальной машины Перед созданием образа, нам сначала необходимо сделать \"шаблон\" ВМ, с которого мы будем клепать и преднастраивать другие ВМ. Первым делом — ставим пакет cloud-init, если таковой отсутствует. После успешной установки пакета, необходимо добавить все службы в автозапуск: Так же необходимо создать свой конфиг по пути /etc/cloud/cloud.cfg.d/, где добавим следующее содержимое: Данный конфиг файл необходим, ибо по моему личному опыту было замечено, что зачастую при отработке cloud-init, он жалуется на datasource_list, а иногда обращается к нашему шлюзу в поисках meta-data. После проделанных махинаций, мы выключаем нашу шаблон ВМ (а если были неудачные попытки, обязательно прописываем cloud-init clean) и переходим к следующему веселому занятию. user-data и meta-data Для того, чтобы cloud-init мог взаимодействовать с нашим iso образом, который будет подгружен через cd‑rom в виртуальную машину, необходимо чтобы присутствовали два файла — user‑data и meta‑data. user‑data и meta‑data — это два типа данных, которые cloud‑init использует для настройки виртуальных машин при их первом запуске. Они служат для передачи различной конфигурационной информации, позволяя автоматизировать процесс развертывания и настройки. user-data • Определение: user-data — это конфигурационные данные, предоставляемые пользователем, которые определяют, как cloud-init должна настроить ВМ. Это, по сути, инструкции, которые вы даете cloud-init, чтобы ВМ была настроена нужным вам образом.• Содержание: user-data может содержать: Команды shell script: Вы можете задать последовательность команд, которые будут выполнены при первом запуске ВМ. Команды shell script: Вы можете задать последовательность команд, которые будут выполнены при первом запуске ВМ. Cloud-config директивы: Вы можете использовать специальный формат YAML (cloud-config), который позволяет настраивать более широкий спектр параметров ВМ (установка пакетов, создание пользователей, настройка сети и т.д.). Cloud-config директивы: Вы можете использовать специальный формат YAML (cloud-config), который позволяет настраивать более широкий спектр параметров ВМ (установка пакетов, создание пользователей, настройка сети и т.д.). Любые другие данные: Вы можете передавать любые текстовые данные, которые будут доступны внутри ВМ. • Формат: Наиболее распространенным форматом является YAML, но поддерживаются и другие форматы, например, текст или shell script. • Примеры использования user-data: Любые другие данные: Вы можете передавать любые текстовые данные, которые будут доступны внутри ВМ. • Формат: Наиболее распространенным форматом является YAML, но поддерживаются и другие форматы, например, текст или shell script. • Примеры использования user-data: Установка конкретных пакетов. Установка конкретных пакетов. Создание пользователей и групп. Создание пользователей и групп. Настройка сети (IP-адреса, DNS). Настройка сети (IP-адреса, DNS). Запуск пользовательских скриптов. Запуск пользовательских скриптов. Установка SSH ключей. Установка SSH ключей. Задание имени хоста. Задание имени хоста. Конфигурация часового пояса. Конфигурация часового пояса. meta-data • Определение: meta-data — это информация о самой виртуальной машине, предоставляемая платформой (облачным провайдером или гипервизором). Это не данные, которые вы задаете напрямую, а скорее информация о среде, в которой запущена ВМ.• Содержание: meta-data включает: ID инстанса: Уникальный идентификатор ВМ. ID инстанса: Уникальный идентификатор ВМ. Имя хоста: Имя, заданное платформой для ВМ. Имя хоста: Имя, заданное платформой для ВМ. IP-адреса: Локальные и публичные IP-адреса ВМ. IP-адреса: Локальные и публичные IP-адреса ВМ. Тип инстанса: Размер ВМ, количество vCPU, объем RAM. Тип инстанса: Размер ВМ, количество vCPU, объем RAM. Регион и зона доступности: Местоположение ВМ. Регион и зона доступности: Местоположение ВМ. Имена дисков: Список присоединенных дисков. Имена дисков: Список присоединенных дисков. Другая специфическая информация платформы: Информация о сети, безопасности, etc. • Формат: meta-data обычно передается в формате JSON или аналогичном структурированном формате. • Примеры использования meta-data: Другая специфическая информация платформы: Информация о сети, безопасности, etc. • Формат: meta-data обычно передается в формате JSON или аналогичном структурированном формате. • Примеры использования meta-data: cloud-init использует meta-data для получения IP-адреса ВМ, чтобы правильно настроить сеть. cloud-init использует meta-data для получения IP-адреса ВМ, чтобы правильно настроить сеть. meta-data помогает cloud-init определить регион и зону, чтобы выбрать правильные настройки. meta-data помогает cloud-init определить регион и зону, чтобы выбрать правильные настройки. meta-data предоставляет информацию об имени хоста, которое можно использовать при настройке. meta-data предоставляет информацию об имени хоста, которое можно использовать при настройке. Написание конфигурации и создание iso образа Сейчас будет просто пример конфигурации meta-data и user-data. Вотссылкана официальный мануал, где описан синтаксис. Содержимое файла meta-data instance-id - указывает id нашего образа (если по простому и без заморочек). Если вдруг вам лень каждый раз писать cloud-init clean - тогда просто меняйте iid. local-hostname - hostname ВМ, который установиться после загрузки и отработки нашего образа (вместо localhost поставится my-server). Содержимое файла user-data #cloud-config - обозначает, что данный файл относится к cloud-init. runcmd - запускает консольные команды (что заметно по скрипту). Вы можете даже на текущем этапе без просмотра мануала изменить содержимое user и meta data файлов под себя. Если вы вдруг потеряли ссылку на мануал -она здесь. Генерация образа Для генерации образа мы вводим следующую команду. ВАЖНО ВЫПОЛНЯТЬ ЕЁ В КАТАЛОГЕ С user-data и meta-data. Тут в -output указываем путь, где появится наш образ (либо оставляем так и он пояявится в каталоге, в котором вы находитесь). Всё, поздравляю, теперь вы без проблем можете подгрузить ваш iso через cd-rom к готовому шаблону vm и проверить, работает или нет. ВАЖНО - user-data СРАБОТАЕТ ЧЕРЕЗ ВРЕМЯ ПОСЛЕ ИНИЦИАЛИЗАЦИИ ВАШЕЙ ВМ, НЕ НАДО СРАЗУ ЕЁ РЕСТАРТИТЬ И СУДОРОЖНО ПИСАТЬ cloud-init clean, cloud-init init. Всем спасибо за чтение статьи, надеюсь вам получится достичь вашего желаемого результата! Пишите в комментарии свои вопросы, я по мере времени буду дополнять статью. "
  },
  {
    "article_id": "https://habr.com/ru/articles/876698/",
    "title": "Переход на новое окружение",
    "category": "Облачные технологии",
    "tags": "миграция данных, posgtresql, sql, dapper, techleads, облако",
    "text": "В этой статье я хочу рассказать историю о переходе на новое окружение – большой задаче, которую пришлось решать, будучи в роли Tech lead команды в одном из продуктов МТС. Статья может быть полезна как «новичкам»-руководителям, которые не знают, как подойти к решению больших и многоэтапных задач, так и просто сочувствующим. Я был бэкенд разработчиком и после ухода предыдущего технического лидера, меня назначили его преемником. Фактически, роль была похожа на швейцарский нож: CTO, технический лидер, team lead команды бекенда и project manager. И одним из первых «испытаний» которое предстояло решить с командой в очень короткий промежуток времени — это переход на новое окружение. Прежде чем рассказать, о чём именно идет речь, начну с небольшой предыстории. Изначально наша команда занималась продуктом А, но в какой-то момент продукт был поглощён крупной командой, которая должна была переписать его на свой лад, а затем – закрыть. Примерные сроки всего мероприятия составляли 12 месяцев. Нас же перевели на новый продукт – продукт Б, и, поскольку его нужно было быстро разработать и запустить, а юридически и технически (на тот момент) мы оставались командой, занимающейся продуктом А, было принято решение разворачивать продукт Б на тех же ресурсах, с теми же учетными записями, токенами и сертификатами, что и продукт А. И вот тут начинается история. Первый вопрос, который мне задали наши DevOps инженеры утром следующего дня после назначения: «нужно что-то делать». Действительно, до конца дедлайна оставалось два месяца, а понимания того, что нужно делать, – не было. Для начала мы решили сформулировать проблему, оценить её критичность и понять, как её можно решить. Как можно понять из предыстории, у нас была следующая ситуация: два несвязанных с точки зрения бизнеса продукта, работающих на одних и тех же серверах, использующих одни и те же микросервисы и учетные записи. При этом юридически — это разные продукты, с разными бюджетами, доступами и уровнями критичности. Так же присутствовал риск, что ресурсы продукта Б заберут несмотря на то, что там работает продукт А. Отсюда напрашивалось очевидное решение – переезд на собственные ресурсы, за которые мы платим, обслуживаем и отвечаем. Другими словами, переход на новое окружение. На одной из встреч совместно с DevOps инженерами был составлен первый вариант плана перехода. Я решил, что лучший способ понять, что нам нужно, – представить, что мы прямо сейчас совершаем переход. То есть исходить из того, что у нас есть два окружения, и мы начинаем переносить весь продукт на новые ресурсы. При составлении плана учитывали каждое действие, например, сборка и доставка кода на стенд, миграция данных, проверка интеграции с внешними системами, тестирование, запуск нового продуктового стенда. Так же мы опирались на следующие требования: Развертывание нового стенда должно быть параллельным старому, чтобы была возможность прекратить переход и вернуться к старому окружению; Развертывание нового стенда должно быть параллельным старому, чтобы была возможность прекратить переход и вернуться к старому окружению; Определить все точки входа для изменения данных; Определить все точки входа для изменения данных; Срок переноса продуктового стенда – не более 8 часов; Срок переноса продуктового стенда – не более 8 часов; Код в репозитории продукта А и продукта Б должен быть одинаковый на момент перехода, то есть вся конфигурация должна происходить через Vault. Код в репозитории продукта А и продукта Б должен быть одинаковый на момент перехода, то есть вся конфигурация должна происходить через Vault. В результате был составлен длинный список шагов и список вопросов, на которые предстояло найти ответы. Концептуально план перехода выглядел так: Ставим заглушки на сайт, останавливаем микросервисы, которые могут изменить сущности в базе данных на старом окружении; Ставим заглушки на сайт, останавливаем микросервисы, которые могут изменить сущности в базе данных на старом окружении; Мигрируем код, Vault, базу данных; Мигрируем код, Vault, базу данных; Переключаем тумблер работы с базой данных на новом окружении; Переключаем тумблер работы с базой данных на новом окружении; Раскатываем микросервисы на новое окружение; Раскатываем микросервисы на новое окружение; Тестируем новое окружение; Тестируем новое окружение; Запускаем все микросервисы, которые могут изменять сущности базы данных на новом окружении; Запускаем все микросервисы, которые могут изменять сущности базы данных на новом окружении; Снимаем заглушку с сайта. Снимаем заглушку с сайта. Резюмируя полученные данные, можно разделить переход на новое окружение на три основных этапа: миграция в MTC Ocean; миграция в MTC Ocean; отделение продукта Б от продукта А; отделение продукта Б от продукта А; переход с Microsoft SQL Server на PostgreSQL (далее расскажу зачем). переход с Microsoft SQL Server на PostgreSQL (далее расскажу зачем). Следующим шагом, мы начали декомпозировать каждый этап. Главная цель перехода – перенос работы продукта на новые ресурсы. Учитывая бюджет и оперативность получения ресурсов, а так же такие параметры как соответствие по безопасности, SLA, инструментарий ипатриотизм, наш выбор пал на MTC Ocean – собственное облачное решение, что-то вроде Yandex.Cloud. В самом переносе продукта на другие сервера было несколько основных проблем: На старом окружении продукт работал на Windows VM, что тянуло за собой IIS и прочие специфические вещи, влияющие на миграцию, тогда как на новом ожидалась работа на Linux серверах. На старом окружении продукт работал на Windows VM, что тянуло за собой IIS и прочие специфические вещи, влияющие на миграцию, тогда как на новом ожидалась работа на Linux серверах. Настройка Microsoft SQL Server (старше 2017) на Linux серверах (а также в условиях урезанных лицензий) оказалась проблематичной задачей. Поэтому появилась потребность в миграции на PostgreSQL. Настройка Microsoft SQL Server (старше 2017) на Linux серверах (а также в условиях урезанных лицензий) оказалась проблематичной задачей. Поэтому появилась потребность в миграции на PostgreSQL. Старое окружение нельзя было изменять, поэтому на новом требовалось всё заводить заново: AD-группы, политики доступа, инфраструктурные сервисы и технических пользователей. Старое окружение нельзя было изменять, поэтому на новом требовалось всё заводить заново: AD-группы, политики доступа, инфраструктурные сервисы и технических пользователей. Специфика работы некоторого функционала, например, работа с.pfxсертификатами, нуждалась в доработке, так как на VM создавалась общая папка со всеми сертификатами, куда могли обращаться микросервисы. На Linux так делать было нельзя. Специфика работы некоторого функционала, например, работа с.pfxсертификатами, нуждалась в доработке, так как на VM создавалась общая папка со всеми сертификатами, куда могли обращаться микросервисы. На Linux так делать было нельзя. В основном, этап включал инфраструктурные задачи, которыми занимались DevOps инженеры: заявки на выделение ресурсов, конфигурация серверов, написание нового CI/CD и разворачивание различных инфраструктурных сервисов (Vault, Prometheus, OpenSearch, ArgoCD). Результатом этапа стало новое окружение, поднятое на более мощных серверах с максимально приближенным к Infrastructure as a Code подходу, чего не было на старом окружении. Следующий этап – отделение продукта А от продукта Б. Проблема заключалась в том, что при использовании общих сертификатов и аккаунтов, а также общих методов и микросервисов, перенести продукт Б на другие ресурсы без влияния на продукт А было невозможно. Например, некоторые микросервисы работали с callback’ами, которые регистрировались во внешних системах и привязывались к аккаунту, другие аккаунты были привязаны к кошелькам с деньгами, откуда списывались средства за различные услуги. Также имелись общие jobs в планировщике задач и общее подключение к коммунальной Kafka. Общие аккаунты так же присутствовали во многих критических частях продукта, например, в микросервисах для оплаты или для SMS-рассылки. Это создавало определённые риски, включая финансовые. Поэтому в рамках этапа было выполнено ряд задач, связанных с отделением функционала, так же была оформлена документация об общих «участках» продуктов А и Б, где были четко описаны пересечения до и после реализации этапа, что позволило гарантировать, что в случае перехода, микросервисы не будут влиять друг на друга. Последний этап подготовки к переезду – это настройка PostgreSQL. Почему мы выбрали именно его – это внутреннее решение, исходя из потенциальных потребностей. Изначально планировалось, что мы самостоятельно поднимем свой PostgreSQL: настроим бэкапы, доступы и балансировку. Однако, оценив объём работы, было предложено воспользоваться Managed PostgreSQL в MTC Ocean, тем самым передав всю поддержку базы данных команде МТС, а самим сосредоточиться на миграции данных. Это решение оказалось правильным, хотя из-за специфики работы ролей и пользователей базы данных пришлось решать дополнительные задачи. После того, как вопрос с развертыванием PostgreSQL был решён, осталось еще два: Как мигрировать данные и проверять их целостность; Как мигрировать данные и проверять их целостность; Убедиться, что микросервисы работают с PostgreSQL так же, как с Microsoft SQL Server. Убедиться, что микросервисы работают с PostgreSQL так же, как с Microsoft SQL Server. Что касается миграции, после исследования объёма данных и отсечения баз данных и таблиц, принадлежащих продукту Б, объём оказался сравнительно небольшим – порядка <400 ГБ, поэтому, перенос осуществлялся с помощью скриптов, без привлечения сторонних провайдеров и библиотек. Однако поддержка PostgreSQL синтаксиса во всех микросервисах стала проблемой. В нашем проекте использовалась мини-ORM Dapper, работающая с SQL скриптами. Синтаксис Microsoft SQL Server и PostgreSQL отличается, что потребовало большого объема доработок со стороны разработчиков. Универсального решения, которое можно было бы применить сразу на все микросервисы не нашлось, поэтому для каждого микросервиса были выполнены доработки связанные работой с обеими базами данных, а также добавлена возможность использования флага, переключающего репозитории в зависимости от окружения. Эта задача оказалась самой объёмной, заняв около месяца. Из упущений этапа – это отсутствие валидации данных по итогам миграции, но в остальном процесс прошел гладко. Отдельно хочу рассказать про интеграции. Наш продукт взаимодействовал с большим количеством внешних систем – порядка 14 интеграций, треть из которых инициировала запросы к нам, а не наоборот. Чтобы определить, что нам нужно предусмотреть в плане интеграций, аналитики подготовили документацию со списком всех внешних систем, который включал в себя: контакты представителей, способы взаимодействия, бизнес-кейсы, авторизационные данные, IP-адреса. После этого мы связались со всеми внешними системами, оповестили их о предстоящих работах и уточнили потенциальные проблемы при взаимодействии с новым окружением. Для нескольких систем завели новые учетные записи, перевыпустили токены и сертификаты, что позволило безболезненно реинтегрироваться с критически важными системами. Из особенностей, которые стоило бы учесть в большой компании – наличие внутреннего firewall, где для доступа к любому ресурсу требовалось заказывать разрешения. Для некоторых IP-адресов заявки были заведены с задержкой, что стало отдельной проблемой. Поскольку в процессе перехода участвовала вся команда, требовалась чёткая координация. Для этого были разработаны инструкции по различным процессам, связанным с переходом. Например, детально расписаны шаги перехода: кто, что и когда должен делать от начала до конца. Также были составлены инструкции по откату – действиям на случай возникновения проблем. Определена «точка невозврата», после которой откат становился невозможен. Были составлены таблицы уровней доступа к ресурсам, ссылки на сами ресурсы и инструкция для отключения старого окружения. Настроены корпоративные доступы и написаны инструкции по работе с новыми инструментами. Был подготовлен план тестирования для каждого стенда: на dev-окружении тестировался весь бизнес-функционал, на preprod – только интеграционные сценарии. С командой были согласованы сроки перехода с учётом отпусков, проводились регулярные обсуждения для сбора вопросов и предложений, а также проводилась консультация команды эксплуатации. Переход прошёл успешно. В связи с недостаточным тестированием сетевых доступов, связанных с интеграциями, мы сдвинулись по срокам, но существенных ошибок получено не было. Но что бы я сделал иначе, будь у меня этот опыт: Интеграции.Нужно было уделить больше внимания интеграциям. Нужно было убедиться, что доступ до каждого адреса был получен, перепроверить все IP-адреса и тщательно разобраться в особенностях их работы (в частности, с легаси типа WCF). Интеграции.Нужно было уделить больше внимания интеграциям. Нужно было убедиться, что доступ до каждого адреса был получен, перепроверить все IP-адреса и тщательно разобраться в особенностях их работы (в частности, с легаси типа WCF). Валидация данных при миграции.Хоть проблем с этим и не возникло, но после того, как мы мигрировали данные, проверка валидации была ручным и хаотичным. Стоило об этом задуматься заранее и предусмотреть механизм валидации и целостности данных. Валидация данных при миграции.Хоть проблем с этим и не возникло, но после того, как мы мигрировали данные, проверка валидации была ручным и хаотичным. Стоило об этом задуматься заранее и предусмотреть механизм валидации и целостности данных. Корпоративные трудности.Работа с внутренними ресурсами, специалистами информационной безопасности, поддержкой МТС Ocean, с корпоративным firewall и WAF требовало много времени. Это нужно учитывать в планировании, что было сделано в недостаточном объеме. Корпоративные трудности.Работа с внутренними ресурсами, специалистами информационной безопасности, поддержкой МТС Ocean, с корпоративным firewall и WAF требовало много времени. Это нужно учитывать в планировании, что было сделано в недостаточном объеме. Поэтапный переход.Выполнять сразу три этапа одновременно было ошибкой. Поэтапный подход позволил бы минимизировать ошибки, но из-за сжатых сроков такой вариант был невозможен. Поэтапный переход.Выполнять сразу три этапа одновременно было ошибкой. Поэтапный подход позволил бы минимизировать ошибки, но из-за сжатых сроков такой вариант был невозможен. Декомпозиция:если есть возможность разбить большую задачу на части – сделайте это; Декомпозиция:если есть возможность разбить большую задачу на части – сделайте это; Пишите инструкции:на каждом этапе должно быть понятно, что и кому делать. Это позволит полностью видеть процесс и вносить в него корректировки до того, как дойдет до дела; Пишите инструкции:на каждом этапе должно быть понятно, что и кому делать. Это позволит полностью видеть процесс и вносить в него корректировки до того, как дойдет до дела; Имейте план Б:описывайте инструкцию по откату изменений, определите точку невозврата и что делать, если всё пошло не так, как планировали; Имейте план Б:описывайте инструкцию по откату изменений, определите точку невозврата и что делать, если всё пошло не так, как планировали; Организуйте коммуникацию внутри команды:регулярные встречи, обсуждения сроков и распределение ролей помогут поддерживать прозрачность перехода и избежать недопонимания; Организуйте коммуникацию внутри команды:регулярные встречи, обсуждения сроков и распределение ролей помогут поддерживать прозрачность перехода и избежать недопонимания; Убедитесь в целостности и консистенции данных:при миграции данных требуется быть уверенным в том, что данные не изменятся в момент миграции, поэтому убедитесь, что ни пользователь, ни очередь, ни планировщик задач, ни внешняя система не изменит сущность. Используйте заглушки для пользователей и флаги в конфигах для отключения функционала. Так же валидируйте данные при миграции, чтобы быть уверенным в том, что ничего не потеряли; Убедитесь в целостности и консистенции данных:при миграции данных требуется быть уверенным в том, что данные не изменятся в момент миграции, поэтому убедитесь, что ни пользователь, ни очередь, ни планировщик задач, ни внешняя система не изменит сущность. Используйте заглушки для пользователей и флаги в конфигах для отключения функционала. Так же валидируйте данные при миграции, чтобы быть уверенным в том, что ничего не потеряли; Тестирование:структурируйте бизнес-кейсы, определите важные, пользовательские, критичные и не критичные. При больших изменениях в коде имейте отдельный «план тестирования», по которому можно отслеживать статус тестирования; Тестирование:структурируйте бизнес-кейсы, определите важные, пользовательские, критичные и не критичные. При больших изменениях в коде имейте отдельный «план тестирования», по которому можно отслеживать статус тестирования; Интеграции:имейте полное понимание с кем и как вы интегрируетесь. Зафиксируйте все необходимые данные в документации, будьте на связи с представителями внешних систем; Интеграции:имейте полное понимание с кем и как вы интегрируетесь. Зафиксируйте все необходимые данные в документации, будьте на связи с представителями внешних систем; Уделяйте внимание документированию и ведению задач:ведите подробную документацию по всем процессам, включая список ресурсов, инструкции, бизнес-кейсы и планы отката, заводите задачи на каждое действие, отслеживайте их статус; Уделяйте внимание документированию и ведению задач:ведите подробную документацию по всем процессам, включая список ресурсов, инструкции, бизнес-кейсы и планы отката, заводите задачи на каждое действие, отслеживайте их статус; Учитывайте корпоративные процессы и ограничения:заранее планируйте дополнительные временные затраты на согласования, заявки на firewall, отпуска, болезни и другие внутренние процессы команды и компании; Учитывайте корпоративные процессы и ограничения:заранее планируйте дополнительные временные затраты на согласования, заявки на firewall, отпуска, болезни и другие внутренние процессы команды и компании; Релизы:учитывайте тот факт, что пока идет подготовка к крупным инфраструктурным задачам, развитие продукта не стоит на месте, поэтому команда может переключаться на продуктовые задачи, что может увеличить сроки. Релизы:учитывайте тот факт, что пока идет подготовка к крупным инфраструктурным задачам, развитие продукта не стоит на месте, поэтому команда может переключаться на продуктовые задачи, что может увеличить сроки. В заключении хочу сказать, что это был очень интересный опыт, который позволил с другой стороны посмотреть на работу продукта и на работу команды в целом. Технические детали были опущены в связи со спецификой доменной области, а так же с тем, что я хотел сделать упор на организационный аспект решения задачи.Команде – спасибо! "
  },
  {
    "article_id": "https://habr.com/ru/companies/h3llo_cloud/articles/875644/",
    "title": "Что не так с OpenStack и почти всеми российскими публичными облаками",
    "category": "Облачные технологии",
    "tags": "опенстек, сервис, openstack, инфраструктура, облако, виртуализация",
    "text": "Архитектура — заявленная как микросервисная, по факту — распределённый монолит, причём взаимодействие с компонентами вроде файловых хранилищ разного типа не вынесено в отдельные модули, а затянуто в ядро. 49 команд разработки, которые делят сервисы по зоне ответственности, а не архитектурной задаче. Десятки комитетов, которые добавляют бюрократии. Документация не соответствует реальности. Иногда баг в одном модуле исправляется специальной утилитой, убирающей его последствия от другой команды разработки, а не апдейтом исходного модуля. Код неоптимальный, сервисы работают медленно, есть бутылочные горлышки. Обновляться очень тяжело. ИБ часто делается по остаточному принципу. Выбор стека Ад подкрадывается незаметно Он ОЧЕНЬ запутанный Его почти невозможно обновить Он медленный. Нет, МЕДЛЕННЫЙ Ваш баг очень важен для нас Интеграция с Kubernetes и контейнерами: конфликт интересов Бюрократия Нестабильность отдельных проектов и «призрачные» сервисы Trove(Database as a Service). Trove создавался для управления СУБД (MySQL, PostgreSQL и др.) в стиле «DBaaS» в рамках OpenStack. Многие операторы вообще не используют Trove, предпочитая управлять базами отдельно (через Kubernetes Operators или внешние PaaS-платформы). Sahara(Big Data as a Service). Sahara предназначен для разворачивания кластеров Hadoop/Spark через OpenStack. С развитием Kubernetes и появлением разнообразных операторов для Big Data-решений (Spark Operator, Airflow и т. п.) популярность Sahara упала. Karbor(Application Data Protection). Он создавался как сервис для бэкапов и восстановления приложений в OpenStack, но полноценной популярности не снискал. Несколько лет назад проект был малоактивным, и на данный момент упоминания о Karbor в комьюнити редки. Rally(Benchmark & Testing). Rally задумывался для нагрузочного тестирования и оценки производительности сервисов OpenStack. Хотя он всё ещё используется некоторыми командами CI/CD, его активное развитие снизилось: многие операторы переключились на собственные фреймворки тестирования или используют Performance-тесты в других инструментах. Костыли При неполном или ошибочном удалении виртуальных машин (например, сбой в момент удаления, прерванная операция миграции) в базе данных Nova могут оставаться «осиротевшие» записи о зарезервированных ресурсах (CPU, RAM, диски). Следствие: гипервизор якобы «загружен» и отказывается принимать новые инстансы, хотя фактически ни одной рабочей ВМ не запущено. Рабочий костыль (workaround): использовать процедуру очистки «orphaned allocations» вручную или скриптами nova-manage placement heal_allocations, а также проверять состояние ВМ (stopped, error и т. д.), чтобы корректно завершать. При серьёзных сбоях — удалять «зависшие» записи напрямую из базы данных (что нежелательно в промышленной среде, но иногда это единственный выход). Официальная документация Nova Troubleshooting — Orphaned Allocations. Сценарий: том был выделен, но виртуальная машина, которая к нему подключалась, удалена с ошибкой. В результате в Cinder остаются «призрачные» тома в статусах deleting, error_deleting или даже «внешне доступные», но фактически они не прикреплены ни к одной ВМ. При повторных запросах на создание/удаление Cinder может сообщать «Не хватает места», «Ресурсы недоступны» и т. д., хотя физически дисковое пространство есть. Рабочий костыль: использовать команды cinder reset-state и cinder force-delete, вручную приводя томы в согласованное состояние. В некоторых случаях — чистить записи в базе данных Cinder или на уровне бэкенда (Ceph, LVM и т. п.), если стандартные инструменты не помогают. При сбоях в процессе удаления инстансов или сетевых ресурсов могут оставаться порты, не привязанные к активным VM. Аналогично могут «зависать» сами сети, если они не были корректно отвязаны. Это мешает созданию новых сетевых ресурсов и приводит к путанице в конфигурации (особенно если используется DVR, L3-HA или VLAN trunking). Рабочий костыль: проверять через openstack port list (или neutron port-list) все «зависшие» порты, удалять их вручную openstack port delete. Если удаление не срабатывает, то искать в логах ошибки (конфликты с другими службами), а в крайних случаях — править базу данных Neutron или восстанавливать целостность через пересоздание сети. Иногда процесс загрузки или удаления образа прерывается ошибкой сети, нехваткой места и т. д. В результате в Glance могут остаться «полусуществующие» записи: метаданные есть, но реального файла нет. Либо наоборот: файл в бэкенде хранится, а метаданные удалены. Это приводит к некорректным подсчётам объёма занимаемого места, а при попытке заново загрузить образ с тем же UUID возможно возникновение конфликтов. Рабочий костыль: проверка статусов образов (например, deleted, но всё ещё существующий) и при необходимости — ручная очистка метаданных или объектов в бэкенде (Swift, Ceph RBD и т. д.). Регулярное сканирование базы данных Glance и соответствующего хранилища для выявления рассинхронов. При сбоях в шаблонах (Templates) или ошибках в ссылках на внешние ресурсы (Nova, Neutron, Cinder) Heat может застревать в состоянии UPDATE_IN_PROGRESS или DELETE_IN_PROGRESS. Пользователь не может ни завершить операцию, ни перейти к следующему обновлению: «раскрутить» это бывает нелегко. Рабочий костыль: перевод стека в состояние FAILED через специальную команду heat stack update --mark-failed, а затем — повторное удаление. В самых тяжёлых случаях используют скрипты для массовой ручной очистки зависимых ресурсов. Если удаление отдельных ресурсов невозможно, то приходится корректировать записи в базе данных Heat. При сбоях в агенте сбора метрик (ceilometer-agent) или при неправильных настройках pipelining возникают «осиротевшие» записи о ресурсах, которые уже не существуют. Это приводит к некорректным показателям в мониторинге и сбоям при выставлении счетов (биллинг), особенно если реализована auto-scaling через Heat или другой механизм. Рабочий костыль: чистить «зависшие» записи в базе Gnocchi, скриптово пересоздавать индекс или вручную выгружать «битые» серии метрик. Отслеживать логи и периодически перезагружать сервисы телеметрии, чтобы избежать накопления устаревших данных. В веб-панели иногда случаются ситуации, когда пользователь инициирует операцию (создание VM, присоединение тома), но обновление страницы «застряло», и остаётся неочевидным, сработало действие или нет. Повторный клик порождает дублирующиеся запросы, в итоге в бэкенде появляются «лишние» ресурсы, а Horizon может не отобразить их корректно. Рабочий костыль: ручной мониторинг через CLI (openstack server list, openstack volume list, openstack network list) в параллель с Horizon, чтобы убедиться в статусе. Если обнаружены дублирующиеся или «зависшие» ресурсы, то удалять их из CLI и/или чистить записи в базах данных. Итог"
  },
  {
    "article_id": "https://habr.com/ru/articles/878596/",
    "title": "О том, почему Китай успешно развивает ИИ, несмотря на экспортные ограничения США",
    "category": "Искусственный Интеллект",
    "tags": "искусственный интеллект, сша, китай, чипы",
    "text": "В 2017 году Китай представил амбициозный план, целью которого было возглавить разработку ИИ, а также закрепить это лидерство к 2030 году. В 2020 году планировалось представить «знаковые достижения», т. е. продемонстрировать прогресс в области ИИ. Но вот настал 2022 год, OpenAI явила миру свой ChatGPT, Китаю же ответить на это было нечем. В то время ведущие китайские технологические компании всё ещё приходили в себя после регуляторного давления правительства. Суровые меры, действовавшие полтора года, сократили китайский технологический сектор примерно на 1 триллион долларов. Так китайским разработчикам чат‑ботов ИИ пришлось ждать почти год, чтобы получить одобрение правительства на выпуск своего продукта. Некоторые полагали, что строгая цензура властей заставит страну отказаться от притязаний в области ИИ. В то же время экспортный контроль, введённый администрацией Байдена всего за месяц до первого выпуска ChatGPT, был направлен на то, чтобы перекрыть доступ Китаю к передовым полупроводникам, необходимых для обучения крупных моделей ИИ. Без современных чипов цель Пекина — превосходство ИИ к 2030 году — казалась все более недостижимой. Но если оценивать положение дел сегодня, то можно сказать, что множество моделей от китайских разработчиков свидетельствует о том, что лидерство США в области ИИ пошатнулось. В ноябре 2024 китайский видеоигровой гигант Tencent показал Hunyuan‑Large, модель с открытым исходным кодом, которая, как показало тестирование компании, превзошла лучшие открытые модели в США по нескольким показателям. В конце декабря 2024 года, DeepSeek выпустила DeepSeek‑v3 — модель, которая опередила ИИ‑модели с открытым исходным кодом в популярной онлайн‑таблице лидеров и не уступает лучшим закрытым системам от OpenAI и Anthropic. Ещё до выпуска DeepSeek‑v3 на эту тенденцию обратил внимание Эрик Шмидт, бывший генеральный директор Google и один из самых влиятельных голосов в политике США в области ИИ. В мае 2024 года Шмидт уверенно заявлял, что США ещё 2–3 года будет сохранять лидерство в области ИИ и это очень большой срок. Однако в ноябре 2024 года, выступая в Школе управления имени Джона Ф. Кеннеди, Шмидт изменил свою позицию. Он сослался на достижения Alibaba и Tencent как на доказательство того, что Китай сокращает отставание. «Для меня это стало потрясением, — говорит он. — Я считал, что ограничения на поставку чипов будут их сдерживать». Победители гонки ИИ, помимо получения мирового престижа, могут изменить мировой баланс сил в свою пользу. Если правительству удастся автоматизировать труд при помощи ИИ‑агентов, это приведёт к значительному росту экономики, автоматизации труда и к достижению военного превосходства. Стремительные успехи Китая поднимают вопросы о том, будет ли экспортный контроль США на полупроводники достаточным для сохранения превосходства Америки. Чтобы создать развитый ИИ, нужны три компонента: данные, продвинутые алгоритмы и вычислительные мощности. Данные для обучения больших языковых моделей, таких как GPT-4o, обычно берутся из интернета. Это означает, что они доступны разработчикам по всему миру. Найти алгоритмы или идеи по улучшению систем ИИ также не составит труда, поскольку новые методы часто публикуются в научных работах. Даже если это не так, кадровый потенциал Китая больше, чем США. Передовые же чипы невероятно сложно производить, и, в отличие от алгоритмов или данных, они являются физическим товаром, который можно остановить на границе. На сегодняшний день поставкой высокопроизводительных полупроводников занимаются преимущественно США и её союзники. Американские компании Nvidia и AMD делят лидерство в производстве видеокарт для ЦОД для обучения ИИ. Их дизайн настолько сложен, что только Тайваньская компания TSMC способна производить такие первоклассные чипы. Для их производства в свою очередь требуется многомиллионная техника, которую TSMC закупает у голландской компании ASML. Правительство США стремилось использовать это преимущество в своих интересах. В 2022 году администрация Байдена ввела систему экспортного контроля ‑ мер, которые запрещают продажу высокопроизводительных чипов Китаю. Такие меры стали продолжением политики Трампа, который также стремился ограничить доступ Китаю к технологии изготовления чипов. Такая политика привела не только к ограничению поставок чипов в Китай, но и к созданию препятствий для внутренних производителей США. Тем не менее трудности с внедрением мер экспортного контроля начались ещё до того, как о них объявили. Китайские разработчики заранее запаслись чипами, которые попадали под ограничения. Так DeepSeek собрал кластер из 10 000 графических процессоров Nvidia A100 за год до того, как их запретили экспортировать в Китай. Контрабандная торговля сыграла не последнюю роль в обходе ограничений. Так в октябре Reuters сообщило, что в устройствах компании Huawei обнаружили запрещённые к экспорту чипы TSMC. Сообщается также, что китайские компании приобретали высокопроизводительные процессоры с помощью подставных компаний за пределами Китая. Другие обходили экспортный контроль, арендуя доступ к графическим процессорам у офшорных облачных провайдеров. В декабре 2024 года The Wall Street Journal сообщила, что США готовят новые меры, которые ограничат возможность Китая приобретать чипы через другие страны. Несмотря на то, что высокопроизводительные полупроводники стали недоступны из‑за ограничений, приобретать менее мощные чипы Китай всё ещё может. Решить, какие процессоры продавать можно, а какие нельзя, оказалось непросто. В 2022 году Nvidia изменила дизайн своего флагманского чипа, чтобы создать урезанную версию для китайского рынка, которая бы удовлетворяла требованиям правительства США. И даже такой чип всё ещё годился для ИИ‑разработки. Это побудило США ужесточить ограничения в октябре 2023 года. «По сути Китай покупал чипы такого же качества, что и обычные», — говорит Леннарт Хайм, ведущий специалист по ИИ и вычислениям в Центре политики технологий и безопасности корпорации RAND. Пока не ясно, удастся ли США держать ситуацию под контролем за счёт ограничений. В ноябре Tencent выпустила языковую модель Hunyuan‑Large, которая по нескольким показателям превзошла самый производительную модель Llama 3.1 от Meta. И хотя тесты отнюдь не совершенный инструмент для сравнения интеллекта ИИ‑моделей, производительность Hunyuan‑Large впечатляет хотя бы потому, что она обучалась с использованием менее мощных графических процессоров Nvidia H20. «Очевидно, им удаётся извлечь больше пользы от железа благодаря лучшему программному обеспечению», — отмечает Ритвик Гупта, автор исследования, консультант Отдела оборонных инноваций Министерства обороны. Её конкурент DeepSeek‑v3 считается самой мощной моделью с открытым исходным кодом. На её обучение ушло гораздо меньше вычислительной мощности, чем для ведущих моделей ИИ. Пока непонятно, как президент Дональд Трамп будет подходить к политике ИИ. Несколько экспертов сообщили TIME в ноябре, что они ожидают сохранения мер экспортного контроля и даже их расширения. В декабре США снова ввели ограничения, однако китайские компании предусмотрительно запаслись процессорами, которые попадали под новые санкции. «Всю эту стратегию нужно переосмыслить», — говорит Гупта. ‑Нет смысла точечно решать проблему. Вместо того, чтобы пытаться замедлить разработку больших языковых моделей, ограничивая доступ к чипам, США нужно сосредоточиться на предотвращении разработки военных систем искусственного интеллекта, которым требуется меньше вычислительной мощности для обучения». Хайм отмечает, что, хотя разрыв между США и Китаем сократился — китайские модели с открытым исходным кодом ничем не уступают американским — отставание в разработке закрытых моделей составляет примерно год. Он добавляет, что тот факт, что разрыв между моделями сокращается, не обязательно свидетельствует о том, что меры экспортного контроля не работают. «Давайте отойдем от этой бинарной модели экспортного контроля, работающей или не работающей», — говорит он, добавляя, что Китаю может потребоваться больше времени, чтобы почувствовать их укус. Количество вычислительных операций для обучения моделей ИИ сильно выросло в последнее десятилетие. Так на обучение GPT-4 от OpenAI, выпущенного в 2023 году, потребовалось, по некоторым оценкам, в 10 000 раз больше вычислений, чем GPT-2, вышедшего в 2019 году. Возможно, эта тенденция сохранится, поскольку американские компании, такие как X и Amazon, строят огромные суперкомпьютеры с сотнями тысяч графических процессоров. Понятно, что их вычислительные мощности превысят мощности для обучения современных моделей ИИ. Если это случится, Хайм прогнозирует, что ограничения на экспорт чипов из США помешают Китаю конкурировать в разработке ИИ. «Экспортный контроль в основном бьёт по количеству», — говорит Хайм. Он также отметил, что если «запрещённые» чипы всё же попадут в руки китайских разработчиков, то обучать ИИ‑модели в больших количествах им вряд ли удастся из‑за экспортных ограничений. «Думаю, что экспортные ограничения будут всё более негативно сказываться со временем, при условии, что вычисления не потеряют своей актуальности», — говорит он. В Вашингтоне «сейчас есть сомнения, стоит ли приглашать Китай за стол переговоров», — говорит Скотт Сингер, научный сотрудник Программы технологий и международных отношений в Фонде Карнеги за международный мир. Подразумевается: «Если США впереди, зачем нам что‑то делить?». «И всё же есть веские причины начать переговоры с Китаем по ИИ, ‑утверждает Хайм. — Китаю не обязательно быть впереди, чтобы стать источником катастрофического риска», — говорит он, добавляя, что постоянный прогресс Китая, несмотря на ограниченные вычислительные мощности, означает, что однажды он сможет создать ИИ с опасными возможностями. «Задумайтесь, какого рода переговоры нам придётся вести с ним, чтобы гарантировать безопасность для обеих сторон,» — говорит Сингер. "
  },
  {
    "article_id": "https://habr.com/ru/articles/878538/",
    "title": "У SAMURAI есть цель — zero-shot решение задачи Visual Object Tracking(VOT)",
    "category": "Искусственный Интеллект",
    "tags": "data mining, artificial intelligence, data science, machine learning",
    "text": "В рамках данной статьи мы обсудим новое zero-shot решение (то есть способное справляться с задачей без дополнительного обучения на данных из конкретного домена)   задачи Visual Object Tracking под названием SAMURAI(SAM-basedUnified andRobust zero-shot visual tracker with motion-AwareInstance-level memory). Эта модель продемонстрировала хороший перфоманс в задаче визуального трекинга, обойдя на нескольких бенчмарках своего прямого предка - SAM 2, а также многие supervised-решения(требующие дообучения под конкретный домен и задачу). Прежде чем перейти непосредственно к SAMURAI, я нахожу полезным кратко ввести читателя в курс дела в области основных вех развития SAM-based моделей. Начнём с родоначальника - самого SAM. SAM SAM задумывалась как фундаментальная модель для сегментации изображений, которая выполняет сегментацию на основе промпта, которым может выступать текст, точка (указывает на объект для сегментации), bounding box или маска и без дообучения адаптируется под различные домены и задачи. SAM обучался на огромном датасете SA-1B, содержащем более миллиарда масок и видел большое количество разнообразных данных, что помогает ей эффективно работать с новыми данными. Это делает её универсальным инструментом, который можно использовать для получения новых данных. Давайте теперь разберём из чего состоит SAM и как он работает Конструкция SAM выглядит следующим образом: После того как изображение подаётся в SAM, оно попадает в тяжёлый энкодер изображений. В качестве энкодера выступает предобученный Vision Transformer(ViT). Энкодер без особой потери в масштабируемости можно сделать потяжелее, поскольку эмбеддинги изображения достаточно получить лишь единожды при прогоне изображения через модель и их же можно несколько раз переиспользовать для последующих промптов. После того как изображение подаётся в SAM, оно попадает в тяжёлый энкодер изображений. В качестве энкодера выступает предобученный Vision Transformer(ViT). Энкодер без особой потери в масштабируемости можно сделать потяжелее, поскольку эмбеддинги изображения достаточно получить лишь единожды при прогоне изображения через модель и их же можно несколько раз переиспользовать для последующих промптов. Для получения эмбеддингов введённого пользователем промпта в архитектуре SAM имеется prompt encoder. В нём промпт в виде bounding box и точек кодируется с помощью позиционного энкодинга, который суммируется с обученными эмбеддингами для каждого типа промпта. Текстовый промпт кодируется с помощью текстового энкодера из модели CLIP, так как этот энкодер предназначен для перевода текста в векторное представление, которое может быть использовано совместно с визуальными данными. Промпт в виде маски с помощью свёрток приводится к размерности кодированного изображения и поэлементно складывается с ним. Для получения эмбеддингов введённого пользователем промпта в архитектуре SAM имеется prompt encoder. В нём промпт в виде bounding box и точек кодируется с помощью позиционного энкодинга, который суммируется с обученными эмбеддингами для каждого типа промпта. Текстовый промпт кодируется с помощью текстового энкодера из модели CLIP, так как этот энкодер предназначен для перевода текста в векторное представление, которое может быть использовано совместно с визуальными данными. Промпт в виде маски с помощью свёрток приводится к размерности кодированного изображения и поэлементно складывается с ним. Роль совмещения эмбеддингов изображения и промпта, а после получения итоговых предсказаний играет декодер. Кодированное изображение и промпт пропускается через два трансформерных блока, которые представляют собой слой self-attention в промпте и cross-attention между промптом и изображением в двух направлениях: от изображения к промпту и наоборот. После прохождения через трансформерные блоки происходит upsampling изображения до исходных размеров. Далее классификационная голова в виде линейного перцептрона(MLP) делает окончательное предсказание маски. Роль совмещения эмбеддингов изображения и промпта, а после получения итоговых предсказаний играет декодер. Кодированное изображение и промпт пропускается через два трансформерных блока, которые представляют собой слой self-attention в промпте и cross-attention между промптом и изображением в двух направлениях: от изображения к промпту и наоборот. После прохождения через трансформерные блоки происходит upsampling изображения до исходных размеров. Далее классификационная голова в виде линейного перцептрона(MLP) делает окончательное предсказание маски. Однако, стоит учесть что во многих случаях промпт может быть довольно многозначным, как показано на примере ниже. Как бороться с подобными кейсами? Чтобы разрешать такие неопределённости, SAM генерирует не одну маску, а три. Это позволяет модели рассмотреть все возможные случаи, так как маски зачастую имеют не более трёх уровней глубины: целый объект, его часть и подчасть. Но как из трёх масок выбрать наиболее подходящую? Для этого у SAM есть дополнительная голова, которая отвечает как раз за это. Она рассчитывает score каждой маски -. В качестве этого скора она учится предсказывать Intersection over Union(IoU) предсказанной и правдивой маски. В качестве результата модель выбирает маску с наибольшим скором. SAM-2 SAM проявил себя отлично в задаче сегментации изображений, продемонстрировав высокую способность к обобщению. Однако, у него есть довольно существенное ограничение - SAM умеет работать только с изображениями. Разработчики SAM-2 пошли дальше, адаптировав SAM для работы с видео. Почему SAM не умеет в видео? Ответ достаточно очевидный: он рассматривает каждое изображение изолированно, как сферического коня в вакууме, в то время как видео - это последовательность изображений, имеющих довольно сильную контекстуальную связь друг с другом. Чтобы научить SAM работать с видео, нужно придумать механизм, с помощью которого SAM сможет учитывать связь между изображениями. Итак, давайте взглянем на внутренности SAM-2 Основное нововведение SAM-2 - это банк памяти(memory_bank), в котором хранится последние N кадров из видео. Давайте рассмотрим подробнее Энкодер изображений - тут идейно без изменений относительно SAM. Единственное, SAM-2 использует иерархический энкодер изображений. То есть он выдаёт на выходе признаки разных уровней, как высокоуровневые(ближе к входу модели), так и более низкоуровневые(ближе к выходу модели). О том, зачем это нужно, будет чуть ниже. Энкодер изображений - тут идейно без изменений относительно SAM. Единственное, SAM-2 использует иерархический энкодер изображений. То есть он выдаёт на выходе признаки разных уровней, как высокоуровневые(ближе к входу модели), так и более низкоуровневые(ближе к выходу модели). О том, зачем это нужно, будет чуть ниже. После энкодера, изображение попадает в memory attention. Memory attention представляет собой несколько блоков self-attention между элементами памяти, cross-attention между изображением и памятью и MLP. Именно на этом этапе изображение приобретает контекстную информацию из прошлых кадров и предсказаний. После энкодера, изображение попадает в memory attention. Memory attention представляет собой несколько блоков self-attention между элементами памяти, cross-attention между изображением и памятью и MLP. Именно на этом этапе изображение приобретает контекстную информацию из прошлых кадров и предсказаний. Prompt encoder - без изменений относительно SAM Prompt encoder - без изменений относительно SAM Mask decoder - по большей части аналогично без изменений относительно SAM, за тем исключением, что в видео целевой объект может скрываться из виду, поэтому необходимо добавить дополнительную голову, которая будет предсказывать скор, отвечающий за наличие объекта в кадре -. Ещё одним нововведением являются skip-connections между разными уровнями иерархического энкодера и декодера(в обход memory attention) по аналогии с тем как это делается в архитектуре Unet(приложена ниже). С помощью этого информация о высоких пространственных разрешениях передаётся в декодер напрямую. Это помогает сохранить детали высокого разрешения, которые могут быть потеряны при проходе через энкодер. Mask decoder - по большей части аналогично без изменений относительно SAM, за тем исключением, что в видео целевой объект может скрываться из виду, поэтому необходимо добавить дополнительную голову, которая будет предсказывать скор, отвечающий за наличие объекта в кадре -. Ещё одним нововведением являются skip-connections между разными уровнями иерархического энкодера и декодера(в обход memory attention) по аналогии с тем как это делается в архитектуре Unet(приложена ниже). С помощью этого информация о высоких пространственных разрешениях передаётся в декодер напрямую. Это помогает сохранить детали высокого разрешения, которые могут быть потеряны при проходе через энкодер. Memory encoder - на этом этапе полученная на выходе из декодера маска прогоняется через свёрточные слои, затем суммируется поэлементно с закодированным изображением и, наконец, всё это \"полируется\" свёртками, после чего отправляется в memory bank. Memory encoder - на этом этапе полученная на выходе из декодера маска прогоняется через свёрточные слои, затем суммируется поэлементно с закодированным изображением и, наконец, всё это \"полируется\" свёртками, после чего отправляется в memory bank. Memory bank - банк памяти хранит последние N кадров, которые затем используются в memory attention для учёта контекста видео. Memory bank - банк памяти хранит последние N кадров, которые затем используются в memory attention для учёта контекста видео. Таким образом, SAM-2 научилась успешно решать задачи видеодетекции, обойдя по метрикам многие существующие трекеры. При этом её можно использовать и для сегментации статичных изображений, так как изображение - это видео, состоящее из одного кадра. SAMURAI Несмотря на то что SAM-2 показал хорошие результаты в задаче Visual Object Tracking, он также столкнулся с рядом трудностей. Во-первых модель до сих пор не очень хорошо чувствует себя в переполненных сценах с быстро движущимися, пропадающими из вида объектами, а также в сценах с похожими объектами(например, стая животных, где каждый экземпляр похож на остальных), так как SAM-2 часто отдаёт предпочтение визуальной схожести объектов, а не пространственной и временной согласованности. У модели отсутствует явная информация о движении, поэтому ей трудно различать похожие объекты. Ещё один источник ошибок - неэффективная конструкция памяти, в которой хранится только последние N кадров, вне зависимости от того, какого качества эти кадры. Такое нерациональное строение памяти может привести к распространению ошибок в видео. Например: На первом кейсе показывает ситуацию, когда на видео присутствуют несколько похожих друг на друга пингвинов. В этом случае модели трудно отличать их. Добавление дополнительной информации о движении этих объектов может дать модели больше важной информации, которую она использует для идентификации правильного объекта. На втором кейсе произошло распространение ошибки из-за того, что на нескольких кадрах объект было плохо видно на изображении => детекции получились с низкой уверенностью и неточные, а на последующих кадрах модель доставала из памяти эти кадры с неточной детекцией и на основе них делала следующее предсказание. Таким образом произошло распространение ошибки сквозь кадры и поведение модели приобрело высокую степень непредсказуемости. Настолько высокую, что вместо велосипеда она начала трекать автомобиль. Модель памяти, в которой модель будет не беспорядочно сохранять кадры в памяти, а выбирать из них только качественные, которые не приведут к подобному результату, поможет решить подобные проблемы. Рассмотрим архитектуру SAMURAI и разберём, какие нововведения помогли ей превзойти своего предшественника SAM-2. Одно из важных нововведений SAMURAI - использование модели движения для задачи трекинга, основанной на фильтре Калмана. На простом примере рассмотрим как работает фильтр Калмана: Допустим, мы глазами следим за мячиком, катящемся по полу. Пусть наше зрение неидеально и мы видим объекты слегка размытыми - соответственно не можем точно определить положение мяча в пространстве в текущий момент времени. Фильтр Калмана позволяет максимально точно вычислить положение мяча, задействуя не только информацию с нашего прибора(глаза), но и информацию о движении мяча(условно, если в текущий момент времени мяч находится в некоторой точке пространства и движется вперёд, то вряд ли он через секунду окажется на 10 метрах сзади начальной точки, это помогает внести бОльшую ясность в вопрос о положении мяча и уточнить показания прибора). В нашем случае \"прибор\", или \"глаз\" который следит за неким объектом - это сама модель детекции. А модель движения задействуется SAMURAI на этапе, когда изображение прошло через декодер и было предсказано M масок - она помогает выбрать наиболее корректную маску на основе информации о движении объекта. Разберём теперь подробнее фильтр Калмана и то, как он помогает делать предсказания: У нас есть вектор-состояние маски, который определяется следующим образом:  Это координаты и скорости bounding box'а маски, который определяется с помощью вычисления её максимальных и минимальных координат x и y. Также вводится ковариационная матрица состояния, которая задаёт неопределённость состояния и обычно является диагональной, при условии что все компоненты нескоррелированы. Также определяется используемая модель движения. Она задаётся оператором F преобразования из состоянияв состояние. С помощью неё мы предсказываем следующее состояние нашего объекта:  В нашем случае это модель равномерного прямолинейного движения(это не значит что объект на протяжении всего видео движется равномерно прямолинейно. Это значит что он движется только на промежутке между кадрами, то есть измерениями). Также предсказываем с помощью модели движения матрицу ковариации в следующий момент времени: , где Q - матрица шума модели перехода, которая описывает неопределённость модели и обусловлена её неточностью. Чем меньше наша модель подходит для описания движения, тем этот шум больше. Например, если движение объекта не равномерное, а равноускоренное, шум будет больше. Значение этой матрицы зависит от задачи и подбирается эмпирически. После получения предсказаний модели для каждой из M масок считаем следующий коэффициент:  Он представляет собой метрику IoU bounding box'a данной маски и предсказанного положения объекта. Это и есть коэффициент, который показывает качество предсказанной маски с точки зрения пространственной и временной согласованности. Окончательную маску выбираем следующим образом:  Для каждой маски считаем взвешенную сумму коэффициентовис коэффициентом, где- оценка IoU с реальной маской, которая предсказывается самой моделью(как в SAM). В оригинальной статье. Таким образом, модель при выборе итоговой маски из предсказанных, использует непосредственно информацию о движении объекта, что довольно сильно увеличивает качество предсказаний, особенно в сценах с множеством похожих объектов, ведь теперь у модели есть ещё один явный способ отличить объекты друг от друга помимо визуального сходства. Полученную маску используем как новое измерениенашего \"прибора\" для уточнения состояния. Считаем инновацию(невязку) между измерением и предсказанием  Где H - матрица - которая извлекает параметрыиз полного состояния Рассчитываем ковариацию невязок: , где матрица R описывает шум измерений. Тоже оценивается эмпирически. Вычисляем коэффициент Калмана:  Коэффициент Калмана показывает, насколько сильно мы верим измерениям(то есть нашему детектору) по сравнению с моделью движения. Действительно, из формулы видно что чем больше матрица шума измерений R, тем меньше коэффициент Калмана. В самом деле, если у нас зрение -10, то в задаче трекинга мяча нам ничего не остаётся кроме как довериться модели движения, а если у нас зрение достаточно хорошее, то мы можем больше доверять ему. Теперь остаётся скорректировать состояние x и матрицу ковариации P:   И далее по-новой. Память Memory Encoder остался неизменным относительно SAM-2, но вот структура памяти претерпела изменения: ранее там просто хранились N последних кадров, теперь хранение происходит более умным образом, учитывая информацию о движении, наличии объекта в кадре и уверенности модели в предсказании. Мы позволяем модели выбрать, какие кадры хранить в памяти на основе 3-х оценок, о которых шла речь выше:- коэффициент, показывающий, насколько данная маска соответствует модели движения,- коэффициент, отвечающий за оценку наличия объекта в кадре и- коэффициент, показывающий уверенность модели в предсказанной маске(предсказание IoU с правильной маской). Кадр считается хорошим тогда и только тогда, когда все три оценки превышают соответствующие пороги(эти пороги - гиперпараметры модели). В итоге в памяти хранитсяхороших кадров, которые выбраны моделью, это решает проблему распространения ошибки из-за хранения в памяти некачественных кадров. Это все нововведения модели SAMURAI. Остальное осталось аналогично SAM-2. Заключение В данной статье мы рассмотрели эволюцию моделей SAM, начиная с оригинального SAM, разработанного для сегментации изображений, затем SAM-2, который адаптировали для работы с видео, и, наконец, SAMURAI — актуальное SOTA zero-shot решение для Visual Object Tracking. SAMURAI смог преодолеть ограничения своих предшественников, благодаря интеграции модели движения на основе фильтра Калмана и улучшенной архитектуре памяти, отбирающей наиболее качественные кадры. Эти улучшения позволили модели лучше справляться с задачами трекинга в сложных сценах, где объекты быстро перемещаются, исчезают из поля зрения или имеют высокую степень визуального сходства. Таким образом, SAMURAI демонстрирует значительный шаг вперёд в области zero-shot трекинга, показывая конкурентные результаты не только по сравнению с SAM-2, но и с многими supervised-методами. "
  },
  {
    "article_id": "https://habr.com/ru/articles/878484/",
    "title": "Нейросетевой интеллект для NPC: Крафтовый интеллект",
    "category": "Искусственный Интеллект",
    "tags": "RPG, craft, NPC, искусственный интеллект",
    "text": "Нейронные сети в играх можно использовать не только для генерации картинок, звука и простыней текста. И даже не для того, чтобы предугадывать желания игрока. А что, если применить их для того, для чего они изначально задумывались – интеллектуального поведения и принятия решений? Начнём с малого: допустим, мы создаем NPC, которые умеют собирать предметы по заданным правилам.  Наша цель: создать «крафтовый» интеллект, т.е. такой интеллект, который выбирает, что будет делать NPC из предметов в его инвентаре. Такую штуку можно попробовать реализовать с помощьюконченныхконечных автоматов, поведенческих деревьев (behaviour tree) или ещё как-нибудь. Но, когда рецептов много, ингредиенты пересекаются, а потребности NPC меняются, такое дерево очень быстро разрастется до трудноподдерживаемого состояния. А если у нас вдруг что-то поменялось в технологической схеме? В статье мы кратко опишем нейросетевой подход к этой задаче и полученные результаты. Крафтовая система Моя система крафта довольно навороченная. Предметы состоят из определенного материала (в указанном количестве), могут быть стэкуемыми (складываться в один слот) и относится к разным типам (любому их сочетанию), например: Код привожу на питоне, для таких вспомогательных утилит он оптимален. Удобно проверять наличие элемента в множестве, работать со строковыми индексами, при желании можно подключать фреймворки машинного обучения вроде Pytorch. В самом проекте на C++, естественно, никаких строк, все обращения к свойствам предметов или рецептов по их индексу. Кроме предметов, есть рецепты. В рецепте указано, какой предмет получается, нужен ли инструмент (инструменты пересекаются с типами предметов) и какие нужны ингредиенты. Ингредиенты могут быть заданы по идентификатору (нужен конкретный предмет), по материалу (нужен определенный материал) и по типу (нужен определенный тип). Предмет в качестве материала можно использовать только если у него в типах есть‘raw’(сокращение от raw material – сырьё). Это единственный захардкоденный идентификатор, остальные типы, материалы и предметы могут называться произвольным образом. Например, для каменного молота нужна одна веревка (любой предмет с‘rope’ in type), одна рукоятка (любой предмет с‘handle’ in type) и один камень (тут прямо именно камень, никаких вариантов). Обратите внимание, что рецепты – это список, а не словарь. Т.е. в принципе, может быть, сколько угодно рецептов, дающих один и тот же предмет. Если представить это в виде диаграммы получится следующая картинка: Как вам? Хотите написать алгоритм под такую задачу? В данном примере граф сильно связанный: многие рецепты требуют веревку, а к этому типу относятся аж 3 предмета: веревка, жилы и кожаная лента. Привожу также саму функцию крафтаcraft_dict, которая находит индексы предметов в инвентаре (список/массив), которые будут использованы для крафта, и возвращает их, вместе с общим результатом (нет инструментов / нет ингредиентов / нет места / нет чего-либо / успех). Функция жадная, не ищет оптимальный вариант, а просто первый попавшийся. Для работы требует вспомогательную функциюfor_craft, которая возвращает какое количество ингредиентов для рецепта данный предмет может дать: В принципе, по этой функции уже видно, что её хочется вызывать как можно реже (и это вы ещё не видели её сишный аналог). Если нам надо проверить, что мы можем сделать, нам нужно прогнать эту функцию по всем рецептам, по всему инвентарю. И вот это то, от чего мы хотим избавится. Мы хотим построить нейронку, которая разок взглянув на инвентарь быстро бы дала нам ответ, какие рецепты возможны и какие нужно делать прямо сейчас. Реализация Чтобы это реализовать будем оперировать не инвентарем и предметами, а их эмбеддингами. Эмбеддинги или скрытое представление – это такой тензор (обычно одномерный, т.е. вектор), который шифрует некую сущность. Пускай у нас будет эмбеддинг всего инвентаря и эмбеддинги предметов. И есть некая нейронная сеть, которая обновляет эмбеддинг инвентаря, всякий раз, когда мы добавляем и удаляем оттуда предмет. Так как эти события происходят не слишком часто, мы несколько снижаем вычислительную нагрузку. Да, тогда получается, что NPC должен кроме инвентаря хранить где-то его эмбеддинг, но это не такая уж большая плата за результат. Это обновление должно быть основано на каких-то очень простых правилах: сложениях/умножениях, чтобы выполнятся быстро. Эмбеддинги предметов во время игрового процесса константы. Они оптимизируются здесь, в этом питоновском скрипте, ну а потом их можно просто распечатать и захардкодить в сишном коде, либо сделать загрузчик. Первый вариант проще и быстрее, а во втором придётся динамически выделять память, но зато можно сделать обновление даже без перезапуска игры. Иногда у нас есть неприятная ситуация, когда разный предмет в одном рецепте может выполнять разные функции (подходить и по типу, и по айдишнику, например). Зачастую в этих ситуациях нейроночка считает, что сделать предмет можно, а на самом деле не хватает ингредиентов. Если не предоставить никакой обратной связи, то можно получить замкнутый цикл. NPC будет раз за разом пытаться сделать предмет, который нейронка советует сделать. Для решения этой проблемы я ещё ввел «эмбеддинг дефицита», который может блокировать рецепты. Он обновляется, когда функция крафта вернула «нет ингредиента» и снимается при добавлении некоторых предметов в инвентарь. Можно формально разделить нейронку на две части: первая отвечает на вопрос «какой из рецептов можно сделать?», а вторая – «какой нужно сделать?». Первую часть можно рассматривать как врожденное знание, незачем это оптимизировать в ходе игры, можно обучить этому нейронную сеть заранее и зафиксировать. Мы ведь не хотим, чтобы NPC сто лет пытался собрать самолёт из каменных топоров в духе reinforcement learning. А вот вторую часть можно сделать динамичной, зависящей от желаний NPC. Сейчас он хочет сделать одно, завтра – другое, в зависимости от текущих потребностей. Для этого предусмотрен режим «целевой предмет», в котором минимизируется количество операций, необходимых для крафта желаемого предмета. Эксперимент Результаты проверены на простом модельном эксперименте. Каждый ход NPC под управлением нейронки предлагается выбор: поднять предмет с пола или попытаться скрафтить предмет. Предмет на полу каждый ход случайный из некоего списка: Какие мы можем посмотреть метрики? Failed Craft Rate: доля проваленных попыток крафта (ИИ пытается сделать предмет, но это невозможно). Craft Rate: отношение успешных попыток крафта к общему числу действий – показывает, насколько часто нейронка выбирает действие крафта. Привожу статистику решений для режима «целевой предмет». Делал 10 прогонов, каждый по 100 ходов, результаты усреднил. Failed Craft Rate равен 0 во всех случаях. Целевой предмет Доля крафта, % Количество созданных целевых предметов за 100 ходов Leather shield 7.2 2.7 Metal mace 22.4 8.9 Привожу так же статистику созданных предметов для одного из прогонов в каждом случае: Согласно рецептам, для создания кожаного щита нужно шило, вот NPC его и создает, а металлическая булава получается из дубинки. При этом NPC зачем-то создает волокна и металлический нож в ряде случаев. Но в целом я доволен результатом – ни разу нейронка не предложила сделать что-то, что невозможно сделать при данном инвентаре. Чтобы не было впечатления, что результат привязан к конкретному крафтовому дереву, попробуем другую технологическую схему. Для простоты назвал предметы абстрактными именами, ингредиенты взял только по айдишнику, но зато заложил длинную цепочку с боковым ответвлением, чтобы проверить, не застрянет ли в них NPC. Достаточно лишь поменять объявление предметов и рецептов в скрипте, чтобы всё работало. Естественно, что это объявление можно вынести, например, в экселевский файл и заставить скрипт парсить его. Тогда можно будет вообще не прикасаться к коду. Для простоты пускай на полу лежит только предмет А, задаём целевой предмет H и, вуаля, за 100 ходов NPC наштамповал:{'B': 18, 'E': 17, 'F': 17, 'G': 8, 'H': 4}; доля крафта - 64%; доля фэйлов – 0. Напомню, что целевой предмет можно задать динамически, делая поведение NPC более хитрым и непредсказуемым. А ещё можно разным NPC задать разные целевые предметы и предложить им обмениваться ими. Простор для фантазии огромен. Итог и дальнейшие планы Как видим, нейросетевой ИИ справляется с выбором рецепта крафта в конкретной ситуации. Он не предлагает делать то, что невозможно сделать и целенаправленно крафтит то, что задано. Мне же не надо писать бесконечный набор правил if…then, и нет нужды переписывать все эти правила, если что-то поменялось в технологической схеме. К сожалению, пока я не внедрил эту разработку в свой проект. Дело в том, что у меня технологическое дерево состоит не только из крафта. Например, металл можно получить из руды только в обжиговых печах, которые нужно строить. Сейчас NPC их строят и умеют ими пользоваться, но это реализовано традиционными алгоритмами. Планирую в скором времени сделать эмбеддинги построек и как-то подружить их с эмбеддингами предметов, чтобы нейросетевой интеллект мог принимать решения по всему технологическому процессу. В идеале хочется сделать, чтобы NPCразвивался в буквальном смысле – постепенно учился какому-то сложному поведению, а не чисто формально увеличивал свои статы и открывал умения по достижению нужных циферок. - Сэм, какое самое сильное животное в Dwarf Fortress? - Карп, Билл - Почему, Сэм? - Пока ты ходишь – карп плавает и качается, пока ты отдыхаешь – карп плавает и качается. "
  },
  {
    "article_id": "https://habr.com/ru/articles/878438/",
    "title": "99 вкладок браузера или «Бесконечность — не предел!»",
    "category": "Искусственный Интеллект",
    "tags": "rag, llm, итмо, искусственный интеллект, интервью с клиентами, расширения браузеров, стартап, опыт",
    "text": "Хабр, привет! Представьте: вы открываете браузер и... 99 вкладок, хаос, поиск той самой нужной страницы превращается в квест. Мы, командаИИнтеграция, столкнулись с этим лично, и именно так родился HoundApp — интеллектуальный ассистент, который призван помочь навести порядок. Всё началось с магистратуры магистратурыAI Talent HubотИТМОxNapoleon IT, где мы совмещаем учебу с акселератором стартапов.  В этой статье мы расскажем, как проводили первый кастдев, определяли целевую аудиторию и сколько шишек при этом набили. И так, начнем! Проблема ли? Сколько у вас сейчас открыто вкладок? 5? 50? Или все 99? Ответы наших респондентов были разными. Кто-то говорил: «Мне и с 50 комфортно!», а кто-то признавался, что теряет нужную информацию уже при пяти открытых. Проблема не в количестве, а в том, как мы управляем этим хаосом. Для некоторых это настоящая боль: нужная информация теряется, искать ее дольше, чем готовится кофе, а работа превращается в борьбу с браузером. Наша команда решила разобраться, насколько массово это явление и что с этим можно сделать. От первого опроса до первых инсайтов Начать исследование мы решили с небольшого эксперимента, выступив на DS-коммьюнити Сбера с докладом «Как бороться с информационным хаосом?». Во время выступления мы попросили слушателей ответить на несколько простых вопросов, чтобы понять их пользовательский опыт. Мы составили проблемное интервью в виде небольшого опросника, где завуалированно подводили к теме проекта (приём из книгиРоба Фицпатрика «Спроси маму»), для того чтобы избежать предвзятых ответов. Уже в самом начале презентации поделились ссылкой на наш опрос и приступили к обсуждению темы. Одни кивали, соглашаясь с описанием проблемы, вторые усмехались: «Ой, у меня 50 вкладок сейчас открыто, и ничего, нормально живу!», ну а третьи мрачно замечали, что браузер уже начинает подтормаживать от большого количества контента. А после  выступления к нам даже подошел один из участников с идей органайзера, который бы автоматически группировал вкладки и наводил порядок в браузере. Для нас это был важный сигнал:проблема реально существует, но ее восприятие сильно зависит от контекста использования браузера. Опрос показал, что пользователей больше всего раздражает именно путаница из-за избытка вкладок, а трудности с поиском в истории браузера оказались менее значимыми, чем мы думали. По результатам опроса мы расставили приоритеты среди проблем: Переполнение браузера вкладками.Пользователи не хотят их закрывать, опасаясь потерять важную информацию Переполнение браузера вкладками.Пользователи не хотят их закрывать, опасаясь потерять важную информацию Сложности в поиске релевантной информации.Прежде чем найти нужный ответ на свой вопрос, приходится перебирать множество страниц в поисковой выдаче Сложности в поиске релевантной информации.Прежде чем найти нужный ответ на свой вопрос, приходится перебирать множество страниц в поисковой выдаче Долгий поиск в истории браузера.Речь про поиск информации на страничках, которые когда-то были открыты и теперь хранятся в истории. Хотя процесс занимает много времени, по результатам опроса такая потребность возникает редко. Долгий поиск в истории браузера.Речь про поиск информации на страничках, которые когда-то были открыты и теперь хранятся в истории. Хотя процесс занимает много времени, по результатам опроса такая потребность возникает редко. Куда копать дальше? После опроса на DS Community стало ясно, что проблему с переполнением вкладок игнорировать нельзя. Но возникает вопрос: решение, которое мы собираемся предложить приведет к чему-то глобальному или просто добавит немного удобства? Чтобы понять масштаб проблемы, нужно было копать дальше. Так мы приступили к проведению проблемных интервью с потенциальной целевой аудиторией. Наши респонденты — коллеги, друзья, одногруппники, и родственники, которые регулярно погружаются в бесконечные потоки информации, учебу и преподавание. Мы расспрашивали их о пользовательском опыте: «Сколько вкладок обычно открыто?», «Как часто возникает переполнение браузера?», «Пытались ли гуглить решение?» Один из самых любопытных вопросов звучал так: «А как вы справляетесь с этой проблемой?» Проанализировав ответы мы составили список рекомендаций для эффективного управления вкладками. Поделимся лайфхаками в конце статьи :) И знаете, что мы услышали в ответ на эти вопросы? В большинстве случаев респонденты признавались, что их не устраивает хаос вкладок, но назвать это настоящей болью могли лишь немногие. На момент проведения проблемных интервью мы почти уже стояли на пороге готового MVP, ведь разработка шла в рамках стартап-трека проектной практики в ИТМО.  И полученные выводы были для нас как снег на голову :( Но, не унывая, мы довели идею до финальной стадии MVP, усвоив важный урок: перед началом любой разработки нужно чётко определить проблему, поговорить с потенциальными пользователями, изучить конкурентов и продумать финансовую модель. Что у нас получилось Изначальная концепция подразумевала интеллектуального ассистента, который не только помогает найти нужную информацию в истории браузера, но и наводит порядок в хаосе вкладок. Основой проекта стала RAG-система (Retrieval-Augmented Generation), способная анализировать данные и отвечать на вопросы пользователя. Мы рассматривали два варианта реализации: Полноценный веб-интерфейс.В такой системе пользователи могли бы добавлять ссылки, файлы и информацию из других сервисов, а затем задавать вопросы, чтобы получать релевантные ответы. Но на рынке уже есть сильные аналоги, напримерChatGPT. А для B2B сегмента свои продукты предлагаютHEBBIAиGLEAN. Полноценный веб-интерфейс.В такой системе пользователи могли бы добавлять ссылки, файлы и информацию из других сервисов, а затем задавать вопросы, чтобы получать релевантные ответы. Но на рынке уже есть сильные аналоги, напримерChatGPT. А для B2B сегмента свои продукты предлагаютHEBBIAиGLEAN. Веб-расширение для браузера.Этот вариант интереснее, его основное преимущество — возможность работать «на ходу»: ассистент отслеживает историю поиска и вкладки, избавляя пользователя от необходимости вручную добавлять данные в интерфейс. Но браузеров много, и писать под каждый свою версию расширения — непростая задача. Веб-расширение для браузера.Этот вариант интереснее, его основное преимущество — возможность работать «на ходу»: ассистент отслеживает историю поиска и вкладки, избавляя пользователя от необходимости вручную добавлять данные в интерфейс. Но браузеров много, и писать под каждый свою версию расширения — непростая задача. Мы решили убить двух зайцев одним выстрелом (мы любим природу) и объединить оба подхода: создать простенькое браузерное расширение, которое будет мониторить пользовательскую активностью в браузере, и веб-сайт, где можно воспользоваться всей мощью нашего функционала. Ну а почему бы и нет? Польза вдвойне, а усилия — в разумных пределах! Что внутри HoundApp? Мы понимали, что наша миссия — не просто реализовать удобный инструмент, а сделать незаметного помощника, который трудится в тени, экономя ваше время. Схема нашего творения представлена на картинке ниже. DS-часть нашего ассистента Создание интеллектуального ассистента стало серьезным вызовом. Для нас важным этапом являлась разработка системы, которая сможет эффективно анализировать веб-контент, находить нужную информацию, оставаясь при этом достаточно быстрой. Вот как мы подошли к решению DS задачи: Ретривер Ключевой частью нашей системы стал ретривер — компонент, который ищет релевантную информацию на основе эмбеддингов. Для этой цели мы использовалиLlamaIndexв связке сChromaDB. Пользователи посещают столько сайтов, что кажется, будто интернет создан исключительно для них. А нам, чтобы справляться с этим контентом, необходимо было внедрить умные механизмы индексации. Мы доверили эту задачу LlamaIndex — инструменту, позволяющему выполнить её минималистично. Мы, конечно, смотрели в сторонуLangChain, но LlamaIndex покорил нас своим удобным api. Для хранения эмбеддингов документов мы использовали ChromaDB — инструмент, который идеально подошел под наши требования, так как он позволяет хранить не только эмбеддинги, но и метаданные (например, теги и ссылки). А для генерации самих эмбеддингов мы выбрали мультиязычную модельmultilingual-e5-base, которая хорошо зарекомендовала себя вбенчмарках. Что по LLM? Развертывать LLM локально мы посчитали нецелесообразным из-за сжатых сроков. Поэтому мы сделали ставку на решения, предлагающие API и протестировали две модели: GigaChatпоказал средние результаты: он честно старался и боролся, однако плохо справлялся с выведением ответа в требуемом json-формате, а ещё часто ограничивал ответы из-за строгой цензуры. GigaChatпоказал средние результаты: он честно старался и боролся, однако плохо справлялся с выведением ответа в требуемом json-формате, а ещё часто ограничивал ответы из-за строгой цензуры. Mistralоказалась настоящей находкой! Эта модель справилась с задачей куда увереннее, чем Gigachat. К тому же, для нас было большим плюсом, что API модели доступно бесплатно, пусть и с некоторыми ограничениями. Mistralоказалась настоящей находкой! Эта модель справилась с задачей куда увереннее, чем Gigachat. К тому же, для нас было большим плюсом, что API модели доступно бесплатно, пусть и с некоторыми ограничениями. Метрики и тесты Чтобы проверить, насколько хорошо ассистент справляется с задачами, мы создали собственный набор тестовых данных. Вот как это выглядело: Сбор данных. Мы выбрали 60 веб-страниц с разным содержанием и разбили текст на батчи Сбор данных. Мы выбрали 60 веб-страниц с разным содержанием и разбили текст на батчи Используя ChatGPT, мы сформулировали по одному вопросу для каждой страницы, ответ на который содержится исключительно в определенном батче текстового контента. Иными словами, вопрос должен быть связан с конкретной частью страницы, а не с ее общим смыслом Используя ChatGPT, мы сформулировали по одному вопросу для каждой страницы, ответ на который содержится исключительно в определенном батче текстового контента. Иными словами, вопрос должен быть связан с конкретной частью страницы, а не с ее общим смыслом Далее на сгенерированные вопросы мы генерировали ответ, используя тот же ChatGPT. Сам себе режиссёр, сам себе зритель — идеальная схема для небольшого шоу ИИ ассистентов! Далее на сгенерированные вопросы мы генерировали ответ, используя тот же ChatGPT. Сам себе режиссёр, сам себе зритель — идеальная схема для небольшого шоу ИИ ассистентов! Сгенерировав датасет мы приступили к расчету метрик. Как основные мы выбрали следующие метрики: Соответствие ответа llm формату json —0.83 Соответствие ответа llm формату json —0.83 Модельная оценка близости ответа к эталонному —0.76 Модельная оценка близости ответа к эталонному —0.76 Эти тесты позволили нам оценить работу системы и проводить эксперименты с промптами и другими параметрами RAG, чтобы добиться максимального качества. Backend — так сказать, основа При выборе технологии для начала работы над проектом, мы остановились на FastAPI. Этот фреймворк обладает рядом преимуществ, таких как быстрая скорость разработки и легкая интеграция с другими инструментами. Нашей задачей было лишь создание API для работы с историей браузера. Когда дело дошло до тяжёлых операций, таких как получение эмбеддингов для контента с веб-страниц или отработка ретривера, мы задумались: а что, если они начнут тормозить весь процесс? Логичным шагом стало выделение этих задач в отдельный сервис: Во время пиковой нагрузки основное API остаётся доступным, а «тяжёлые» операции обрабатываются в своём темпе. Во время пиковой нагрузки основное API остаётся доступным, а «тяжёлые» операции обрабатываются в своём темпе. Если запросов становится слишком много, мы просто добавляем ещё несколько серверов, как будто ставим дополнительные кассы в супермаркете перед праздниками). Если запросов становится слишком много, мы просто добавляем ещё несколько серверов, как будто ставим дополнительные кассы в супермаркете перед праздниками). Такой подход делает систему устойчивой и позволяет справляться даже с резкими всплесками активности. Для эффективной организации работы нашего сервиса мы выбрали Temporal. Мы также могли бы использовать другую очередь, например, RabbitMQ, и прикрутить к ней веб-интерфейс для мониторинга. В таком случае для выполнения задач по расписанию потребовалась бы отдельная библиотека, а при усложнении логики обработки пришлось бы разрабатывать собственную стейт-машину. Однако, благодаря использованию Temporal, нам не нужно беспокоиться обо всем этом. Данный инструмент предоставляет готовые решения  из коробки для упомянутых проблем Frontend: лицо нашего ассистента Как говорилось выше, мы решили реализовать как веб-сервис, так и расширение для браузера. Давайте рассмотрим эту связку подробнее Сервис: SPA, где оживают взаимодействия Cервис реализован как SPA (Single Page Application), что идеально подходит для нашего ассистента. Мы не фокусируемся на SEO, что позволяет сосредоточиться на улучшении пользовательского опыта. SPA предоставляет гибкость в разработке, давая возможность легко добавлять функциональные элементы и создавать современный интерфейс без ограничений. На чём всё держится? Чтобы сделать интерфейс быстрым, гибким и приятным, мы выбрали проверенные инструменты: Vue.js 3 — лёгкий, но мощный фреймворк, который стал основой нашего фронтенда Vue.js 3 — лёгкий, но мощный фреймворк, который стал основой нашего фронтенда Vite — быстрый инструмент сборки, позволяющий экономить время на каждом этапе разработки Vite — быстрый инструмент сборки, позволяющий экономить время на каждом этапе разработки Vue Router — отвечает за плавные переходы между страницами Vue Router — отвечает за плавные переходы между страницами Pinia — легковесная библиотека, для создания хранилища Vue Pinia — легковесная библиотека, для создания хранилища Vue Внешний вид сервиса основан на: PrimeVue — библиотеке готовых компонентов, которые можно настраивать под любой вкус PrimeVue — библиотеке готовых компонентов, которые можно настраивать под любой вкус PrimeIcons — набору стильных иконок PrimeIcons — набору стильных иконок Sass — препроцессору, который добавляет гибкости в стилизации и избавляет от монотонного CSS Sass — препроцессору, который добавляет гибкости в стилизации и избавляет от монотонного CSS Архитектура: порядок прежде всего Мы решили не изобретать велосипед и использовали подходFSD (Feature-Sliced Design). Конечно, не строго по учебнику, но с умом. Вот как мы всё организовали: App — корневой слой приложения App — корневой слой приложения Pages — отдельные страницы, где разворачивается основное действие Pages — отдельные страницы, где разворачивается основное действие Widgets — крупные модули, которые можно использовать повторно Widgets — крупные модули, которые можно использовать повторно Features — функциональные блоки для решения конкретных задач Features — функциональные блоки для решения конкретных задач Entities — представление данных Entities — представление данных Shared — всё общее: от утилит до компонентов Shared — всё общее: от утилит до компонентов Для большей гибкости мы внедрили собственную системуlayouts, чтобы менять контент страницы без перезагрузки приложения. А для разграничения доступа к страницам добавили слой middlewares — всё чётко, никаких лишних привилегий. Расширение: минимализм как искусство Теперь про расширение. Это компактная и практичная часть нашего ассистента, работающая внутри браузера. Его задача проста: Захватывать содержимое веб-страниц Захватывать содержимое веб-страниц Извлекать текстовую информацию Извлекать текстовую информацию Отправлять её на сервер для анализа Отправлять её на сервер для анализа С точки зрения технологий, расширение тоже построено на Vue.js 3, а чтобы интеграция с браузером была максимально плавной, мы используем плагин crxjs для Vite. Ну а теперь, как говорится, достаточно слов — время для картинок! Также мы подготовиликороткое демо, чтобы наглядно продемонстрировать функционал MVP HoundApp. А с кодом нашего проекта (кроме приватного API) вы можете ознакомиться в нашемрепозитории. А что дальше? Мы осознали, что прежде чем отправляться на Марс, важно убедиться, что под ногами твердая почва, а не зыбучие пески или океан. Разработка, особенно в сочетании с инвестициями, требует постепенного и вдумчивого подхода. Важно прощупывать каждый шаг, чтобы не оказаться в неудобном положении перед партнерами, инвесторами и командой. Возможно, эта статья не об «успешном успехе», но она о чем-то более ценном — о реальном опыте, ошибках и движении вперед. Поэтому команда ИИнтеграция продолжает свой путь, за которым вы можете следить в нашемTelegram-канале. А как вы боретесь с проблемой переполнения браузера вкладками? Делитесь с нами — будем рады обратной связи! На путь борьбы с информационным хаосом отважились пятеро героев: Агапитов Денис– PMЕншов Владимир– DSЦарева Екатерина– NLP EngineerКапралов Александр– FrontendГаббасов Никита– Backend Обещанные лайфхаки для борьбы с вкладками Использование сочетаний клавишСочетаний клавиш значительно ускоряет работу за компьютером. Например, Ctrl+Tab и Ctrl+Shift+Tab позволяют переключаться между вкладками, Ctrl+W закрывает их (можно сразу несколько), а Ctrl+Shift+T восстанавливает последнюю закрытую. Если привыкнуть к этим комбинациям, работать в браузере станет гораздо удобнее Использование сочетаний клавишСочетаний клавиш значительно ускоряет работу за компьютером. Например, Ctrl+Tab и Ctrl+Shift+Tab позволяют переключаться между вкладками, Ctrl+W закрывает их (можно сразу несколько), а Ctrl+Shift+T восстанавливает последнюю закрытую. Если привыкнуть к этим комбинациям, работать в браузере станет гораздо удобнее Использование нескольких окон браузераЧтобы сосредоточиться на задаче, открывайте новое окно браузера (Ctrl+N) или используйте другой браузер для разделения постоянных и временных вкладок Использование нескольких окон браузераЧтобы сосредоточиться на задаче, открывайте новое окно браузера (Ctrl+N) или используйте другой браузер для разделения постоянных и временных вкладок Создание тематических папок и закрепленных вкладоксохраняйте часто используемые страницы в специально организованные разделы, чтобы минимизировать время на поиск нужной информации Создание тематических папок и закрепленных вкладоксохраняйте часто используемые страницы в специально организованные разделы, чтобы минимизировать время на поиск нужной информации "
  },
  {
    "article_id": "https://habr.com/ru/companies/garage8/articles/878220/",
    "title": "Продуктовый дизайн во власти карго-культа: как перестать бесконечно исследовать и начать делать",
    "category": "Искусственный Интеллект",
    "tags": "research, design, product design, rap, ux, ux дизайн",
    "text": "Всем привет. Меня зовут Артур Арсёнов, я Product Design Lead вGarage Eight. За 20 лет в продуктовом дизайне я успел поработать дизайнером, арт-директором, владел собственной компанией и стал дизайн-лидом. Десять лет назад я был одним из тех дизайнеров, кто говорил: «Давайте исследовать, давайте тестировать, давайте разбираться как можно глубже». И вот спустя десять лет и 200+ проектов я могу сказать, что исследования из решения превратились в проблему. Как так вышло и что с этим делать, разбираю в статье. Если в вашей компании из-за бесконечных исследований цикл разработки продукта растянулся на месяцы и годы, то го читать. Что такое карго-культ исследований и откуда он взялся Чаще всего сегодня на рынке встречается два основных типа компаний. Первые вообще ничего не исследуют и тратят ресурсы, делая продукты вслепую. Вторые превращают исследования в культ, поклоняясь им так же, как островные племена поклонялись грузам, сбрасываемым во время Второй мировой. Исследование из банального инструмента, который помогает быстро проверить гипотезу, в какой-то момент стало самоцелью. Вместо того чтобы взять на себя ответственность и сказать: «Ребят, мы делаем что-то не то», мы стали говорить: «А давайте это исследуем». Сверяться с пользователями стало модно, а исследователи стали очень востребованной кастой. Мы стали забывать, что, как у любого другого инструмента, у исследований тоже есть свои проблемы. Во-первых, пользователи врут, стараясь казаться лучше, так что даже глубинные интервью не всегда позволяют выявить реальные боли во взаимодействии с продуктом. Во-вторых, мало кто умеет нормально интерпретировать результаты. Раньше самой большой проблемой было то, что мы делали продукты для себя. Сегодня на смену ей пришла новая: мы топорно интерпретируем результаты и пытаемся сделать ровно то, что говорит нам коллективный пользователь. В итоге вместо того чтобы опираться на свою экспертность, мы поддаемся синдрому самозванца и начинаем по каждой мелочи сверяться с пользователем. В профессиональном плане это выглядит как попытка снять с себя ответственность и ничего не решать. Исследований много [не] бывает Еще десять лет назад потребность в исследованиях объяснялась очень просто: невозможно сделать что-то по-настоящему классное, если ты не понимаешь, что и для кого ты делаешь. То есть у исследования была конкретная цель — быстро проверить гипотезу. Сейчас у нас в распоряжении десятки методов, которые позволяют выявить предпочтения пользователей: от фокус-групп и глубинных интервью до А/В-тестов. Мы настолько увлеклись исследованиями, что на одном и том же продукте пытаемся использовать все виды сразу Я много раз видел, как сначала компании проводили бесконечное количество глубинных интервью, за которыми потом следовали фокус-группы. По результатам фокус-групп устраивали экспертные интервью, воркшопы, строили Customer Journey Map и прототипировали. На основе прототипов делали сценарные исследования, запускали А/В-тесты и проверяли в вебвизоре. И всё это только для того, чтобы поменять расположение кнопки. В моей практике был случай, когда 200 участников продуктовых команд два года не могли договориться, на чьих экранах будут точки входа и ключевые элементы продукта, которым регулярно пользуются клиенты. Я на собственном опыте понял: когда мы хотим сделать что-то классно, достаточно одного качественного исследования и небольшой группы участников. Если у вас есть пять человек, которые реально пользуются вашим продуктом, и вы нормально проводите интервью, то на основе этих данных уже можно делать выводы. Чем больше берешь респондентов, тем больше вероятность, что начнешь копать не туда. Каждый из респондентов будет давать плюс-минус похожую информацию, и на этапе интерпретации ты, скорее всего, сломаешься. Вместо этого при грамотной выборке из пяти человек можно провести глубинное интервью и построить базовые гипотезы. Да, здесь всегда есть место для ошибки. Но ошибка — это тоже результат, и нужно уметь правильно ее интерпретировать. Интервью тоже может стать циклом поставки. В методологии Continuous Discovery и Customer Driven Development интервью клиентов — это процесс валидации, который постоянно обновляет требования и с каждым новым интервью увеличивает понимание потребностей и снижает риски. И по-другому тоже можно, важно с умом. Мы сами всё усложнили Исследования стали отдельной субкультурой со своим языком. Мы на каждом шагу применяем специальные аббревиатуры и термины. Возьмем для примера кастдев. На западном рынке такого понятия вообще не существует. Это просто глубинное интервью, которое мы обернули новым словечком и стараниями одной известной личности превратили в стандарт рынка. Сам язык, на котором мы говорим, усложнился. Вместо «предположения, которое нужно проверить» у нас теперь есть «гипотеза интерфейса», «гипотеза решения» и еще с десяток других разновидностей гипотез. Кастдевы, воркшопы, АВСД/МВТ-тесты: никто, кроме нас самих, уже не понимает, о чем речь, когда мы используем сложные термины, которые изобрела исследовательская и продуктовая субкультура. Мы усложнили само понятие исследований, чтобы стоить дороже и казаться умнее. Но главная проблема даже не в том, что мы стали сложно разговаривать. Культ, сложившийся вокруг исследований, увеличил цикл поставки решений. Мы не только говорим сложным языком — мы откладываем принятие решений, потому что не исследовали вопрос достаточно. Я думаю, многие знакомы с подходом Double Diamond. В компаниях, где синдром самозванца слишком разыгрался, можно встретить тройной и даже четверной Diamond. То есть они бесконечно прогоняют циклы исследований, чтобы добавить какую-нибудь маленькую фишечку в продукт. Здесь важно вовремя вспомнить: мы не исследовательская, а производственная компания, и наша задача — как можно скорее поставить пользователю конечный продукт. Так что теперь, отказаться от исследований? За 20 лет работы с цифровыми продуктами я прошел все стадии отношений с исследованиями и на собственном опыте проверил разные постулаты, которыми теперь пользуюсь. Всё что угодно можно сделать за три месяца Эту идею я позаимствовал у Стива Джобса, и она реально работает. Когда я это понял, мне стало хватать трехмесячного цикла производства, чтобы довести любую идею до выпуска. Весь фокус в том, что я ограничил период исследований: на любом проекте это две недели — не больше. Этого времени оказалось достаточно, чтобы собрать предпочтения пользователей. На выходе получаю три документа, которые помогают мне разобраться, что и как делать дальше. Один документ — про рынок, второй — про пользователя, третий — про бизнес. Оставшегося времени хватает, чтобы довести продукт до пользователя и уложиться в три месяца. Невозможно застраховаться от ошибок на 100% Есть замечательная методология под названием ТРИЗ — теория решения изобретательских задач, которую придумал наш советский ученый Генрих Альтшуллер. Мы очень часто используем ее в своей повседневной работе: итеративный подход, анализ ошибок, построение гипотез. В этой теории зашита одна очень важная мысль: вы можете проводить бесконечное количество итераций, но если вы не ошибаетесь, то, скорее всего, вы просто ничего не делаете. Без выпуска готового продукта любые предположения так и останутся предположениями, и даже А/В-тест не спасет вас от ошибки. Как показалоисследование Pendo,компании тратят 29,5 млн долларов на выпуск фич, которые потом никогда не используются или используются крайне редко. Выкатить фичу в продакшн и проверить, насколько она будет востребованной, куда эффективнее, чем месяцами проводить опросы пользователей, а в итоге всё равно сделать мимо. Исследования не решают проблем на 100%. Они могут лишь задать вектор. И только выпуск продукта и реакция реальных клиентов позволяют провести аналитику и понять, насколько вы попали в цель. Как противостоять карго-культу в своей компании Приведу ретроспективный опыт. У нас была серьезная проблема с двухлетним циклом поставки продукта. Мы долго мучились, пытаясь понять, как осчастливить пользователя. А потом как-то раз собрались вместе с коллегой-лидом и за один вечер нарисовали то, что нам самим понравилось, во что мы поверили. К тому моменту мы оба были большими экспертами в своей области и сделали 100500 интерфейсов. Мы сразу принесли нашу идею на заключительный этап — количественное исследование. Пользователи итогово подтвердили наши гипотезы, и мы получили прирост в метрике удовлетворенности пользователей ×2. Весь цикл занял две ночи против двух лет, а ключевые метрики этой идеи на 40% превышали все предыдущие. На мой взгляд, это единственный способ саботировать карго-культ исследований: быть уверенным в себе и уметь доказывать свою экспертность. Ничему не верить и бесконечно исследовать можно, но это попахивает синдромом самозванца. Не надо превращать нашу деятельность в бесконечный набор сложных тезисов, терминов и инструментов, которые должны избавить от всех проблем и снизить риски. Нужно как можно быстрее поставлять продукт в продакшен, как можно быстрее доставлять ценность до пользователей. Да, иногда нужно пойти и поговорить с клиентами, чтобы понять, что болит. Но иногда достаточно сделать продукт для себя, чтобы получилось что-то настолько крутое, что изменит мир."
  },
  {
    "article_id": "https://habr.com/ru/articles/878044/",
    "title": "Наш бот просто жжет. Людей, машины, но в основном – здания",
    "category": "Искусственный Интеллект",
    "tags": "светодизайн, нейросети, нейросети и machine learning, бот-дизайнер",
    "text": "И вы так сможете, если прочитаете эту статью. Да, она о светодизайне. Конечно, в 2025-м все знают, что нужно использовать нейросети в дизайне, потому что это стильно, модно, экономит силы, деньги и время. Однако не все инструменты просты в использовании, многие требуют если не обучения, как когда-то фотошоп, то по крайней мере, танцев с бубнами и уверенных референсов, потому что по промту «сделай красиво, я подожду» работать не будет (мы проверяли). А значит, для обработки изображений и работы с освещением объектов нужна не только нейросеть, но и талантливый дизайнер, который к ней прилагается. Наша задача заключалась в том, чтобы изъять из уравнения талантливого дизайнера или, как минимум, сократить время его присутствия в проекте – то есть дать потенциальному клиенту некий рабочий инструмент, с которым он, до определенного момента, может забавляться сам. И тогда мы придумали Светобота. С одной стороны, это было просто программное решение, которое использовало алгоритмы машинного обучения для анализа изображений и создания дизайнерских проектов по подсветке зданий. С другой – мы хотели обучить его всему, что знаем сами – редактированию изображений, преобразованию фотографий в световые схемы, комбинации лучших цветовых решений, и главное – изобретению множества вариантов для различных пространств. Многие современные платформы используют ИИ для генеративного проектирования, некоторые боты умеют самостоятельно определять цветовые палитры на основе загруженных фотографий, предлагать гармоничные сочетания, моделировать, как будет выглядеть освещение в помещении или всё здание снаружи, в зависимости от расположения источников света и вообще, они вундеркинды. Наш бот был не таким. Он учился, но медленно, делал кучу ошибок, не распознавал фото и каждому его успеху мы радовались так, будто он уже основал свой стартап и переехал с ним в Силиконовую долину. Ну или хотя бы в Сколково. Тот факт, что малыш-Светобот может работать как начинающий, пусть и не слишком старательный светодизайнер в какой-то момент наполнил нас оптимизмом. Ведь если мы смогли научить его рендерить дизайн подсветки сложных объектов “как бы на скорую руку”, значит сможем заставить углубиться в детали и постепенно научим выдавать более уверенную и качественную базу для любых объектов. Но тогда мы еще ничего не знали о проблеме Голубой Стены. Дело в том, что довольно много зданий в нашей стране красят в неброский серо-голубой цвет. И если говорить о реальных конструкциях, а не просто обучающих фото, то приходится мириться с этой блеклой цветовой гаммой. Но Светобот решил просто игнорировать голубые конструкции. Превращая день в ночь и дорисовывая подсветку, он просто убирал стены, которые ему не нравились. Обратите внимание на правую часть фото. Так было. А так стало. Как видим, это уже не тот же самый объект, это его вольная интерпретация. С полотнами Сальвадора Дали нейросети проделывают подобное сплошь и рядом, но с реальным заданием по светодизайну так запросто поступать нельзя. Цель светодизайна - так подсветить ГЭС, чтобы это можно было прикинуть, рассчитать и воплотить в реальность, желательно сэкономив деньги. А мы тут стены двигаем. Ну, чтобы не придираться к себе, мы решили придраться к изображению. Возможно, слишком малый/большой размер не позволил Светоботу отличить стену от пустого места. Родился вопрос, влияет ли качество изображения на качество рендеринга. Увеличили, уменьшили, перепробовали разные варианты. Верным решением оказалось вшивание метаданных в фотографии, чтобы можно было их воспроизвести и как следует поколдовать над параметрами, поапскейлить, поразмышлять, что идет не так. В данном случае - исчезновения стены удалось избежать, просто уменьшив изображение в два раза. Из минусов - поиграть с промтами не получилось, так как периодически Светобот начинал ругаться на размер изображения и буквально тушить свет: С другой стороны, стало понятно, что дело не только в стенах, просто нашему боту требуется более тонкая настройка, потому что помимо стен есть еще масса интересных фрагментов. Например, у нас было много проблем с флагами, которые он высвечивал, игнорировал или перерисовывал на свой вкус или удалял. А ведь каждый светодизайнер вам скажет, что подсветка правительственных зданий - это довольно частотный запрос. Но несмотря на все эти (и многие другие!) подводные камни, мы смогли методом проб, тестов, совещаний и магической настройки привести нашего бота в боеспособное рабочее состояние. Теперь он готов к труду и вашим запросам на обработку фото. Теперь он с радостью создаст для вас уникальные варианты архитектурного светодизайна и предложит целый спектр фасадных решений для любых задач, которые могут прийти в голову вам или вашим клиентам. Наш бот дружелюбен, прост в использовании и его можно бесплатно тестировать прежде чем решиться на серьезные отношения. Все работает довольно просто: вы знакомитесь с ботом, проходите регистрацию, выбираете в меню справа тип здания (многоэтажное, малоэтажное, частный дом и т.д.), выбираете тип подсветки, а затем переходите к основному этапу – загрузке фотографии. Это может быть любое фото здания, даже сделанное на телефон, но нужно помнить, что чем четче фото – тем лучше конечный результат. После загрузки кадра бот немного подумает и произойдет магия: он выдаст вам четыре варианта светодизайна за одну генерацию. Вы сможете понять, что вам нравится или не нравится, и попробовать другие варианты при последующих генерациях – с нуля или с помощью кнопки “заказать доработку”. Так это выглядит сейчас. На очереди – возможность сохранять ваши проекты в папку, чтобы вы могли видеть процесс и результат своего труда, персонализация по стилям (вдруг вам хочется модерн, а бот все время подсовывает классику), инструкции по правильной съемке здания – вещи довольно очевидные, но ведь не лишне напомнить, что снимать лучше в солнечный день, с расстояния, чтобы фасада влезал целиком, а не под дождем из окна такси. Ну, это по мелочи, а если серьезно - нашего бота ждет большое расширение функционала не только с сфере обработки фото (нужно добавить возможность тематического освещения - скажем, к Новому году или каким-то другим поводам), но и для выдачи инструкций по монтажу и эксплуатации светового оборудования, которое понадобится для воплощения дизайнерских идей. Таким образом, вы сможете получить не только варианты светодизайна здания, но и документ в PDF, где будет сказано, чего и сколько вам понадобится приобрести в реальной жизни для такого дизайна. Однако человеческих специалистов все равно увольнять не советуем. Лучше познакомить их с новым членом команды - вдруг они подружатся. "
  },
  {
    "article_id": "https://habr.com/ru/companies/croc/articles/878010/",
    "title": "Как видят свои главные задачи 12 тысяч опрошенных ИТ-директоров",
    "category": "Искусственный Интеллект",
    "tags": "генеративный ии, искусственный интеллект, ит-директор, исследования в ит, развитие ии",
    "text": "Как считают аналитики Gartner, в 2025 году ИТ-директора столкнутся с рядом задач, от решения которых зависит успешность развития их компании. На первое место Gartner поставил умение извлекать пользу из такого инструмента управления, как генеративный искусственный интеллект (ИИ). Далее идут способность работать с данными и аналитикой, минимизировать риски киберугроз, доносить до бизнеса ценность внедрения новых технологий. Хабр-редакция КРОК подготовила перевод исследования Gartner о том, какие алгоритмы решений поставленных задач могут пригодиться в наступившем году ИТ-директорам и насколько эти решения могут быть эффективными. Исследование мы представили в четырех частях: Стратегия развития ИИ Стратегия развития ИИ Стратегия работы с данными и аналитикой Стратегия работы с данными и аналитикой Отработка новых угроз безопасности Отработка новых угроз безопасности Демонстрация ценности ИТ Демонстрация ценности ИТ Стратегия ИИ Стратегия ИИ: от формирования видения до реализации инициатив на базе ИИ, дающих реальную ценность бизнесуЭффективнее всего строить стратегию развития ИИ на четырех основных столпах. Это видение, осознание ценности, риски и планы по внедрению. Генеративный искусственный интеллект (ИИ) –  это один из видов ИИ, который неожиданно приглянулся руководителям, но чтобы извлекать из него пользу и уверенно управлять сопутствующими рисками, необходима продуманная, целостная и реализуемая стратегия внедрения ИИ. Четыре ключевых элемента любой стратегии ИИ приведены ниже: • Определить цели, преимущества генеративного ИИ и критерии успешности внедрения; • Увязать ваше видение развития генеративного ИИ с выгодой, которую получит бизнес; • Оценить и минимизировать основные риски, связанные с ИИ; • Расставить приоритеты среди инициатив, касающихся генеративного ИИ. Построение стратегии развития ИИ, в том числе генеративного ИИ, требует скрупулезности во всем: от продумывания видения с учетом потребностей бизнеса до планирования и обоснования конкретных инициатив. Видение ИИ: определите, какие стратегические возможности открывает генеративный или другие виды ИИ Все вдруг заинтересовались генеративным ИИ, но некоторые уже давно и успешно применяют ИИ в различных подразделениях и процессах. По данным исследования Gartner, среди тех, кто сейчас экспериментирует с ИИ, доля таких опытных игроков составляет всего 10%, но потенциальные пользователи генеративного ИИ могут многому у них научиться. Генеративный ИИ способен радикально изменить существующие экономические и социальные парадигмы, как когда-то это удалось интернету и более ранним инновациям, например, электричеству. Главный вопрос с точки зрения бизнеса: как ИИ поможет достичь масштабных бизнес-целей и добиться более внушительных результатов? При правильном внедрении генеративный ИИ станет конкурентным преимуществом и даже будет иметь решающее значение, учитывая, что ИИ в принципе умеет автоматизировать однообразные и рутинные задачи, собирать аналитику, генерировать новые идеи и инновации с помощью предиктивной аналитики, машинного обучения и других методов ИИ. Генеративный ИИ может существенно повысить капитализацию компании, создавая новые прорывные возможности для достижения корпоративных целей, таких как: –Увеличение выручки.ИИ ускорит создание новых продуктов. Фармацевтика, здравоохранение и промышленность (производство потребительских товаров, продуктов питания и напитков, химикатов и материалов) – в этих отраслях ИИ пригодится в первую очередь и поможет как с разработкой новых лекарств, менее токсичных бытовых чистящих средств, новых вкусов и ароматов, новых сплавов, так и с проведением более быстрой и качественной медицинской диагностики. –Повышение вовлеченности клиентов.С помощью генеративного ИИ можно повысить вовлеченность клиентов: организации отказываются от текущих бизнес-моделей и цепочек создания ценности, чтобы создавать и транслировать контент клиентам напрямую, минуя традиционных посредников (СМИ и дистрибьюторов). –Снижение затрат и повышение производительности.Генеративный ИИ может упростить процессы и ускорить получение результатов, например, помогая сотрудникам обобщать, упрощать и классифицировать контент, создавая программный код или оптимизируя работу чат-бота. Кроме того, генеративный ИИ поможет задействовать ранее неиспользуемые (а значит, пока бесполезные) данные. Недавно компания Gartner опросила больше 600 организаций, внедривших ИИ, и выяснила, что наиболее опытные из них, кто давно работает с ИИ, не измеряют успех ни объемом проекта, ни количеством выполненных задач или выпущенной продукции. Вместо этого они решили: 1.Смотреть на бизнес-метрики, а не на финансовые показатели, и использовать свои модели атрибуции и критерии для каждого сценария; 2.Проводить сравнительную оценкувнутренних подразделений и сравнивать свою организацию с другими; 3.Заранее определять показатели, а затем с их помощью быстро и последовательно измерять успешность сценариев использования ИИ. К бизнес-метрикам относятся: ·Рост бизнеса.Например, возможность перекрестных продаж, повышение цен, оценка спроса, монетизация новых активов. ·Успех у заказчиков.Например, показатели удержания, удовлетворенности заказчиков, доля в кошельке заказчика. ·Экономическая эффективность.Например, сокращение запасов, производственные затраты, производительность труда, оптимизация активов. В исследовании Gartner отдельно подчеркивается, что организации, в которых команда ИИ участвует в определении показателей успешности, на 50% чаще используют ИИ в стратегических инициативах. При выборе показателей команда ИИ должна учитывать мнение групп, управляющих данными, бизнес-аналитиков, экспертов по доменам, руководителей отделов по управлению рисками, специалистов по обработке и анализу данных (data scientists), а также руководителей ИТ-подразделений и разработчиков. Ценность ИИ Новые инструменты, например, ChatGPT, подогрели интерес к возможностям ИИ, но чтобы извлечь выгоду из этих инструментов, руководителям необходим более широкий взгляд на их пользу для бизнеса, риски, кадровые проблемы и инвестиционные приоритеты, а также топ-менеджерам нужно подготовиться к полному пересмотру привычных бизнес-моделей и стратегий. На сегодняшний день бизнес-ценность ИИ в основном создается за счет разовых решений. Для получения большей пользы в масштабе, в том числе от проектов на базе генеративного ИИ, могут потребоваться серьезные изменения бизнес-процессов, новые наборы навыков, роли и организационные структуры, и даже новые методы работы. В отсутствие таких изменений не получится использовать по максимуму все выявленные возможности. Продумайте, как по мере интеграции генеративного ИИ в повседневную работу ваша организация будет преобразовывать процессы и системы, а также повышать квалификацию сотрудников. Осознанное внедрение ИИ с учетом будущих потребностей может обеспечить вам успех на годы вперед, а отсутствие такого взвешенного подхода может привести к катастрофе. Gartner выдвигает следующие стратегические гипотезы: · К 2026 году больше 100 млн человек будут обращаться к робоколлегам (искусственным виртуальным коллегам) за помощью в своей повседневной работе. · К 2033 году решения на основе ИИ, внедряемые для дополнения или автономного выполнения задач, действий или заданий, приведут к созданию больше 500 млн новых рабочих мест. Определите проблемы, которые могут замедлить внедрение проектов с генеративным ИИ или помешать вам извлечь из них максимальную пользу. Сопоставьте решения и действия и назначьте ответственного за проведение необходимых организационных изменений. Например, если в вашей организации не хватает компетенций для работы с данными, а без этого не обходится ни один проект с ИИ, то запишите топ-менеджеров (а не только рядовых сотрудников) на обучение и тренинг по работе с данными, и пусть директор по данным и аналитике отвечает как за проведение программы, так и за участие в ней других топ-менеджеров. Риски, связанные с ИИ Государственные нормативно-правовые акты, касающиеся ИИ, только начинают появляться, поэтому следите за тем, какие постановления принимают в соответствующих юрисдикциях. Поскольку использование ИИ все еще вызывает вопросы, когда речь заходит об этике и ответственности, смена общественных настроений в отношении использования ИИ может привести к появлению новых законов. Однако в целом следует подготовиться к следующим основным типам рисков: 1.Нормативно-правовые.Используя ИИ, компания рискует получить судебные иски из-за контента, информации и данных, защищенных авторским правом или охраняемых законом. Нормативные акты быстро меняются, поэтому лучше знать о нормативных актах в области ИИ в конкретной стране или юрисдикции, чтобы не нарушать закон. Кроме того, следует следить за отраслевыми регламентами, например, в области медико-биологических наук и финансовых услуг. 2.Репутационные.ИИ способен возвести погрешности данных в абсолют и создать «черный ящик», т.е. систему ИИ, в которой пользователь не видит входных данных и операций. Если не ясно, какие данные используются для обучения системы, возникает риск получить вредоносные данные на выходе. Непроверенные сервисы ИИ также могут представлять опасность из-за некачественного принятия решений и/или выполнения задач. При создании или покупке сервисов на базе генеративного ИИ организациям необходимо выстраивать надежные защитные механизмы, чтобы предотвратить потерю интеллектуальной собственности или утечку данных заказчиков. 3.Компетентностные.Для использования ИИ необходим уникальный набор навыков, которые нужно специально развивать у текущих сотрудников или через работу с научным сообществом или стартапами. В ближайшей перспективе будут востребованы навыки создания промптов и знание стандартов ответственного подхода к использованию ИИ. Угрозы, связанные с ИИ и компрометацией данных (как преднамеренной, так и случайной), продолжают постоянно развиваться, поэтому необходимо разработать принципы и политики управления, благонадежности, достоверности, надежности, защищенности, эффективности и конфиденциальности ИИ. Без этого организации с гораздо большей вероятностью столкнутся с негативными последствиями использования ИИ и утечками. Модели не будут работать как задумано, а это чревато сбоями в системе безопасности, нарушением конфиденциальности, финансовыми и репутационными потерями, а также ущербом для отдельных людей. Когда генеративный ИИ создает новые версии контента, стратегий, разработок и методов, обучаясь на исходных данных из больших репозиториев, возможны определенные последствия: ·Ложные выводы.Генеративный ИИ может давать сбои и допускать ошибки в рассуждениях и фактах, не полностью учитывать контекст, действовать недостаточно прозрачно и понятно для людей, а также может быть необъективен. ·Безопасность.В настоящее время любая конфиденциальная информация, которая попала в публичные приложения, сохраняется и может быть использована для обучения новых версий модели. Конфиденциальные данные и интеллектуальная собственность могут стать доступны пользователям за пределами организации, в том числе попасть в руки злоумышленников. ·Право.Работа генеративного ИИ может порождать юридические риски, связанные с интеллектуальной собственностью и конфиденциальностью, включая нарушение авторских прав, незаконное присвоение коммерческих секретов, нарушение конфиденциальности данных, искажение и небезопасность модели. Внедрение ИИ При выборе сценариев использования ИИ, в том числе генеративного ИИ, представители компании должны четко сформулировать, какие ощутимые преимущества для бизнеса они ожидают получить. Для этого им необходимо ответить на следующие вопросы: · Какую проблему пытается решить компания? · Кто главный потребитель этой технологии? · В каком бизнес-процессе будет использоваться ИИ? · Кто из профильных экспертов бизнес-подразделений может руководить разработкой решения? · Как будет измеряться эффект от внедрения технологии? · Как и кто будет отслеживать и обеспечивать полезность технологии? Прежде чем внедрять комплексную стратегию ИИ, стоит поэкспериментировать с методами на ее отдельных компонентах. Выполните следующие пять шагов для внедрения методов ИИ: 1.Задачи:укомплектуйте портфель задачами, которые влияют на бизнес и которые можно измерить и быстро решить. 2.Навыки:подберите специалистов с навыками, необходимыми для решения задач. 3.Данные:соберите подходящие данные под выбранные задачи. 4.Технологии:выберите методы ИИ, подходящие под ваши задачи, навыки и данные. 5.Организация:структурируйте опыт и накопленные ноу-хау в области ИИ. Из этих пяти шагов складывается тактический подход к внедрению технологий ИИ, который быстро принесет пользу бизнесу. Данный подход нельзя назвать долгосрочным и стратегическим. Помимо осознания пользы для бизнеса, не менее, а возможно даже более важно понимать, насколько такие задачи или сценарии использования в принципе реализуемы Сначала определите те сценарии использования, которые могут принести наибольшую пользу бизнесу. Необходимо ориентироваться на проекты реальных улучшений и ощутимые бизнес-результаты. При этом важно понимать, насколько эти сценарии реализуемы. Как правило, при высоком риске и низкой реализуемости можно рассчитывать на больший эффект, но проекты, которые невозможно реализовать с помощью доступных технологий и данных, не стоит и начинать, даже несмотря на их очевидную ценность для бизнеса. Критерии реализуемости: ·Технический.Позволяют ли имеющиеся технологии поднять заявленный сценарий использования до уровня «передовых практик»? ·Внутренний.Здесь необходимо оценить наличие соответствующей культуры (или ее отсутствие), лидерства, вовлеченности, навыков и этических норм. ·Внешний.Здесь необходимо оценить наличие нормативных актов (или их отсутствие), внешней инфраструктуры и одобрения со стороны общества. Если сценарий использования легко реализовать и он еще сулит огромную ценность для бизнеса, то мы либо на пороге нового технологического прорыва, либо рынок почему-то в упор не видит отличную возможность. Об остальных задачах, поставленных перед ИТ-директорами по всему миру, читайте в следующих частях этого исследования. Продолжение следует... "
  },
  {
    "article_id": "https://habr.com/ru/companies/pt/articles/877666/",
    "title": "В тренде VM: под угрозой продукты Microsoft, сайты на WordPress и веб-приложения на Apache Struts",
    "category": "Искусственный Интеллект",
    "tags": "уязвимости, microsoft, windows, apache struts, apache, wordpress, удаленное выполнение кода, rce, vulnerability management, трендовые уязвимости",
    "text": "Хабр, привет! Я Александр Леонов, ведущий эксперт лаборатории PT Expert Security Center. Мы с командой аналитиков Positive Technologies каждый месяц исследуем информацию об уязвимостях из баз и бюллетеней безопасности вендоров, социальных сетей, блогов, телеграм-каналов, баз эксплойтов, публичных репозиториев кода и выявляем во всем этом многообразии сведений трендовые уязвимости. Это те уязвимости, которые либо уже эксплуатируются вживую, либо будут эксплуатироваться в ближайшее время. В декабре мы отнесли к трендовым четыре уязвимости: Уязвимости в Microsoft Уязвимость в драйвере ksthunk.sys, связанная с повышением привилегий (CVE-2024-38144), Уязвимость в драйвере ksthunk.sys, связанная с повышением привилегий (CVE-2024-38144), Уязвимость в драйвере CLFS.sys, связанная с повышением привилегий (CVE-2024-49138). Уязвимость в драйвере CLFS.sys, связанная с повышением привилегий (CVE-2024-49138). Уязвимость в Apache Struts, фреймворке для создания веб-приложений Уязвимость, связанная с обходом каталога (CVE-2024-53677). Уязвимость, связанная с обходом каталога (CVE-2024-53677). Уязвимость в Hunk Companion, плагине для WordPress Уязвимость, связанная с установкой уязвимых плагинов (CVE-2024-11972). Уязвимость, связанная с установкой уязвимых плагинов (CVE-2024-11972). Уязвимости в продуктах Microsoft Уязвимость в драйвере ksthunk.sys, связанная с повышением привилегий при помощи переполнения буфера в Windows 💥CVE-2024-38144 (оценка по CVSS — 8,8; высокий уровень опасности) Уязвимостьиз августовского дайджеста Microsoft Patch Tuesday. В обзорах ее не выделяли как трендовую. Было известно только то, что локальный атакующий может получить привилегии уровня SYSTEM — максимального в операционной системе.Через три с половиной месяца, 27 ноября, компания SSD Secure Disclosure, специализирующаяся на раскрытии уязвимостей, опубликовалаисследованиес кодом эксплойта. Оказалось, что эксплуатация этой уязвимости была представлена на конкурсеTyphoonPWN 2024в конце мая. Исследователь получил за нее 70 000 $.Специалисты SSD отмечают, что коммуникация с Microsoft была непростой и «на момент проверки на последней версии Windows 11 уязвимость все еще эксплуатировалась». Непонятно, когда именно был этот момент проверки: до августовского дайджеста Microsoft Patch Tueasday или перед выходом исследования. Если второе, то получается, что уязвимость может быть до сих пор не исправлена и ее можно отнести к 0-day. Ждем разъяснений от SSD или Microsoft.Про эксплуатацию уязвимости в реальных атаках пока не слышно. Признаки эксплуатации:Microsoftне отмечаетфактов эксплуатации уязвимости. Количество потенциальных жертв:все пользователи Windows, которые не скачали обновления безопасности. Публично доступные эксплойты:есть в открытом доступе. Уязвимость в драйвере CLFS.sys, связанная с повышением привилегий при помощи переполнения буфера в Windows 💥CVE-2024-49138 (оценка по CVSS — 7,8; высокий уровень опасности) Уязвимостьиз декабрьского дайджеста Microsoft Patch Tuesday. Недостаток безопасности был обнаружен в стандартном компоненте Windows во всех версияхначиная с Windows Server 2003 R2. Описание у нее типично для уязвимости, связанной с повышением привилегий (EoP) в Windows: в случае успешной эксплуатации локальный злоумышленник может получить привилегии уровня SYSTEM. Причина уязвимости —heap-based buffer overflow. Переполнение буфера в области данных кучи возникает, когда программа записывает в буфер памяти, выделенный для кучи, больше данных, чем туда может поместиться.Microsoft сразу подтвердили случаи эксплуатации вживую, однако не предоставили информации о том, где эксплуатировалась уязвимость и насколько масштабными были атаки. Публичный эксплойт появился 15 января, его представил Alessandro Iandoliиз HN Security. На GitHub доступен исходный код и видео с демонстрацией работы эксплойта: локальный атакующий запускает EXE-файл в PowerShell и через секунду получает максимальные привилегии уровня SYSTEM. Исследователь протестировал работоспособность эксплойта на Windows 11 23H2. Он также обещает опубликовать пост с подробным анализом уязвимости. Признаки эксплуатации:Microsoftотмечаетфакты эксплуатации уязвимости. Компания CrowdStrikeсообщает, что уязвимость активно эксплуатируется злоумышленниками. Количество потенциальных жертв:все пользователи Windows, которые не скачали обновления безопасности. Публично доступные эксплойты:в открытом доступе опубликован PoC. Способ устранения описанных уязвимостей:установить обновления безопасности, которые представлены на официальном сайте Microsoft (CVE-2024-38144,CVE-2024-49138). Уязвимость во фреймворке для создания веб-приложений Apache Struts Уязвимость, связанная с обходом каталога в Apache Struts 💥CVE-2024-53677 (оценка по CVSS — 9,5; критический уровень опасности) Apache Struts — это открытая программная платформа для создания веб-приложений на языке Java. Она позволяет разработчикам отделять бизнес-логику приложения от пользовательского интерфейса. Apache Struts часто используется в крупных корпоративных проектах.Бюллетень безопасности с описанием уязвимостивышел14 декабря. Ошибка в логике загрузки файлов позволяет неаутентифицированному злоумышленнику выполнить атаку типа path traversal, загрузить зловредный файл и при определенных условиях осуществить удаленное выполнение кода (remote code execution). Публичный эксплойт появился 20 декабря. Естьсообщения о попытках эксплуатации, но информации об успешных атаках пока нет.Вендор рекомендует обновиться до версии 6.4.0 или выше и мигрировать приложения на новый безопасныймеханизм File Upload. Признаки эксплуатации:несколько вредоносных узлов, пытающихся использовать уязвимость, заметилив GreyNoise, но информация об успешных атаках отсутствует. Количество потенциальных жертв:по данным Maven Central, уязвимую версию фреймворка установили около 40 000 раз. Публично доступные эксплойты:есть в открытом доступе. Способы устранения, компенсирующие меры:всем пользователям рекомендуетсяобновитьApache Struts минимум до версии 6.4.0 и перейти на использование нового механизма загрузки файлов, не имеющего обратной совместимости со старым. Уязвимость в плагине Hunk Companion для WordPress Уязвимость, связанная с установкой уязвимых плагинов через плагин Hunk Companion для WordPress 💥CVE-2024-11972 (оценка по CVSS — 9,8; критический уровень опасности) КомпанияThemeHunkразрабатывает коммерческие темы для WordPress CMS, аплагин Hunk Companionпредназначен для дополнения и расширения функциональности этих тем. У плагина более 10 000 установок.Бюллетеньот WPScan с описанием уязвимости в версиях плагина Hunk Companion ниже 1.9.0 вышел 10 декабря. Уязвимость позволяет неаутентифицированному атакующему устанавливать и активировать на сайте произвольные плагины из репозитория WordPressOrg. Эксплойт доступен на GitHub с 28 декабря.Несанкционированная установка плагинов опасна тем, что устанавливаемые плагины могут содержать свои уязвимости. Так, в инциденте, зафиксированном WPScan, злоумышленники установили на сайте плагинWP Query Consoleс уязвимостью, связанной с удаленным выполнением кода (CVE‑2024‑50498), и проэксплуатировали ее для установки бэкдора.Если вы используете WordPress, старайтесь минимизировать количество плагинов и регулярно их обновляйте! Признаки эксплуатации:WPScanсообщаето фактах эксплуатации уязвимости. Количество потенциальных жертв:по данным WordPress, уязвимая версия установлена более чем у 6800 пользователей. Публично доступные эксплойты:есть в открытом доступе. Способы устранения, компенсирующие меры:всем пользователям рекомендуется обновитьHunk Companionдо исправленной версии 1.9 и выше. 👷 Как защититься Использование продуктов, содержащих трендовые уязвимости, может поставить под угрозу любую компанию. Такие недостатки безопасности являются наиболее опасными и требуют немедленного исправления. В систему управления уязвимостямиMaxPatrol VMинформация о подобных угрозах поступает в течение 12 часов с момента их появления, что позволяет вовремя принять меры по устранению и защитить инфраструктуру компании. Кроме того, мы рекомендуем использовать межсетевые экраны уровня веб-приложений, напримерPT Application Firewall, которые позволяют обезопасить общедоступные ресурсы. В статье приведены примеры уязвимостей, которые активно эксплуатируются в последнее время. Информация о них и о публично доступных эксплойтах представлена по состоянию на 31 декабря 2024 года. Ведущий эксперт PT Expert Security Center "
  },
  {
    "article_id": "https://habr.com/ru/companies/vasexperts/articles/877632/",
    "title": "Как меняются [и меняются ли] DDoS-атаки",
    "category": "Искусственный Интеллект",
    "tags": "vas experts, информационная безопасность, ddos, исследования и прогнозы в it",
    "text": "Ранее мы ужеотмечалитенденцию роста общего количества и мощности кибератак — злоумышленники активно пользуются особенностями протоколов, известными уязвимостями и находят новые. Обсудим, стабилизировалась ли ситуация, какие приёмы все еще применяют злоумышленники сейчас, и как дорого обходятся DDoS-атаки. Становятся мощнее Формально первая DoS-атака произошла более 50 лет назад, когда 13-летний подросток смог «положить» тридцать один терминал Университета Иллинойса. С тех пор вмешательства в сетевую инфраструктуру быстро переросли из спонтанных шуток в изощренные киберпреступления против различных организаций. В 90-е злоумышленники ужепроводилиполноценные DoS-атаки на инфраструктуру интернет-провайдеров, а также приступили к DDoS-атакам на государственные организации. С тех пор частота и интенсивность кибератак только возрастала. Если 15 лет назад их пиковая мощность была на уровне 100 Гбит/сек, то сегодня онадоходитдо 4,2 Тбит/сек. Причём длительность таких атак может составлять всего несколько секунд. Например, в отчете Cloudflare речь идет о сотнях подобных сверхмасштабных атак в рамках года. В среднем это — DDoS с мощностью более 3 Тбит/сек и 2 млрд пакетов в секунду. Кстати, поданныманалитиков облачной платформы Fastly, от ноября к декабрю 2024 года рост объема DDoS-атак превысил несколько сотен процентов, а пиковые значения атак такого типа обычно приходились на первую половину рабочей недели. Остаются верны «традициям» Злоумышленники хоть и постепенно внедряют в работу новые методы и системы ИИ, но в основном они полагаются на «классику» — средства и уязвимости, которые известны не первый год, а в некоторых случаях — не первый десяток лет. Так, одним из основных инструментов для проведения DDoS-атак, конечно же, остаются ботнеты. И мощность зараженных сетей продолжает увеличиваться. Например, согласноисследованию, проведенному поставщиком услуг информационной безопасности StromWall, в третьем квартале прошлого года наблюдался резкий рост активности ботнетов в африканском регионе и на Ближнем Востоке. Мощнейшие DDoS-атаки, зафиксированные специалистами (они произошли в ОАЭ), имели поток в 1,5 Тбит/сек. Увеличился и средний размер ботнета — до 28 тысяч устройств. Для сравнения, в 2023 году эта цифра едва ли превышала 7 тыс. При этом ботнеты продолжают эксплуатировать известные слабые места в инфраструктуре онлайн-сервисов. Один из ботнетов — Mozi — использует уязвимости в ряде популярных роутеров и устройств интернета вещей, организуя с их помощью DDoS-атаки. Так, одна из эксплуатируемых уязвимостей (CVE-2023-1389) позволяет проводить инъекции команд без аутентификации. Примечательно, что в какой-то момент времени ботнет дажегенерировалбольшую часть всего IoT-трафика. Примером другого эксплойта, который подходит к категории «старых» уязвимостей, можно быть атака DNSBomb. Она основана на методе DDoS через TCP-пульсации, который был раскрыт внаучной работе2003 года. Суть заключается в отправке на сервер жертвы повторяющихся серий кратковременных импульсов с огромным трафиком. Даже в небольших масштабах интенсивность атаки может достигать десятков Гбит/сек. В то же время метод пульсации потенциально применим и к CDN для генерации DoS-трафика. Однако перед злоумышленниками открываются и новые «возможности» — специалисты по информационной безопасности регулярно находят уязвимости. В прошлом году эксперты немецкого Центра информационной безопасности имени Гельмгольца (CISPA) отметили метод, которыйназвалиLoop DoS. Идея состоит в запуске бесконечной петли запросов между приложениями, подверженными уязвимости в протоколе UDP. В теории атаку можно инициировать единственным сообщением от одного хоста. По оценкам экспертов, эксплойт затрагивает порядка 300 тыс. устройств по всему миру, хотя об успешной реализации атаки такого типа на практике пока ничего не слышно. Наносят все более масштабный ущерб Старые методы и новые технологии позволяют наносить все более серьёзный ущерб. Например, в январе крупнейший японский оператор связи NTT Docomoстолкнулсяс массовой DDoS-атакой, которая затронула порядка 90 млн пользователей и вывела из строя большую часть сервисов на двенадцать часов. Эксперты связывают эту атаку с другой,произошедшейв сентябре 2023 года (естьмнение, что NTT Docomo шантажирует команда хакеров-вымогателей). Также от подобных атакпострадаликрупные финансовые организации страны — Банк Мидзухо и холдинговая компания Resona Holdings. Во время кибератак у них не работали услуги онлайн-банкинга, приложения и сайты. Оценить ущерб от подобных атак, как правило, невероятно сложно. Нужно учитывать не только прямой финансовый урон, но и репутационные издержки, эффект от которых может быть отложенным. Но можно взглянуть на усредненные цифры, которые приводят аналитики. Согласно оценкам компании MazeBolt, одна атака типа «отказ в обслуживании»обходитсякрупному бизнесу в полмиллиона долларов. Другая ИБ-компания — Zayo — в своем исследовании приводит иные, но сопоставимые цифры. Специалистыпроанализировали62 тыс. DDoS-инцидентов, с которыми столкнулись компании (выборка покрывала четырнадцать индустрий) из Северной Америки и Европы в первой половине прошлого года. Оказалось, что в среднем бизнес теряет 270 тыс. долларов за 45-минутную DDoS-атаку. Иными словами, каждая минута обходится организации примерно в 6 тыс. долларов. Однако стоит заметить, что 86% всех вредоносных вмешательствсоставляликибератаки длительностью меньше десяти минут. Учитывая скорость развития технологий, распространения цифровых сервисов и повсеместной диджитализации производств, можно с уверенностью сказать, что размер ущерба, которые способны нанести DDoS-атаки, будет только увеличиваться. Также сами цели атак становятся более разнообразными. В 2024 году киберпреступники атаковали даже некоммерческие и исследовательские организации. В апреле DDoS-атаке мощностью в 3,5 млн запросов в часподвергсясайт ЦЕРН. С похожей угрозойстолкнулсяпроект Internet Archive, который подвергался DDoS-воздействию около недели. Что почитать по теме Что такое IDS — система обнаружения вторжений. Рассказываем о комплексах мер по борьбе с уязвимостями и взломами. Рассматриваем методы выявления на основе машинного обучения, сигнатур. Объясняем, как работают системы NIDS и HIDS. Что такое IDS — система обнаружения вторжений. Рассказываем о комплексах мер по борьбе с уязвимостями и взломами. Рассматриваем методы выявления на основе машинного обучения, сигнатур. Объясняем, как работают системы NIDS и HIDS. Сетевые технологии с нуля[или почти с нуля] — наша подборка общедоступных книг по теме. Основная часть литературы обладает минимальным порогом вхождения и призвана помочь с погружением в сферу начинающим специалистам. Сетевые технологии с нуля[или почти с нуля] — наша подборка общедоступных книг по теме. Основная часть литературы обладает минимальным порогом вхождения и призвана помочь с погружением в сферу начинающим специалистам. Подборка открытых ресурсов с информацией о выявленных уязвимостях. Это — наш обзор актуальных ресурсов и баз, посвященных различным эксплойтам. Среди них общепризнанный CVE и дополнительные базы на его основе NVD, OpenCVE и др. Подборка открытых ресурсов с информацией о выявленных уязвимостях. Это — наш обзор актуальных ресурсов и баз, посвященных различным эксплойтам. Среди них общепризнанный CVE и дополнительные базы на его основе NVD, OpenCVE и др. В копилку уязвимостей BGP — как устроена атака Kirin. За годы существования протокола BGP в нем обнаружили немало уязвимостей. И продолжают находить новые. Рассказываем про эксплойт, который позволяет осуществлять DDoS-атаки на маршрутизаторы. Кстати, некоторые специалисты призывают пересмотреть интернет-архитектуру и заменить BGP новыми решениями — рассказываем и о них. В копилку уязвимостей BGP — как устроена атака Kirin. За годы существования протокола BGP в нем обнаружили немало уязвимостей. И продолжают находить новые. Рассказываем про эксплойт, который позволяет осуществлять DDoS-атаки на маршрутизаторы. Кстати, некоторые специалисты призывают пересмотреть интернет-архитектуру и заменить BGP новыми решениями — рассказываем и о них. Руководство по FOSS — про CLI, приватность, безопасность и self-хостинг. Это — онлайн-гайд, посвящённый безопасности в Linux, работе с шифрованием, подписями и методологиями, в целом направленными на Linux Hardening. Руководство наполнено сравнениями различных решений и технологий. К примеру, автор описывает плюсы и минусы методов мультифакторной аутентификации. Руководство по FOSS — про CLI, приватность, безопасность и self-хостинг. Это — онлайн-гайд, посвящённый безопасности в Linux, работе с шифрованием, подписями и методологиями, в целом направленными на Linux Hardening. Руководство наполнено сравнениями различных решений и технологий. К примеру, автор описывает плюсы и минусы методов мультифакторной аутентификации. Исследования: QUIC может быть медленнее, чем ожидалось. Специалисты утверждают, что в ряде кейсов QUIC менее эффективен по сравнению с «классическим» TCP. В статье обсуждаем различные точки зрения и научные работы: как в поддержку этого утверждения, так и против. Исследования: QUIC может быть медленнее, чем ожидалось. Специалисты утверждают, что в ряде кейсов QUIC менее эффективен по сравнению с «классическим» TCP. В статье обсуждаем различные точки зрения и научные работы: как в поддержку этого утверждения, так и против. "
  },
  {
    "article_id": "https://habr.com/ru/companies/quanttelecom/articles/878444/",
    "title": "Отчего зависит безопасность квантовой сети? Часть 2",
    "category": "Кибербезопасность",
    "tags": "шифрование, безопасность данных, канал связи, протокол передачи данных, измерительные приборы, повторитель, квантовое распределение ключей",
    "text": "ООО «СМАРТС-Кванттелеком» Продолжаем говорить о безопасности квантовой сети. Начало статьи можно прочитать в нашем блоге по ссылке —Отчего зависит безопасность квантовой сети? Часть 1 / Хабр Проблема безопасности при передаче ключей При рассмотрении производительности сети КРК мы используем следующие предположения: 1.    Узлы являются доверенными. Протокол аутентификации работает должным образом и даёт сбой с вероятностью не более εauth для каждого узла. Все узлы атакуются по отдельности и одновременно в каждом сеансе передачи ключей. 2.    КРК-соединения могут быть любого типа (через оптоволокно, свободное пространство, соединение «точка-точка» или с недоверенными узлами между ними, например, см. рис. 3),  должны работать должным образом между всеми узлами, и каждое соединение независимо должно быть εqkd-безопасным (для простоты мы считаем их одинаковыми для каждого соединения). Все КРК-соединения атакуются по отдельности и одновременно в каждом сеансе передачи ключей. 3.    Расстояние между двумя соседними узлами не превосходит предельного. Предельным должно рассматриваться расстояние между наиболее удалёнными друг от друга связанными (через c − 1) узлами. Проблемы, с которыми приходится сталкиваться при оценке безопасности КРК-сети, схожи с проблемами в КРК-соединениях типа «точка-точка». Таким образом, приходится иметь дело с атаками как на аутентификацию узлов, так и на протокол КРК для каждого соединения. В принципе, вероятности отказа протокола аутентификации (εauth) и протокола КРК (εqkd) могут быть рассмотрены по отдельности согласно принципу композиции [48]. Однако, принимая во внимание/имея в виду сетевую реализацию, мы должны корректно масштабировать обе нотации с учётом определенных ограничений. Более того, мы должны прояснить ещё один важный̆ момент. Неправильная реализация  может привести к уязвимостям, поскольку в открытом канале связи происходит неоднократная передача сообщений Ki, зашифрованных на одном и том же квантовом ключе kpq. Подобная уязвимость распространена среди естественных языков/шифров: при использовании постоянно повторяющегося ключа (в нашем случае — сообщения) простое гаммирование может быть легко обращено/инвертировано с использованием частотного анализа. Существует два возможных решения данной̆ проблемы: первое (и самое простое) — применение хэш-функции к нашему сообщению Ki для перемешивания битов, после чего применяется гаммирование. Второе решение более полемичное. Давайте введём несколько основных правил, раз уж мы ничем не ограничены при построении сообщений. Для устранения любых взаимосвязей̆, присущих естественным языкам, битовая строка (соответствующая сообщению Ki) должна быть сгенерирована с использованием случайных чисел, а также обладать свойством равномерного распределения. Проблема масштабирования аутентификации Существует вероятность проведения атаки типа «человек посередине»: по сути, это атака на протокол аутентификации, когда злоумышленник полностью копирует один из узлов, точь-в-точь выполняя его функции, при этом не вызывая подозрений у своих соседей. После обмена квантовыми ключами между каждой парой узлов (включая подставные) в момент, когда доверенными узлами выполняется транспортировка ключей, скомпрометированный узел может получить передачу информации. Фактически, это атака на всю классическую схему передачи ключей (с квантово-распределёнными ключами), а не только на её КРК-«часть» («квантово-распределённую часть»). Чтобы максимизировать количество скомпрометированных узлов, нарушитель должен одновременно атаковать все доверенные узлы в определённом сегменте. Критерием успешной атаки является компрометация не менееcузлов подряд (по порядку узлов в самом длинном пути). Это условие следует из топологии сети, так как если скомпрометированыcузлов подряд, то нет возможности построить маршрут между первым и последним узлами. Иначе тогда все Ki могут стать известны злоумышленнику. Следует упомянуть, что подслушивающий не может оставить скомпрометированный узел, так как требуется знание предварительного общего ключа. Преступник может попытаться провести новую атаку в начале каждого сеанса. Давайте рассмотрим не всю задачу, а только приближенное решение (т. е. приближение самого низкого порядка, которое хорошо работает с малыми вероятностями) следующим образом: наименьшее количество скомпрометированных узлов для успешной атаки равно c, и существует (N − c − 1) возможных конфигураций, которые могут быть расположены в ряд в пределах (N − 2) узлов (мы не рассматриваем компрометацию первого и последнего узлов). Здесь и далее мы предполагаем, что N и c могут принимать любые значения, но с условием c ≤ N − 2. Тогда очевидно, что общая вероятность успешной атаки на протокол аутентификации: ε1≈(N – c − 1) (εauth)c.  (1) Данное приближение приемлемо для малых что может быть легко удовлетворено. Проблема масштабирования безопасности КРК Для оценки безопасности сети следует также рассмотреть одновременные атаки на все КРК-связи, которые могут предоставить переданные ключи противнику. В некотором смысле эта задача тесно связана с задачей min-cut ИЛИ проблемой минимизации ИЛИ задачей наименьшего разреза [49] в теории графов. Общая идея рассматриваемой задачи заключается в нахождении минимального количества связей/рёбер, которые необходимо разрезать/удалить, чтобы разделить граф на две части. Основа метода весьма похожа на ту, что использовалась в предыдущем подразделе. Для обеспечения успешной атаки следует перехватить все входящие или исходящие связи из c узлов подряд. Однако в случае перехвата связи эта величина,a, может быть оптимизирована. Рисунок 6 иллюстрирует невырожденный min-cut/минимальный разрез для случая c = 3. Хоть и в случае нотации безопасности эта величина не является оптимальной. Основной причиной является выбранный протокол передачи ключей, который приводит к вырождению необходимой̆ величины при приближении к крайним узлам, как показано на рис. 6. Таким образом, количество связей для min-cut лежит внутри следующего интервала: Так как мы рассматриваем худший случай, то используется приближение наименьшего порядка. Злоумышленнику необходимо перехватить квантовый̆ ключ из определённых связей таким образом, чтобы все Ki передавались через эти связи, например, см. рис. 5. Очевидно, что наименьшее количество таких связей, которые передают все Ki, это те, которые связаны с первым или последним узлом; в любом случае, естьcсвязей (более высокие члены имеют порядок не менее c + 1). Следовательно, успешная атака на связи КРК может быть выполнена с вероятностью: Данное приближение приемлемо для которое, в свою очередь, легко выполняется. Особенно нужно отметить случай сc=1, при котором последнее выражение принимает вид ε2 =(N−1)·εqkd. В общем случае, для любых c и N > c + 2 будет только два варианта для прослушивания всех Ki: перехват всех связей, либо исходящих из первого узла, либо всех входящих в последний̆ узел. Однако следует отметить, что существует только одно исключение из этого рассмотрения. Оно связано с предельным случаем минимально необходимого количества узлов, N = c + 2, для каждого числа последовательных соединений c, например, c = 2, N = 4. Объяснение довольно простое: в предельном случае всегда существует прямое соединение между вторым и последним узлами, что приводит к дополнительным возможностям прослушивания. Это увеличивает количество комбинаций для успешного перехвата связей сc, и, таким образом, вероятность отказа следует рассматривать как ε ≈ (c + 1)(εqkd)c. Безопасность квантовой сети Основной результат нашей статьи заключается в нотации безопасности для произвольных конфигураций квантовых сетей, использующих описанный протокол передачи ключей. Следуя принципу композиции, мы должны ограничить вероятность отказа квантовой сети (εqn) масштабированными вероятностями отказа протокола аутентификации (ε1) и протокола КРК (ε1) с помощью следующего выражения: εqn =ε1 +ε2 ≈(N−c−1)(εauth)c +2(εqkd)c. Полученная формула будет полезна и для построения будущих квантовых сетей, и может использоваться для оптимизации уже существующих. Pirandola, U. Andersen, L. Banchi, M. Berta, D. Bunandar, R. Colbeck, D. Englund, T. Gehring, C. Lupo, C. Ottaviani, J. Pereira, M. Razavi, J. S. Shaari, M. Tomamichel, V. C. Usenko, G. Vallone, P. Villoresi, and P. Wallden, “Advances in quantum cryptography,” arXiv:1906.01645 (2019). Pirandola, U. Andersen, L. Banchi, M. Berta, D. Bunandar, R. Colbeck, D. Englund, T. Gehring, C. Lupo, C. Ottaviani, J. Pereira, M. Razavi, J. S. Shaari, M. Tomamichel, V. C. Usenko, G. Vallone, P. Villoresi, and P. Wallden, “Advances in quantum cryptography,” arXiv:1906.01645 (2019). C. Elliott, “Building the quantum network,” New J. Phys. 4, 46 (2002). C. Elliott, “Building the quantum network,” New J. Phys. 4, 46 (2002). C. Elliott, A. Colvin, D. Pearson, O. Pikalo, J. Schlafer, and H. Yeh, “Current status of the DARPA quantum network,” Proc. SPIE 5815, 138–149 (2005). C. Elliott, A. Colvin, D. Pearson, O. Pikalo, J. Schlafer, and H. Yeh, “Current status of the DARPA quantum network,” Proc. SPIE 5815, 138–149 (2005). C. Elliott and H. Yeh, “DARPA quantum network testbed,” Tech. Rep. (BBN Technologies, 2007). C. Elliott and H. Yeh, “DARPA quantum network testbed,” Tech. Rep. (BBN Technologies, 2007). A. Poppe, M. Peev, and O. Maurhart, “Outline of the SECOQC quantum-key-distribution network in Vienna,” Int. J. Quantum Inf. 6, 209–218 (2008). A. Poppe, M. Peev, and O. Maurhart, “Outline of the SECOQC quantum-key-distribution network in Vienna,” Int. J. Quantum Inf. 6, 209–218 (2008). J. Dynes, A. Wonfor, W.-S. Tam, A. W. Sharpe, R. Takahashi, M. Lucamarini, A. Plews, Z. L. Yuan, A. R. Dixon, J. Cho, Y. Tanizawa, J.-P. Elbers, H. Greißer, I. H. White, R. V. Penty, and A. J. Shields, “Cambridge quantum network,” NPJ Quantum Inf. 5, 101 (2019). J. Dynes, A. Wonfor, W.-S. Tam, A. W. Sharpe, R. Takahashi, M. Lucamarini, A. Plews, Z. L. Yuan, A. R. Dixon, J. Cho, Y. Tanizawa, J.-P. Elbers, H. Greißer, I. H. White, R. V. Penty, and A. J. Shields, “Cambridge quantum network,” NPJ Quantum Inf. 5, 101 (2019). M. Peev, C. Pacher, R. Alléaume, et al., “The SECOQC quantum key distribution network in Vienna,” New J. Phys. 11, 075001 (2009). M. Peev, C. Pacher, R. Alléaume, et al., “The SECOQC quantum key distribution network in Vienna,” New J. Phys. 11, 075001 (2009). F. Xu, W. Chen, S. Wang, Z. Yin, Y. Zhang, Y. Liu, Z. Zhou, Y. Zhao, H. Li, D. Liu, Z. Han, and G. Guo, “Field experiment on a robust 942 Vol. 14, No. 11 / November 2022 / Journal of Optical Communications and Networking Research Article hierarchical metropolitan quantum cryptography network,” Chin. Sci. Bull. 54, 2991–2997 (2009). F. Xu, W. Chen, S. Wang, Z. Yin, Y. Zhang, Y. Liu, Z. Zhou, Y. Zhao, H. Li, D. Liu, Z. Han, and G. Guo, “Field experiment on a robust 942 Vol. 14, No. 11 / November 2022 / Journal of Optical Communications and Networking Research Article hierarchical metropolitan quantum cryptography network,” Chin. Sci. Bull. 54, 2991–2997 (2009). M. Sasaki, M. Fujiwara, H. Ishizuka, et al., “Field test of quantum key distribution in the Tokyo QKD network,” Opt. Express 19, 10387–10409 (2011). M. Sasaki, M. Fujiwara, H. Ishizuka, et al., “Field test of quantum key distribution in the Tokyo QKD network,” Opt. Express 19, 10387–10409 (2011). S. Wang, W. Chen, Z.-Q. Yin, et al., “Field and long-term demonstration of a wide area quantum key distribution network,” Opt. Express 22, 21739–21756 (2014). S. Wang, W. Chen, Z.-Q. Yin, et al., “Field and long-term demonstration of a wide area quantum key distribution network,” Opt. Express 22, 21739–21756 (2014). T. R. Beals and B. C. Sanders, “Distributed relay protocol for probabilistic information-theoretic security in a randomly-compromised network,” in International Conference on Information Theoretic Security (Springer, 2008), pp. 29–39. T. R. Beals and B. C. Sanders, “Distributed relay protocol for probabilistic information-theoretic security in a randomly-compromised network,” in International Conference on Information Theoretic Security (Springer, 2008), pp. 29–39. L. Salvail, M. Peev, E. Diamanti, R. Alléaume, N. Lütkenhaus, and T. Länger, “Security of trusted repeater quantum key distribution networks,” J. Comput. Secur. 18, 61–87 (2010). L. Salvail, M. Peev, E. Diamanti, R. Alléaume, N. Lütkenhaus, and T. Länger, “Security of trusted repeater quantum key distribution networks,” J. Comput. Secur. 18, 61–87 (2010). S. M. Barnett and S. J. Phoenix, “Securing a quantum key distribution relay network using secret sharing,” in IEEE GCC Conference and Exhibition (GCC) (IEEE, 2011), pp. 143–145. S. M. Barnett and S. J. Phoenix, “Securing a quantum key distribution relay network using secret sharing,” in IEEE GCC Conference and Exhibition (GCC) (IEEE, 2011), pp. 143–145. S. J. Phoenix and S. M. Barnett, “Relay QKD networks & bit transport,” arXiv:1502.06319 (2015). S. J. Phoenix and S. M. Barnett, “Relay QKD networks & bit transport,” arXiv:1502.06319 (2015). C. Ma, Y. Guo, and J. Su, “A multiple paths scheme with labels for key distribution on quantum key distribution network,” in IEEE 2nd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC) (IEEE, 2017), pp. 2513–2517. C. Ma, Y. Guo, and J. Su, “A multiple paths scheme with labels for key distribution on quantum key distribution network,” in IEEE 2nd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC) (IEEE, 2017), pp. 2513–2517. H. Zhou, K. Lv, L. Huang, and X. Ma, “Quantum network: security assessment and key management,” IEEE/ACM Trans. Netw. 30, 1328–1339 (2022). H. Zhou, K. Lv, L. Huang, and X. Ma, “Quantum network: security assessment and key management,” IEEE/ACM Trans. Netw. 30, 1328–1339 (2022). S. Rass, A. Wiegele, and P. Schartner, “Building a quantum network: how to optimize security and expenses,” J. Netw. Syst. Manag. 18, 283–299 (2010). S. Rass, A. Wiegele, and P. Schartner, “Building a quantum network: how to optimize security and expenses,” J. Netw. Syst. Manag. 18, 283–299 (2010). N. R. Solomons, A. I. Fletcher, D. Aktas, N. Venkatachalam, S. Wengerowsky, M. Loncˇ aric´ , S. P. Neumann, B. Liu, Ž. Samec, M. Stipcˇ evic´ , R. Ursin, S. Pirandola, J. G. Rarity, and S. K. Joshi, “Scalable authentication and optimal flooding in a quantum network,” arXiv:2101.12225 (2021). N. R. Solomons, A. I. Fletcher, D. Aktas, N. Venkatachalam, S. Wengerowsky, M. Loncˇ aric´ , S. P. Neumann, B. Liu, Ž. Samec, M. Stipcˇ evic´ , R. Ursin, S. Pirandola, J. G. Rarity, and S. K. Joshi, “Scalable authentication and optimal flooding in a quantum network,” arXiv:2101.12225 (2021). M. Pattaranantakul, A. Janthong, K. Sanguannam, P. Sangwongngam, and K. Sripimanwat, “Secure and efficient key management technique in quantum cryptography network,” in 4th International Conference on Ubiquitous and Future Networks (ICUFN) (IEEE, 2012), pp. 280–285. M. Pattaranantakul, A. Janthong, K. Sanguannam, P. Sangwongngam, and K. Sripimanwat, “Secure and efficient key management technique in quantum cryptography network,” in 4th International Conference on Ubiquitous and Future Networks (ICUFN) (IEEE, 2012), pp. 280–285. S. Das, S. Bäuml, M. Winczewski, and K. Horodecki, “Universal limitations on quantum key distribution over a network,” Phys. Rev. X 11, 041016 (2021). S. Das, S. Bäuml, M. Winczewski, and K. Horodecki, “Universal limitations on quantum key distribution over a network,” Phys. Rev. X 11, 041016 (2021). S. Pirandola, “End-to-end capacities of a quantum communication network,” Commun. Phys. 2, 51 (2019). S. Pirandola, “End-to-end capacities of a quantum communication network,” Commun. Phys. 2, 51 (2019). L. Jiang, J. M. Taylor, K. Nemoto, W. J. Munro, R. Van Meter, and M. D. Lukin, “Quantum repeater with encoding,” Phys. Rev. A 79, 032325 (2009). L. Jiang, J. M. Taylor, K. Nemoto, W. J. Munro, R. Van Meter, and M. D. Lukin, “Quantum repeater with encoding,” Phys. Rev. A 79, 032325 (2009). Z. Zhao, T. Yang, Y.-A. Chen, A.-N. Zhang, and J.-W. Pan, “Experimental realization of entanglement concentration and a quantum repeater,” Phys. Rev. Lett. 90, 207901 (2003). Z. Zhao, T. Yang, Y.-A. Chen, A.-N. Zhang, and J.-W. Pan, “Experimental realization of entanglement concentration and a quantum repeater,” Phys. Rev. Lett. 90, 207901 (2003). T.-J. Wang, S.-Y. Song, and G. L. Long, “Quantum repeater based on spatial entanglement of photons and quantum-dot spins in optical microcavities,” Phys. Rev. A 85, 062311 (2012). T.-J. Wang, S.-Y. Song, and G. L. Long, “Quantum repeater based on spatial entanglement of photons and quantum-dot spins in optical microcavities,” Phys. Rev. A 85, 062311 (2012). M. Ghalaii and S. Pirandola, “Capacity-approaching quantum repeaters for quantum communications,” Phys. Rev. A 102, 062412 (2020). M. Ghalaii and S. Pirandola, “Capacity-approaching quantum repeaters for quantum communications,” Phys. Rev. A 102, 062412 (2020). A. S. Cacciapuoti, M. Caleffi, F. Tafuri, F. S. Cataliotti, S. Gherardini, and G. Bianchi, “Quantum Internet: networking challenges in distributed quantum computing,” IEEE Netw. 34, 137–143 (2019). A. S. Cacciapuoti, M. Caleffi, F. Tafuri, F. S. Cataliotti, S. Gherardini, and G. Bianchi, “Quantum Internet: networking challenges in distributed quantum computing,” IEEE Netw. 34, 137–143 (2019). A. S. Cacciapuoti, M. Caleffi, R. Van Meter, and L. Hanzo, “When entanglement meets classical communications: quantum teleportation for the Quantum Internet,” IEEE Trans. Commun. 68, 3808–3833 (2020). A. S. Cacciapuoti, M. Caleffi, R. Van Meter, and L. Hanzo, “When entanglement meets classical communications: quantum teleportation for the Quantum Internet,” IEEE Trans. Commun. 68, 3808–3833 (2020). J. Illiano, M. Caleffi, A. Manzalini, and A. S. Cacciapuoti, “Quantum Internet protocol stack: a comprehensive survey,” arXiv:2202.10894 (2022). J. Illiano, M. Caleffi, A. Manzalini, and A. S. Cacciapuoti, “Quantum Internet protocol stack: a comprehensive survey,” arXiv:2202.10894 (2022). H.-K. Lo, M. Curty, and B. Qi, “Measurement-device-independent quantum key distribution,” Phys. Rev. Lett. 108, 130503 (2012). H.-K. Lo, M. Curty, and B. Qi, “Measurement-device-independent quantum key distribution,” Phys. Rev. Lett. 108, 130503 (2012). S. L. Braunstein and S. Pirandola, “Side-channel-free quantum key distribution,” Phys. Rev. Lett. 108, 130502 (2012). S. L. Braunstein and S. Pirandola, “Side-channel-free quantum key distribution,” Phys. Rev. Lett. 108, 130502 (2012). K. Tamaki, H.-K. Lo, C.-H. F. Fung, and B. Qi, “Phase encoding schemes for measurement-device-independent quantum key distribution with basis-dependent flaw,” Phys. Rev. A 85, 042307 (2012). K. Tamaki, H.-K. Lo, C.-H. F. Fung, and B. Qi, “Phase encoding schemes for measurement-device-independent quantum key distribution with basis-dependent flaw,” Phys. Rev. A 85, 042307 (2012). X. Ma and M. Razavi, “Alternative schemes for measurementdevice- independent quantum key distribution,” Phys. Rev. A 86, 062319 (2012). X. Ma and M. Razavi, “Alternative schemes for measurementdevice- independent quantum key distribution,” Phys. Rev. A 86, 062319 (2012). Y. Liu, T.-Y. Chen, L.-J. Wang, H. Liang, G.-L. Shentu, J. Wang, K. Cui, H.-L. Yin, N.-L. Liu, L. Li, X. Ma, J. S. Pelc, M. M. Fejer, C.-Z. Peng, Q. Zhang, and J.-W. Pan, “Experimental measurementdevice- independent quantum key distribution,” Phys. Rev. Lett. 111, 130502 (2013). Y. Liu, T.-Y. Chen, L.-J. Wang, H. Liang, G.-L. Shentu, J. Wang, K. Cui, H.-L. Yin, N.-L. Liu, L. Li, X. Ma, J. S. Pelc, M. M. Fejer, C.-Z. Peng, Q. Zhang, and J.-W. Pan, “Experimental measurementdevice- independent quantum key distribution,” Phys. Rev. Lett. 111, 130502 (2013). K. Goodenough, D. Elkouss, and S. Wehner, “Optimizing repeater schemes for the quantum internet,” Phys. Rev. A 103, 032610 (2021). K. Goodenough, D. Elkouss, and S. Wehner, “Optimizing repeater schemes for the quantum internet,” Phys. Rev. A 103, 032610 (2021). C. Ottaviani, C. Lupo, R. Laurenza, and S. Pirandola, “Modular network for high-rate quantum conferencing,” Commun. Phys. 2, 118 (2019). C. Ottaviani, C. Lupo, R. Laurenza, and S. Pirandola, “Modular network for high-rate quantum conferencing,” Commun. Phys. 2, 118 (2019). G.-J. Fan-Yuan, F.-Y. Lu, S. Wang, Z.-Q. Yin, D.-Y. He, Z. Zhou, J. Teng, W. Chen, G.-C. Guo, and Z.-F. Han, “Measurementdevice- independent quantum key distribution for nonstandalone networks,” Photon. Res. 9, 1881–1891 (2021). G.-J. Fan-Yuan, F.-Y. Lu, S. Wang, Z.-Q. Yin, D.-Y. He, Z. Zhou, J. Teng, W. Chen, G.-C. Guo, and Z.-F. Han, “Measurementdevice- independent quantum key distribution for nonstandalone networks,” Photon. Res. 9, 1881–1891 (2021). G.-J. Fan-Yuan, F.-Y. Lu, S. Wang, Z.-Q. Yin, D.-Y. He, W. Chen, Z. Zhou, Z.-H. Wang, J. Teng, G.-C. Guo, and Z.-F. Han, “Robust and adaptable quantum key distribution network without trusted nodes,” Optica 9, 812–823 (2022). G.-J. Fan-Yuan, F.-Y. Lu, S. Wang, Z.-Q. Yin, D.-Y. He, W. Chen, Z. Zhou, Z.-H. Wang, J. Teng, G.-C. Guo, and Z.-F. Han, “Robust and adaptable quantum key distribution network without trusted nodes,” Optica 9, 812–823 (2022). M. Lucamarini, Z. L. Yuan, J. F. Dynes, and A. J. Shields, “Overcoming the rate–distance limit of quantum key distribution without quantum repeaters,” Nature 557, 400–403 (2018). M. Lucamarini, Z. L. Yuan, J. F. Dynes, and A. J. Shields, “Overcoming the rate–distance limit of quantum key distribution without quantum repeaters,” Nature 557, 400–403 (2018). S. Pirandola, R. Laurenza, C. Ottaviani, and L. Banchi, “Fundamental limits of repeaterless quantum communications,” Nat. Commun. 8, 15043 (2017). S. Pirandola, R. Laurenza, C. Ottaviani, and L. Banchi, “Fundamental limits of repeaterless quantum communications,” Nat. Commun. 8, 15043 (2017). S. Pirandola, S. L. Braunstein, R. Laurenza, C. Ottaviani, T. P. Cope, G. Spedalieri, and L. Banchi, “Theory of channel simulation and bounds for private communication,” Quantum Sci. Technol. 3, 035009 (2018). S. Pirandola, S. L. Braunstein, R. Laurenza, C. Ottaviani, T. P. Cope, G. Spedalieri, and L. Banchi, “Theory of channel simulation and bounds for private communication,” Quantum Sci. Technol. 3, 035009 (2018). M. Minder, M. Pittaluga, G. Roberts, M. Lucamarini, J. Dynes, Z. Yuan, and A. Shields, “Experimental quantum key distribution beyond the repeaterless secret key capacity,” Nat. Photonics 13, 334–338 (2019). M. Minder, M. Pittaluga, G. Roberts, M. Lucamarini, J. Dynes, Z. Yuan, and A. Shields, “Experimental quantum key distribution beyond the repeaterless secret key capacity,” Nat. Photonics 13, 334–338 (2019). S. Wang, D.-Y. He, Z.-Q. Yin, F.-Y. Lu, C.-H. Cui, W. Chen, Z. Zhou, G.-C. Guo, and Z.-F. Han, “Beating the fundamental rate-distance limit in a proof-of-principle quantum key distribution system,” Phys. Rev. X 9, 021046 (2019). S. Wang, D.-Y. He, Z.-Q. Yin, F.-Y. Lu, C.-H. Cui, W. Chen, Z. Zhou, G.-C. Guo, and Z.-F. Han, “Beating the fundamental rate-distance limit in a proof-of-principle quantum key distribution system,” Phys. Rev. X 9, 021046 (2019). X. Zhong, J. Hu, M. Curty, L. Qian, and H.-K. Lo, “Proof-of-principle experimental demonstration of twin-field type quantum key distribution,” arXiv:1902.10209 (2019). X. Zhong, J. Hu, M. Curty, L. Qian, and H.-K. Lo, “Proof-of-principle experimental demonstration of twin-field type quantum key distribution,” arXiv:1902.10209 (2019). V. Chistiakov, A. Kozubov, A. Gaidash, A. Gleim, and G. Miroshnichenko, “Feasibility of twin-field quantum key distribution based on multi-mode coherent phase-coded states,” Opt. Express 27, 36551–36561 (2019). V. Chistiakov, A. Kozubov, A. Gaidash, A. Gleim, and G. Miroshnichenko, “Feasibility of twin-field quantum key distribution based on multi-mode coherent phase-coded states,” Opt. Express 27, 36551–36561 (2019). S. Wang, Z.-Q. Yin, D.-Y. He, W. Chen, R.-Q. Wang, P. Ye, Y. Zhou, G.-J. Fan-Yuan, F.-X. Wang, W. Chen, Y.-G. Zhu, P. V. Morozov, A. V. Divochiy, Z. Zhou, G.-C. Guo, and Z.-F. Han, “Twin-field quantum key distribution over 830-km fibre,” Nat. Photonics 16, 154–161 (2022). S. Wang, Z.-Q. Yin, D.-Y. He, W. Chen, R.-Q. Wang, P. Ye, Y. Zhou, G.-J. Fan-Yuan, F.-X. Wang, W. Chen, Y.-G. Zhu, P. V. Morozov, A. V. Divochiy, Z. Zhou, G.-C. Guo, and Z.-F. Han, “Twin-field quantum key distribution over 830-km fibre,” Nat. Photonics 16, 154–161 (2022). F. Grasselli, H. Kampermann, and D. Bruß, “Conference key agreement with single-photon interference,” New J. Phys. 21, 123002 (2019). F. Grasselli, H. Kampermann, and D. Bruß, “Conference key agreement with single-photon interference,” New J. Phys. 21, 123002 (2019). R. Renner, “Security of quantum key distribution,” Int. J. Quantum Inf. 6, 1–127 (2008). R. Renner, “Security of quantum key distribution,” Int. J. Quantum Inf. 6, 1–127 (2008). C. Portmann and R. Renner, “Cryptographic security of quantum key distribution,” arXiv:1409.3525 (2014). C. Portmann and R. Renner, “Cryptographic security of quantum key distribution,” arXiv:1409.3525 (2014). T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms (MIT, 2022). T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms (MIT, 2022). J. L. Carter and M. N. Wegman, “Universal classes of hash functions,” J. Comput. Syst. Sci. 18, 143–154 (1979). J. L. Carter and M. N. Wegman, “Universal classes of hash functions,” J. Comput. Syst. Sci. 18, 143–154 (1979). C. Portmann, “Key recycling in authentication,” IEEE Trans. Inf. Theory 60, 4383–4396 (2014). C. Portmann, “Key recycling in authentication,” IEEE Trans. Inf. Theory 60, 4383–4396 (2014). Y. Li and M. Chen, “Software-defined network function virtualization: a survey,” IEEE Access 3, 2542–2553 (2015). Research Article Vol. 14, No. 11 / November 2022 / Journal of Optical Communications and Networking 943 Y. Li and M. Chen, “Software-defined network function virtualization: a survey,” IEEE Access 3, 2542–2553 (2015). Research Article Vol. 14, No. 11 / November 2022 / Journal of Optical Communications and Networking 943 V. Martin, A. Aguado, J. Brito, A. Sanz, P. Salas, D. R. López, V. López, A. Pastor-Perales, A. Poppe, and M. Peev, “Quantum aware SDN nodes in the Madrid quantum network,” in 21st International Conference on Transparent Optical Networks (ICTON) (IEEE, 2019). V. Martin, A. Aguado, J. Brito, A. Sanz, P. Salas, D. R. López, V. López, A. Pastor-Perales, A. Poppe, and M. Peev, “Quantum aware SDN nodes in the Madrid quantum network,” in 21st International Conference on Transparent Optical Networks (ICTON) (IEEE, 2019). A. Aguado, V. Martin, D. Lopez, M. Peev, J. Martinez-Mateo, J. Rosales, F. de la Iglesia, M. Gomez, E. Hugues-Salas, A. Lord, R. Nejabati, and D. Simeonidou, “Quantum-aware software defined networks,” in International Conference on Quantum Cryptography (QCrypt) (2016). A. Aguado, V. Martin, D. Lopez, M. Peev, J. Martinez-Mateo, J. Rosales, F. de la Iglesia, M. Gomez, E. Hugues-Salas, A. Lord, R. Nejabati, and D. Simeonidou, “Quantum-aware software defined networks,” in International Conference on Quantum Cryptography (QCrypt) (2016). E. Hugues-Salas, F. Ntavou, D. Gkounis, G. T. Kanellos, R. Nejabati, and D. Simeonidou, “Monitoring and physical-layer attack mitigation in SDN-controlled quantum key distribution networks,” J. Opt. Commun. Netw. 11, A209–A218 (2019). E. Hugues-Salas, F. Ntavou, D. Gkounis, G. T. Kanellos, R. Nejabati, and D. Simeonidou, “Monitoring and physical-layer attack mitigation in SDN-controlled quantum key distribution networks,” J. Opt. Commun. Netw. 11, A209–A218 (2019).  "
  },
  {
    "article_id": "https://habr.com/ru/companies/tomhunter/articles/875640/",
    "title": "Топ самых интересных CVE за январь 2025 года",
    "category": "Кибербезопасность",
    "tags": "cve, vulnerabilities, vulnerability, vulnerability assessment, vulnerability scanning, уязвимости",
    "text": "⚠ Внимание ⚠Вся информация, представленная ниже, предназначена только для ознакомительных целей. Автор не несет ответственности за вред, который может быть причинен с помощью предоставленной им информации. В этой подборке представлены самые интересные уязвимости за январь 2025 года.Подведем вместе итоги первого месяца этого года, поехали! Навигация по уязвимостям:➡️Исправленная 0-day уязвимость в продуктах Apple➡️Многочисленные уязвимости в продуктах Cisco➡️Выполнение команд ОС в шлюзах SonicWall➡️Уязвимости в решениях Ivanti➡️Исправленные уязвимости в Microsoft➡️Обход механизма безопасности в 7-Zip 🟣 Исправленная 0-day уязвимость в продуктах Apple ▶ CVE-2025-24085 Об уязвимости:В системах iOS/iPadOS, macOS, tvOS, watchOS, visionOS компанииAppleобнаружена уязвимость Use-After-Free в компоненте CoreMedia, который отвечает за обработку аудио и видео на устройствах Apple. Затронуты версии iOS до iOS 17.2. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику повышать свои привилегии в системе. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление устройств до последней версии. Подробнее:CVE-2025-24085 🟣 Многочисленные уязвимости в продуктах Cisco ▶ CVE-2025-20156 Об уязвимости:В системе видеоконференцсвязиCiscoMeeting Managementдо 3.9 версии  обнаружена уязвимость, связанная с недостатками механизма авторизации пользователей REST API. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, прошедшему проверку подлинности, повысить свои привилегии в системе до администратора, отправляя API-запросы к определённому эндпоинту. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление продукта до последней версии (подробнее). Оценка уязвимости по шкале CVSS 3.1 —9.9баллов. Подробнее:CVE-2025-20156 ▶ CVE-2025-20165 Об уязвимости:ВCisco BroadWorksобнаружена уязвимость, связанная с некорректной обработкой SIP-запросов и позволяющая отправлять их в большом объеме в уязвимую систему, что может привести к исчерпанию памяти, выделенной сетевым серверам BroadWorks, обрабатывающим SIP-трафик. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, вызвать отказ в обслуживании (DoS). Исправление:Всем пользователям рекомендуется как можно скорее провести обновление продукта до последней версии (подробнее). Оценка уязвимости по шкале CVSS 3.1 —7.5баллов. Подробнее:CVE-2025-20165 ▶ CVE-2025-20128 Об уязвимости:В антивирусес открытым исходным кодомClamAVот компанииCiscoобнаружена уязвимость в механизме расшифровки файлов формата Object Linking and Embedding 2 (OLE2), связанная с целочисленным переполнением при проверке границ, что приводит к чтению из области за пределами выделенного буфера. В Cisco отметили, что для данной уязвимости уже доступна проверка концепции. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, вызвать отказ в обслуживании (DoS) и завершение процесса сканирования, отправив специально созданный файл (содержащий OLE2) на сканирование в ClamAV. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление продукта до версий 1.4.2 и 1.0.8 (подробнее). Оценка уязвимости по шкале CVSS 3.1 —5.3балла. Подробнее:CVE-2025-20128 🟣 Выполнение команд ОС в шлюзах SonicWall ▶ CVE-2025-23006 Об уязвимости:В шлюзах безопасного доступаSonicWallсерииSMA 1000вAppliance Management Console (AMC) и Central Management Console (CMC) обнаружена уязвимость внедрения команд ОС, связанная с некорректной работой механизма десериализации. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, при определенных условиях выполнять произвольные команды ОС. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление продукта до исправленной версии 12.4.3-02854 или более поздней версии. Также рекомендуется ограничить доступ к AMC и CMC только для доверенных источников. Оценка уязвимости по шкале CVSS 3.1 —9.8баллов. Подробнее:CVE-2025-23006 🟣 Уязвимости в решениях Ivanti ▶ CVE-2025-0282 Об уязвимости:В решенияхIvanti Connect Secureдо версии 22.7R2.5, Policy Secureдо версии 22.7R1.2, Neurons for ZTA Gatewayдо версии 22.7R2.3обнаружена уязвимость, связанная с переполнением буфера на основе стека. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, при определенных условиях выполнять произвольный код в целевой системе. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление продуктов до исправленных версий (подробнее). Оценка уязвимости по шкале CVSS 3.1 —9.0баллов. Подробнее:CVE-2025-0282 ▶ CVE-2025-0283 Об уязвимости:В решенияхIvanti Connect Secureдо версии 22.7R2.5, Policy Secureдо версии 22.7R1.2, Neurons for ZTA Gatewayдо версии 22.7R2.3обнаружена уязвимость, связанная с переполнением буфера на основе стека. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, прошедшему проверку подлинности, повышать свои привилегии в системе. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление продуктов до исправленных версий (подробнее). Оценка уязвимости по шкале CVSS 3.1 —7.0баллов. Подробнее:CVE-2025-0283 🟣 Исправленные уязвимости в Microsoft В традиционный Patch Tuesday компаниейMicrosoftбыло исправлено159 уязвимостей, из которых восемь являлись уязвимостями нулевого дня. Всем пользователям остается как можно скорее провести обновление своих продуктов до исправленных версий. ▶ Исправленные 0-day уязвимости CVE-2025-21333,CVE-2025-21334,CVE-2025-21335— уязвимости вWindows Hyper-V NT Kernel Integration VSP,позволяющие удаленному злоумышленнику повышать свои привилегии в системе до уровня SYSTEM.Оценка уязвимости по шкале CVSS 3.1 —7.8баллов. CVE-2025-21333,CVE-2025-21334,CVE-2025-21335— уязвимости вWindows Hyper-V NT Kernel Integration VSP,позволяющие удаленному злоумышленнику повышать свои привилегии в системе до уровня SYSTEM.Оценка уязвимости по шкале CVSS 3.1 —7.8баллов. CVE-2025-21275— уязвимость вWindows App Package Installer, позволяющая удаленному злоумышленнику повышать свои привилегии в системе до уровня SYSTEM.Оценка уязвимости по шкале CVSS 3.1 —7.8баллов. CVE-2025-21275— уязвимость вWindows App Package Installer, позволяющая удаленному злоумышленнику повышать свои привилегии в системе до уровня SYSTEM.Оценка уязвимости по шкале CVSS 3.1 —7.8баллов. CVE-2025-21186,CVE-2025-21366,CVE-2025-21395—RCE-уязвимостивMicrosoft Access,позволяющие удаленному злоумышленнику выполнять произвольный код. Успех эксплуатации зависит от пользователя, который должен открыть специально созданный документ Microsoft Access в целевой системе. Для защиты от этой проблемы Microsoft заблокировал доступ к документам Microsoft Access с расширениями: accdb, accde, accdw, accdt, accda, accdr и accdu, если они были отправлены по электронной почте.Оценка уязвимости по шкале CVSS 3.1 —7.8баллов. CVE-2025-21186,CVE-2025-21366,CVE-2025-21395—RCE-уязвимостивMicrosoft Access,позволяющие удаленному злоумышленнику выполнять произвольный код. Успех эксплуатации зависит от пользователя, который должен открыть специально созданный документ Microsoft Access в целевой системе. Для защиты от этой проблемы Microsoft заблокировал доступ к документам Microsoft Access с расширениями: accdb, accde, accdw, accdt, accda, accdr и accdu, если они были отправлены по электронной почте.Оценка уязвимости по шкале CVSS 3.1 —7.8баллов. CVE-2025-21308— уязвимость вWindows Themes, позволяющая удаленному злоумышленнику осуществлять спуфинг-атаки, используя специально созданный файл темы, отображаемый в Windows Explorer. Успех эксплуатации зависит от пользователя, который должен загрузить специально созданный файл в целевую систему, а затем выполнить определенные действия с этим файлом, при этом не требуется на него нажимать или открывать данный файл. Для защиты от этой проблемы предлагается отключить NTLM или активировать политику «Restrict NTLM: Outgoing NTLM traffic to remote servers».Оценка уязвимости по шкале CVSS 3.1 —6.5баллов. CVE-2025-21308— уязвимость вWindows Themes, позволяющая удаленному злоумышленнику осуществлять спуфинг-атаки, используя специально созданный файл темы, отображаемый в Windows Explorer. Успех эксплуатации зависит от пользователя, который должен загрузить специально созданный файл в целевую систему, а затем выполнить определенные действия с этим файлом, при этом не требуется на него нажимать или открывать данный файл. Для защиты от этой проблемы предлагается отключить NTLM или активировать политику «Restrict NTLM: Outgoing NTLM traffic to remote servers».Оценка уязвимости по шкале CVSS 3.1 —6.5баллов. ▶ Исправленные критические уязвимости CVE-2025-21298— уязвимость вWindows Object Linking and Embedding (OLE), позволяющая удаленному злоумышленнику выполнять произвольный код в целевой системе. Успех эксплуатации зависит от пользователя, который должен открыть специально созданное письмо с помощью почтовой программы Outlook, заранее отправленное злоумышленником.Опубликована проверка концепции, которая доступна поссылке.Оценка уязвимости по шкале CVSS 3.1 —9.8баллов. CVE-2025-21298— уязвимость вWindows Object Linking and Embedding (OLE), позволяющая удаленному злоумышленнику выполнять произвольный код в целевой системе. Успех эксплуатации зависит от пользователя, который должен открыть специально созданное письмо с помощью почтовой программы Outlook, заранее отправленное злоумышленником.Опубликована проверка концепции, которая доступна поссылке.Оценка уязвимости по шкале CVSS 3.1 —9.8баллов. CVE-2025-21311— уязвимость в протоколе аутентификацииWindows NTLM V1, позволяющая удаленному злоумышленнику повышать свои привилегии в системе.Оценка уязвимости по шкале CVSS 3.1 —9.8баллов. CVE-2025-21311— уязвимость в протоколе аутентификацииWindows NTLM V1, позволяющая удаленному злоумышленнику повышать свои привилегии в системе.Оценка уязвимости по шкале CVSS 3.1 —9.8баллов. CVE-2025-21307—RCE-уязвимостьвWindows Reliable Multicast Transport Driver (RMCAST), позволяющая удаленному злоумышленнику, не прошедшему проверку подлинности, выполнять произвольный код, совершив отправку специально созданных пакетов открытому сокету Windows Pragmatic General Multicast (PGM) на сервере. Успех эксплуатации зависит от наличия у пользователя программы, прослушивающей порт Windows PGM.Оценка уязвимости по шкале CVSS 3.1 —9.8баллов. CVE-2025-21307—RCE-уязвимостьвWindows Reliable Multicast Transport Driver (RMCAST), позволяющая удаленному злоумышленнику, не прошедшему проверку подлинности, выполнять произвольный код, совершив отправку специально созданных пакетов открытому сокету Windows Pragmatic General Multicast (PGM) на сервере. Успех эксплуатации зависит от наличия у пользователя программы, прослушивающей порт Windows PGM.Оценка уязвимости по шкале CVSS 3.1 —9.8баллов. 🟣 Обход механизма безопасности в 7-Zip ▶ CVE-2025-0411 Об уязвимости:В архиваторе7-Zip, начиная с версии 22.00,обнаружена уязвимость, связанная с некорректной обработкой архивных файлов и позволяющая обходить механизм безопасности Mark of the Web (MotW). Эксплуатация:Уязвимость позволяет удаленному злоумышленнику обойти предупреждения безопасности с целью выполнения произвольного кода в целевой системе. Успех эксплуатации зависит от пользователя, который должен открыть вредоносный файл. Исправление:Всем пользователям рекомендуется как можно скорее провести ручное обновление архиватора до исправленной версии 24.09 (подробнее). Оценка уязвимости по шкале CVSS 3.1 —7.0баллов. Подробнее:CVE-2025-0411 "
  },
  {
    "article_id": "https://habr.com/ru/companies/pt/articles/878392/",
    "title": "Вредоносные пакеты deepseeek и deepseekai были опубликованы в Python Package Index",
    "category": "Кибербезопасность",
    "tags": "python, pypi, open source, cybersecurity, открытое программное обеспечение, devsecops, pyanalysis, stealer, deepseek",
    "text": "В рамках исследования и отслеживания угроз группа Supply Chain Security департамента Threat Intelligence экспертного центра Positive Technologies (PT ESC) обнаружила и предотвратила вредоносную кампанию в главном репозитории пакетов Python Package Index (PyPI). Атака была нацелена на разработчиков, ML-специалистов и простых энтузиастов, которым могла быть интересна интеграция DeepSeek в свои системы. Репозиторий пакетов PyPI используется по умолчанию в пакетных менеджерах pip, pipenv, poetry. Злоумышленник, создавший учетную записьbvkв июне 2023 года и до этого не проявлявший активности, зарегистрировал вредоносные пакеты deepseeek и deepseekai 29 января 2025 года. Функции найденных пакетов направлены на сбор данных о пользователе, его компьютере и кражу переменных окружения после установки пакетов. Вредоносная функциональность выполняется при вызове консольных командdeepseeekилиdeepseekai,в зависимости от установленного пакета. Переменные окружения часто содержат конфиденциальные данные, необходимые для работы приложений, например API-ключи для S3-хранилища, учетные данные для баз данных и доступы к другим инфраструктурным ресурсам. В качестве контрольного сервера, куда выгружаются данные, разработчик этих двух пакетов использовал сервис Pipedream, который является интеграционной платформой для разработчиков. Примечательно, что код создан с использованием ИИ-ассистента, на что указывают характерные комментарии, объясняющие строки кода. Мы оперативно уведомили администраторов PyPI, пакеты уже удалены. Несмотря на быструю реакцию, пакет успели скачать 36 раз пакетным менеджером pip и средством зеркалирования bandersnatch и 186 раз — при помощи браузера, библиотеки requests и других средств. Время Событие 1/29/25 15:52 Опубликован пакет deepseeek 0.0.8 1/29/25 16:13 Опубликован пакет deepseekai 0.0.8 1/29/25 16:21 В рамках нашего репорта оба пакета отправлены в карантин и недоступны для скачивания через менеджеры пакетов 1/29/25 16:41 Администрация PyPI удалила пакет deepseeek и уведомила нас 1/29/25 16:42 Администрация PyPI удалила пакет deepseekai и уведомила нас Страна Способ скачивания Количество США Браузер 33 requests 19 pip 8 bandersnatch 2 Иной 55 Китай bandersnatch 8 pip 6 Браузер 4 Иной 18 Россия Браузер 3 requests 1 Иной 8 Гонконг pip 4 Браузер 4 bandersnatch 2 requests 1 Германия bandersnatch 4 requests 4 Браузер 2 Канада Браузер 3 requests 2 Швейцария requests 3 Браузер 2 Хорватия Браузер 4 Швеция requests 4 Польша Браузер 2 Великобритания Браузер 2 Ирландия Иной 2 Норвегия Браузер 2 Сингапур bandersnatch 2 Франция requests 2 Украина Браузер 1 Саудовская Аравия Браузер 1 Хакеры всегда следят за современными трендами и в нужный момент могут подсуетиться, используя хайп в своих целях. В этом кейсе мы разобрали достаточно безобидную атаку. Впрочем, из-за ажиотажа вокруг сервиса DeepSeek она могла бы привести к большему количеству жертв, если бы вредоносная активность пакета дольше оставалась незамеченной. Эти пакеты были выявлены с помощью сервиса для выявления подозрительных и вредоносных пакетовPT PyAnalysis. Сервис анализирует на предмет нежелательной активности новые релизы, создаваемые пользователями PyPI, в режиме реального времени. Рекомендуем проявлять бдительность в отношении ранее неизвестных вам пакетов, которые выдают себя за обертки для популярных сервисов. IoC Тип deepseeek пакет PyPI deepseekai пакет PyPI eoyyiyqubj7mquj.m.pipedream.net C2 "
  },
  {
    "article_id": "https://habr.com/ru/companies/pt/articles/878360/",
    "title": "Начинаем в багбаунти: распродажа уязвимостей в Juice Shop, или Как искать простейшие баги в веб-приложениях",
    "category": "Кибербезопасность",
    "tags": "cybersecurity, уязвимости и их эксплуатация, багхантинг, багбаунти, juice shop, burp suite, stored xss, idor, content-type",
    "text": "Привет, меня зовут Анна Куренова (@SavAnna), в IT я уже девять лет. Начинала как разработчик и тестировщик, потом начала искать уязвимости и перешла в ИБ. Сейчас работаю DevSecOps-инженером и веду телеграм-канал8ug8eer. В этой статье поговорим о базовых вещах в вебе и некоторых простых уязвимостях, поиск которых под силу даже начинающим. Текст будет полезен новичкам в сфере безопасности и тестирования софта. Если вам удобнее воспринимать информацию на слух, смело включайтевидео. В нем я рассказываю также о том, что не вошло в статью. Заодно советую посмотреть ипредыдущие выпуски, в которых другие участники Standoff Bug Bounty делятся историями о том, как они погружались в багхантинг. Всем, кто пропустил предыдущие серии нашего цикла, крайне рекомендую ознакомиться с текстом Олега Уланова (@olegbrain) про поискуязвимостей SSRFи статьей Александра aka bytehope о багах,связанных с контролем доступа. Статья носит исключительно информационный характер и не является инструкцией или призывом к совершению противоправных действий. Наша цель — рассказать о существующих уязвимостях, которыми могут воспользоваться злоумышленники, предостеречь пользователей и дать рекомендации по защите личной информации в интернете. Авторы не несут ответственности за использование опубликованной информации. Помните, что нужно следить за защищенностью своих данных. Поскольку мы начинаем с азов, то кратко расскажу о принципах работы веб-приложений. В случае рядового пользователя современный интернет работает на основе клиент-серверного взаимодействия. Клиент — это устройство пользователя, например компьютер или смартфон, а сервер — это удаленный компьютер, который обрабатывает запросы клиента. Для обмена запросами клиент и сервер используют различные протоколы, в нашем случае речь пойдет об HTTP. На основе него также написано много других протоколов, например HTTP Live Streaming (для потоковой передачи видео) или gRPC, разработанный компанией Google. Одно из популярных направлений в поиске уязвимостей — перехват и исследование запросов, с помощью которых «общаются» веб-приложения и браузер. Из них мы узнаем, какие методы HTTP-запросов применяются (GET, POST, PUT, DELETE и т. д.), какие заголовки и параметры передаются в запросах и ответах. Все это мы сейчас и будем применять на практике. Не все соки одинаково полезны Нашим подопытным станет магазин соков под незамысловатым названием Juice Shop. Это уже готовое приложение, которое работает на HTTP/1.1. Я развернула его на своем домене, который содержит множество уязвимостей и отлично подходит для обучения. Конечно, для анализа запросов можно ограничиться и стандартными инструментами браузера, такими как DevTools в Chrome. Для просмотра запросов и работы с ними я использую Burp Suite. Считаю, что это мастхэв для пентестеров и хорошая альтернатива OWASP ZAP. Burp Suite работает как прокси, то есть позволяет видеть все запросы, которые идут на сервер и обратно. А теперь давайте посмотрим, какие базовые уязвимости скрыты в нашем онлайн-магазине. Сразу оговорюсь, что здесь приведу лишь часть примеров, больше багов и теории ищитев записи трансляции. Первый улов: stored XSS Начинается все с главной страницы, где мы сразу же видим поле поиска. Первое, что приходит в голову, — попробовать ввести какой-то запрос и посмотреть на результат. Вводим цифру 1 и отмечаем, что сайт выводит наш текст на экран. Далее можно попробовать вписать туда какой-то заголовок (header), в своем примере я введу просто <lol>. Сайт принял и обработал его, следовательно, он уязвим к HTML-инъекции. Наше приложение неправильно обрабатывает введенные данные и позволяет внедрить сторонний код, который затем интерпретируется браузером. Теперь я попробую внедрить вредоносный JavaScript-код через запрос на добавление картинки. Впишем в поле поиска запрос на загрузку изображения с адреса X с обработчиком On Error. То есть, если приложение не сможет найти нашу картинку (а по адресу X этого сделать нельзя), на экране появится предупреждение. Таким образом, мы нашли XSS-уязвимость — ведь вместо алерта вполне реально «скормить» нашему сайту вредоносный JavaScript-код. Он будет выполняться каждый раз, когда другие пользователи заходят на страницу. С его помощью я могу попробовать, например, выкрасть чужие данные или перенаправить жертву на вредоносный ресурс. Вторая уязвимость: IDOR, или BOLA Продолжим изучать наш Juice Shop и попробуем добавить что-то в корзину, например яблочный сок. С помощью прокси я вижу два запроса — GET (корзины) и POST (товара). Помимо прочего, в них можно увидеть ID пользователя, который совершил эти действия. Попробуем изменить эти запросы, подставив другой идентификатор, — так мы проверим сайт на наличие уязвимости IDOR (insecure direct object reference), или BOLA (broken object level authorization). Суть здесь в том, что приложение позволяет получить информацию о любом пользователе. В нашем случае простая подмена ID позволила заглянуть в корзину другого любителя соков, что является очень опасным багом. Третья находка: подмена контента Сайт поддерживает загрузку контента. Например, пользователь может прикрепить файл к своему отзыву, что мы и сделаем. Нажимаем «Подтвердить» и смотрим, что нового появилось в нашем прокси. Видим запрос и строку file upload, а в поле Content-Type — значение application/pdf. Проще говоря, браузер сообщает сайту, что собирается загрузить PDF-файл. Разобравшись, как «общаются» клиент и сервер при обмене файлами, попробуем модифицировать этот запрос. Меняем тип контента и вписываем вместо PDF формат HTML. У нас получилось. А значит, и злоумышленник сможет добавить к отзыву вредоносный скрипт, сайт попробует его обработать и запустит полезную нагрузку. Подставим еще один тип контента и заменим значение в Content-Type на application/XML. Такой файл наше приложение тоже принимает, поэтому есть смысл добавить полезную нагрузку. Здесь важно знать о том, что документы XML включают такое понятие, как сущность. Это имя, которое будет использовано вместо того или иного содержимого. При этом каждая сущность может преобразовываться в другую. В своем XML-документе я пропишу, что имя and будет заменено на сущность SYSTEM (file:///etc/passwd) с данными о всех зарегистрированных аккаунтах. Сайт обработал наш запрос, а значит, он уязвим к атаке XML External Entity (XXE) — есть возможность выполнить сторонний код через XML-документ. Таким образом можно получить любые файлы, доступные пользователю, от имени которого запущено приложение. Если получится вернуть file:///etc/shadow, то приложение запущено от имени пользователя root. Заключение Несмотря на скромные вводные о работе веб-приложений, нам удалось проверить три гипотезы: Найти уязвимость XSS в поле поиска сайта, что позволяет атаковать всех его пользователей. Найти уязвимость XSS в поле поиска сайта, что позволяет атаковать всех его пользователей. Проэксплуатировать IDOR-уязвимость и посмотреть чужие данные. Проэксплуатировать IDOR-уязвимость и посмотреть чужие данные. Найти уязвимость XXE через подмену контента. Найти уязвимость XXE через подмену контента. Как видите, для этого мне хватило даже базовых знаний о принципах работы веба, а значит, получится и у вас. Если вам тоже нравится тестировать приложения и искать уязвимости, не бойтесь трудностей и смело начинайте свой путь багхантера! "
  },
  {
    "article_id": "https://habr.com/ru/companies/rshb/articles/878314/",
    "title": "Как о нас заботятся безопасники",
    "category": "Кибербезопасность",
    "tags": "информационная безопасность, информационные технологии, интернет-шоппинг, мошенничества в интернете, учетная запись, таргетированные атаки, фишинг, IoT, кибергигиена, уязвимости",
    "text": "«Защитим банк вместе» — под таким слоганом выходят регулярные выпуски корпоративного СМИ на актуальные темы в сфере ИБ от Департамента информационной безопасности@RSHB_tsyfra. В этой статье делаем обзор 12 выпусков — смотрим, какие ключевые темы затрагивали безопасники и какие рекомендации давали. Запустили голосование! Определите лучший выпуск с точки зрения важности и качества подачи информации в области ИБ для банков. Почему это важно? По подсчётам компании McAfee и Центра стратегических и международных исследований (CSIS), только за 2023 год прямой ущерб от компьютерных инцидентов в мире составил более 1 трлн долларов (это 1% мирового ВВП). С начала 2022-го по август 2024 года в России было совершено около 1,5 млн преступлений в ИТ-сфере, при этом кибермошенники похитили у россиян более 350 млрд рублей, по данным следственного департамента МВД РФ. Согласно оценкам компании Positive Technologies, основными последствиями успешных атак на организации стали утечки конфиденциальной информации (41%)и сбои в их основной деятельности (37%). В случае атак на частных лиц чаще всего происходили утечки чувствительных данных (69%) и прямые финансовые потери (32%). Голосуем за лучший выпуск в конце статьи. 1. Современные таргетированные атаки В этом выпуске безопасники рассказывают о том, что такое целевая атака, как она проходит и что нужно делать для противодействия. Проявлять осмотрительность при общении в социальных сетях, мессенджерах, на форумах, при заполнении различных анкет и опросов, не публиковать без необходимости информацию о своей должности, обязанностях, деталях технического оснащения и мерах безопасности, помнить, что фишинг остается одним из основных каналов проникновения вредоносного кода в сеть, не использовать внешние накопители, происхождение которых неизвестно, не заходить на посторонние сайты с рабочих компьютеров — основные рекомендации по этому блоку. 2. Правила безопасности при удалённой работе В этом выпуске — об обеспечении необходимого уровня безопасности при удалённой работе  с использованием корпоративных и личных устройств. Важно регулярно устанавливать обновления безопасности, информировать о вирусных заражениях, защищать корпоративные гаджеты от утери и кражи, соблюдать правила подключения к беспроводной сети, к сети банка. 3. Актуальные угрозы цифрового мира Знать правила безопасности в банке — это обязательно. В выпуске содержится обзор актуальных угроз информационной безопасности, с которыми сталкивается банк и его клиенты в цифровом мире — утечка конфиденциальной информации (кража данных о клиентах, сотрудниках и самом банке, которые могут быть использованы для мошенничества и шантажа), кражи и мошенничество (использование социальной инженерии, вирусов и других методов для хищения денежных средств со счетов клиентов и самого банка), атаки на инфраструктуру и кибервойны (целенаправленные атаки на критическую информационную инфраструктуру банка и других организаций, которые могут нанести серьезный ущерб), заражение вирусами, DDoS-атаки —  перегрузка каналов связи и систем банка с целью сделать их недоступными для клиентов. 4. Информационная безопасность и бизнес — новые реалии Рассказываем о том, как обеспечить информационную безопасность и выполнение бизнес-целей с минимальными рисками. Полностью избавиться от риска невозможно, но его можно уменьшить до приемлемого уровня. Для снижения риска можно уменьшить вероятность или ущерб от реализации угрозы, либо оба показателя сразу (например, внедрение средств защиты, резервное копирование). Помимо прямого снижения риска, возможны варианты с перекладыванием его на третью сторону (страхование, неустойки). Если риск невозможно снизить до приемлемого уровня, можно устранить его причину (отказаться от использования критичных технологий). ИБ призывает сотрудников влиять на оценку рисков и эффективность мер информационной безопасности. 5. Какие формальные внешние требования регулируют деятельность нашего Банка в области информационной безопасности? Выпуск информирует сотрудников о том, что требования по информационной безопасности (ИБ) в банковской сфере — это не ограничения, а инструкции для правильной организации и функционирования банковских процессов. Требования ИБ устанавливаются Банком России, а также платежными системами, они направлены на защиту информации, в том числе персональных данных клиентов и коммерческой тайны. Соблюдение требований ИБ является обязанностью всех сотрудников банка, а не только специалистов по ИБ. 6. Моя личная учётная запись Выпуск рассказывает об основных параметрах обеспечения безопасности личной учётной записи. Учетная запись (идентификатор (логин), пароль, права доступа и другие данные) может быть заблокирована в случае длительного отсутствия пользователя на рабочем месте или при шести неверных попытках ввода пароля. Для обеспечения безопасности учетной записи важно не использовать очевидные и легко подбираемые пароли, не передавать пароли другим лицам, не хранить пароли в открытом виде, не открывать подозрительные файлы, не посещать небезопасные сайты. 7. Социальная инженерия — как не стать проводником для киберпреступников Основная идея этого выпуска — рассказать о самых распространенных сценариях социальной инженерии: Запуск вредоносных макросов в документах (например, под видом продажи мебели, инвентаризации, новых методик расчета зарплаты). Запуск вредоносных макросов в документах (например, под видом продажи мебели, инвентаризации, новых методик расчета зарплаты). Требование ввода пароля (например, под видом приглашения на конференцию, оповещения о входе в аккаунт, записи на вакцинацию) Требование ввода пароля (например, под видом приглашения на конференцию, оповещения о входе в аккаунт, записи на вакцинацию) Запуск вредоносных файлов (например, под видом установки нового мессенджера, программ, технологии удаленного доступа) Запуск вредоносных файлов (например, под видом установки нового мессенджера, программ, технологии удаленного доступа) Кроме того, авторы выпуска обозначают признаки фишинговых писем: требование немедленного действия, кажущаяся выгода для пользователя, эксплуатация любопытства. ИБ рекомендует не разрешать запуск макросов, быть осторожными с подозрительными письмами, даже если они кажутся выгодными. 8. Что такое уязвимости? Технические уязвимости постоянно выявляются и закрываются производителями ПО, но также активно мониторятся злоумышленниками, которые могут использовать их для атак. Уязвимости могут быть как техническими (ошибки в программном обеспечении), так и организационными (ошибки персонала). Организационные уязвимости возникают из-за ошибок сотрудников при настройке систем или их эксплуатации. Например, случайное предоставление излишних прав доступа. Банк проводит регулярные аудиты и тестирование на проникновение, чтобы выявлять технические и организационные уязвимости. Но для этого также нужна помощь всех сотрудников, которые должны сообщать о замеченных проблемах. 9. Онлайн-шоппинг Основные акценты выпуска — рекомендации для защиты от кибермошенников, которые обманывают покупателей в сезон распродаж. Проверяйте адрес сайта, на который переходите, чтобы он не был похож на фишинговый. Не регистрируйтесь на неизвестных мошеннических сайтах, это может привести к краже данных. Проверяйте адрес сайта, на который переходите, чтобы он не был похож на фишинговый. Не регистрируйтесь на неизвестных мошеннических сайтах, это может привести к краже данных. Читайте отзывы о продавце, сайте. Читайте отзывы о продавце, сайте. Следите за предупреждениями операционной системы, браузера и антивируса. Следите за предупреждениями операционной системы, браузера и антивируса. Используйте отдельный email и банковскую карту для онлайн-покупок. Используйте отдельный email и банковскую карту для онлайн-покупок. Не совершайте покупки и банковские операции в публичных Wi-Fi сетях. Не совершайте покупки и банковские операции в публичных Wi-Fi сетях. По возможности используйте платежные сервисы вместо ввода данных карты. По возможности используйте платежные сервисы вместо ввода данных карты. 10. Основы кибергигиены В этом выпуске напоминают о правилах безопасности при использовании мессенджеров в мобильных устройствах. Не оставляйте телефон без присмотра  в общественных местах, обязательно установите пароль или биометрическую защиту, не повышайте привилегии на устройстве (jailbreak или root), не устанавливайте сторонние сертификаты, следите за актуальностью ОС и приложений, устанавливайте их только из официальных магазинов, используйте сложные пароли, меняйте их часто, используйте менеджеры паролей, пользуйтесь только необходимыми программами, ограничьте использование публичных Wi-Fi сетей. 11. Как избежать горя от ума Выпуск предупреждает о рисках использования IoT-устройств. Основные проблемы безопасности IoT-устройств — возможность физического доступа и модификации устройств, использование уязвимого программного обеспечения с известными уязвимостями, использование производителями легко взламываемых паролей, применение небезопасных протоколов передачи данных. Пользователям IoT-устройств необходимо проверять отзывы о безопасности устройств перед покупкой, менять стандартные пароли на сложные и регулярно их обновлять, устанавливать последние обновления ПО, включать доступные функции безопасности в настройках устройств. 12. К безопасности во всеоружии Выпуск призывает сотрудников узнавать больше о процессе обеспечения информационной безопасности, так как большинство атак на корпоративный сектор предполагает использование обычных пользователей в качестве точки проникновения. Предлагается использовать дополнительные внешние ресурсы для повышения уровня знаний в области информационной безопасности, помимо внутренних обучений и тренингов в компании: Курс по основам информационной безопасности— курс для тех, кто хочет систематизировать свои знания в области информационной безопасности)Портал Anti-Malware.ru— портал, в основном посвященный средствам защиты информации. Приводятся сравнения разных продуктов, в том числе ответы на вопрос «Какой антивирус лучше». Курс по Безопасности АСУ ТП— рассматриваются различные атаки на промышленные системы и способы защиты от них. Курс по безопасности веб-проектов— рассматриваются атаки на веб-сервисы и способы защиты, а также применяемые в веб-сервисах технологии. Блог Алексея Лукацкого— рассказывается про изменения в действующем отечественном законодательстве, поясняются спорные моменты, описываются новые угрозы и тенденции. Ресурс от Лаборатории Касперского— содержательный дайджест, рассказывающий о современных угрозах, технологиях, событиях простым и понятным языком.Подборки публикаций компании R-Vision— здесь можно найти как официальные релизы, так и посты известных журналистов и блогеров, в том числе англоязычных. Справочник законодательства РФ в области информационной безопасности— самый полный и постоянно обновляемый справочник. "
  },
  {
    "article_id": "https://habr.com/ru/companies/ozontech/articles/877692/",
    "title": "Маскирование данных от А до Я",
    "category": "Кибербезопасность",
    "tags": "sql, oracle sql, postgres, mssql, безопасность данных, маскирование данных, маскирование, ddm",
    "text": "Всем привет! Меня зовут Светлана Анохина, я младший бизнес-партнёр по информационной безопасности в Ozon. Несмотря на небольшой стаж, опыт работы у меня довольно разносторонний, ведь моя роль предполагает участие практически во всех аспектах безопасности (если интересно, кто такие бизнес-партнёры по ИБ, то рекомендую почитать статьи моих коллегДашиилиМаксимана эту тему). До этого я работала в команде Compliance, где активно занималась защитой данных. Сейчас моя зона ответственности охватывает безопасность в логистике и товарных операциях: склады, пункты выдачи заказов, курьерская доставка и все сопутствующие процессы. На первых этапах погружения в профессию, когда я была студенткой и только начинала разбираться в защите данных, проблемы безопасности казались мне чем-то далёким и почти гипотетическим. Мысль о том, что личная информация — адрес, электронная почта или данные банковской карты — может попасть в чужие руки, не вызывала у меня особого беспокойства. Утечка данных? Да, неприятно, но ведь это происходит где-то далеко и вряд ли касается меня напрямую. Но всё изменилось, когда я впервые устроилась работать в команду безопасности данных крупной компании. Я увидела, как это выглядит в реальной жизни: настоящие утечки, реальные данные и конкретные последствия. Тогда я осознала, что даже самая простая ошибка или недосмотр могут привести к тому, что чья-то личная информация окажется у злоумышленников, и защита данных — это не прихоть, а насущная необходимость. Каждая строка кода, каждое принятое решение могут стать барьером между безопасностью и катастрофой. В компаниях с развитой культурой информационной безопасности обычно уже испробовано многое: от простых решений до сложных, почти инженерных конструкций. На первых порах, будучи младшим специалистом, я поняла, что изобрести нечто радикально новое вряд ли удастся. Тогда я решила попробовать другой подход: взять уже известный метод защиты и попробовать применить его по-новому. Конечно, не всё сразу получалось, но на ошибках я училась, развивала навыки и продолжаю это делать до сих пор. За время своей работы в e-commerce-компаниях я собрала некоторый перечень подходов и инструментов защиты, которые реализую в своём направлении. Сегодня я хочу рассказать о маскировании вам, моим коллегам, которые так же, как и я, стремятся сделать данные безопасными, а жизнь пользователей — спокойной. Этот инструмент помогает нам обеспечивать безопасность в ключевых бизнес-процессах, и я уверена, что он будет интересен многим из вас! Так что, специалисты по информационной безопасности, администраторы и разработчики баз данных, аналитики, студенты IT-направлений и просто заинтересованные безопасностью пользователи, заваривайте вкусный чай, садитесь поудобнее и готовьтесь вместе со мной погрузиться в мир защиты данных! Итак, нам известно, что утечка даже небольшой части конфиденциальных данных может привести к серьёзным последствиям: потере репутации компании, крупным штрафам от регуляторов и многомиллионным убыткам. Причины таких инцидентов могут быть разными: от целенаправленных атак хакеров до ошибок сотрудников или недостаточной защиты тестовых сред. Как же мы можем защититься от утечек? Защита данных — это многослойный процесс, где каждая мера важна. Начнём с базовых методов. Первое, что приходит на ум, — этошифрование, использование сложных паролей и ограничение доступа извне. Это фундамент безопасности, своеобразная «основа», без которой нельзя строить систему защиты. Следующий этап —мониторинг: кто заходит в базу, что делает, какие данные запрашивает. Логирование здесь — инструмент номер один. Оно позволяет не только выявлять подозрительные активности, но и разбираться в причинах возможных инцидентов. Кроме того, важным этапом является защита на сетевом уровне: настройка брандмауэров, использование VPN для защищённого подключения, ограничение IP-адресов, с которых можно получить доступ к системе. Всё это создаёт дополнительный барьер для злоумышленников. Но вот что часто остаётся за кадром. Даже если вы настроили идеальную защиту от внешних угроз, данные внутри компании продолжают активно использоваться: их анализируют, передают разработчикам, копируют для тестирования. И именно здесь, внутри организации, утечка может произойти в самый неожиданный момент. И здесь на помощь приходитмаскирование данных. Что это такое? По сути, это фильтр, который подменяет реальные данные на фиктивные или обезличенные. Например, разработчики или тестировщики видят подставные данные, которые выглядят как настоящие, но при этом не содержат никакой чувствительной информации. Почему этот метод так эффективен? Во-первых, даже если данные утекут, они будут бесполезны для злоумышленников. Во-вторых, это помогает компаниям соблюдать GDPR или ФЗ-152, которые требуют строгого ограничения и минимизации доступа к персональным данным. И, что немаловажно, процесс маскирования довольно просто внедрить, если подойти к нему правильно. Важно лишь, чтобы замаскированные данные могли использоваться для тестирования или анализа. В большинстве случаев этот метод применяется для защиты персональных данных и корпоративных секретов. Существует множество алгоритмов маскирования, каждый из которых подходит для своих задач. 1.     Начнём с самого популярного — подменазначений. Здесь всё просто: берём реальные данные и заменяем их чем-то похожим. Например, вместо имени «Иван Иванов» подставляем «Алексей Смирнов». Формат сохраняется, данные выглядят правдоподобно, и для тестирования это идеальный вариант. Но здесь стоит учитывать, что при подмене есть вероятность случайно «попасть» в реальные данные другого человека и начать нелегитимно их обрабатывать. Можно минимизировать этот риск, используя вымышленные имена и фамилии, генерируя псевдонимы, частично модифицируя имена «Иван» -> «Ивак» или совмещая этот алгоритм с генерацией синтетических данных. 2.     Дальше идётобфускацияили, проще говоря, зашифровывание данных таким образом, чтобы они стали нечитаемыми. Например, ваш телефон «123-456-7890» превращается в «XXX-XXX-7890» или адрес электронной почты превращается в email@domen.ru. Отличный способ скрыть чувствительные данные, при этом структура остаётся понятной. 3.     Для простых случаев подойдётредактирование данных— мы просто скрываем или удаляем часть информации. Например, вместо адреса «Москва, ул. Ленина, д. 10» показываем «Москва, ул. Ленина, д. ***». Быстро, просто, но эффективно. 4.     Есть ещёперестановка данных, или «перемешивание». Этот метод особенно полезен, если у вас большие наборы данных. Например, зарплаты сотрудников можно перемешать, чтобы никто не мог понять, какая из них кому принадлежит. 5.     Отдельно стоит упомянутьгенерацию фейковых данных. Здесь вообще всё заменяется синтетическими значениями. Это идеальный вариант, если вам нужно создать абсолютно безопасный тестовый набор данных. Например, генерируем вымышленные адреса, телефоны и всё остальное. Ещё один значимый плюс этого способа: можно генерировать значения с помощью детерминированных алгоритмов. То есть если один и тот же алгоритм используется в разных системах с одним и тем же ключом или правилами, то результат преобразований будет одинаковым. Благодаря этому исходные данные будут всегда иметь идентичные значения, что позволяет идентифицировать совпадения в разных системах, не раскрывая реальные данные. 6.     А воттокенизация— это уже уровень посложнее. Мы заменяем реальные данные токенами, то есть уникальными идентификаторами, а связь с исходной информацией хранится отдельно. Это особенно полезно для работы с кредитными картами и другой финансовой информацией. Например, вместо номера карты «1234-5678-9012-3456» видим что-то вроде «ABC123DEF456». 7.     И наконец,дифференциальная приватность. Это что-то из продвинутой аналитики. Суть в том, что к данным добавляется шум, чтобы сохранить общую картину, но скрыть индивидуальные значения. Например, при анализе среднего возраста система добавляет случайные отклонения, чтобы нельзя было узнать возраст конкретного человека. Как же выбрать алгоритм?Всё зависит от задачи. Если нужно сохранить формат — выбирайте подмену или генерацию фейковых данных. Для высоких требований безопасности — токенизацию или обфускацию. А для аналитики подойдёт дифференциальная приватность. Всё это — инструменты, которые можно и нужно комбинировать. Нет единственного «идеального» алгоритма, но правильно подобранное маскирование позволяет защищать данные без ущерба для их использования. С алгоритмами стало более-менее понятно. Но как именно внедрить маскирование правильно и безболезненно для бизнес-процессов компании? В зависимости от задач и среды, где данные используются, маскирование также делится на несколько видов. Каждый из них имеет свои особенности, преимущества и области применения. Давайте поговорим остатическом маскировании данных. Этот метод на самом деле — один из самых понятных, но при этом очень эффективный, особенно если правильно его настроить. Итак, что это такое? Статическое маскирование (SDM) — это процесс, при котором мы берём реальные данные, модифицируем их (заменяем, обфусцируем или редактируем) и сохраняем в этой изменённой форме в новой базе. То есть у нас получается отдельная копия данных, которая выглядит как настоящая, но на самом деле не содержит ничего чувствительного. Какие у этого метода преимущества? Безопасность на уровне среды: когда данные уже замаскированы, даже если кто-то получит доступ к копии базы, ничего критичного он из неё не извлечёт. Безопасность на уровне среды: когда данные уже замаскированы, даже если кто-то получит доступ к копии базы, ничего критичного он из неё не извлечёт. Идеально для тестирования и разработки: разработчики и тестировщики могут спокойно работать с маскированными данными, которые полностью отражают логику оригинальных данных. Идеально для тестирования и разработки: разработчики и тестировщики могут спокойно работать с маскированными данными, которые полностью отражают логику оригинальных данных. Допустим, у вас есть база с персональными данными курьеров, и вам нужно протестировать новую систему мониторинга доставки. Очевидно, что разработчики не должны видеть настоящие данные: имена, почты, номера телефонов. Вы используете SDM: заменяете настоящие имена на фейковые, номера телефонов — на набор цифр, а реальные почты — на случайно сгенерированные, сохраняя при этом структуру данных. Как это реализовать? В большинстве СУБД, таких как Oracle, SQL Server или PostgreSQL, есть инструменты для статического маскирования: в Oracle это можно сделать с помощью Data Masking Pack; в Oracle это можно сделать с помощью Data Masking Pack; в SQL Server есть специальные функции для генерации псевдонимов и обработки данных; в SQL Server есть специальные функции для генерации псевдонимов и обработки данных; в PostgreSQL можно использовать сторонние инструменты вроде pgmask или писать свои скрипты. в PostgreSQL можно использовать сторонние инструменты вроде pgmask или писать свои скрипты. Главный нюанс здесь — нужно понимать, какие данные маскировать. Например, если это паспортные данные или номера карт, то важно, чтобы они выглядели реалистично. Иначе тестовые системы могут просто не принять их. SDM хорошо работает, если ваши данные часто копируются для анализа или тестирования. Это простое, но надёжное решение для того, чтобы конфиденциальные данные не выходили за пределы защищённой среды. Однако в тех случаях, когда необходимы гибкость и сохранение данных для авторизованных пользователей, SDM неприменимо. И здесь нам на помощь приходит динамическое маскирование данных. Этот способ более сложный, но зато невероятно гибкий и удобный, особенно когда вы хотите защитить данные в реальном времени. Что же такое динамическое маскирование данных? Динамическое маскирование (DDM)— это процесс, при котором данные остаются в своей оригинальной форме в базе, но при обращении к ним система автоматически подменяет или скрывает определённые значения в зависимости от настроек доступа. То есть пользователь, который запрашивает данные, видит только маскированную версию, в то время как сами данные остаются нетронутыми. Как это работает на практике? Допустим, у вас есть база клиентов интернет-магазина. Специалист службы поддержки видит только часть данных: например, вместо полного ФИО отображается «Иванов К.». А вот администратор системы, обладающий соответствующими правами, видит эти данные полностью. Эта настройка может зависеть от роли пользователя или даже от типа запроса. Пример реализации ВSQL Serverдинамическое маскирование данных уже встроено. Вы можете настроить маскирование на уровне базы, используя специальные политики. Например: В этом случае пользователи без особых прав доступа будут видеть замаскированное имя и часть номера карты. ВPostgreSQL и MySQLдинамическое маскирование можно реализовать с помощью триггеров, представлений или расширений. Например, вы можете создать представление, которое возвращает маскированные данные в зависимости от пользователя: Когда это особенно полезно? Если у вас много пользователей с разными уровнями доступа к базе — что очень актуально для больших компаний с огромным количеством сервисов и бизнес-процессов. Если у вас много пользователей с разными уровнями доступа к базе — что очень актуально для больших компаний с огромным количеством сервисов и бизнес-процессов. Когда важна защита данных в реальном времени, например, в CRM-системах. Когда важна защита данных в реальном времени, например, в CRM-системах. Если нужно минимизировать риски утечки данных через UI систем. Если нужно минимизировать риски утечки данных через UI систем. Если вдруг вам всё ещё не до конца понятно, как это работает, и остались вопросы, предлагаю рассмотреть следующий сценарий применения динамического маскированияв компании. Компания обрабатывает персональные данные клиентов, такие как номера телефонов, email, и финансовую информацию. Сотрудники отдела маркетинга должны видеть только контактные данные клиентов в обезличенном формате, а полный доступ нужен только аналитикам и администраторам. Реализация Создание политики маскирования Создание политики маскирования Настраиваем маскирование для столбцов, содержащих данные: 2. Управление правами доступа Менеджеры получают доступ к маскированным данным: Аналитики и администраторы имеют полный доступ: 3. Пример запросов: Запрос от имени менеджера: Результат: Email Phone CreditCard ------------------- -------------- ------------ uXXX@XXXX.com XXX-XXX-XXXX XXXXXX Запрос от имени администратора: Результат: Email Phone CreditCard ------------------- ------------------ --------------- user@example.com +77777777777 1234567890123456 Вот так это работает в Microsoft SQL Server, где настройки довольно интуитивны и встроены в базовый функционал. Oracle Теперь давайте посмотрим, как DDM реализуется в Oracle. Oracle предлагает ещё больше гибкости, позволяя настраивать политики маскирования через мощный пакет DBMS_REDACT. Эта технология позволяет не только подменять данные «на лету», но и задавать сложные условия, при которых маскирование будет применяться. Перейдём к конкретному примеру, чтобы увидеть, как это работает. 1. Создаём таблицу Clients, где храним персональные данные клиентов: 2. Настраиваем политики динамического маскирования Используем пакет DBMS_REDACT, чтобы замаскировать контактные данные (номер телефона, email) и финансовую информацию. Маскирование номера телефона:показываем только последние 4 цифры. Маскирование email:скрываем часть адреса, оставляя первую букву и домен. Маскирование номера карты:показываем нулевые значения вместо реального баланса. 3. Проверка политики Запрос от сотрудника отдела маркетинга (без привилегий): Результат: FullName Phone Email CreditCard ------------ ------------ ------------------- ------- Johny Jo *-*-4567 jXXX@XXXX.com *-*-*-7654 Jenny Je *-*-6543 jXXX@XXXX.com *-*-*-4567 Запрос от аналитика или администратора (с привилегиями): аналитики и администраторы видят данные без маскирования. Для этого им предоставляется привилегия EXEMPT REDACTION POLICY: Результат: FullName Phone Email CreditCard ------------ ------------ --------------------- ------- Johny Jo 555-123-4567 johny.jo@example.com 1234567890987654 Jenny Je 555-987-6543 jenny.je@example.com 09876543212345678 4. Удаление политики (при необходимости) Если потребуется удалить политику маскирования: Теперь давайте поговорим о преимуществах и ограничениях динамического маскирования данных. Этот инструмент на первый взгляд кажется почти магическим, но важно понимать, где он действительно хорош, а где есть нюансы. Почему DDM — это круто? Простота реализации. Экономия времени и ресурсов. Это, пожалуй, самый сильный аргумент. Вам не нужно переписывать код приложения, добавлять сложные процедуры или создавать отдельные базы. За счёт того, что маскирование можно внедрять в уже готовые решения, оно помогает очень легко скрыть критичные данные, не переделывая ролевую модель и не ломая бизнес-процессы. Простота реализации. Экономия времени и ресурсов. Это, пожалуй, самый сильный аргумент. Вам не нужно переписывать код приложения, добавлять сложные процедуры или создавать отдельные базы. За счёт того, что маскирование можно внедрять в уже готовые решения, оно помогает очень легко скрыть критичные данные, не переделывая ролевую модель и не ломая бизнес-процессы. Гибкость. Представьте, что один и тот же набор данных видят разные пользователи по-разному. Например, аналитик видит только обрезанные номера карт, а администратор с доступом — всё, что нужно. Это реально удобно, когда в системе много ролей. Гибкость. Представьте, что один и тот же набор данных видят разные пользователи по-разному. Например, аналитик видит только обрезанные номера карт, а администратор с доступом — всё, что нужно. Это реально удобно, когда в системе много ролей. Снижение риска утечек реальных данных через UI. Даже если произойдёт что-то неприятное, например, SQL-инъекция, злоумышленник получит только замаскированные данные. Да и при ошибках в конфигурации системы это тоже дополнительный уровень защиты. Снижение риска утечек реальных данных через UI. Даже если произойдёт что-то неприятное, например, SQL-инъекция, злоумышленник получит только замаскированные данные. Да и при ошибках в конфигурации системы это тоже дополнительный уровень защиты. Соответствие стандартам. DDM может закрыть сразу несколько пунктов в требованиях к безопасности, будь то ФЗ-152, GDPR или PCI DSS. Это не то чтобы панацея, но настроить — проще простого. Соответствие стандартам. DDM может закрыть сразу несколько пунктов в требованиях к безопасности, будь то ФЗ-152, GDPR или PCI DSS. Это не то чтобы панацея, но настроить — проще простого. Звучит это всё слишком сказочно. В чем же подвох? DDM не защищает от «своих» с доступом UNMASK. Если у кого-то есть права «видеть всё», то никакое маскирование уже не поможет. Поэтому таких людей должно быть минимум, и каждый должен знать, зачем ему этот доступ. DDM не защищает от «своих» с доступом UNMASK. Если у кого-то есть права «видеть всё», то никакое маскирование уже не поможет. Поэтому таких людей должно быть минимум, и каждый должен знать, зачем ему этот доступ. Не заменяет шифрование. Тут нужно быть честными: DDM маскирует только на уровне визуализации. Данные в базе остаются в исходной форме, и, если кто-то получит прямой доступ к ней, он их увидит. Не заменяет шифрование. Тут нужно быть честными: DDM маскирует только на уровне визуализации. Данные в базе остаются в исходной форме, и, если кто-то получит прямой доступ к ней, он их увидит. Не работает на резервных копиях. Если кто-то украл вашу бэкап-базу, маскирование на ней работать не будет. Это важно учитывать, особенно если у вас нет политики шифрования бэкапов. Не работает на резервных копиях. Если кто-то украл вашу бэкап-базу, маскирование на ней работать не будет. Это важно учитывать, особенно если у вас нет политики шифрования бэкапов. Итого мы получаем следующее. Динамическое маскирование — это отличный инструмент, чтобы быстро и просто защитить данные на уровне доступа. Он решает много задач и экономит ресурсы, но нужно помнить, что это не замена всем остальным мерам безопасности. Это одна из частей комплексного подхода, и, если вы её внедрите, она точно сделает вашу жизнь немного спокойнее. Главный вызов динамического маскирования — это аналитика и настройка. Для больших систем с множеством ролей и сложными запросами потребуется время, чтобы правильно описать все сценарии доступа. Тем не менее это мощный инструмент, который позволяет защищать данные там, где статическое маскирование становится неудобным. Динамическое маскирование идеально подходит для современных систем, где доступ к данным предоставляется большому количеству пользователей с разными уровнями привилегий. Кажется, что динамическое маскирование превосходит статическое по всем параметрам: гибкость, защита данных в реальном времени, минимальные затраты на поддержку. Но все же в ситуациях, когда необходимо обезличить чувствительные данные для всех пользователей (к примеру, на DEV/STG-средах), SDM безусловно выигрывает. Не стоит ограничиваться одним методом, следует адекватно оценить процессы и чувствительность данных и применять разные виды маскирования в зависимости от ваших потребностей. Заключение Мы разобрали, что такое маскирование, какие существуют алгоритмы, где их лучше применять и как этот метод может стать частью общей стратегии защиты данных. Как и любая мера безопасности, маскирование эффективно только в рамках комплексного подхода: оно дополняет шифрование, контроль доступа, мониторинг и другие инструменты. Внедрение маскирования — это не сложность, а инвестиция в будущее вашей компании. Это шаг к созданию защищённой и надёжной среды, где данные остаются под контролем, даже если они активно используются внутри организации. Помните: защита данных — это не только про технологии, но и про осознанное отношение к информации, с которой мы работаем. Если вы подойдёте к маскированию с вниманием и пониманием, то сможете превратить его в мощный элемент вашей системы безопасности. Теперь у вас есть всё необходимое, чтобы внедрить маскирование правильно, эффективно и с минимальными затратами. Время действовать! "
  },
  {
    "article_id": "https://habr.com/ru/companies/otus/articles/877960/",
    "title": "Особенности написания эксплоитов под х64",
    "category": "Кибербезопасность",
    "tags": "reverse-engineering, exploit, msfvenom, реверс-инжиниринг, buffer overflow, эксплоиты",
    "text": "Современные механизмы защиты от уязвимостей переполнения буфера существенно усложняют реализацию таких атак, однако buffer overflow по‑прежнему остается одним из самых распространенных видов уязвимостей. В этой статье мы поговорим об особенностях написания эксплоитов под 64-битную архитектуру. В сети присутствует множество публикаций, посвященных эксплоитам в 32-битной архитектуре, но на практике такие приложения можно встретить все реже, поэтому мы будем говорить об х64. Прежде чем переходить к рассмотрению эксплоита, немного поговорим об отличиях 32 и 64-битных архитектур. Начнем с регистров. Регистры В 64-битной архитектуре у нас добавляются новые регистры большего размера. Но ничего сложного нет: для того, чтобы получить доступ к 64-битной версии каждого регистра, замените префикс E из 32-битной нотации на R. Как видно на рисунке ниже, есть восемь новых регистров (R8-R15). Существует также множество других регистров, например, 16 векторных регистров (xmm0-xmm15), но в этой статье мы о них говорить не будем. Таким образом, как и ранее при переходе с 16 бит на 32, мы можем получить доступ к младшей части регистра. Младшие 32 бита от регистра RAX — это EAX, младшие 16 бит — это AX, который, в свою очередь, можно разделить на старшую часть AH и младшую AL. Соглашение о вызове В качестве соглашения о вызове для x86–32 обычно используется cdecl или stdcall (cdecl для большинства случаев и stdcall для Windows API). Однако почти во всех случаях в x86–64 используется соглашение о вызове fastcall. Fastcall помещает первые четыре аргумента, передаваемые функции, в регистры RCX, RDX, R8 и R9, а дальнейшие параметры помещаются в стек. Регистры RAX, RCX, RDX, R8, R9, R10 и R11 считаются непостоянными: их значения не сохраняются при вызове функции, в отличие от значений, размещенных в RBX, RBP, RDI, RSI, RSP, R12, R13, R14 и R15, которые сохраняются. Важно, что когда функцияfastcallвозвращает значение, оно будет храниться в RAX (само значение, если оно меньше восьми байт, или указатель на него, если больше восьми байт). Таким образом, если мы можем перезаписать указатель возврата функции указателем на инструкцию JMP RAX, мы можем перейти к месту в памяти, где находится значение, возвращенное этой функцией. Теперь давайте рассмотрим вопросы, связанные непосредственно с переполнением буфера. Перезапись RIP Напомним, что в приложениях 32-битной архитектуры, если буфер переполняется соответствующим образом, в регистр EIP будет загружен перезаписанный адрес указателя возврата в стеке. То есть, скормив уязвимому приложению большой блок букв А, вы увидите в значении регистру EIP 0×41 414 141. Однако это не относится к 64-битным приложениям, которые будут загружать в регистр RIP только канонические адреса. Например, вы не можете переполнить буфер значением A и ожидать, что в RIP будет загружен 0×41, поскольку 0×414 141 414 141 414 141 не является каноническим адресом. Однако мы можем перезаписать указатель возврата каноническим адресом какого‑либо набора инструкций, который мы хотим выполнить, и этот адрес будет загружен в RIP и запущен как обычно. В 64-битной архитектуре не все возможные адреса используются для обычных пользовательских приложений. Обычно для пользовательских процессов доступны адреса от 0×0 000 000 000 000 000 до 0×00 007FFFFFFFFFF, а процессы ядра используют диапазон 0xFFFF0000`00 000 000 — 0xFFFFFFFFFFFFFF. Любые адреса за пределами этих диапазонов являются неканоническими, и мы не сможем записать такие значения в RIP. Находим уязвимость в коде Давайте посмотрим на практике, как может выявиться наличие потенциальной уязвимости в приложении. Для этого нам нужно будет передать программе нагрузку, которая вызовет аварийное завершение работы. Поскольку эта программа считывает файл на вход, мы запишем 1 000 символов A в файл с помощью скрипта python, приведенного ниже, и передадим его программе. Запустим уязвимую программу под отладчиком x64dbg и передадим ей сгенерированный файл. Как видно, мы перезаписали указатель возврата (0×41). Однако в представлении регистров видно, что RIP не был перезаписан символами A. Это происходит потому, что RIP не будет загружать неканонический адрес (как обсуждалось ранее). Таким образом, мы подтвердили, что уязвимость существует и, похоже, может быть проэксплуатирована, так что теперь мы можем продолжить разработку полноценного эксплойта. Разработка эксплойта Поскольку теперь мы знаем, что можем перезаписать сохраненный указатель возврата, мы можем приступить к созданию рабочего эксплойта. Здесь в качестве полезного инструмента мы будем использовать плагин ERC для X64dbg. Итак, давайте убедимся, что все наши файлы генерируются в нужном месте, выполнив следующие действия: Теперь, когда мы назначили наш рабочий каталог и установили автора проекта, следующей задачей будет определить, как далеко в нашей строке из символов A был перезаписан указатель возврата. Чтобы определить это, мы сгенерируем неповторяющийся шаблон из 1000 символов и включим его в наш следующий буфер, передаваемый приложению. Для этого выполним команду: Сгенерированный шаблон мы снова передадим уязвимому приложению. Наша задача — понять, на каком объеме передаваемых данных происходит переполнение. Значение «3 978 413 878 413 778», представленное на изображении выше, в переводе на ascii равно «9xA8xA7x». Мы можем использовать это значение вместе с командой смещения шаблона, чтобы определить, как далеко в нашей входной строке был перезаписан возвращаемый указатель. Команда: Итак, мы определили, как далеко в нашем вредоносном блоке данных был перезаписан указатель возврата. Пришло время определить, указывает ли RAX на полезный участок памяти: то есть на участок, который мы можем в дальнейшем использовать для запуска эксплоита. Щелкните правой кнопкой мыши на регистре RAX в x64dbg и выберите «Follow in Dump» — и теперь мы можем увидеть начало нашего неповторяющегося шаблона. Как видно, RAX указывает на область памяти, содержащую наш паттерн. Теперь, когда мы знаем, что можем управлять RIP, и у нас есть регистр, указывающий на область памяти, которую мы контролируем, всё, что нам нужно, — это указатель на инструкцию JMP RAX, и выполнение будет перенаправлено на начало нашего вредоносного буфера. Для начала давайте определим, какие шестнадцатеричные коды соответствуют JMP RAX ВыполнимERC ‑assemble JMP RAX. Полученные значения FF E0. Затем поищем эти коды в памяти: Из предложенного списка мы выберем 0×00 000 000 007D6AF0. Поскольку это канонический адрес, мы можем поместить его в точку, необходимую для перезаписи нашего указателя возврата, и посмотреть, загрузится ли он в RIP. Собственно, дальнейший процесс отладки нашего эксплоита аналогичен созданию эксплоита в 32-битной архитектуре. Нам также необходимо сгенерировать полезную нагрузку, например, с помощью Msfvenom. И нам также необходимо будем добавить NOP Sled в передаваемый буфер данных. В качестве полезной нагрузки у нас будет выступать запуск калькулятора. Для генерации выполним следующую команду: Здесь мы указали архитектуру 64 бит, формат вывода Python и плохой байт 0×1А. В 64-битной архитектуре проблема плохих байтов также актуальна, как и в 32-битной. Однако, в данном примере для нас опасен только 0×1А, 0×00 — не опасен, так как передается файл. Что в итоге Соберем вместе результаты наших исследований. Представленный ниже скрипт на Python создает файл, содержащий полезную нагрузку, NOP Sled и адрес, на который должен быть выполнен переход для выполнения полезной нагрузки. После некоторых манипуляций по отладке нашего эксплоита получаем запуск калькулятора. Заключение Разработка эксплоитов для 64-битной имеет некоторые отличия от 32-битной архитектуры, но в целом здесь используются аналогичные принципы; и написание эксплоитов для потенциально уязвимых приложений не должно составить большого труда. Пользуясь случаем, напомню об уроках, которые проведут мои коллеги из Отус в рамках курса по реверс-инжинирингу: 5 февраля: «Техника внедрения шеллкода в user mode приложение из режима ядра».Подробнее 5 февраля: «Техника внедрения шеллкода в user mode приложение из режима ядра».Подробнее 18 февраля: «Скрываем процесс с помощью DKOM».Подробнее 18 февраля: «Скрываем процесс с помощью DKOM».Подробнее"
  },
  {
    "article_id": "https://habr.com/ru/articles/878660/",
    "title": "Свой Cheat Engine с нуля! Часть 1 — Получаем список процессов и модули в нем",
    "category": "Кибербезопасность",
    "tags": "cheat engine, c++",
    "text": "Чтобы сформировать понимание, как происходит получение списка процессов, просто заглянем в исходники самого Cheat Engine. Здесь у нас есть процедура GetProcessList, в которую мы подаем массив строк, в который она запишем нам имена и айди процессов. Первое, на что обратим внимание - структура, куда записывается информация о процесе, в СЕ она выглядит так Мы можем ее записать так. Далее в процедуре идет блок с переменными Как итог я оставил так Для замены под C++ объектаProcessList: TStrings, я использовалstd::unordered_map<std::wstring, PProcessListInfo>& process_list. Потому что в коде, была логика схожая с мапой, когда у нас по имени процесса идет связка с объектом, содержащим информацию о нем (фактически одинProcessID…) Что у нас эквивалентно И в конце при необходимости это включается в лист Заменил на По этой процедуре особо добавить и нечего, она просто делает снимок и пробегается по процессам, выгружая данные… Но тут мое любопытство увело меня в сторону от Cheat Engine, и я решил посмотреть, что там у Process Hacker, это утилита позволяет работать с процессами. Самой интересной частью является - список модулей. Потому, что там сразу можно увидеть кто-где и какой размер в памяти занимает каждый из них. Через поиск по файла по фразеEnumModulesя вышел на вот такую вот функцию так же у нее есть 32битная реализация Результатом работы оных будет вот такая вот структура Самыми интересными для нас будутPVOID DllBase- начало модуля относительно процесса,ULONG SizeOfImage- размер модуля (сколько байт он занимает внутри процесса) иUNICODE_STRING FullDllName. Но с именем все не так просто. Структура выглядит так Казалось бы, вот же она…PWSTR Bufferстрока… выводись в студаут!! А вот и нет, здесь находится адрес в чужом пространстве, чтобы прочитать эту строку, придется сделать следующее. И тут я на радостях побежал смотреть все модули, но не тут-то было… Если получить список процессов вполне себе легитимная процедура, то читать память другого процесса уже не всегда дозволяется авторами софта. Но на этот случай у ProcessHacker есть свой собственный драйвер, на то он и хакер. Все, до чего я докопался - это метод Данные о драйвере выглядят так Но, к сожалению, с наскока подключиться к драйверу не удалось и лучший ответ, который я получилъ По итогу, я решил пока оставить драйвер и заняться гуевой частью, чтобы сразу под каждый инструмент прорабатывать внешний вид, как, что и куда будет выводиться. Конец! А кто слушал - можете прокачать свои навыки накрутейшем курсе по реверсу ММОРПГ :)"
  },
  {
    "article_id": "https://habr.com/ru/articles/878170/",
    "title": "Стандартная библиотека С не потокобезопасна: проблему не решает даже Rust",
    "category": "Кибербезопасность",
    "tags": "C, Python, Rust, linux, gil, отказы, анализ",
    "text": "Мы работаем над базой данных EdgeDB и в настоящее время портируем с Python на Rust существенную часть кода, отвечающего за сетевой ввод/вывод. В процессе работы мы узнали много всего интересного. Отказ, который происходит только на ARM64 Мы  разрабатывали для EdgeDB новую возможность выборки HTTP. В качестве клиентской HTTP-библиотеки мы использовали reqwest. Всё шло гладко: фича работала на локальной машине, проходила тесты на сборочных агентахx86_64и казалась стабильной. Но мы заметили кое-что странное: тесты то и дело не проходили на сборочных агентах для ARM64. На первый взгляд ситуация напоминала взаимную блокировку. Сборочный агент запускался, зависал на неопределённое время, после чего время на выполнение сборочного задания истекало. В логах не отображалось никаких ошибок — просто тест крутился впустую. Затем, спустя несколько часов, задание завершалось отказом и выдавало ошибку «время истекло». Вот как выглядел вывод системы непрерывной интеграции: Тут мало что происходит. Создаётся впечатление, как будто из-за взаимной блокировки асинхронная задача не может работать и с нашей точки зрения заблокируется. Но оказалось, что мы ошибались. Наши исходные соображения Почему проблема в ARM64? Сначала мы не могли найти этому объяснения. Среди первого мы предположили, что существуют различия между моделями памяти, применяемыми в Intel и ARM64. В Intel принята достаточно строгая модель памяти. Притом, что определённые необычные варианты поведения всё-таки возможны, при записях в память поддерживается общий порядок, который соблюдается во всех процессорах Intel ([1],[2],[3]). В ARM действует значительно менее строгая модель памяти[4], в которой (среди прочего) порядок следования записей может отличаться с точки зрения разных потоков. Салли написал по этому поводу квалификационную работу на Ph.D.[5], поэтому мы и пригласили его посмотреть, что здесь происходит. Отладка на машине для непрерывной интеграции Наши машины для ночных сборок методом непрерывной интеграции работают на серверах Amazon AWS. Преимущество таких серверов в том, что на них можно смоделировать настоящего неконтейнеризованного пользователя с правами администратора. Притом, что можно подключиться к агентам github через ssh[6], хорошо бы иметь возможность подключаться в качестве полноценного администратора, чтобы получать доступ к dmesg и другим системным логам. Чтобы выяснить, что происходит, мы (Салли и Мэтт) решили подключиться непосредственно к агенту для ARM64 и посмотреть, что происходит под капотом. Сначала мы зашли через SSH на машину для непрерывной интеграции, попытались найти этот подвисший процесс и подключиться к нему: Oказалось, это верно! Мы запускаем сборку в контейнере Docker, а у него — собственное пространство имён для процессов: Постойте-ка. Здесь этого зависшего процесса тоже нет. Значит, это была не взаимная блокировка — процесс аварийно завершился. Как выясняется, наш тестовый агент просто не смог этого зафиксировать. Но ладно, эту ошибку мы исправим как-нибудь потом. Можем посмотреть, остался ли от процесса дамп памяти. Поскольку контейнер Docker — это просто отдельное пространство имён для процессов, дамп памяти передаётся на хост с Docker. Можно попытаться докопаться до него извне контейнера при помощиjournalctl: Ага! Нашли его. Как и ожидалось, дамп для этого процесса лежит в/var/lib/systemd/coredump/. Обратите внимание: именно из-за разницы в пространствах имён здесь наблюдаются разные идентификаторы процессов. Вне контейнера мы увидим pid 59530, а внутри — 1000. Мы загрузили дамп памяти в gdb, желая проверить, что же произошло. К сожалению, в ответ нам высыпался ворох ошибок: Ладно. Это не помогло. Мы не располагаем необходимыми файлами вне контейнера, а наши контейнеры весьма минималистичные, поэтому в них не так просто установитьgdb. Вместо этого нам потребуется скопировать из контейнера все релевантные библиотеки и сообщитьgdb, где находятся файлы.so: Гораздо лучше! Но в нашем новом HTTP-коде обратная трассировка выявила не отказ, а нечто неожиданное: Мы дизассемблировали аварийно завершающуюся функциюgetenv. Поскольку мы знали, что для сборки контейнеров применяется GLIBC 2.17, мы также отыскали соответствующий исходный кодgetenv, чтобы его можно было проследить[7]: Уф, значит, аварийное завершение происходит во время загрузки байта и в ходе поиска интересующей нас переменной окружения. Можно вывести дамп актуального состояния всех регистров: Итак,getenvотказывала, пытаясь загрузить данные из недействительного региона памяти (0x220 – очевидно, такого значения быть не может). Но как? Что же происходило? Поначалу нас это озадачило. Отказ происходил глубоко вlibc. Мы подозревали, дело может быть в том, что переменная окружения просто повреждена — учитывая, что происходил вызов getenv, но для дальнейшего расследования нам не хватало информации. Мы принялись проверять блок окружения при помощиgdb. Напомню: в соответствии со стандартом POSIX[8]environопределяется какchar **и фактически представляет собой список указателей на строки окружения, а конец списка отмечается какNULL-указатель. Но ведь это нонсенс — мы наблюдаем решительно невозможную операцию загрузку из пространства памяти, а окружение ведёт себя так, как будто эта операция совершенно валидна и непротиворечива. Кстати, а почему мы вообще вызываем здесьgetenv? А потом пожаловал Юрий и скинул в комментарии ссылку на один старый пост: <yury> Кажется, что некоторые операции, относящиеся к вводу/выводу, выливаются в ошибки, а  Python пытается при помощиPyErr_SetFromErrnoWithFilenameObjectsсконструировать изerrnoисключение  <yury> По-видимому, эта функция отмечается наgettext(заготовка для трансляции?) И дальше идёт вgetenv  <yury> Возможно, именно поэтомуgetenvне потокобезопасна https://rachelbythebay.com/w/2017/01/30/env/ Вот где собака зарыта: setenv и getenv Функцию setenv небезопасно вызывать в многопоточной среде. Зачастую это представляет проблему, и данный феномен то и дело переоткрывается, когда наш брат-разработчик сталкивается с таинственными отказами функцииgetenvизlibc[9],[10],[11],[12]. Казалось вероятным, что причина именно в этом, но, учитывая явную нехватку символов, мы не могли понять, каков вклад каждого из эксплуатируемых нами потоков в этот отказ. Почитав дизассемблированный код и соотнеся его с кодом на C, мы определили, что регистрx20соответствует переменнойep. Это указатель, используемый для обхода массиваenviron. Но оказалось, чтоx20соответствует 0x248b5000, аenviron— 0x28655750, почти на 60 мегабайт позже в памяти. Посколькуx20— это указатель, применяемый для считывания старого окружения, можно осмотреть окружающую область памяти и проверить, не сохранились ли там какие-то крупицы интересующей нас информации — а потом сравнить её с актуальным состояниемenviron. Интересно! Значения указателей в двух областях памяти очень похожи! А где они начинают различаться? Это последние записи по адресам0x0000ffff6401f0b0и0x0000ffff6401f8d0: они соответствуютSSL_CERT_FILE=...иSSL_CERT_DIR=...! Всё это явно подсказывает, что мы верно догадывались насчёт гонки данных, и другой поток перемещал environ в рамкахsetenv! При рассмотренииsetenvказалось, что блок окружения заключён в слишком тесной области памяти и, возможно, имела место повторная аллокация, чтобы в памяти уместились новые переменные[13]. При этом мы всё ещё не выяснили, какой код вызывает setenv. Казалось возможным, что обвал происходит из-за OpenSSL и/или какой-то другой зависимостиreqwest, относящейся к TLS (rust-native-tls), но как? Подключение к openssl_probe Погуглив эти переменные окружения в связке сrust-native-tls, мы выудили старинную проблему:[14]. А в одном из комментариев скрывалось следующее: Не уверен насчётopenssl. Создаётся впечатление, как будто сейчас он  загружает системные сертификаты при помощи библиотеки openssl-probe, и при этом устанавливаются переменные окруженияSSL_CERT_FILEиSSL_CERT_DIR, а после этого в дело вступаетSslConnector::builder, вызывающийctx.set_default_verify_paths, который и заглядывает в эти переменные окружения. Учитывая, что переменные окружения устанавливаются глобально и всего один раз, максимально разумно было бы, пожалуй, просто пытаться очищать хранилище,  когда вся работа закончена. По-видимому, у меня на локальной машине это срабатывало. Интересно. Итак,openssl-probeустанавливает эти переменные. Причём, само собой, под Linux мы пользуемся серверным интерфейсомrust-native-tls openssl, из которого идут вызовы в эти функции! Вот совершенно безобидные на вид строки из библиотекиopenssl-probe, в которых просто не стоитunsafe[15]: Именно так мы и нарвались на аварийное завершение. Оно происходило из-за кода на Rust, в котором отсутствуютunsafe, и который патологически взаимодействует сlibcгде-то в другой точке программы. В качестве отступления: что же такое RISC? Притом, что оба мы имеем опыт обратной разработки, Мэтт был растренирован в работе с aarch64, a Салли этого вообще не умел. Поэтому мы некоторое время вместе потупили, рассматривая один из главных циклов в ассемблере. По-видимому, в коде был расчёт на то, что значениеx20изменится, и этот регистр наиболее явственно просился на представление ep, но он, по-видимому, не фигурировал в левой части ни одной из инструкций. И тут мы заметили любопытный восклицательный знак: Оказывается, это «пре-индексный» режим адресации, действующий по принципуx19 = *(x20 + 8); x20 = x20 + 8[16]. Такой маленький миленький оператор, но мы достаточно повидали и помним, что нам рассказывали: режимы адресации с автоматическим инкрементом — это наследие старинных машин CISC, таких как VAX. Без них обходились даже более современные машины из категории CISC, например, x86, и уж конечно элегантные и простые архитектуры RISC. Полагаю, как раз тот случай, когда всё новое — хорошо забытое старое. (Апдейт: ну, на самом деле, не такое уж новое. В ARM это существует с самого начала, RISC, думаю, опоздал примерно на неделю). Так почему же только на ARM64 и на Linux? Поскольку данная авария возникает из-заrealloc, перемещающей память, а сама эта ситуация провоцируетсяsetenv, и всё это происходит в тот самый момент, когда другой поток вызывает getenv. Этот пазл складывается сразу из множества кусочков. Переменных окружения должно быть ровно столько, чтобы потребовалась повторная аллокация. Отказ ввода/вывода — отдельная проблема, но он подхватываетсяasyncio, и здесь требуется вызвать getenv, которая, в свою очередь, извлечёт переменную окружения LANGUAGE в самый неподходящий момент. Значение 0x220 подозрительно напоминает размер старого окружения, выраженный в 64-разрядных словах (0x220 / 8 = 68), и этим значением затирался завершающийNULLблока окружения ещё до того, как он был перемещён. Вероятно, это делалось, чтобы указать функции malloc размер свободного блока. Но при этом программа как раз минировалась опасным невалидным указателем, и на нём подрывались жертвы, испытывавшие на себе использование после высвобождения. Учитывая все эти предусловия, нам ещё повезло, что удалось в основном воспроизвести данный отказ на отдельно взятой платформе. Исправление В итоге мы решили переезжать с серверного интерфейсаrust-native-tls/opensslотreqwestнаrustlsпод Linux. Исходно мы полагали, что, пользуясь нативным TLS-бекэндом, мы обойдёмся без одновременного применения двух TLS-движков в процессе портирования кода с Python на Rust. Столкнувшись с этой проблемой, мы решили, что в краткосрочной перспективе работать одновременно с двумя движками вполне нормально. Был и другой вариант: на первый раз вызыватьtry_init_ssl_cert_env_vars, удерживая в Python глобальную блокировку интерпретатора (та самая GIL, которой вас пугали). В Rust предусмотрена внутренняя блокировка, предотвращающая гонки между фрагментами кода Rust, одновременно читающими и пишущими окружение. Но эта блокировка не мешает коду из других языков напрямую использоватьlibc. Удерживая GIL, мы, как минимум, страховались от гонки с нашими Python-потоками. В проекте Rust эта проблема уже зафиксирована, и в версии 2024 планировалось сделать небезопасными функции, устанавливающие окружение[17]. В проектеglibcтакже (совсем недавно) усилили потокобезопасность вgetenv, избегая использования realloc и просачивания в более старые окружения[18]. "
  },
  {
    "article_id": "https://habr.com/ru/companies/ru_mts/articles/878302/",
    "title": "Тихой сапой: кремний-углеродные батареи приходят на смену литий-ионным?",
    "category": "Мобильные технологии",
    "tags": "смартфоны, гаджеты, it-стандарты, honor, Si-C, Li-Ion, аккумуляторы",
    "text": "В феврале прошлого года сразу несколько телефонов китайской компании Honorвышлис непривычными для этого типа устройств аккумуляторами — Si-C вместо Li-Ion. Samsung и Apple, насколько известно, тоже планируют выпустить свои флагманы с новыми элементами питания. Давайте разбираться, что это за новинка, зачем ее создали и какие у нее достоинства и недостатки. Что вообще произошло Honor представила новые телефоны Magic 6 и Magic 6 Pro с кремний-углеродным аккумулятором. По словам производителя, он почти не боится мороза — а значит, смартфон не теряет заряд в условиях сильного холода. Еще у батареи высокая емкость — около 5 600 мА•ч. Габариты при этом меньше, чем у литиевых элементов питания (про точную разницу в размерах пока не сообщили). Компания рассказала про эксперименты, когда телефон Magic 6 Pro поместили в криокамеру с температурой в -20 °С. Он оставался включенным в течение 13 часов, показывая видео. Смартфон с обычным элементом питания на такое не способен. Это оказалось не маркетинговой уловкой: потом экспериментповторяли, и устройство действительно работало больше 10 часов. Кремний-углеродные аккумуляторы в телефонах китайской компании — уже второго поколения. Первым оснащался Magic 5 Pro с емкостью батареи 5 450 мА•ч. Он продавался только на внутреннем рынке Китая и был фактически грандиозным тестом в масштабах всей Поднебесной. Появление таких батарей прошло без особого шума, ну установили и установили. Si-C элементы питания показали себя неплохо. Ими заинтересовались и другие производители. В том же 2024 году десятки разных компанийполучили лицензиина эту технологию. Год назад появились и аккумуляторы второго поколения. Но что они собой представляют? Сорвем покровы тайны Нюанс в том, что Si-C — это не какая-то сакральная технология, а разновидность все тех же литиевых элементов питания. Отличие — в аноде, он выполнен изнаноструктурированногокремний-углеродного композитного материала. Он заменяет традиционный графит, применяемый в большинстве современных литий-ионных аккумуляторов. Катод, как и в стандартных аккумуляторах, изготавливается из литий содержащих материалов, таких как оксиды лития-кобальта или лития-марганца. Графитовый анод впервые появился в батареях Sony в 1991 году и стал стандартом. Но не потому, что был идеальным, а потому, что на тот момент просто не нашли альтернативы лучше. Теоретически есть много других материалов, которые могли бы работать эффективнее. Но на практике каждый из них оказывается проблемным: одни дают слишком низкую емкость, другие перегреваются или глючат на холоде. Третьи не выдерживали больше нескольких десятков циклов заряда. Короче, графит оказался самым надежным и универсальным вариантом. Попытки найти достойную замену не прекращались. Технология литий-ионных аккумуляторов уже давно подошла к пределу энергетической плотности. Увеличение емкости чаще всего реализовывали за счет повышения размеров батарей, но часто это шло в ущерб другим компонентам смартфона. После ряда неудачных проб и попыток инженеры Honor сделали ставку на кремний-углеродный материал. Принципы работы аккумулятора остались прежними. Но благодаря новой структуре анода, которая способна удерживать больше ионов лития, удалось значительно улучшить характеристики батареи. В новом типе аккумуляторов наночастицы кремния встроены в наноуглеродную структуру с пустотами, что позволяет избежать растрескивания и отслаивания материала при циклах зарядки и разрядки. Номинальная емкость таких аккумуляторов в среднем на 12,8%выше, по сравнению с традиционными аналогами. Если говорить о реальных версиях батарей, установленных в Honor Magic 5 Pro, прирост составляет 6,8%. Но есть нюанс: ключевое преимущество проявляется при падении напряжения до 3,5 В — в таких условиях емкость кремний-углеродных батарей оказывается выше на впечатляющие 240%. В Li-Ion на этом этапе большая часть заряда уже потрачена, и элемент питания быстро теряет остаточную энергию. К тому же Si-C аккумуляторы очень компактны: толщина рабочего образца батареи Honor на 10 000 мА·ч — всегонесколько миллиметров. Что еще? Максимальная теоретическая удельная емкость кремниясоставляет4 200 мА·ч/г против 372 мА·ч/г у графита, так что использование кремний-углеродных аккумуляторов открывает для производителей новые возможности. Теперь они могут создавать более легкие и компактные устройства с прежней емкостью батареи или выпускать гаджеты стандартных размеров, но с заметно увеличенной емкостью аккумулятора. Эти элементы питания не только лучше переносят низкие температуры, но и обеспечивают повышенную безопасность за счет меньшего нагрева при активном использовании и особенно при зарядке. Лабораторные тестыпоказывают, что аккумуляторы нового типа еще и более износостойкие. Ресурс кремний-углеродных батарей может достигать 5–7 лет, что заметно превышает стандартные 2-3 года у современных аккумуляторов. Это особенно важно, учитывая, что флагманские смартфоны сохраняют свою работоспособность и актуальность даже спустя 2–3 года, когда оригинальная батарейка обычно уже выходит из строя. Немногие пользователи решаются заменить аккумулятор (иногда стоимость такой работы в сертифицированном сервисном центре — солидная часть цены нового такого телефона), большинство просто покупает новый смартфон. Если стандартная батарея сможет работать до 5 лет, пользователь дольше будет оставаться с одной и той же моделью. А значит, будет меньше электронного мусора. Еще одно важное преимущество кремний-углеродных аккумуляторов — поддержка сверхбыстрой зарядки мощностью до 60 Вт на один элемент. В современных телефонах чаще всего применяются схемы с двумя «банками» в одном «пакете», это дает общую мощность зарядки до 120 Вт. Получается, полная зарядка даже увеличенной батареи занимаетменьше времени, чем у предыдущих поколений устройств, — вплоть до 30 минут, чтобы пополнить заряд с 0 до 100%. Что с недостатками Тут пока сложно сказать что-то определенное: вопросов больше, чем ответов. Ну например, если смартфоны действительно смогут служить на 2–3 года дольше, как это повлияет на текущую схему цикла смены моделей у производителей? Сейчас он рассчитан как раз на пару-тройку лет, после чего человек обычно идет и покупает себе что-то новое. Не станет ли бизнес терять деньги из-за этого и встраивать что-то для «планируемого устаревания» телефона? Пока неясно, насколько дороже для покупателя обойдется смартфон с Si-C батареей. Речь, конечно о равнозначных по мощности и характеристиках устройства — с Si-C и Li-Ion. Проблема еще и в том, что Honor и другие компании, запатентовавшие кремний-углеродные аккумуляторы для смартфонов, установили высокие лицензионные сборы за использование этой технологии. Так они фактически сформировали новый сегмент на рынке — «суперфлагманы». В них внедрение таких инновационных решений становится признаком премиального уровня устройства. Но здесь возможны скорые послабления: чем больше компаний будут изучать и осваивать эту технологию, тем дешевле она станет. Ну а раз за деловзялисьтакие гиганты, как Samsung и Apple, то массовый выпуск девайсов с Si-С батарейками не за горами. Возможно, в этом году «кремний-углерод» пропишется в Galaxy Z Fold 7, Galaxy S25 Slim и iPhone 17 Air. Там и минусы с плюсами станут более явными, мы с вами о них уже точно узнаем. А пока ждем и надеемся, что телефоны будут заряжаться быстрее, а жить — дольше. "
  },
  {
    "article_id": "https://habr.com/ru/companies/selectel/articles/878222/",
    "title": "Liberux NEXX: Linux-смартфон с 32 ГБ ОЗУ и аппаратными переключателями",
    "category": "Мобильные технологии",
    "tags": "selectel, Liberux NEXX, смартфоны, гаджеты",
    "text": "Что за зверь 100% уверенность в отключении. Когда вы переводите переключатель в положение «выкл.», устройство физически разрывает цепь. Это значит, что даже если злоумышленник или вредоносное ПО попытаются получить доступ к камере или микрофону, у них ничего не выйдет. В отличие от программного отключения, где всегда есть риск, что система или приложение «случайно» включит компонент снова. Защита от слежки. В эпоху, когда даже умные телевизоры могут шпионить за своими владельцами, аппаратные переключатели – это как глоток свежего воздуха. Вы можете быть уверены, что ваш смартфон не записывает ваши разговоры или не отслеживает ваше местоположение без вашего ведома. Простота и прозрачность. Вам не нужно копаться в настройках или устанавливать дополнительные приложения для контроля доступа. Всё, что нужно, – это переключить тумблер. Это особенно удобно для тех, кто не хочет разбираться в сложных настройках конфиденциальности. Защита от взлома. Даже если злоумышленник получит доступ к вашему устройству через сеть или вредоносное ПО, он не сможет включить отключённые компоненты. Аппаратные переключатели – это как замок на двери: без физического доступа к устройству их не обойти. Уязвимость системы. Если в операционной системе есть уязвимость, злоумышленник может обойти программные ограничения. Скрытые процессы. Некоторые приложения или системные службы могут включать компоненты без вашего ведома. Например, голосовые помощники вроде Siri или Google Assistant могут активировать микрофон, даже если вы этого не хотите. Сложность контроля. Чтобы быть уверенным в своей безопасности, нужно постоянно проверять настройки и разрешения. Это требует времени и внимания. Характеристики ОЗУ: 32 ГБ LPDDR4x Память: 256 ГБ eMMC (да, это слабое место) + поддержка microSD до 2 ТБ Процессор: Rockchip RK3588S (4x Cortex-A76 + 4x Cortex-A55, Mali-G610 MP4) Дисплей: 6.34\" OLED, Full HD+ (2400x1080) Аккумулятор: 5300 мАч Камеры: основная – 32 МП, фронтальная – 13 МП Порты: 2x USB-C 3.1, 3.5 мм аудиоразъем Сети: 5G (модем Qualcomm Snapdragon X62), 4G LTE Беспроводные интерфейсы: Wi-Fi 5, Bluetooth 5.0 Датчики: сканер отпечатков пальцев, акселерометр, гироскоп, магнитометр ОС: LiberuxOS (на базе Debian 13) Кому это надо"
  },
  {
    "article_id": "https://habr.com/ru/companies/ru_mts/articles/878244/",
    "title": "Осторожно, подделка: чудесное превращение iPhone 11 в «iPhone 14 Pro»",
    "category": "Мобильные технологии",
    "tags": "iphone, смартфоны, гаджеты, носимая электроника, подделки",
    "text": "Привет, жители Хабра! Меня зовут Дима Фролов, и я уже лет 10 ремонтирую технику. В прошлый разрассказывало рефах и китайских подделках смартфонов. А теперь по горячим следам решил показать, как превратить оригинальный iPhone 11 в неоригинальный «iPhone 14 Pro». Спойлер: это ну очень легко, поэтому напомню, что не стоит вестись на слишком приятные цены, а информацию о телефоне всегда нужно проверять. Считайте, дальше будет мини-косплей мастера из Поднебесной. Поехали! Зачем я за это взялся Предыстория такая. Около месяца назад один из постоянных клиентов принес неработающий iPhone 11. Он давно лежал у него дома мертвым грузом, не включался, корпус покоцан. В общем-то, телефон ему был больше не нужен, ремонтировать он не захотел, так что я выкупил его за копейки для своих запасов. Как появилось время, сел изучать «пациента»: АКБ полуживая, дисплей внешне выглядел целым, но подсветка не работала. Но когда я накинул новый, внезапно оказалось, что материнская плата в порядке и даже отвязана от аккаунта. В общем, поторопился парень продавать телефон. И тут во мне проснулась жажда экспериментов. Периодически в наш сервисный центр залетают интересные клиенты, которые просят вставить какой-нибудь из древних оригинальных IPhone в корпус от 14, 15 или 16 Pro. Про ХR в корпусе 16 Pro как раз писалв прошлом посте— тут суть та же. Мы от такой работы сразу отказываемся. Замена корпуса на неоригинальный в целом то еще «развлечение», даже если модель та же. Пример. Приносит клиент IPhone со слегка потертым корпусом и говорит:«А поменяйте, пожалуйста, на китайский, чтобы подешевле было». Предупреждаю о рисках — соглашается, меняем. А через неделю возвращается и жалуется, что за это время новый корпус стал выглядеть в десять раз хуже оригинального — того самого, который был покоцан. Краска отвалилась, кнопки люфтят, беспроводная зарядка и NFC не всегда корректно работают — короче, что и требовалось доказать. Какой корпус придет по факту, как он будет вести себя потом, как в него встанет плата — всегда загадка. Все это мы подробно и в красках объясняем, прежде чем взять смартфон в ремонт. На работу даем гарантию месяц, так что повторки для нас — самый страшный кошмар, мы ничего на них не зарабатываем. Но вернемся к экспериментам. На новогодних праздниках клиенты притихли, даже рабочий телефон молчал, а у меня очень чесались руки. И тут я подумал, почему бы не сыграть в китайского мастера. Решил попробовать переставить внутренности IPhone 11, который внезапно достался мне от клиента, в «корпус от iPhone 14 Pro» и показать вам, что получилось. Приключение начинается! Приступаем к «прокачке» Эксперименты — дело хорошее, но для такого случая я даже не стал заморачиваться с поиском корпуса у поставщиков запчастей. Все мы знаем, где можно максимально ненапряжно заказать этого зверя. Через сутки он уже был у меня — ПВЗ как раз по соседству, даже идти никуда не пришлось. Начинаем распаковку: Щедрости продавцов нет предела. В комплекте даже есть проклейка для «влагостойкости», набор инструментов, которые могут развалиться прямо во время распаковки, и потрясающее защитное стекло: типа керамическое и 100D. Ну что ж. Теперь приступаем к разбору «пациента». Как снимал дисплей, фоткатьзабылне стал. Но вы точно уже тысячу раз такое видели. Снял дисплей с пациента и убрал кожухи: Отклеил АКБ, снял вибромотор, динамик, sim-коннектор: Почти не дыша отклеил шлейф от корпуса. Перед этим шагом лучше чуть погреть его феном. Тогда клейкий материал, на котором это добро держится, утратит свойства — и шанс оторвать микрофончики на шлейфе будет ниже. У неопытных умельцев чаще всего отрываются именно они. Надо сказать, я человек педантичныйтолько наоборот. Стоит отвлечься — как все, что было аккуратно разложено, куда-то разбегается. Чтобы избежать таких моментов, сразу устанавливаю то, что снял, в новый корпус. Пока все довольно неплохо, встало без проблем: Снимаю металлические крепления со старого корпуса, прикручиваю к новому: Извлекаю материнскую плату: Аккуратно отклеиваю NFC-антенну: Снимаю камеры и шлейфы кнопок. Очень осторожно — тут тоже лучше немного погреть: Про держатели кнопок хочется сказать отдельно. Они испытывают мою нервную систему еще со времен 7 IPhone. Уж не знаю, почему, но китайцы в свои корпусы эти штуки не кладут — их нужно снимать с оригинального. Сделать это тяжело, особенно с непривычки, а еще они разлетаются во все стороны. Если вдруг такая штука потеряется, кнопка держаться не будет. Я даже не в курсе, продаются ли они отдельно. Если бы потерялись, пришлось бы искать доноров. Сервисные центры скупают мертвые аппараты за символическую цену именно для таких случаев. Как же приятно порой из мусора вытащить какой-нибудь «ценняк». Оригинальный шлейф кнопки включения, микрофона и вспышки не подходят к китайскому корпусу. Он слишком короткий, вспышка не дотягивается до предназначенного для нее отверстия. И тут я почти было растерялся:  уже подумал, что на этом мой эксперимент и закончится. Но потом решил заглянуть в коробку с посылкой — и не зря! Нашел там модифицированный шлейф. Да, он шел в комплекте: китайцы предусмотрели такой расклад и любезно положили его в коробку. Дальше внимательно вкручиваю винтики и ставлю кожухи, чтобы не быть как «классический» мастер сервисного центра — ну вы же знаете этих ремонтников… На удивление все проходит неплохо, даже не верится. Ставим неоригинальные АКБ и дисплей — тоже лежали в моих запасах. И вуаля! Перед вами «IPhone 14 Pro»прости господи. Как я уже рассказывал в предыдущем своем посте, похожие девайсы продают не только в переходах под видом оригинальных, но и на маркетплейсах — тут все прозрачно, так и пишут, например, «IPhone ХR в корпусе 16 Pro». Теперь вы знаете, как собираются такие франкенштейны. На всякий случай напомню: если есть сомнения, какой Айфон вы держите в руках, всегда можно зайти в настройки и посмотреть реальное название модели. Жизнь после «прокачки» Подытожим, что там с моим творением. Корпус оказался вполне вменяемого качества. Помню времена, когда каждую китайскую реплику приходилось дорабатывать напильником, чтобы хоть как-то использовать. А этот приятный для мастера: платы и шлейфы встали на места без каких-либо усилий. Может, мне попался такой качественный экземпляр и у других производителей все гораздо печальнее. Или китайцы не дремлют, и их подделки в целом стали приятнее на ощупь. После «прокачки» телефон включился и как будто работает. Но если вы хотя бы раз держали в руках оригинальный IPhone, то сразу поймете: это не он. Корпус на ощупь хоть и приятный, но даже близко не айфоновского качества. У кнопок включения и громкости люфт, чего никогда не будет в оригинале. Цветопередача хромает, скроллинг подвисает — и это только на старте. Как такой телефон будет работать дальше — лотерея. Возможно, проживет какое-то время, а может, уже через два дня начнет перезагружаться и выключаться сам по себе. Сюда же фантомные нажатия и мертвые зоны на тачскрине — все это тоже обычная история. Что дальше делать с телефоном, долго думать мне не пришлось. Отдал детям как игрушкус чем только ни играют дети ремонтников, даже симку не вставил. В последний раз попадался мне на глаза, когда шестилетняя дочь примеряла на него чехлы с единорогами и котятами, а вот дальнейшая судьба пока неизвестна. "
  },
  {
    "article_id": "https://habr.com/ru/articles/877028/",
    "title": "Apple Pro Weekly News (20.01 – 26.01.25)",
    "category": "Мобильные технологии",
    "tags": "Apple, iOS, iPadOS, macOS, watchOS, iPhone, iPad, Mac, Apple Watch, Слухи",
    "text": "Показываем что нового в iOS и iPadOS 18.3, а также статистика установок свежих систем от Apple. Как прошло открытие магазина в Майами, какой необычный подарок получили первые посетители и что за новый ремешок для Apple Watch компания презентовала. Утечки и слухи: про iPhone 17 Air и SE 4-го поколения и какие перестановки могут произойти в руководстве Apple. Как поиграть в топовые игры на visionOS и причём тут браузер Safari. Разработчикам дали новый API для внутренних подписок в приложениях, а в Индии и Великобритании у компании проблемы. Погнали к новостям! Релиз: iOS/iPadOS 18.3, macOS 15.3 и другие системы – рассказываем, что там нового. Стали известны данные, сколько пользователей установило свежие системы Apple. На этой неделе Apple выпустит в общедоступный релиз свои новые версии систем: • iOS 18.3 (22D63) • iPadOS 18.3 (22D63) • macOS Sequoia 15.3 (24D61) • tvOS 18.3 (22K557) • HomePod 18.3 (22K557) • watchOS 11.3 (22S553) • visionOS 2.3 (22N896) • macOS Sonoma 14.7.3 (23H417) • iPadOS 17.7.4 (21H414) Также на прошлой неделе компаниявыпустила бета-версию для AirPods Pro 2 (обе версии) и AirPods 4 (обе версии) – firmware update 7.4 beta (7E5067b), скорее всего, сборки станут доступны в релизе позднее после выхода в релиз iOS 18.3 Что нового в iOS/iPadOS 18.3, macOS 15.3 Sequioa и watchOS 11.3? Эти обновления содержат улучшения функций Apple Intelligence (который доступен не везде), а также другие усовершенствования, исправления ошибок и обновления для системы безопасности. Теперь Apple Intelligence будет автоматически активироваться на поддерживаемых устройствах после обновления на iOS/iPadOS 18.3 и macOS 15.3 Sequoia‌. Пользователи при необходимости могут отключить эту функцию в пункте Apple Intelligence и Siri в Настройках. Visual Intelligence с помощью Контроллера Камеры, доступного во всех моделях iPhone 16, получил доработки: Сводка уведомлений (все модели iPhone 16, а также iPhone 15 Pro/15 Pro Max): Apple отмечает месяц афроамериканской истории с новым спортивным нейлоновым ремешком Black Unity для WATCH (подробнее о нём чуть ниже), циферблатом Unity Rhythm и обоями для iPhone и iPad. Циферблат доступен с обновлением watchOS 11.3, фирменные обои также будут доступны в коллекции с обновлением iOS/iPadOS 18.3 Это обновление также включает в себя следующие улучшения и исправления ошибок: Устранена проблема, из-за которой клавиатура могла исчезать при вводе запроса Siri. Исправлена ​​проблема, из-за которой на экране блокировки некоторых моделей iPhone в режиме фокусировки «Не беспокоить» при проведении пальцем вниз по экрану «Домой» больше не появлялась надпись «Проведите пальцем, чтобы открыть». Для систем tvOS/HomePod 18.3 и visionOS 2.3 компания не предоставляла подробного списка новшеств, кроме дежурного «исправления ошибок и другие улучшения». Некоторые функции могут быть доступны не во всех регионах или не на всех устройствах Apple. Тем временем, Apple объявила статистику по iOS и iPadOS 18: 68% всех iPhone используют iOS 18, при этом 76% всех iPhone, представленных за последние 4 года используют iOS 18 и лишь 19% до сих пор на iOS 17. Уже 53% всех пользователей iPad установили систему iPadOS 18, при этом 63% пользователей планшетов, представленных за последние 4 года – на iPadOS 18, а 27% до сих пор на iPadOS 17. Параллельно: новый Parallels Desktop для Mac с чипами M теперь поддерживает эмуляцию x86 Вышла свежая версияParallels Desktop 20.2.0,где теперь можно использовать эмуляцию x86 на компьютерах Mac с чипами Apple Silicon. Это значит, что теперь можно запускать некоторые виртуальные машины (VM) на базе Intel (x86_64) на Mac с чипами M1 и новее через механизм эмуляции от Parallels. Добавлена даже поддержка функции Writing Tools на базе Apple Intelligence для Windows-приложений (Microsoft Word, PowerPoint и Outlook). Также добавлена автоматическая синхронизация даты и времени в виртуальной машине – ранее это нужно было устанавливать вручную. Магазин: открылся новый Apple Store в Майами и новый ремешок для WATCH Black Unity 24 января открылся 10-й магазин Apple в городе Майами, штат Флорида, США. Компания поделилась фотографиями с открытия магазина Apple Miami Worldcenter Apple представила новый спортивный нейлоновый ремешок Black Unity для WATCH в честь месяца афроамериканской истории, он также дополняется циферблатом Unity Rhythm и обоями для iPhone и iPad Слухи: утечки панели iPhone 17 Air, макеты iPhone SE4, след новых устройств в коде системы, что могут показать на неделе и какие перестановки в компании ожидаются В начале прошлой недели инсайдер Majin Bu опубликовал фотографию задней панели якобы iPhone 17 Air, но достоверность снимка спорная, да и качество в лучших традициях – «на тапок» А по свежим данным от Мин-Чи Куо, размер Dynamic Island в серии iPhone 17 не изменится, вопреки различным инсайдам про уменьшение выреза в экране Но эти устройства лишь осенью, а из ближайшего – iPhone SE 4-го поколения. Всплыли свежие снимки детализированных макетов грядущего устройства: Ещёна видео(оно короткое) можно рассмотреть форму «чёлки» как в iPhone 14, что отличается отданных инсайдера evleaks,который ссылаясь на внутренние документы указал на возможный Dynamic Island в iPhone SE 4-го поколения. Тем временем, вкоде iOS 18.3 RC найдены упоминания грядущих iPhone SE4, iPad 11 и iPad Air 7: iPhone SE (4-го поколения) имеет заводскую сборку iOS 18.3 с номером 22D8062. По описанию в коде, чип устройства имеет идентификатор «T8140», что значит версию A18. Базовый iPad 11-го поколения и iPad Air 7-го поколения получат заводскую iPadOS 18.3 с номером 22D2060 или 22D2062. Процессор в базовом iPad прописан как «T8120» – идентификатор скорее всего соответствует чипу A16, вместо чипа A17 Pro, о котором ходили слухи ранее. Если это будет так, то iPad 11 не получит поддержку Apple Intelligence. А вот iPad Air 7-го поколения будут иметь чип M3, о чём недавно также появились слухи. Версий диагонали всё также будет две: 11” и 13”. Также в коде iOS нашли упоминания пока невыпущенных функций для записи экрана iPhone: поддержка HDR и стереозвука, а также отображение камеры в режиме «Картинка в Картинке». На этой неделе могут быть анонсированы свежие MacBook Air на чипе M4. По данным Марка Гурмана, в магазинах компании на этой неделе стартуют обучающие тренинги для сотрудников по новым продуктам. По данным Bloomberg, Apple назначила опытного ветерана компании помогать с Siri и Apple Intelligence. Ким Воррат работает в Apple уже 36 лет и она была первым менеджером по продуктам для операционной системы iPhone, а позже управляла всем управлением проектами для iOS и macOS. Также она помогала вывести visionOS на рынок с 2019 года управляя отделом разработки. По данным Гурмана, руководитель Apple по искусственному интеллекту, Джон Джаннандреа, раскрыл план грядущих изменений в письме, отправленном подчинённым. В нём говорилось, что команда теперь сосредоточена на обновлении базовой инфраструктуры Siri и улучшении внутренних моделей ИИ, включая работу над новым собственным LLM. Основные текущие проблемы Siri носят критический статус внутри компании. Самая первая команда, работавшая над голосовым ассистентом – больше не работает в Apple, а различные слухи указывают на то, что команда Siri и Apple Intelligence находится в хаосе с организационной точки зрения. Также в Apple есть ещё один член совета директоров, которому исполняется 75 лет в этом году и по правилам компании он должен уйти на пенсию – это председатель совета директоров, Артур Левинсон. В компании не хотят его быстро выпускать на пенсию, а сам Левинсон уже готов к переизбранию в следующем месяце на ежегодном собрании акционеров Apple. Однако для этого Apple либо нужно придумать причину, по которой ему нужно остаться, либо он скорее всего уйдёт на пенсию к 2026 году. По данным Гурмана, компания Apple ищет по всему миру новых членов совета директоров – компании необходимо выяснить, кто может вступить в должность председателя после ухода Левинсона. Apple обычно разделяет роли генерального директора и председателя, но учитывая положение и текущую должность Тима Кука, есть большая вероятность, что именно он получит обе должности. Если бы Кук стал председателем, когда Левинсон уйдёт на пенсию, то это помогло бы ему в момент, когда Кук должен будет уйти в отставку с поста генерального директора на пенсию, ведь тогда он останется в качестве председателя совета директоров на какое-то время. Игры: NVIDIA GeForce NOW теперь доступен на visionOS Игровой облачный сервис NVIDIA GeForce NOW обновился до версии 2.0.70, где появилась поддержка Vision Pro и других VR-устройств – всё работает прямо на сайтеplay.geforcenow.comв браузере Safari. Доступно более 2100 игр, включая ААА-уровня. На видео пример работы сервиса в браузере Safari на visionOS с подключенным геймпадом в игре “Cyberpunk 2077”: App Store: новое API для встроенных покупок, проблемы в Индии и Великобритании Apple анонсировала для разработчиков Advanced Commerce API – новый инструмент для расширенных встроенных покупок Новинка запущена для поддержки соответствующих бизнес-моделей в App Store и более гибкого управления покупками внутри приложений. Компания предлагает для таких покупок в рамках нового API надёжную коммерческую систему в App Store, включая комплексную обработку платежей, налоговую поддержку, обслуживание и многое другое. Чтобы иметь право на доступ к API Advanced Commerce, приложение должно использовать StoreKit2, работать на iOS/iPadOS 15.0 (и новее), а также соответствовать одной из следующих бизнес-моделей: • приложения с большими каталогами разовых покупок, в том числе включающие в себя добавляемый контент от нескольких авторов; • приложения с большими каталогами подписного контента – включая часто добавляемый контент от нескольких авторов, с частыми дополнениями, работающее как единая возобновляемая подписка; • приложения с доступом к подпискам с дополнительным контентом в виде единой покупки в том же приложении в рамках его основного предложения – включая варианты подписок на определенную услугу, а также дополнительно на контент или услуги, которые автоматически продлеваются как единая подписка в том же приложении. Запросить доступ и отправить приложение на рассмотрение, а также уточнить подробностиможно здесь Тем временем, индийское правительство требует от Apple возможности установки государственного магазина приложений Как сообщают местные медиа, в декабре Министерство технологий Индии вызвало к себе представителей Apple, Google и других производителей смартфонов с требованием предоставить индийцам прямой доступ к обязательному набору приложений – под это дело создали магазин «GОV in». От Apple требуется сделать поддержку установки стороннего государственного магазина приложений прямо как в ЕС. Ранее в Индии уже рассматривали к принятию закон, который повторяет европейский «Закон о DMA» и Apple тогда высказалась против. Теперь дело имеет реальные положения и обязательства к запуску. При этом сроков и последствий при неисполнении пока не озвучивалось. А в Великобритании Apple столкнулась с антимонопольным судебным разбирательством Конкретных обвинений пока не выносили, поскольку антимонопольная комиссия запустила процесс расследования – он продлится до 22 октября 2025 года и также охватит компанию Google. На этом пока все новости, до встречи в следующем выпуске! Или в наших соцсетях, где ещё больше разных новостей из мира Apple: 𝕏 (бывший Twitter)/Telegram/VK "
  },
  {
    "article_id": "https://habr.com/ru/companies/timeweb/articles/875496/",
    "title": "Жизнь после помойки — ремонтируем легендарный игровой смартфон Nokia N-Gage",
    "category": "Мобильные технологии",
    "tags": "bodyawm_ништячки, timeweb_статьи, телефоны, смартфоны, nokia, n-gage, symbian, ремонт, diy",
    "text": "Недавно мне удалось купить в Китае легендарный игровой смартфон, о котором наверняка слышали многие Хабровчане, а именно — Nokia N-Gage Classic. Однако полностью рабочий экземпляр в хорошем состоянии сейчас ценится как коллекционный девайс и стоит не менее 10.000 рублей. Но у меня таких денег не было... да и простых путей я тоже не ищу, поэтому я приобрел себе телефон не из рук ценителя, а напрямую из металлоприёмки после воды и работы другого мастера. В конечном счете, мне удалось практически полностью восстановить телефон и в сегодняшней статьей я расскажу вам о: диагностике некоторых аппаратных неисправностей классических телефонов Nokia, принципе работы матричной клавиатуры, ремонте телефона на практике с подробными изображениями и в заключительной части мы посмотрим с вами, во что можно было на нём поиграть! ❯ Предисловие Так уж получилось, что почти все январские статьи у нас посвящены телефонам-игровым консолям. Мы с вами не только вкратце рассмотрели предысторию появления мобильного гейминга и игровых телефонов в целом, но и успели на практике пощупать два необычных телефона с функциями консоли: современный кнопочный телефон с встроеннымаппаратнымклоном денди и редчайший прототип Android-смартфона для геймеров. Однако, думаю, многие читатели ждали статью о серийном и относительно массовом игровом смартфоне, который вышел в далёком 2003 году. И да, речь сегодня пойдет об оригинальном Nokia N-Gage, который в наше время получил постфикс «Classic». Но начнём с предыстории. В 2002 году, рынок портативных игровых консолей переживал свои лучшие годы. Самой популярной «портативкой» на рынке была свежая GameBoy Advance от Nintendo, которая отличалась неплохой, по меркам хэндхэлда, производительностью, хорошим цветным дисплеем с высоким разрешением и обильной библиотекой игр. Ещё в 2001 году, компания Nokia выпустила свой первый смартфон — Nokia 7610, на операционной системе Symbian, которая была прямым наследником ОС EPOC с карманных компьютеров Psion. Аппаратная платформа смартфона называлась WD2 и состояла из ARMv5 процессора TI OMAP 310, работающего на частоте ~104-126МГц, от 8 до 16 мегабайт оперативной памяти типа SDRAM, а также около 16 мегабайт постоянной памяти и бейсбенд-процессора (иными словами — модема) от обычного S40-телефона. В Nokia смекнули, что в сравнении с GBA, такие характеристики были как минимум достойными для портативной консоли и на базе смартфонной платформы вполне можно сделать игровой девайс! В одном устройстве необходимо было объединить две концепции — телефона и игровой консоли, поэтому компания решила использовать весьма необычный форм-фактор, который назывался «Тако» и подразумевал горизонтальное расположение аппаратных кнопок. Причём первый телефон с таким дизайном, Nokia 5510, имел QWERTY-клавиатуру! Уже в 2002 году, Nokia анонсировала N-Gage, который должен был перевернуть рынок портативных игровых консолей. В начале 2003 года, вышла Nokia 3300, представляющая из себя телефон в формате «гаги», но при этом ориентированный на мультимедийные возможности и работающий на платформе S40. И хотя производитель не позиционировал его как игровой, это был один из первых цветных телефонов с поддержкой Java-приложений и на нём можно было играть в самые первые мобильные игры. Ну а раз есть игры — то чем не игровой? :) 7 октября 2003 года, мир наконец-то увидел N-Gage: смартфон, на который Nokia возлагала большие надежды... Однако телефон получил лишь умеренный успех на рынке из-за ряда инженерных особенностей устройства. Например, говорить предлагалось повернув телефон торцом к уху, а для смены игры необходимо было вытащить аккумулятор, достать прошлую MMC-флэшку, установить новую, снова установить аккумулятор и включить телефон — и весь процесс занимал около полутора минут, во время которого вы были не в сети! Помимо этого, у телефона был странно реализован драйвер MMC-флэшек: плеер мог эксклюзивно заблокировать карту памяти и если «аська» была установлена на MMC-карту, вы не могли параллельно общаться и слушать музыку! Игры для N-Gage распространялись на картриджах в виде MMC-флэшек с какой-никакой защитой от копирования. Развитие WAP-сайтов и мобильного интернета в целом было отнюдь не на руку Nokia в этом случае, поскольку игры очень быстро сдампили с картриджей, пропатчили и выложили в интернет — совершенно бесплатно, пользователю оставалось лишь скопировать игру на свою карту памяти. В те годы, на Symbian можно было ставить всё что угодно, никаких сертфикатов и трюков с переводом даты на телефоне не было! В 2004 году, Nokia выпустила второе поколение N-Gage, которое называлось QD и имело как улучшения, так и упрощения. Из улучшений можно выделить разговорный динамик, перенесенный на фронтальную часть устройства, хороший дисплей с гораздо более яркой подсветкой и возможность замены картриджей без перезагрузки, а из упрощений — зачем-то убрали разъём для синхронизации с ПК и уменьшили габариты телефона — лично мне с моими большими руками на Classic'е играть удобнее! Конечно и мне хотелось обзавестись своим собственным N-Gage Classic. Однако, как я уже сказал в вводной части статьи, цены на них очень сильно кусаются: классическая версия редкая и выпускалась не очень большим тиражом, из-за чего ценники на вторичке в России достигают 15 тысяч рублей за рабочее устройство в хорошем состоянии и 10 тысяч рублей за девайс с небольшими недостатками. У меня таких денег нет, но зато я обожаю ремонтировать и пытаться дать новую жизнь различным ретро-устройствам, поэтому я решился на рисковый шаг - купить смартфон из утиля в Китае! Смартфон продавался на онлайн-барахолке Goofish, аналоге нашего Avito. Продавец писал о том, что у смартфона не работает подсветка и возможно есть какие-то другие недостатки. Ну, подсветка это несложно на первый взгляд, поэтому я решил рискнуть и заказал его себе. Купить устройство и доставить его на склад в Китае помог мой подписчик Роман, а привезти в мой город - Ейск, помог сервис самостоятельных покупок YouCanBuy, за что вам большое спасибо! При получении, я проверил смартфон: он включился, но требовал SIM-карту. Подсветка хоть и работала — но только в момент включения устройства до фактической загрузки ОС. Ну что ж, давайте перейдем к процессу дриставрации! ❯ Ремонтируем Поскольку телефон я купил из утиля, за годы лежания в неизвестных условиях, он очень сильно покрылся пылью и грязью, а все прижимные модули телефона, по типу разъёма зарядки, были в серьезной коррозии. Поэтому в первую очередь, смартфон необходимо было хорошенько отмыть! Я разобрал телефон и пошёл тщательно отмывать каждый корпусной элемент устройства с зубной щёткой и шампунем. Особо тщательно я вымывал труднодоступные места в фронтальной панели и силиконовой резинке клавиатуры — там скопилось ну просто неприличное количество грязи. После принятия душа, все корпусные элементы были предварительно высушены феном при температуре в 100 градусов с средним потоком, а затем отправлены окончательно досыхать на стол. Теперь самое время посмотреть на саму плату устройства. Когда я вставил SIM, включил смартфон и начал проверять кнопки — я обнаружил, что «меню», «музыка» и «левая софт-клавиша» не функционируют: При детальном осмотре платы обнаружилось, что мембранный слой с кнопками уже когда-то отклеивался... и я увидел как кто-то криво залудил контакты кнопки «меню» в надежде её починить. С виду, эта часть платы точно топилась и возможно мастер, обнаружив следы коррозии на контактах, решил попробовать отремонтировать её вот таким путём. Само собой это не дело. Я перезалудил контакты и снял лишний припой оплеткой, однако это, очевидно, не помогло. Если мы обратимся к схеме устройства, то увидим что клавиатура выполнена по матричному принципу — простыми словами, процессор выдаёт высокий уровень на GPIO каждой колонки матрицы, при этом напряжение с колонки идёт на первый вывод каждой кнопки, а второй вывод присоединён к соответствующему столбцу в процессоре. По итогу получаем очень простую схему: процессор выдаёт VIO-напряжение на каждый ряд кнопок и если какая-то из них нажата, то получает это же напряжение на одной из своих ножек. Итого делаем вывод что один из row или column-сигналов банально не доходит до кнопки! Для уменьшения помех на сигнальных линиях от радиотракта, в смартфонах Nokia использовались т.н EMIF-фильтры в BGA-корпусах, ещё их называют «стекляшки». Фильтры ставятся на линии клавиатуры и на дисплей, однако сами стекляшки очень хрупкие и практически гарантированно выходят из строя при попадании в воду и иногда при падении. Поэтому если у вашей Нокии белый дисплей, но при этом есть звуки и все кнопки работают, либо же часть кнопок не работает при общей работоспособности устройства — скорее всего, вышел из строя один или два EMIF-фильтра. Однако если фильтр вышел из строя — не беда, его можно заменить перемычками. Для этого сам фильтр необходимо снять: добавляем флюс под «пузо» фильтра и греем его паяльником сверху. Если не получается — можно добавить немного припоя, главное не пытайтесь снять его насильно — иначе есть риск сорвать пятачки! После этого, необходимо сделать перемычки  на всех I и O пинах площадки под чип. Где они находятся, можно узнать в даташите на фильтр, в моем случае это верхние два пятака и нижние два пятака. Это помогло лишь частично — у меня наконец-то заработала кнопка музыки. Тут я уже взял в руки мультиметр и начал прозванивать где у нас обрываются дорожки с кнопок. До выхода фильтров все прозванивалось замечательно, при этом я обнаружил тестпоинты всех колонок и столбцов кнопок... кроме одного. И тут мне стало всё очевидно: поскольку от кнопки меню идёт общий сигнал ROW0 с левым-софткеем — у нас банально отгнили две дорожки на кнопке меню! Сигнал COL я взял с тест-поинта, а ROW0 я нашёл, счистив маску с ближайшей кнопки дорожки сверху и установив между ними перемычку. Теперь всё наконец-то заработало! За кадром я пообрезал хвосты и поставил перемычки потоньше (с изначально тонкими на фото ничего не было бы видно), но УФ-маску наносить пока не стал. Был бы у меня микроскоп — сделал бы вообще идеально, но и так вполне неплохо :) Однако остался вопрос с дисплеем, из-за пребывания во влажной среде клей под поляризационной пленкой вспух, поэтому у нас останутся перманентные артефакты на дисплее. Что ж, бывает и такое Пришло время собрать смартфон и посмотреть что же у нас получилось! Поковырявшись в меню, я обнаружил программу nLights для управления подсветкой устройства... прошлый хозяин зачем-то выкрутил подсветку клавиатуры и дисплея в ноль — в этом и была причина её «неработоспособности». На этом наш процесс дриставрации завершен. Аппарат собран, выглядит вполне неплохо и что самое главное — полностью работает! Весь ремонт занял у меня часа 2 от силы вместе с диагностикой. Давайте же посмотрим, на что смартфон способен в 2024 году! ❯ Смотрим на девайс После включения смартфона, нас встречает ламповый и любимый интерфейс Symbian 6.1! Телефонные функции доступны и сейчас, в России 2G ещё не отключили, поэтому при желании можно пользоваться телефоном по прямому назначению. И в целом, для этих целей он подходит весьма неплохо! Казалось бы, нестандартная форма клавиатуры может показаться неудобной, однако на практике всё оказывается совершенно наоборот. Тоже самое касается серфинга WAP-сайтов, когда это было актуально! В отличии от QD, у Classic был весьма широкий мультимедийный функционал — он из коробки поддерживал mp3 и wav, FM-радио, а также имел поддержку стерео-звука. Качество звука для тех лет было вполне неплохим — представляю, какой мечтой было заполучить такой телефон в момент выхода и сидеть с пацанами «у падика», слушая музычку и поигрывая по очереди в Asphalt 2! После установки MMC-флэшки с приложениями и играми, смартфон раскрывался в полную силу. Помимо нативных sis-приложений, N-Gage поддерживал также и Java-приложения, что ещё больше расширяло библиотеку софта и игр. Лет 10 назад можно было даже Хабр почитать, пока работала Opera Mini 5. Однако MIDP 1.0 потерял свою актуальность уже к 2006 году. Но мы ведь пришли с вами за играми! Давайте посмотрим, что-же умел N-Gage на практике: вместо статичных скриншотов, я приложил к каждой игре таймкод с геймплеем. С двухмерными играми телефон справляется без каких либо проблем, благодаря довольно шустрому железу, у разработчиков появлялась возможность использования продвинутых фишек по типу аффинных трансформаций спрайтов и сложных параллакс-фонов. Первой игрой у нас будет легендарная SonicN, которая является прямым портом Sonic Advance. На глаз игра идёт в стабильные 30 FPS и, скажу вам честно, ни одна Java-игра под MIDP 1.0 не могла выдать такой уровень графики! Переходим к трёхмерным играм. Поскольку на N-Gage не было 3D-ускорителя, все игры с честным 3D использовали программные растеризаторы. Особо отличилась здесь компания Ideaworks, которая реализовала спецификацию GPU PS1 в виде очень быстрого софтрендера, что позволило портировать различные игры с PS1. Одной из таких была Tomb Raider, которая идёт здесь просто замечательно! И последней игрой в тестах у нас будет Asphalt 2. В те годы множество разработчиков мобильных игр соревновались в скорости и красоте своих игр и Asphalt 2 от ещё совсем молодой Gameloft выглядел прямо как AAA-игра! На N-Gage игра выглядит красиво и при этом работает в 20-25 кадров в секунду: ❯ Заключение Вот такая ретроспективная статья о легендарном игровом смартфоне у нас с вами получилась. Удалось ли мне вдохнуть новую жизнь в смартфон, который побывал в утиле, на котором по сути в своё время поставили крест и почти отправили в переработку? Пишите своё мнение в комментариях! Друзья! Если вам интересен мой контент, то будет здорово если вы подпишитесь на мойканал на YouTubeилипаблик ВК. Сами понимаете, для видеоблогеров в РФ сейчас время сложное, на ютубе охваты сильно упали, а в ВК нет никаких механизмов для продвижения контентмейкеров-новичков — так что приходится искать зрителей среди читателей :) Также если вам интересна тематика ремонта, моддинга и программирования для гаджетов прошлых лет, подписывайтесь на мойTelegram-канал, куда я публикую бэстейджи статей и видео, ссылки на новый контент и немножко щитпоста! Друзья! Для подготовки статей с разработкой самопальных игрушек под необычные устройства, объявляется розыск телефонов и консолей! В 2000-х годах, китайцы часто делали дешевые телефоны с игровым уклоном — обычно у них было подобие геймпада (джойстика) или хотя бы две кнопки с верхней части устройства, выполняющие функцию A/B, а также предустановлены эмуляторы NES/Sega. Фишка в том, что на таких телефонах можно выполнять нативный код и портировать на них новые эмуляторы, чем я и хочу заняться и написать об этом подробную статью и записать видео! Если у вас есть телефон подобного формата и вы готовы его задонатить или продать, пожалуйста напишите мне в Telegram (@monobogdan) или в комментарии. Также интересуют смартфоны-консоли на Android (на рынке РФ точно была Func Much-01), там будет контент чуточку другого формата :) А также я ищу старые (2010-2014) подделки на брендовые смартфоны Samsung, Apple и т. п. Они зачастую работают на весьма интересных чипсетах и поддаются хорошему моддингу, парочку статей уже вышло, но у меня ещё есть идеи по их моддингу! Также может у кого-то остались самые первые смартфоны Xiaomi (серии Mi), Meizu (ещё на Exynos) или телефоны Motorola на Linux (например, EM30, RAZR V8, ROKR Z6, ROKR E2, ROKR E5, ZINE ZN5 и т.п, о них я хотел бы подготовить специальную статью и видео т. к. на самом деле они работали на очень мощных для своих лет процессорах, поддавались серьезному моддингу и были способны запустить даже Quake!). Всем большое спасибо за донаты! А ещё я держу все свои мобилы в одной корзине при себе (в смысле, все проекты у одного облачного провайдера) — Timeweb. Потому нагло рекомендую то, чем пользуюсь сам —вэлкам: "
  },
  {
    "article_id": "https://habr.com/ru/articles/876562/",
    "title": "Обзор смартфона Realme GT6: Убийца флагманов или просто агрессивный маркетинг?",
    "category": "Мобильные технологии",
    "tags": "Realme GT6",
    "text": "Realme GT6 вышел на международный рынок летом 2024 года и моментально весь интернет заполонили обзоры в стиле: «убийца флагманов», «лучший смартфон в своем классе», «топ за свои деньги» и прочая маркетинговая лабуда. Однако как только пена улеглась, оказалось, что большинству пользователей GT6 оказался не интересен. Во-первых, сыграла роль достаточно высокая цена, особенно на фоне с китайской версией аппарата. Во-вторых, искушенный пользователь уже «наелся» рекламы и очередной самоубийца флагманов воспринимался не более, чем очередной трюк пиарщиков. И смартфон, который заслуженно мог стать бестселлером, отправился на задворки прилавков, где сейчас им интересуются лишь гики и случайные прохожие. В этом, кстати, есть вина и самой компании Realme. Привычка максимально преувеличивать незначительные преимущества, завышать характеристики и всячески пускать пыль в глаза, вылилось в то, что многие просто перестали обращать на это внимание. Что касается меня, то я приобрел смартфон в ноябре, после чего меня ждали неслабые «американские горки». Вначале я был дико заинтригован заявленными характеристиками (до момента получения). Потом немного расстроился, когда оказалось, что это никакой не убийца флагманских смартфонов, а типичный субфлагман (с некоторыми оговорками). А потом смартфон мне как зашел! Случилось это тогда, когда я спокойно изучил его реальные преимущества и недостатки. В итоге я пришел к мнению, что смартфон явно недооценен пользователями и сильно переоценен производителем. Сейчас, когда цена немного снизилась, а я хорошо изучил все преимущества и недостатки, решил написать обзор. Для тех, кто вдумчиво ищет свой следующий смартфон, не гонясь за новинками — будет полезно. Смартфон точно стоит своих денег, но нужно четко понимать за что конкретно платятся эти деньги. Технические характеристики Realme GT6 Дисплей 6,78″ AMOLED 8T LTPO с разрешением 2780 x 1264 пикселей и адаптивной частотой обновления от 1 до 120 Hz, яркость 1000 нит (типичная)/1600 нит (на солнце)/6000 нит (максимальная), цветовой охват 100% DCI-P3, защищен закаленным стеклом Corning Gorilla Glass Victus 2, есть регулировка ШИМ для уменьшения мерцания Чипсет 8 ядерный Qualcomm Snapdragon 8s Gen 3 с частотой до 3 GHz, техпроцесс 4 нм, графика Adreno 735 Оперативная память 8GB/12GB/16GB LPDDR5X Постоянная память (накопитель) 256GB или 512GB UFS 4.0 Основная камера Главный сенсор 50MP Sony LYT-808 с оптической стабилизацией f/1.69, 1/1.4″ + телефото 50MP Samsung S5KJN5 f/2.0, 1/2.8″ + ультраширик 8MP Sony IMX355 Фронтальная камера 32МР Sony IMX615 f/2.45, 1/2.74″ Беспроводные интерфейсы WiFi 6 (802.11ax) 2x2 MIMO and MU-MIMO, Bluetooth 5.4, NFC, двухдиапазонный GPS L1 + L5 Дополнительно Вибромотор Tactile Engine Pro, подэкранный сканер отпечатков пальцев, стерео звук, система охлаждения Iceberg, защита IP65, ИК передатчик Аккумулятор 5500 mAh с поддержкой ультра быстрой 120W зарядки Операционная система Realme UI 5.0 на базе Android 14 Размеры 162 х 75.1 х 8.6mm Вес 199g Цена на момент публикации обзора 45 390 ₽ (уточнить актуальную стоимость) Комплектация Еще раз хочу уточнить, что я взял именно глобальную версию, а не CN (китайскую), которая отличается, как в плане железа, так и в плане ПО. Да и региональные блокировки никто не отменял, а изучать тонкости прошивки желания не было никакого. Смартфон помечен у продавца, как Russian Version и имеет на борту предустановленные региональные приложения (по типу яндекс браузера, кинопоиска и прочего мусора). Все это можно конечно почистить и отключить, если не нужно. Зато есть локальная гарантия 12 месяцев. Коробка самая обыкновенная, но наклейка с информацией об импортере, сервисном обслуживании и прочем — на русском языке. Цвет смартфона в такой интерпретации называется «зеленый туман». Звучит романтично, даже немного мистически. И вполне достоверно соответствует ощущениям от расцветки. Комплектация отличная, ни в какое сравнение с флагманами от Samsung (да и не только), где кроме смартфона и кабеля вы найдете разве что иголочку для извлечения лотка. Здесь же производитель дает нам фирменное SuperVOOC зарядное устройство с мощностью 120W и кабель с поддержкой соответствующего протокола. На экран наклеена качественная защитная пленка, есть даже силиконовый чехол для тех, кто хочет максимально сберечь свой смартфон. В чехле он конечно на 90% теряет свою мистичность, т. к от «зеленого тумана» остается лишь зеркальный вырез в районе камер. Большую часть корпуса теперь закрывает чехол. Зато в чехле смартфон не скользкий и будет защищен от потертостей и падений с небольшой высоты. Я всегда использую смартфоны в защитных чехлах, не исключением стал и Realme GT6. Только на момент заказа я был не в курсе, что в комплекте уже есть чехол и параллельно заказал свой вариант. Он нравится мне больше комплектного. По материалам это что-то вроде искусственного камня, с шероховатой цепкой поверхностью. Максимально тонкий и компактный, поэтому почти не увеличивает габариты. Кроме прочего он полностью закрывает зеркальную площадку и в таком виде смартфон выглядит более целостно. Зарядное устройство и скорость зарядки О зарядке расскажу более подробно, т. к здесь используется технология быстрой зарядки SuperVOOC, которую разработала компания Oppo. Технология используется в смартфонах Oppo, OnePlus и Realme. В нашем случае мы получили блок питания SuperVOOC с мощностью 120W, который по словам производителя может зарядить смартфон на 50% всего за 10 минут. В характеристиках можно увидеть, что зарядное работает с напряжением от 5 до 11V и выдает до 11А. Но чтобы получить максимальную мощность зарядки, нужен и соответствующий кабель. Если взять первый попавшийся, то мощность зарядки будет сильно ниже, т. к некоторые контакты и жилы в нем будут не задействованы. Поэтому если нужен дополнительный кабель, то нужно искать по запросу «supervooc 120w cable». Комплектный кабель естественно соответствует требованиям, на разъеме указан максимально поддерживаемый ток 12А. Что нам это дает? Возможность использовать смартфон весь день, подключив его на зарядку всего на несколько минут. Хотел подключить тестер, чтобы снять график зарядки и наглядно показать ток, напряжение и соответственно скорость набора заряда. Но ни один мой тестер не потянул 120W, да и с протоколом SuperVOOC никто не дружит. Но я решил не сдаваться и просто в ручном режиме (с секундомером) фиксировал процесс набора заряда каждые несколько минут. В итоге, на полную зарядку от 0 до 100% ушло28 минут 14 секунд. Но особенно быстро он заряжается первые 15 минут, вот поминутная статистика: 5 минут — 28% 5 минут — 28% 10 минут — 48% 10 минут — 48% 20 минут — 80% 20 минут — 80% 25 минут — 94% 25 минут — 94% 28 минут 14 секунд — 100% 28 минут 14 секунд — 100% Если вы опасаетесь, что столь быстрая зарядка негативно скажется на аккумуляторе, то не стоит этого делать. Смартфон имеет встроенный оптимизатор батареи, который следит за ее состоянием. Пользуюсь смартфоном уже пару месяцев и оценочное значение емкости все еще находится на уровне 100%. Обязательно стоит использовать функцию «умная зарядка». Основываясь на вашей статистике использования, смартфон будет сам подбирать мощность зарядки. Например, среди дня вы получите максимальные 120W, чтобы быстро зарядить смартфон. А если поставите заряжаться на ночь, то мощность будет существенно ниже (чтобы увеличить ресурс батареи). В конце концов вы можете использовать не только комплектную зарядку, а что-то из своих запасов. Например, моя зарядка Ugreen с поддержкой протокола PD заряжает смартфон мощностью 27W. Искусственное ограничение мощности, если хотите. А еще в настройках смартфона вы можете установить лимит зарядки 80% и тогда аккумулятор прослужит вам, если не вечно, то уж точно очень и очень долго. Но даже при стандартном использовании производитель гарантирует 1600 циклов до заметной потери емкости. Если заряжать каждый день, то это 1600 дней или почти 4,5 года. Внешний вид и особенности Realme GT6 получил противоречивый дизайн. Если прошлые аппараты серии GT были вдохновлены такими понятиями как скорость и спорт (вспомним яркие, как у спорткаров расцветки и полосы), то новая версия хочет быть более стильной и изысканной. И первое время это действительно работает: потрясающая расцветка, полу матовый корпус и зеркальная вставка в районе камер выглядят восхитительно. Но когда вы начинаете смартфоном пользоваться, корпус быстро теряет гламурный лоск и покрывается отпечатками. И если матовая часть справляется с этим неплохо, то зеркальная вставка буквально за несколько часов покрывается отпечатками и разводами. А еще, между камерами собирается пыль, которую очень сложно очистить. Меня все эти загрязнения раздражают, поэтому предпочитаю использовать чехол, фактически положив болт на дизайнеров и разработчиков. Возможно вы не из тех пользователей, кто судорожно протирает корпус после каждого касания? Тогда сможете пользоваться смартфоном как есть. Тактильные ощущения от смартфона приятные. По материалам имеем пластиковую рамку (покрашена под металл) и крышку из стеклопластика. Это такой хитрый пластик, который на вид и по ощущения очень похож на обычное стекло. У Samsung такой материал назвали гластик и уже несколько лет использут, в том числе и у флагманских моделей. В зависимости от окружающего освещения цвет корпуса может меняться от серо-зеленого до зелено-бирюзового, это еще одна особенность материала. Что касается блока камер, то здесь установлены 3 объектива. Основной это 50MP объектив с сенсором Sony LYT-808 с оптической стабилизацией. Он отлично справляется с любыми условиями и дает возможность получить четкий снимок даже при слабом освещении. В помощь ему поставили телефото объектив 50MP Samsung S5KJN5 с 2х кратным оптическим приближением и простенький ультраширик 8MP Sony IMX355. Камера делает снимки с последующей обработкой искусственным интеллектом, поэтому в большинстве случаев фото получаются четкие, красочные и детализированные. Органы управления реализованы в виде качельки громкости и отдельной кнопки блокировки с правой стороны. На верхнем торце можно заметить инфракрасный передатчик, который используется для управления бытовой техникой. Для настройки используется приложение IR Remote, которое предустановлено в системе. Вы выбираете тип бытовой техники и следуете инструкциям на экране, после чего смартфон подбирает нужные параметры. Буквально за пару минут я настроил дистанционное управление на свой телевизор Sony и кондиционер Midea. На нижнем торце разместили аудио динамик, разъем USB-C и лоток для сим карт. Несмотря на то, что аудио динамик здесь один, смартфон выдает стереозвук с приличным уровнем громкости. Роль второго динамика играет разговорный. Звук хорошо сбалансирован по частотным характеристикам, нет перекоса по громкости, как у многих недорогих решений. В горизонтальной ориентации отчетливо ощущается стерео эффект, чувствуется объем. Лоток поддерживает работу с двумя sim картами формата нано. Поддержка карт памяти не предусмотрена, поэтому стоит подумать какая версия подойдет вам больше: на 256 GB или на 512 GB. Память быстрая UFS 4.0, поэтому работает все очень быстро, приложения запускаются мгновенно. В целом внешний вид смартфона мне нравится, почему-то он напоминает мне Redmi Note 8 Pro зеленого цвета, который свое время был очень популярен. Что касается лицевой части, то здесь Realme постарались максимально: дисплей флагманского уровня, минимальные рамки и аккуратная фронтальная камера в виде точки. Экран закрыт стеклом Corning Gorilla Glass Victus 2, которое отлично противостоит ударам и падениям с высоты до 2 метров. Дисплей Дисплей визуально не хуже, чем у последних флагманских смартфонов от Samsung или IPhone. Качественная AMOLED матрица с цветовым охватом 100% DCI-P3. Матрица здесь на 10 бит и поддерживает вывод изображения Dolby Vision. Цветопередача настраивается как в сторону реалистичности, так и в сторону красочности. Зависит от выбранного пресета. Можно переключить цветовой охват на DCI-P3 или на sRGB. Есть профессиональный режим, где можно вручную подкорректировать цвета (Red, Green, Blue), оттенок, насыщенность и другие параметры. Также регулировке поддается и цветовая температура. Частота обновления увеличена до 120 Hz, благодаря чему все анимации выглядят очень плавными. Причем есть адаптивный режим, который автоматически меняет частоту обновления в зависимости от происходящего, что позволяет экономить заряд. Смартфон просто уменьшит частоту до минимума, если будет статичное изображение и снова увеличит ее в динамичных сценах. Разрешение изображения также можно изменить. По умолчанию стоит 2780 х 1264, что дает более высокую детализацию и четкость. При желании можно уменьшить до 2376 х 1080, что увеличит время работы, так как снизит нагрузку на процессор и графическое ядро. Отмечу наличие различных режимов для защиты зрения, где, в том числе используется искусственный интеллект. ИИ выявляет признаки усталости глаз при помощи фронтальной камеры и регулирует цветовую температуру экрана в режиме реального времени. Есть естественно и ручной режим, где вы сами настраиваете интенсивность фильтра синего излучения. Можно задать расписание. Еще одной особенностью экрана является его высокая яркость. Но здесь производитель пошел на некоторые уловки. В промо материалах описания смартфона указана рекордная яркость 6000 нит и это конечно же вызывает удивление и неподдельный интерес. Однако если перейти в характеристики, то мы увидим, что типичная яркость составляет 1000 нит, а пиковая (под прямыми солнечными лучами) — 1600 нит. Значение 6000 нит там тоже указано, но показатель этот помечен как APL. Что это за показатель такой никто не знает (в том числе интернет), но в описании смартфона на официальном сайте мне все же удалось интересующую информацию. В общем 6000 нит это локальная пиковая яркость, которую получили в лаборатории Realme при освещенности в 100 000 люкс. Т. е по факту это просто маркетинг и к реальным условиям использования отношения не имеет. Я отталкиваюсь от типичной яркости 1000 нит и пиковой 1600 нит, что действительно кажется правдой. Экран яркий и хорошо читается как в помещении при ярком освещении, так и на улице. Визуально экран кажется таким же ярким, как у Galaxy S23, с которым я имел возможность сравнить в магазине электроники. Если нужно получить максимальную яркость, то в настройках нужно активировать пункт «повышенная яркость», которая разблокирует ее увеличение еще на 10 — 15% Если нужно получить максимальную яркость, то в настройках нужно активировать пункт «повышенная яркость», которая разблокирует ее увеличение еще на 10 — 15% Настройка яркости работает в обе стороны. В том плане, что вы также можете задать минимальную яркость, при которой вам будет комфортно работать в полной темноте Настройка яркости работает в обе стороны. В том плане, что вы также можете задать минимальную яркость, при которой вам будет комфортно работать в полной темноте Что касается ШИМ, который всегда присутствует в AMOLED-дисплеях, то здесь Realme предлагает использовать функцию One-Puls EM Mode. Она уменьшает мерцание на средних и высоких уровнях и снижает нагрузку на глаза Что касается ШИМ, который всегда присутствует в AMOLED-дисплеях, то здесь Realme предлагает использовать функцию One-Puls EM Mode. Она уменьшает мерцание на средних и высоких уровнях и снижает нагрузку на глаза При помощи анализатора мерцания проверим как это работает. Сначала сделал замеры мерцания на разных уровнях яркости без использования функции One-Puls EM Mode. Даже на максимальной яркости мы наблюдаем высокую модуляцию на очень низкой частоте, никуда она не делась и на средних значениях яркости. Лишь на низкой яркости мы наблюдаем увеличение частоты. Включаю функцию One-Puls EM Mode и провожу измерения еще раз. Как видим, даже на осциллограмме есть видимые изменения. При том же уровне модуляции, частота на высоких и средних значениях яркости увеличилась в 6 раз. Т. е функция действительно работает и чувствительным к мерцанию пользователям лучше ее активировать. Также хочу обратить ваше внимание на максимально зафиксированную яркость 944 нит (первый скриншот), что очень близко к заявленным производителем 1000 нит. А вот такую картину показывает Radex Lupin, он считает что коэффициент пульсаций у смартфона находится на уровне 15%, что является предельно допустимым значением. В общем я считаю, что дисплей у смартфона установлен действительно классный, фактически флагманского уровня: яркий, четкий, с хорошей детализацией и большим количеством полезных настроек. Высокая частота обновления 120 Hz лишь дополняет его, обеспечивая максимальную плавность анимаций. А то, что присутствует пугающий некоторых ШИМ (Широтно-импульсная модуляция), так это не страшно. Начиная с Galaxy S8, я постоянно пользовался аппаратами от Samsung (рекордсмены по ШИМ) и никаких негативных эффектов на себе не ощущал. К тому же здесь One-Puls EM Mode, которая увеличивает частоту пульсаций. Программное обеспечение Realme GT6 работает на Realme UI 5.0, которая базируется на Android 14. Это сильно переработанная система с огромным количеством настроек и заметно расширенными, по сравнению со стоковой системой, возможностями. По своему подходу и визуализации очень напоминает One UI от Samsung, которой скорее всего и вдохновлялась при ее создании. Все возможности системы рассматривать мы конечно не будем, но немного внимания уделю. В плане персонализации смартфон очень гибкий и позволяет настроить все до мелочей: обои, значки, темы, шрифты, рингтоны, анимации отпечатка пальцев и даже световые сигналы на кромке. Есть фирменный «магазин» с соответствующим контентом, причем все абсолютно бесплатно. AOD (всегда включенный дисплей) может отображать часы, дату, время, заряд батареи и уведомления. Он может работать в энергосберегающем режиме (включаться только если вы возьмете смартфон в руки) или отображаться постоянно. В качестве AOD можно использовать цифровые или аналоговые часы, анимацию, текст или пользовательское изображения, а также различные интерактивные штуки, вроде животных, природы или Omoji (ваше цифровое изображение). Наиболее энергоэффективны минималистичные решения, которые отображают основную информация. Например, вот так выглядит стандартный вариант с цифровыми часами. Realme всегда славилась тем, что уделяет большое внимание визуальным эффектам и дает возможность гибкой настройки смартфона под свой стиль. От визуальных моментов переходим к практическим. Для большей реалистичности при взаимодействии со смартфоном здесь использовали функцию O-Haptics, которая при помощи специального вибромоторчика может имитировать текстуру и эффекты различных материалов, дать ощущение большей естественности при взаимодействии с аппаратом. Например, держа смартфон в руках, вы реально ощущаете как по экрану катится шарик, щелкает регулятор или лопаются пузырьки. Следующей фирменной «фишкой» является режим GT, который увеличивает быстродействие, позволяя ядрам процессора и графического ускорителя работать на повышенных частотах. Однако подробней об этом я расскажу в разделе о производительности и играх. Также в настройках можно обнаружить целый ворох функций, связанный с использованием искусственного интеллекта. Не зря же на коробке красуется аббревиатура AI? Например, полезные функции «распознавание экрана «и «умный цикл», которые позволяют смартфону автоматически распознавать текст и изображения с экрана. Потом их можно использовать для своих целей (перевести, найти информацию об объекте, сохранить на смартфоне, передать другому пользователю или отредактировать). В разделе «умное обнаружение» доступна поддержка бесконтактных жестов, двойное касание спинки для сканирования Qr-кода и даже регулировка громкости взглядом при входящем звонке. Для игр возможно зарезервировать определенный объем оперативной памяти, в том числе в автоматическом режиме при помощи ИИ. Реально много всего намешано в системе, отчего первое время даже теряешься. Но со временем система становится понятней, а настройки уже не пугают структурой. Чего только не найдешь в Realme UI 5.0. Например, для меня была в диковинку возможность измерения пульса сканером отпечатков пальцев. Также нашел различные смарт панели и плавающие кнопки, куда можно «повесить» наиболее часто используемые приложения и функции, а также организовать доступ к хранилищу и некоторым ИИ функциям (тот же, перевод с экрана). И конечно важно то, что работа над прошивкой не останавливается ни на минуту. Периодически приходят обновления, которые исправляют ошибки, добавляют новые функции и в целом улучшают работу смартфона. Производительность и тесты Итак, что мы имеем на текущий момент? Смартфон получил флагманский дисплей, но материалы корпуса это то, на чем производитель решил сэкономить. Возможно по производительности он сможет приблизиться к топовым решениям? Realme GT6 получил свежий процессор Snapdragon 8s Gen 3, который вышел весной прошлого года. Его запросто можно спутать с обычным Snapdragon 8 Gen 3, ведь разница в маркировке всего одна буковка. Тем не менее это менее мощная версия процессора, хотя и относится к высокопроизводительной линейке. По производительности она находится где-то посередине между Snapdragon 8 Gen 2 и Snapdragon 8 Gen 3. Также здесь поставили более простой видео ускоритель Adreno 735. Т. е видим, что и по «железу» смартфон скорее ближе к субфлагманам, чем к самым топовым решениям. Хотя память вот порадовала, производитель использовал здесь какую-то разогнанную версию UFS 4.0. По объему уже смотрите сами, т. к есть версия на 256GB и версия на 512GB. В синтетических тестах смартфон показывает такие результаты: В Antutu V10 почти 1,5 млн очков В Antutu V10 почти 1,5 млн очков Geekbench 6: 4855 — многоядерный режим, 1932 — одноядерный Geekbench 6: 4855 — многоядерный режим, 1932 — одноядерный Open Cl — 9126 очков Open Cl — 9126 очков Для сравнения, смартфоны на Snapdragon 8 Gen 3 набирают в среднем 2 млн. очков в Antutu и 7300\\2000 очков в Antutu V10. Т. е в целом можно говорить о 30% превосходстве старшей платформы. Что касается графики, то смартфон набрал 1015 баллов в тесте Steel Nomand, 4823 балла в тесте Solar Bay и 2881 балл в тесте Wild Life Extreme. Далее я включил режим GT и провел тесты повторно, получив 1030\\4879\\3012 очков. Как видите, результат действительно немного улучшился, ноприрост составил от 1,2 до 4,5%. Эффекта от этого особо не будет, а вот нагрев увеличивается весьма заметно. В общем на мой взгляд режим GT в этом смартфоне использовать не стоит. Скорость накопителя очень высокая. Как обычно, в разных бенчмарках результат может сильно отличаться, ведь они используют разные алгоритмы для тестирования, но в целом картина предельно ясна. Достаточно понимания, что скорость здесь измеряется не в MB/s, а в GB/s. Гигабайты в секунду. Как раз тот случай, когда внутреннюю память реально можно использовать в качестве расширения оперативной. Хотя и оперативки здесь конечно достаточно, да и по скорости LPDDR5X будет еще приятней. В общем с памятью все отлично, благодаря чему смартфон шустрый и отзывчивый. Игры Линейка GT у Realme всегда была сильна в игровом плане. Не исключением стал и GT6, где в совокупности с «мощным» железом использовали систему парового охлаждения Iceberg, которая позволяет процессору дольше работать на высоких частотах без перегрева. Также в устройстве используется специальный игровой режим HyperBoost, который оптимизирует быстродействие и позволяет более тонко настроить смартфон для игр. Просто во время игры вы можете вызвать панель, где видна текущая частота кадров, температура, задержка передачи данных в интернете. И конечно же там есть различные полезные для геймеров настройки. Есть переключение режима производительности, частоты обновления экрана и увеличение чувствительности сенсорного слоя. Все эти параметры влияют на производительность, расход аккумулятора и конечно же нагрев. Более того, вы можете самостоятельно ограничить или увеличить частоту ядер процессора или графического ускорителя до нужного значения В каждом из разделов (большие, малые ядра и т. д) есть ручные настройки частоты. То же самое и с графическим процессором. Есть дополнительное увеличение качества рендеринга до Ultra HD качества, есть улучшение цветопередачи в режиме HyperHDR. И это я еще не касался оптимизации сети, оптимизации касаний и прочих вспомогательных функций. Ну давайте же посмотрим, на что способен смартфон в играх. В War Thunder на максимальных настройках графики смартфон выдает 55 — 60 FPS. В Call Of Duty mobile у нас фактически есть два пути, по которому мы можем пойти. Первый это максимальное качество графики при 60 FPS. Можно еще увеличить частоту кадров в секунду до 100+, снизив настройки до средних. Это будет интересно киберспортсменам, т. к игра становится более отзывчивая и целиться становится проще. Ну и конечно Genshin. Здесь с максимальными настройками графики FPS находится в районе 45 — 50. Причем в начале это даже 60, но потом видимо процессор нагревается и снижает частоту. В любом случае играть комфортно. Аудио и видео Realme GT6 хорошо подходит на роль развлекательного устройства. Он поддерживает Dolby Vision и HDR10+, современные кодеки (в том числе AV1), а также получил DRM Widewine L1. Все это позволяет смотреть онлайн кинотеатры, фильмы и ролики с максимально доступным качеством. Где-то в дороге, в командировке или во время ожидания в очереди можно посмотреть Youtube или любимый сериал на одном из популярных сервисов. Я, например, люблю смотреть видосики во время длительных пробежек на беговой дорожке. Не подкачал смартфон и в плане звука. Аналоговый аудио выход ушел в историю и сейчас является уделом бюджетных аппаратов. Нам же предлагают использовать переходник со встроенным DAC или беспроводные наушники. Насыщенное объемное звучание с высокой детализацией обеспечивается поддержкой высококачественных кодеков, а именно aptx HD. Фото Смартфон предлагает неплохие возможности в плане фото и видеосъемки, делая упор на реалистичности и детализации. Сейчас сложно показать все преимущества камеры, но даже в такую «серую» погоду заметны основные достоинства. Быстрая точная фокусировка, хорошая работа с мелкими деталями, корректный баланс белого и широкий динамический диапазон это то, что видно невооруженным взглядом. Есть возможность двухкратного оптического приближения, которое умело совмещается с цифровым. Даже гибридное 4Х приближение выглядит достойно и точно передает детали на снимке. Вот пара примеров использования зума. В помещении при искусственном освещении камера не пасует. Снимки получаются четким, естественными и с хорошей детализацией. При этом фокус отрабатывает безошибочно и почти в 100% случаев мы получаем четкий кадр без смазывания, даже при съемке «с руки». В темное время суток камера также показывает себя хорошо. Смартфон приглушает яркие объекты и увеличивает освещенность затемненных участков. Здесь конечно не флагманский уровень, но довольно неплохо. При записи видео доступно разрешение 4К со скоростью 60 кадров в секунду. Есть хорошая стабилизация, а автофокус срабатывает мгновенно, переключаясь с дальнего на ближний объект (и наоборот). Небольшой тестовый ролик в максимальном качестве вы можете посмотреть ниже. Автономность Смартфона стабильно хватает на пару дней работы при обычном использовании и на день, если добавить в сценарий игры. При воспроизведении видео с Youtube на яркости экрана 50% смартфон проработал 21 час 26 минут. На максимальной яркости экрана это же видео воспроизводилось 14 часов 42 минуты. В тесте Work 3.0, который имитирует смешанный режим использования, на яркости 50%, смартфон проработал 16 часов 50 минут. По графику заметно, что спустя какое-то время (примерно после 40 — 50 минут теста), производительность снизилась. Т. е смартфон не может работать на максимуме под высокой нагрузкой без уменьшения производительности. От таких сценариев не спасет никакое пассивное охлаждение. Такую же картину мы видим и в стресс тесте CPU Slowdown: при постоянной максимальной нагрузке смартфон теряет до 26% производительности. Что в принципе никаким образом не ощущается в повседневной жизни, т. к создать такую нагрузку обычным использованием не реально. Хотя в особо требовательных играх, как мы видели (в Genshin падение FPS составило от 60 до 45 — 50 кадров в секунду) снижение производительности вполне реально. В целом автономность Realme GT6 находится на достаточно высоком уровне, а совместно с ультра быстрой зарядкой, способной восполнить заряд за считанные минуты, он становится хорошим выбором для тех, кого принято называть очень активными пользователями. Итоги Отвечая на заглавный вопрос «Убийца флагманов или просто агрессивный маркетинг?» я могу с уверенностью сказать, что оба эти варианта в равной степени описывают этот смартфон. С одной стороны мы получаем высокую производительность, приличные камеры и отличный экран. С другой стороны его характеристики в рекламных описаниях выглядят слишком завышенными и преувеличенными. Вспомнить хотя бы о рекордной яркости экрана, которая оказалась лишь маркетинговым трюком. В итоге мы имеем добротный субфлагман, который находится на ступень ниже, чем передовые решения от Samsung или Apple, но гораздо интересней различных решений от Redmi или линейки смартфонов Samsung A. Здесь хорошо видно на чем сэкономили по сравнению с флагманами: процессор упрощенный с индексом S, рамки и задняя крышка из пластика, камеры хоть и очень приличные, но все же не флагманский уровень. При этом и стоит он конечно сильно дешевле, чем флагманы. Поэтому я думаю этот смартфон подойдет скорее не тем, кто хочет флагман. А тем, кто его не хочет. В том плане, что он не готов сильно переплачивать за стеклянные крышки, металлический корпус, незначительно лучшие камеры и мощь процессора, которую все равно никогда не сможет использовать на все 100%. В Realme GT6 мне понравилось: Экран с хорошей яркостью, поддержкой Dolby Vision и частотой обновления 120 Hz Экран с хорошей яркостью, поддержкой Dolby Vision и частотой обновления 120 Hz Хорошая автономность и быстрая зарядка, способная зарядить смартфон на 50% за 10 минут Хорошая автономность и быстрая зарядка, способная зарядить смартфон на 50% за 10 минут Система Realme UI и фирменные «фишки», вроде ИК передатчика Система Realme UI и фирменные «фишки», вроде ИК передатчика Производительность, достаточная для любых задач. В том числе игры на максималках Производительность, достаточная для любых задач. В том числе игры на максималках Камера Камера Комплектация Комплектация В Realme GT6 мне не понравилось: Маркий и скользкий корпус, который лучше сразу спрятать в чехол Маркий и скользкий корпус, который лучше сразу спрятать в чехол Снижение производительности в играх при длительных сеансах Снижение производительности в играх при длительных сеансах Различия между CN версией и Global, в том числе и в плане цены Различия между CN версией и Global, в том числе и в плане цены "
  },
  {
    "article_id": "https://habr.com/ru/articles/875116/",
    "title": "Apple Pro Weekly News (13.01 – 19.01.25)",
    "category": "Мобильные технологии",
    "tags": "Apple, iOS, iPhone, iPad, Mac, AirPods, Apple Intelligence, Beats, App Store, Слухи",
    "text": "Дайджест интересных и ярких новостей из мира Apple за первые недели нового года: какие обновления вышли и что в них нового, какие продукты от компании ожидать в ближайшие месяцы и даже подробности с характеристиками, какой ещё ИИ-сервис Apple запустит в будущем и чем провинилась Siri, небольшая сводка из магазинов компании, где накинулись на компанию и почему, как промоутировали «Разделение» и как поживает App Store. Двойная порция, погнали к новостям! Вышли: iOS 18.2.1, iOS 18.3 beta 3 и другие системы – рассказываем, что нового и когда ждать релизы Уже 6 января в Apple вылезли из салатов и выпустили долгожданное обновление с исправлениями ошибок: • iOS 18.2.1 (22C161) • iPadOS 18.2.1 (22C161) А потом ещё довыпустили tvOS 18.2.1 (22K160) с исправлениями ошибок и улучшениями производительности. Также 14 января, внезапно, вышлообновление ПО для MagSafe Charger (только модель A2580) – firmware update (2A143) За последние недели были выпущены и бета-версии для разработчиков, актуальные сборки на данный момент: • iOS 18.3 Developer beta 3 (22D5055b) • iPadOS 18.3 Developer beta 3 (22D5055b) • macOS 15.3 Sequoia Developer beta 3 (24D5055b) • visionOS 2.3 Developer beta 3 (22N5894a) • tvOS 18.3 Developer beta 3 (22K5553a) • HomePod OS 18.3 Developer beta 3 (22K5553a) • watchOS 11.3 Developer beta 3 (22S5550a) А ещё Release Candidate сборки для прошлых macOS: • macOS Sonoma 14.7.4 RC 3 (23H415) • macOS Ventura 13.7.3 RC 3 (22H415) А что нового в бетах? Слухи: iPhone SE4, iPad Air M3, iPad 11, MacBook Air M4, PowerBeats Pro 2 – скоро; Vision Pro 2 и «HomePad» – отложены Пачка свежих слухов про грядущий iPhone SE 4-го поколения, который уже успели назвать “iPhone 16E”, но потом опровергли. Сначала всплыли чертежи чехлов, где заметно, что будетоснова корпуса как у iPhone 14, но с одной камерой, зарядным портом USB-C, кнопкой Action Button. Но интересный момент: чехол от iPhone 14 почти подходит для нового iPhone SE, за исключением немного другого расположения боковой кнопки и динамиков с микрофоном. Также появился слух, что iPhone SE4 могут показать уже в этом месяце вместе с релизом iOS 18.3. Однако Марк Гурман из Bloomberg назвал этот слух совершенно неверным:«Да, новые iPad и iPhone SE разрабатываются на базе iOS 18.3, но это не значит, что они выйдут вместе в этом месяце. Это значит, что они выйдут до iOS 18.4, к апрелю, если всё пойдет по плану. Запасы iPhone SE 3-го поколения иссякают в большинстве магазинов, что обычно предвещает скорый выход новой модели.» Позже инсайдер Сони Диксон поделился свежими фотографиями муляжей iPhone SE (4-го поколения): А затем появляется инсайдер Evleaks, который известенвесьма достоверными утечкам, и делится информацией: называться устройство будет именно iPhone SE 4-го поколения, а ещё оно будет иметь Dynamic Island, о чём свидетельствует опубликованный документ, на котором изображение специально выкручено по яркости, чтобы это было видно: Кроме того, инсайдер поделился скриншотом документа, согласно которому новые iPad Air выйдут на чипе M3. В то же время Марк Гурман из Bloombergранее предполагал,что новинка пропустит этот чип, получив сразу M4. Кто окажется прав – узнаем уже совсем скоро. По слухам, все перечисленные новинки выйдут в марте-апреле этого года. А вот в базовом iPad 11-го поколения может быть чип A17 Pro в упрощённой модификации с 5 ядрами GPU и 8ГБ оперативной памяти – также как уже сейчас в iPad mini (A17 Pro) А вот MacBook Air на чипе M4 может получить опцию нанотекстурного покрытия для дисплея. Ещё один слухот Марка Гурмана: Apple планирует провести пресс-брифинги по PowerBeats Pro 2 через неделю или около того, чтобы рассказать про новые функции избранным медиа, которые получат наушники на обзор. Поданным из кода iOS,уже стало известно, что новые наушники будут иметь функцию мониторинга сердечного ритма во время тренировок. Наушники уже начали собирать на фабрике во Вьетнаме, а их старт продаж ожидается в феврале. А теперь о том, чего ещё придётся немного подождать: Vision Pro 2 выйдет в 2026 году, но может быть анонсирован в конце этого года Умные очки от Apple с камерами и AirPods с камерами – находятся на ранних этапах проектирования, их выпуск не состоится в ближайший год. Также задержится и выпуск домашнего умного хаба от Apple. По словам инсайдера Марка Гурмана, Apple планирует представить новый домашний центр«HomePad»в марте, но старт продаж может начаться гораздо позже. Всё потому, что система устройства (кодовое название Pebble) тесно связана с функциями App Intents в Apple Intelligence, которые появятся лишь в iOS 18.4, а значит само устройство не появится в продаже до релиза системы, то есть, не ранее апреля. Ещё один ИИ-сервис нас ждёт в ближайшем будущем:Apple близка к выпуску в этом году нового платного «виртуального ИИ-тренера по здоровью и фитнесу». По данным Bloomberg, работа над новой услугой «коучинговой службы на основе искусственного интеллекта», велась последние годы, основная её задача – укрепить огромные амбиции Apple в области здравоохранения. По проекту услуга будет иметь своё собственное приложение, ежемесячную плату и систему мотиваций пользователей. Наиболее вероятно, что этот платный сервис «Apple Coach» войдёт и в пакет услуг One. Пока неизвестно, когда он будет запущен, но судя по слухам о грядущих WATCH Ultra 3 и Series 11, а также AirPods Pro 3с новыми датчиками для здоровья– услуга может быть частью новых устройств, которые ожидаются до конца этого года. Из ближайшего, как сообщил также Гурман, стоит ждатьновый дизайн приложения Почта на macOS 15.4 и iPadOS 18.4 – в апреле. А тем временем, появился концепт дизайна приложения Камера в iOS 19 – именно так он будет выглядеть по словам блогера Джона Просера, утверждающего, что он видел рабочий прототип, оно будет в стиле visionOS. В магазинах: в Китае – новый Apple Store и специальные AirPods 4, в Майами (США) – новый магазин, в Европе – iPhone 15 refurbished, а в Азии акция для учащихся. В Китае, Гонконге и некоторых других азиатских странах стала доступна специальная версия AirPods 4 (ANC) с гравировкой символа года на зарядном футляре.Продаетсяона ограниченным тиражом по цене HK$1,499 Кроме того, на азиатских региональных сайтах Apple в честь наступающего Нового Года по восточному календарю представлены товары с символом года: чехолOtterBox Lumen Seriesдля iPhone 16 серии, внешний аккумуляторBelkin BoostCharge Proс поддержкой MagSafe, брелок для AirTag отOtterBox,органайзердля путешествий ипапкадля ноутбука от Bellroy со специальным артом, магнитный кошелек-подставка отSatechi,адаптер питания 30W с кабелем в «змеином» дизайне отAnker,а Tech21 выпустили целую серию аксессуаров EvoArt, включающую брелок дляAirTag,а также защитные чехлы дляAirPods Max,AirPods Pro 2иAirPods 4 Кроме того, Apple поделилась фирменными новогодними обоями с узором в виде змеиной чешуи для iPhone, iPad, Mac и WATCH Скачать тут  А 18 января открылся  магазин Apple 合肥万象城 (Apple MixC Hefei) – первый в китайской провинции Аньхой. Также компания анонсировала открытие нового розничного магазина Apple Miami Worldcenter в Майами (США), который распахнет двери покупателям 24 января По этому случаю также выпущены фирменные обои с логотипом магазина,скачать их можно тут Ну и мы сделали свою версию обоев с логотипом Apple Miami Worldcenter,скачать можно тут Также Apple Store Online вместе с приложением магазина официально запущены в Индии. Компания также подтвердила планы открыть ещё четыре розничных магазина на территории Индии в этом году. Акция Apple «Back to School» запущена вАвстралии,БразилиииЮжной Кореевпервые с бесплатными AirPods 4 или Pencil Pro при покупке Mac или iPad А в Европе Apple начала продавать официально восстановленные iPhone 15, iPhone 15 Pro и iPhone 15 Pro Max с дисконтом до 15%. Модели могут оставаться в наличии воФранции,Германии,ИталиииИспании Тим Кук: задонатил Трампу, получил за это вопросы, посетил инаугурацию, дал интервью в подкасте В первых числах января стало известно, что Тим Кук из личного бюджета задонатил Трампу $1 млн. Такую информацию раздобыло издание Axios, уточнив, что сумма $1 млн от главы Apple в счёт инаугурационного фонда избранного президента США Дональда Трампа, не была отправлена от компании, а лично от Тима Кука. Об отношении к пожертвованию внутри компании ничего неизвестно. Сам факт такого доната может означать, что Кук считает инаугурацию – отличной традицией, потому жертвует на это в духе единства, сообщили источники издания. Но руководитель Apple не единственный, кто так сделал: было замечено несколько донатов от представителей и сотрудников Amazon, Uber, Toyota, Ford, GM и даже OpenAI. Именно вся эта группа представителей большой технологической отрасли присутствовала на инаугурации 20 января. А тем временем, Американские сенаторы добиваются от Тима Кука пояснений из-за доната в инаугурационный фонд Трампа. Точно также, как к Тиму Куку из Apple, сенаторы обратились и к другим представителям технологической отрасли, которые внесли свои пожертвования в фонд. Многие из тех, кто получили данные требования, пока никак не отреагировали на это. «Эти пожертвования поднимают вопросы о коррупции и влиянии корпоративных денег на администрацию Трампа, Конгресс и общественность заслуживают ответов. Поэтому мы просим вас предоставить ответы на следующие вопросы до 31 января 2025 года: 1) Когда и при каких обстоятельствах ваша компания решила сделать эти взносы в инаугурационный фонд Трампа? 2) Каково ваше обоснование этих взносов? 3) Какие люди в компании также решили сделать эти пожертвования? 4) Был ли Совет проинформирован об этих планах, и если да, то дали ли они положительное согласие на это? Проинформировала ли ваша компания акционеров о планах сделать эти пожертвования? 5) Были ли у должностных лиц компании какие-либо контакты об этих пожертвованиях с членами переходной команды Трампа или другими партнерами президента Трампа? Если да, пожалуйста, перечислите все такие сообщения, включая время разговора, участников и характер любого общения.» Впрочем, об этом узнаем потом. А пока генеральный директор Apple, Тим Кук побывал на подкасте«Table Manners»с Джесси и Ленни Уэйр в Лондоне. Тим Кук о своей пенсии: «Пенсия для меня это нетрадиционное определение. Я не вижу, чтобы я просто мог быть дома, ничего не делая и не был бы чем-то озадачен. Я думаю, что я всегда захочу работать. Я работал, когда мне уже было 11 или 12 лет. Я начал работать в фастфуде \"Tasty Freeze\", когда мне было, наверное, 14-15 лет. Сначала я начинал с подкладывания бумаги, а потом перешел на заворачивание бургеров. Я зарабатывал $1,10/час – минимальная плата, законная в тот момент.» И коротко о самом интересном из интервью: — «На завтрак я беру кофе и немного хлопьев.» — «Я на самом деле утром читаю почту, в день писем приходит 500-600 штук, на какие-то я отвечаю: нашим сотрудникам и даже пользователям.» — «В некоторые дни, когда происходит что-то необычное, утренняя почта занимает гораздо больше времени, чем обычно.» — «Несмотря на то, что моя фамилия Cook (англ. cook – готовить), я лично обычно не готовлю еду. Наше корпоративное кафе \"Caffè Macs\" в Park просто невероятное. Я там обедаю и ужинаю, иногда забирая домой из \"Caffè Macs\" что-то. Обычно я хожу за рыбой.» — «Apple делает варенье из плодов фруктовых деревьев, посаженных в Park.» — «Один из любимых ресторанов в Пало-Альто – \"Ethel's Fancy\".» — «Любимым блюдом в детстве была жареная курица, а ещё я большой поклонник шоколада, особенно тёмного шоколада.» — «Если вино, то \"Kistler chardonnay\" – моя личная любовь.» Послушать интервью можно в приложенииПодкасты Apple: компания защищает данные пользователей Siri На фоне завершившегося судебного спора Apple по делу о «прослушке Siri», которое было начато ещё в 2019 году, а в декабре прошлого года закончившегося урегулированием с компенсацией без выражения претензий со стороны пострадавших, компания выступила с заявлением. В нём она утверждает, что «конфиденциальность является основополагающей частью процесса проектирования продуктов, руководствуясь принципами, которые включают в себя минимизацию собираемых данных, работу интеллекта на устройстве, прозрачность и контроль, а также надёжные средства защиты, которые работают вместе, чтобы предоставить пользователям невероятный опыт и душевное спокойствие.» «Apple никогда не использовала данные Siri для создания маркетинговых профилей, никогда не делала их доступными для рекламы и никогда никому не продавала их ни для каких целей. Мы постоянно разрабатываем технологии, чтобы сделать Siri ещё более приватной, и будем продолжать это делать.» В заявлении,опубликованном на сайте Apple,подробно рассказано как именно Siri защищает пользовательские данные Card: Goldman Sachs не исключает прекращение сотрудничества с Apple и уже известны парочка кандидатов на смену Гендиректор банка Goldman Sachs рассказал о будущем Card. Дэвид Соломон в комментарии для Reuters в ходе отчётного конференц-звонка: «У нас есть контракт с Apple на развитие партнерства до 2030 года по Card, хотя есть некоторая вероятность, что оно не будет продолжаться до конца этого срока. Card снизила рентабельность собственного капитала Goldman в прошлом году на 75-100 базисных пунктов, хотя это может улучшиться в 2025 и 2026 годах.» Тут же появились слухи о том, что компания Apple ведёт переговоры с банками Barclays и Synchrony Financial в качестве потенциальной замены Goldman Sachs для обеспечения работы Card Год новый, проблемы старые: Индонезии мало $1 млрд, в Бразилии штраф за приложение, в Великобритании суд из-за комиссии в магазине, а в США бан TikTok Бан iPhone 16 в Индонезии сохраняется, даже после предложения от Apple по инвестициям в $1 млрд и строительства фабрики. Министр промышленности Индонезии Агус Гумиванг Картасасмита, 7 января встретился с вице-президентом Apple по глобальным правительственным делам Ником Амманном, чтобы обсудить инвестиционное предложение Apple. Так стало известно, что планируемая фабрика Apple совместно с партнёрами на территории Индонезии – будет предназначена для сборки AirTag и начнёт работу в следующем году. Также обсудили инвестиции Apple в местную промышленность в размере $1 млрд. Но похоже, министру промышленности Индонезии и этого мало: «Если это будет только $1 миллиард, то этого будет недостаточно. У министерства сейчас нет оснований для выдачи местной сертификации для Apple на разрешение продавать iPhone 16, потому что планируемая фабрика на территории Индонезии не имеет прямого отношения к iPhone. Министерство будет учитывать только фабрики с компонентами для телефона.» А в Бразилии компанию Apple оштрафовали за сбор данных приложением «FaceApp». Бразильский суд постановил, что Apple (и Google) несут ответственность за распространение приложения «FaceApp», которое обвиняется в: - ненадлежащем сборе конфиденциальных данных от пользователей; - нарушении Бразильской системы гражданских прав в Интернете; - отсутствии условий использования и политики конфиденциальности на португальском языке. Apple была оштрафована на 19 миллионов бразильских реалов (около 312 млн ₽) и в дополнение к этому, решением судьи также наложено обязательство выплатить компенсацию в размере 500 бразильских реалов (~8200₽) каждому человеку в Бразилии, который загрузил и использовал «FaceApp» с июня 2020 года. Также Apple столкнулась с судебным иском на £1,5 млрд в Великобритании из-за комиссии в App Store. Против компании Apple подали иск от лица доктора Рэйчел Кента из Королевского колледжа Лондона, а также «от имени 19,6 миллионов других пользователей iPhone и iPad в Великобритании», как указано в судебной заявке. Причина судебного спора – комиссия в App Store за покупки приложений и внутреннего контента, требуемая сумма возмещения ущерба оценена в £1,5 млрд: «Apple не имеет права взимать с нас 30%, потому что мы и так платим за наши телефоны. Особенно это неуместно, когда Apple сама ограничивает наш доступ к сторонним платформам и разработчикам, которые могут предложить нам гораздо более выгодные предложения.» А самое интересное произошло в минувшие выходные: Apple выпустила заявление об удалении приложений владельца TikTok из американского App Store, но пострадали и некоторые пользователи из России. В официальном документеПоддержкикомпания Apple отметила, что обязана соблюдать законы в юрисдикциях, в которых она работает. В соответствии с «Законом о защите американцев от приложений, контролируемых иностранными компаниями», приложения, разработанные китайской компанией ByteDance Ltd. и её дочерними компаниями, включая «TikTok», «CapCut» и другие, больше не будут доступны для скачивания или обновлений в App Store для пользователей в США с 19 января 2025 года. Вот полный перечень приложений, который был удалён из американского App Store: - «TikTok» - «TikTok Studio» - «TikTok Shop Seller Center» - «CapCut» - «Lemon8» - «Hypic» - «Lark – Team Collaboration» - «Lark – Rooms Display» - «Lark Rooms Controller» - «Gauth: AI Study Companion» - «MARVEL SNAP» Компания пояснила, что если вы живете в США, используете американский аккаунт для App Store и у вас уже установлены эти приложения – они останутся на вашем устройстве, без возможности переустановки. Но вот первые 14 часов работать они всё равно не могли, поскольку при подключении к интернету, приложения выдавали своё сервисное уведомление об ограничении доступа к сервису на территории США. На данный момент приложение работает у тех, кто его установил, сервис не заблокирован в стране, однако в магазин его пока до сих пор не вернули – судя по всему, компания дожидается указа Трампа. В российском App Store приложения ByteDance и TikTok остаются в доступе. Но поскольку лента сервиса ограничена на территории России, многие использовали модифицированные версии приложения на iOS – и они тоже пострадали. Дело в том, что большинство таких приложений использовали версию с подменой региона как раз на США, включая методы регистрации, запуска и обхода ограничений со стороны TikTok через американские данные. И именно поэтому большинство аккаунтов были связаны с американской платформой, а значит также оказались «заморожены». По словам разработчиков данных модифицированных версий, обойти эту «заморозку» было никак невозможно, до тех пор, пока TikTok заблокирован в США. Сейчас все ждут указа об отсрочке запрета TikTok в США, который пообещал подписать Трамп, только в этом случае приложения будут возвращены в магазин, судя по всему. Впрочем, сервис уже работает у американских и российских пользователей с модифицированными приложениями. Премьера tv+: «Разделение» 2 сезон и масштабное промо В рамках промо-кампании к премьере второго сезона сериала tv+ компания запустила масштабные акции в том числе в оффлайне. Например, Apple организовала специальную инсталляцию на Центральном вокзале в Нью-Йорке в виде офиса Lumon, которую посетили актеры и режиссер сериала Бен Стиллер. Команда tv поделилась видеоотчётом со специальной инсталляции по сериалу «Разделение» на Центральном вокзале в Нью-Йорке: Рекламная кампания второго сезона сериала «Разделение» коснулась и розничных магазинов – Apple украсила витрины и окна Apple SoHo в Нью-Йорке специальной инсталляцией с шариками: В онлайне Apple запустиласпециальный сайтк премьере второго сезона сериала tv+«Разделение»,где можно напечатать свою фотографию на синий шарик Первая серия второго сезона с рейтингом свежести 94% на Rotten Tomatoesуже доступна к просмотруна tv+ Открывающая сцена второго сезона «Разделения» хоть и выглядит снятой одним дублем, на самом деле была снята за 5 месяцев и склеена из 10 разных отрывков с использованием во время съемки беговой дорожки, хромакея и скоростной камеры на роботизированной руке (как в рекламе«Big and Bigger»iPhone 14 Plus). Сам актер Адам Скотт признался, что при подготовке к этой сцене вдохновлялся бегом Тома Круза (на которого кстати похож по мнению многих критиков) App Store: Apple не знает, сколько он приносит дохода, а также рубрика «На этой неделе могли скачать из App Store, но не скачали» В ходе судебного разбирательства по антимонопольному делу против App Store выяснилось, что Apple не знает точную цифру чистой прибыли от магазина приложений. Кевин Парех, финансовый директор Apple: «Компания Apple не распределяет все расходы и доходы на конкретные продукты или услуги. У нас есть только общая цифра для этого сектора услуг и сервисов, но нет процента конкретно для App Store. Любая попытка распределить эти виды расходов будет включать в себя неточные и субъективные суждения.» Из российского App Store были удалены первые в этом году приложения. С начала нового года для установки на iOS из магазина перестали быть доступны: - «SmartBilet» (Почта Банк) - «Смарт Бюджет» (Совкомбанк) - «Вместе: учёт расходов» (Альфа Банк) - «Я в деле» (Альфа Бизнес) - «Эндорфин» (РНКБ) - «Бонус+» (Клуб скидок Русский Стандарт) - «С.Авто» (СберАвто) - «Цифровой Ключ» (СберПодпись) - «Мой QR» (МТС Банк и МТС Деньги) - «МТС Тревел» - «М! Клуб» (Матч! ТВ) - «СНГБ Онлайн» (Сургутнефтегазбанк) - «СНГБ Онлайн Бизнес» - «СНГБ Бизнес QR» - «СНГБ Инвестиции» - «КЛЮЧ: СЕРВИС ПАРОЛЕЙ» (Газпром Ключ) - «АЗС ОПТИ» - «ОПТИ24 – топливо для бизнеса» - «АЗС «Газпромнефть» Последнее было интересно тем, что могло пригодиться не только для владельцев автомобилей, но и для тех, кто хотел вернуть быстрый вызов Wallet на iPhone с экрана блокировки по двойному нажатию кнопки включения, если банковские карты для Pay перестали работать или их вовсе не было. Всё работало благодаря тому, что для бонусной программы заправок добавлялась карточка, которая работала через NFC-метод, прямо как банковская карта – это и позволяло открыть Wallet с заблокированного экрана (для использования скидочных карт и прочего). Теперь рекомендуется не удалять данные приложения – установить заново из App Store их уже не получится! Для некоторых сервисов сохраняется доступ в веб-версиях. На этом наш первый в этом году выпуск в обновлённом формате подошёл к концу. До встречи через неделю со свежим дайджестом интересных и важных новостей из мира Apple! А ещё больше можно увидеть в наших соцсетях: 𝕏 (бывший Twitter)/Telegram/VK "
  },
  {
    "article_id": "https://habr.com/ru/companies/timeweb/articles/874512/",
    "title": "Как я случайно купил последний в мире прототип игрового смартфона и доработал его до ума",
    "category": "Мобильные технологии",
    "tags": "bodyawm_ништяки, гаджеты, телефоны, смартфоны, mops, игры, игровые консоли, эмуляторы, прототипы, денди",
    "text": "В своём блоге я время от времени рассказываю о различных устройствах, предназначенных для разработчиков портативных гаджетов — девкитах, инженерных прототипах и т.п. Недавно я ковырял китайские барахолки в поисках интересных девайсов для будущих статей и мой взор привлёк весьма интересный игровой смартфон на очень редком процессоре всего за 1.000 рублей. Изучив информацию в сети, я узнал что устройство было разработано небольшим стартапом для локального рынка Китая, а когда телефон приехал ко мне... оказалось, что это не серийное устройство, а редчайший инженерный прототип, о котором в сети нет никакой информации. В сегодняшней статье мы с вами узнаем: почему этот прототип вероятно последний в мире, что из себя представляет смартфон и что у него «под капотом», а также пофиксим некоторые баги в прошивке, которые не успели поправить разработчики устройства. Интересно? Тогда жду вас под катом! ❯ Предисловие Недавно я писал статью о том, как купил китайский игровой телефон аж с двумя процессорами «под капотом», где инженеры решили установить в устройство полноценный аппаратный клон денди вместо заморочек с быстрой эмуляцией! В ней мы с вами вспомнили историю появления мобильного гейминга, вкратце затронули Nokia N-Gage и его предка 3300, а также поговорили о весьма интересных разработках китайских «полуподвальных» производств. Типичные характеристики кнопочного телефона нулевых были весьма небольшими: обычно это был ARMv5 процессор с частотой 100-200МГц, 4-32Мб оперативной памяти, ~32-64Мб постоянной памяти и TN-матрица с разрешением от 128x160 до 240x320. Однако даже при таких небольших ресурсах, разработчики умудрялись писать быстрые программные растеризаторы 3D-графики и вмещать в небольшой телефончик целые трёхмерные RPG: К моменту выхода Android и iOS на мобильный рынок, в смартфонах начали массово появляться 3D-ускорители, которые позволяли достичь невиданного доколе уровня графики, достигающий практически уровня PSP! Например, в первый iPhone предусмотрительно установили чипсет Samsung с GPU PowerVR MBX Lite, а в первый серийный Android-смартфон — HTC Dream, установили процессор Qualcomm MSM8201A с видеочипом ATI Imageon Z430 (aka Adreno 200). После этого, мобильный гейминг изменился навсегда: начали появляться клоны Need For Speed, GTA и других игр, которые выглядели весьма достойно и работали с стабильным FPS. А со временем, на смартфонах появились и полноценные порты! В 2011 году, Шведы и Японцы из Sony Ericsson быстро смекнули, что на базе современного железа можно сделать действительно продвинутый смартфон, который будет совмещать в себе функции телефона и игровой консоли. Так, 1 апреля появился на свет легендарный Xperia Play с уникальным механизмом слайдера, позаимствованным с PSP Go и весьма интересными сенсорными стиками. Однако у Плея был небольшой недостаток: аппарат стоил довольно дорого, но при этом его железо было почти идентично другим среднячкам, таким как Xperia Arc или Xperia Neo. По производительности смартфон несколько уступал флагманскому Galaxy S II и топовому Galaxy Note, а поскольку прогресс в смартфонах тогда шёл семимильными шагами, со временем некоторым пользователям начало не хватать его производительности. В том же 2011 году, некая китайская компания Bejing Wenhe TImes Technology, представляющая из себя судя по всему стартап, показала своё собственное видение пусть и не очень мощного, но зато недорого игрового смартфона на Android, дав ему имя MOPS Shadow T800. У компании были Наполенововские планы: был развернут сайт, форум, разработан магазин приложений и подписаны соглашения с такими компаниями, как например Gameloft, благодаря чему T800 поставлялся с игрой Assassin's Creed Altair Chronicles. Помимо весьма интересного и эргономичного внешнего вида с аналоговым стиком и четырьма игровыми кнопками, смартфон был интересен и своим железом: «под капотом» трудился необычный процессор 2009 года — Marvell PXA 920 с видеоускорителем Vivante GC600, в устройстве было 512Мб ОЗУ, 256Мб встроенной памяти и TN-дисплей с разрешением 480x800. Примерно к 2012 году, MOPS начала разработку второй версии своего игрового смартфона — T810, однако в 2013 году, сайт компании по каким-то причинам оказался закрыт, скорее всего произошло это из-за банкротства, за которым обычно следует утилизация активов компании — куда вошли и прототипы телефонов, как например тот, что сейчас находится у меня в руках! По какой-то счастливой случайности, смартфон уцелел за все эти годы и отправился к некоему продавцу полурабочими устройствами в Китае. В описании продавец указал, что телефон «тормозит», работает на «старой версии Android» и у него «не функционирует аналоговый стик», а цена стояла всего-лишь 70 юаней (~1.000 рублей). Учитывая что я люблю копаться и ремонтировать ретро-гаджеты — я его сразу же купил. Но я не знал одного подвоха... Поскольку смартфон я покупал на китайском аналоге Авито — Сяньюй, для перевозки мне нужен был посредник с загранпаспортом. В качестве оного выступил мой подписчик Роман, за что ему огромное спасибо, а привезти девайсы в Россию в целостности и сохранности мне помог сервис самостоятельных покупок в КитаеYouCanBuy. А также хотелось бы сказать спасибо подписчику Андрею, который занял мне кругленькую сумму — именно благодаря этим людям, я смог подготовить данную статью и получить крутейший девайс в коллекцию :) ❯ Нюанс Когда смартфон приехал ко мне, я сразу же его включил, проверил и понял что устройство, в целом, полностью работает за исключением аналогового стика. При этом изначально я не знал где стик должен работать, а где нет: в те годы, в Android толком не было какого-либо соглашения как должны обрабатываться кнопки геймпада, поэтому, например, стиком Xperia Play нельзя было управлять в меню устройства. Меня сразу же смутило то, что кнопка Game, открывающая магазин с играми, вызывает приложение камеры. Ради теста, я решил написать маленькое приложение, которое перехватывает все события ввода в системе и выводит их на экран. Игровые кнопки работали нормально, как и все остальные аппаратные элементы управления, но стик никак не отзывался — ни на Generic motion event, ни на событие трекболла. Далее я проверил файл keylayout, который связывает скан-коды из драйвера ввода Linux и подсистему ввода, и обнаружил что стик там описан как «стрелочки» — то есть система о нём как-бы знает. Я подумал что проблема в аппаратной неисправности устройства и решил его разобрать — благо делается это несложно. Процесс разборки похож на Nokia N8: откручиваем 6 винтов под задней крышкой, снимаем заглушки с верхней и нижней части обратной стороны устройства и откручиваем несколько винтиков и там. Затем заднюю часть корпуса можно снять и перед нами открывается вид на материнскую плату смартфона: Когда я снял пластиковую заглушку и вытащил стик, я почувствовал дежавю — ведь это джойстик от PSP 3000! Да, да, сюда действительно решили установить стик с портативной консоли от Sony! Меня немного смутил шлейф джойстика, поэтому я его переподключил — но результата не было, он всё также не работал... С этого момента, у меня начали появляться первые догадки о том, что это не обычный серийный смартфон. Я решил проверить — подключен ли джойстик вообще к чему либо и решил замерить сопротивления все пинов коннектора относительно массы. Обычно аналоговые джойстики подключаются либо к ADC (аналогово-цифровой преобразователь) процессора, либо к внешнему ADC, которые подключен к процессору через шину общего назначения типа i2c или SPI. Пины явно были куда-то подключены — но куда я определить не смог. Я решил что с устройством скорее всего всё нормально и секрет кроется где-то в программной части устройства. Далее я решил проверить, какие драйверы устройств вообще загружены в системе. Для этого я зашёл в adb shell и проверил все event-устройства в директории /dev/input/ — всё было с виду нормально, однако ни одно event-устройство не откликалось на джойстик. Я решил проверить файл build.prop с конфигурацией системы и пазл в моей голове начал складываться... Во первых, я обратил внимание на то, что прошивка собрана с тестовыми-ключами. Это дебаг-сборка, предназначенная для разработчиков в процессе отладки устройства, которое ещё пока только находится в разработке, серийные устройства с тестовыми-ключами встречаются довольно редко. Помимо этого, нормально работала команда adb root, благодаря которой я получил рут-права. Во вторых, я обратил внимание на то, что MicroSD-флэшка не монтируется в системе, а если её вытащить — пишет ошибку, чего быть не должно. Однако в mount, флэшка числилась как примонтированная, просто в некорректную директорию. Благодаря наличию рута я смог правильно перемонтировать флэшку и заставить её работать. В третьих, я решил проверить IMEI-смартфона, который числился как «352273017386340» — это стандартная заглушка на китайских телефонах если аппарату не ещё выдали пул IMEI, или он «подпольный», при этом сам IMEI принадлежит Nokia 7610. Кроме этого, под крышкой серийного T800 есть информация об IMEI, а здесь её нет — как и нет никаких следов клея. И в четвёртых, на сайте устройства нет никакого упоминания T810 — ни на форуме, ни на самом сайте, а дата сборки прошивки (октябрь 2012) довольно близка к дате закрытия сайта в 2013 году. По итогу, мы можем сделать предположение, что этот смартфон — ранний инженерный прототип, который просто не успели доделать по причине вероятного банкротства компании. То есть драйвер аналогового стика был просто не готов и не включен в текущий билд прошивки. А поскольку в небольших компаниях с маленькими R&D ранних прототипов может быть менее 10-20 штук, то нетрудно догадаться, что если большинство прототипов ушли в утилизацию, этот смартфон вполне может оказаться последним прототипом в мире... Ну что-ж, может аналоговый джойстик у нас и не работает, но с кнопками все впорядке — так что предлагаю посмотреть, на что был бы способен такой смартфон, если бы он в свое время вышел в серийное производство! ❯ Смотрим поближе После включения нас встречает самый обычный Android 4.0.3, без каких либо кастомных оболочек. Поскольку это прототип, набор приложений здесь минимальный — установлены только стандартные Android-приложения. Поскольку смартфон был предназначен для китайского рынка, Google Play здесь соответственно нет. Несмотря на то, что Android 4.0.3 уже почти не поддерживается никакими приложениями, всё равно находятся энтузиасты по типу меня, которые пилят полезные приложения дабы продлить жизнь любимой версии операционной системы. Вот я в прошлом году запилил клиент Telegram на Android 1.5, запустив его на первом серийном Android-смартфоне в мире и на одном из самых маленьких QWERTY-слайдеров: Давайте же сначала узнаем характеристики устройства. Заходим в CPU-Z и видим, что наш смартфон построен на базе уже упомянутого чипсета Spreadtrum SC8810, который имеет 1 ядро Cortex-A5 с частотой 1ГГц и GPU Mali 300. Объём ОЗУ здесь 512Мб, что в то время было нормой для бюджетных и среднебюджетных устройств и 256Мб постоянной памяти, чего категорически не хватало и смартфоном нельзя было полноценно пользоваться без MicroSD флэш-накопителя! В качестве дисплея здесь используется всё та же матрица как и в прошлой модели — TN, 480x800. В целом, типичные характеристики бюджетника конца 2012 года. Давайте-же протестируем игровой потенциал нашего смартфона, ведь он проектировал именно как аппарат для геймеров! В тестах будут участвовать эмуляторы игровых консолей, а также нативные Android-игры и первым делом мы начнём с эмулятора NES. С автопропуском кадров, игры идут в 50-60 «FPS», однако если отключить пропуск кадров — мы получаем несколько хрипящий звук. Но в целом, всё равно результат неплохой и играть можно: Однако NES — детские игрушки по сравнению с эмуляцией Sega Mega Drive. И вот здесь уже смартфону чуточку потяжелее, но с автопропуском кадров он всё так же продолжает выдавать 60 кадров в секунду. Эх, вот если бы здесь стик работал... был бы просто идеальный смартфон для ретро-гейминга! Тест нативных игр начинаем с лёгкой в графической части игрушки Speedx3D, в которую я часто залипал будучи школьником... и что интересно — на смартфоне с точно таким-же чипсетом! Работает она здесь отлично, без каких-либо лагов и фризов, однако она неплохо работала даже на Symbian-смартфонах без видеоускорителя! Далее запускаем Temple Run 2, которая является, в некоторой степени, классикой мобильных игр. Благодаря не очень сложному рендереру, игра идёт в стабильных 25-30 кадров, однако кнопками управлять нельзя — только гироскопом, который тоже здесь работает нормально. Переходим к AC Altair Chronicles. И здесь на первый взгляд у нас всё нормально, однако после запуска уровня и при попытке куда-то пойти, игра крашится без фикса. Вероятно кнопки можно заставить работать, если подсунуть в build.prop модель Xperia Play. Все тесты на практике вы можете посмотреть в моем видео ниже: ❯ Заключение Вот такой интересный смартфончик мне удалось купить всего за 1.000 рублей. Не каждый день видишь прототипы смартфонов, тем более игровых, а тут ещё и настолько редких! В целом, сама концепция игрового Android-смартфона очень интересная и на локальном рынке Китая были весьма занимательные серийные устройства, однако у MOPS'а была своя особенная изюминка — ведь у него была эргономика обычного смартфона: Надеюсь вам было интересно и сегодняшняя статья вам понравилась! Пишите своё мнение в комментариях: нужны ли игровые телефоны в 2025 году? Было ли у вас что-то подобное? Также если вам интересна тематика моддинга, программирования и ремонта ретро-гаджетов, подписывайтесь на мой Telegram-канал каналКлуб фанатов балдежа, куда я публикую бэкстейджи статей и видео, ссылки на новые статьи, немного щитпоста и время от времени интересные длинные посты. Если вы хотели бы помочь мне материально, то это можно сделатьв моём Бусти, а ещё я каждую неделю публикую видео на своёмYouTube-канале. Очень важно! Разыскиваются девайсы для будущих статей! Друзья! Для подготовки статей с разработкой самопальных игрушек под необычные устройства, объявляется розыск телефонов и консолей! В 2000-х годах, китайцы часто делали дешевые телефоны с игровым уклоном — обычно у них было подобие геймпада (джойстика) или хотя бы две кнопки с верхней части устройства, выполняющие функцию A/B, а также предустановлены эмуляторы NES/Sega. Фишка в том, что на таких телефонах можно выполнять нативный код и портировать на них новые эмуляторы, чем я и хочу заняться и написать об этом подробную статью и записать видео! Если у вас есть телефон подобного формата и вы готовы его задонатить или продать, пожалуйста напишите мне в Telegram (@monobogdan) или в комментарии. Также интересуют смартфоны-консоли на Android (на рынке РФ точно была Func Much-01), там будет контент чуточку другого формата :) А также я ищу старые (2010-2014) подделки на брендовые смартфоны Samsung, Apple и т. п. Они зачастую работают на весьма интересных чипсетах и поддаются хорошему моддингу, парочку статей уже вышло, но у меня ещё есть идеи по их моддингу! Также может у кого-то остались самые первые смартфоны Xiaomi (серии Mi), Meizu (ещё на Exynos) или телефоны Motorola на Linux (например, EM30, RAZR V8, ROKR Z6, ROKR E2, ROKR E5, ZINE ZN5 и т.п, о них я хотел бы подготовить специальную статью и видео т. к. на самом деле они работали на очень мощных для своих лет процессорах, поддавались серьезному моддингу и были способны запустить даже Quake!). Всем большое спасибо за донаты! А ещё я держу все свои мобилы в одной корзине при себе (в смысле, все проекты у одного облачного провайдера) — Timeweb. Потому нагло рекомендую то, чем пользуюсь сам —вэлкам: "
  },
  {
    "article_id": "https://habr.com/ru/companies/dihouse/articles/872390/",
    "title": "Обзор CMF Phone 1: смартфон-конструктор под настроение",
    "category": "Мобильные технологии",
    "tags": "смартфоны, cmf phone 1, cmf by nothing, cmf, мобильные устройства, мобильный телефон, гаджеты, гаджеты и девайсы",
    "text": "На примере CMF Phone 1 можно показать, как молодой бренд приходит на устоявшийся рынок и наводит свои правила. Nothing с суббрендом CMF создала смартфон с оригинальным дизайном, напомнив о прошедших временах, когда сменные панельки в телефонах менялись так же легко, как ремешки на часах. Кроме того у него хорошая начинка, яркий экран с насыщенными цветами, а ёмкая батарея позволяет забыть о зарядке на день или два. Устали от однообразия и хочется экспериментов? Вот одно из самых интересных устройств года в своей категории. Корпус CMF Phone 1 выполнен из матового поликарбоната. Материал приятный на ощупь, при этом он не царапается и не скользит, покрытие приятное и практичное. Для комфортного использования одной рукой смартфон великоват, но в целом габариты в рамках нормы. У него размеры 164×77×8 мм и вес 197 граммов. У смартфона плоские грани, получился минималистичный дизайн без излишеств. Смотрится аккуратно, но без видимой явной экономии. На корпусе выделяется металлический блок камер овальной формы. Все остальные детали можно найти на привычных местах: на правом боку кнопки, а на нижнем торце гибридный слот SIM+microSD. Скрепка для извлечения SIM-карты тоже заслуживает упоминания: её дизайн подчёркивает общее внимание к деталям. У смартфона нет 3,5 мм разъёма, а звучание единственного динамика среднего качества. Смотреть фильмы лучше в наушниках, но мобильный кинотеатр никто и не обещал. Самая интересная особенность CMF Phone 1 — это сменные задние крышки. Доступно три варианта: зелёный с матовым покрытием, оранжевый с текстурой под кожу и классический чёрный. Замена крышки занимает всего минуту, хотя процесс требует небольшой подготовки: нужно открутить четыре винта. Для этого в комплекте прилагается отвёртка. Под крышкой находится аккумулятор, доступ к нему прикрыт пломбами. Формально заменить батарею можно, но скорее всего это приведёт к потере гарантии. У смартфона оптический сканер отпечатков пальцев, расположенный в нижней части дисплея. Он работает быстро и точно, без ложных срабатываний. Модульность помогает заменить дизайн под настроение, а также использовать аксессуары. CMF предлагает интересный набор дополнительных предметов. Для хранения пластиковых карт можно приобрести картхолдер с крышкой, он крепится к задней панели. Ещё есть подставка, незаменимая вещь для родителей с детьми. Она решает проблему с упором, когда не на что опереть смартфон, когда в поездке или в кафе пытаешься развлечь ребёнка. Более того, чертежи крышек и креплений находятся в открытом доступе. Поэтому изобретайте собственные аксессуары, если пользуетесь 3D-принтером. Теперь перейдём к характеристикам AMOLED-дисплея: диагональ составляет привычные в этом классе 6,67-дюймова, разрешение 2400×1080 пикселей. Заявлена поддержка 120 Гц, а пиковая яркость разгоняется до 2000 нит. Получить максимальную яркость можно лишь на HDR-контенте на отдельных участках дисплея. А в обычном режиме получаем около 700 нит, что не очень много для солнечных дней. Минимальная яркость составляет 4 нита. Это удобно для использования в тёмное время суток, когда дисплей не бьёт подсветкой по глазам. Экран поддерживает 8-битный цвет и два цветовых профиля: стандартный (sRGB) и насыщенный (DCI-P3). Картинка выглядит яркой и сочной, с естественными оттенками. Углы обзора отличные, без заметных искажений цветов. Частота ШИМ составляет 960 Гц, что могут почувствовать люди с высокой чувствительностью к мерцанию. Частота обновления экрана регулируется в диапазоне 60–120 Гц. В режиме 120 Гц дисплей работает на максимальной частоте только во время использования, а в состоянии покоя автоматически снижает её до 60 Гц. У смартфона 8-ядерный 4-нанометровый чипсет MediaTek Dimensity 7300 5G. У него 8 ГБ оперативной памяти типа LPDDR4X, а встроенной 128 или 256 ГБ типа UFS 2.2. С синтетическими тестами устройство справляется отлично, показатели на уровне нормы, результаты на скриншотах. Система охлаждения эффективная, в стресс-тестах смартфон нет троттлинга и перегрева, падение в пределах 10%. Система работает стабильно без микролагов и тормозов. Можно и поиграть, но требовательные штуки вроде Genshin Impact пойдут на минимальных настройках, а Call of Duty: Mobile или PUBG тоже запустятся с простыми параметрами. Срок поддержки устройства следующий: ожидается два  крупных обновления Android, а в течение трёх лет будут приходить патчи безопасности. Иными словами, устройство отлично подойдёт для обычного использования без хардкорных игр. Но из принципиальных моментов отмечу, что CMF Phone 1 не получил NFC для бесконтактной оплаты. Смартфон работает под управлением Android 14, поверх которого установлена система Nothing OS 2.6. Этот интерфейс хорошо знаком по смартфонам Nothing, но в данном случае получаем ещё больше возможностей кастомизации, сохраняя при этом минимализм и чистоту системы. Приятно сказать, что смартфон поставляется без лишнего предустановленного ПО, за исключением базового набора Google-приложений (звонилка, галерея, мессенджеры). Не придётся удалять ненужные программы, освобождая место в памяти системы. Одной из интересных функций является возможность создавать уникальные обои с помощью генеративного искусственного интеллекта. Создавайте собственные картинки с оригинальным стилем. Кроме того, система предлагает интеграцию с ChatGPT: владельцы наушников Nothing или CMF могут общаться с ботом прямо через гарнитуру. Это полезно для быстрого получения ответов на вопросы или выполнения задач. Но с учётом ограничений в России, функция скорее интересна лишь в теории. При первом запуске после распаковки смартфон предлагает выбрать между базовым Android и кастомным Nothing UI. Второй вариант интереснее как раз в том случае, когда хочется попробовать что-то новое и свежее. Глобальная кастомизация: оформляете главный экран и экран блокировки под свой стиль. Например, в системе есть возможность активировать минималистичный тёмный режим, который смотрится современно и аккуратно. Нестандартный шрифт: фирменный шрифт nDOT 57 cyrillic создаёт впечатление, что текст состоит из множества соединённых кругов. Это не только выглядит оригинально, но и подчёркивает общую философию дизайна устройства. Даже привычные элементы управления тоже отличаются: шторка уведомлений получила крупные плитки и необычный ползунок регулировки громкости. У CMF Phone 1 50-мегапиксельный сенсор Sony IMX882 с диафрагмой f/1.8 и электронной стабилизацией. При хорошем освещении камера выдаёт снимки с естественными цветами, достойным динамическим диапазоном и приличной контрастностью. Однако детализация страдает: при увеличении заметна нехватка резкости. Это особенно ощущается в портретном режиме, где применяется повышенная резкость, которая иногда выглядит излишне искусственно. В условиях недостаточного освещения автоматически включается ночной режим. Он позволяет получить снимки приемлемого качества, хотя детализация остаётся на среднем уровне. Дополнительный модуль на 2 Мп вряд ли можно назвать полезным. Формально он используется для портретных снимков, но скорее он тут для проформы. Фронтальная камера получила 16-мегапиксельный сенсор GalaxyCore GC16B3 с диафрагмой f/2.0. Снимки с неё выглядят нормально, но ничего выдающегося ждать не стоит. Цвета передаются корректно, а детализация остаётся на среднем уровне. Для соцсетей хватит, но на большее рассчитывать не стоит. Фронтальная камера умеет записывать видео в Full HD при 60 FPS, но качество роликов ниже среднего: динамический диапазон узкий, детализация слабая. Основная камера поддерживает запись в 4K при 30 FPS и Full HD при 60 FPS. Есть режим замедленной съёмки с частотой 120 кадров в секунду, а также ночной режим, где видео записывается в 1080p при 30 FPS. Временами даже при дневной съёмке заметны микровибрации, а ночью шумы становятся более выраженными. CMF Phone 1 получил аккумулятор ёмкостью 5000 мАч. Это типичное решение для современного смартфона, при умеренной нагрузке он работает до двух дней. При активном использовании зарядки уверенно хватает на весь день, когда в сумме получается около 7 часов экранного времени. Например, час просмотра YouTube на 50% яркости расходует всего 4% заряда, а в играх, таких как Genshin Impact, за час уходит около 13%. В тесте PCMark устройство показало почти 11 часов работы экрана на максимальной яркости, отличный результат. Смартфон поддерживает зарядку мощностью до 33 Вт, но адаптера в комплекте нет. В реальных условиях зарядка занимает около 1 часа 20 минут: за первые 30 минут аккумулятор наполняется до 50%. Смартфон также поддерживает обратную зарядку мощностью 5 Вт, что может быть полезным для зарядки аксессуаров вроде наушников. CMF Phone 1 — это устройство для тех, кто ищет что-то необычное в мире бюджетных смартфонов. Его модульный дизайн и возможность кастомизации делают его ярким и запоминающимся, а сменные крышки и аксессуары превращают смартфон в настоящую площадку для самовыражения. Если Nothing продолжит развивать идею модульности, улучшит камеры, добавит NFC и стереодинамики, то её следующий смартфон может стать настоящим хитом. Но даже сейчас CMF Phone 1 заслуживает внимания благодаря своему свежему подходу и оригинальности. Это смартфон, который точно не останется незамеченным. "
  }
]