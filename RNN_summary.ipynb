{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDFhyOmQan0v"
      },
      "source": [
        "# Text generation with deep learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TslNQEFlan0z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m>>> [2025-02-17 | 04:08:50] logger:70 - WARNING - Log file wasn't created due to file_log=False\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging\n",
        "import optuna\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "from src.logger import setup_logger\n",
        "\n",
        "setup_logger(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uB9dp0eman1B"
      },
      "outputs": [],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.model_type = model.lower()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        rnn_class = nn.GRU if self.model_type == \"gru\" else nn.LSTM\n",
        "        self.rnn = rnn_class(\n",
        "            hidden_size, hidden_size, n_layers,\n",
        "            dropout=dropout if n_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        batch_size = input.size(0)\n",
        "        encoded = self.encoder(input)\n",
        "        output, hidden = self.rnn(encoded, hidden)\n",
        "        output = self.dropout(output)\n",
        "        decoded = self.decoder(output.contiguous().view(-1, self.hidden_size))\n",
        "        return decoded.view(batch_size, -1, self.output_size), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        if self.model_type == \"lstm\":\n",
        "            return (\n",
        "                torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n",
        "            )\n",
        "        else:\n",
        "            return torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VZf7PBqGan1F"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, chunk_len=200, stride=50):\n",
        "        self.text = text\n",
        "        self.chunk_len = chunk_len\n",
        "        self.stride = stride\n",
        "        self.unique_chars = sorted(set(text))\n",
        "        self.char_to_idx = {c: i for i, c in enumerate(self.unique_chars)}\n",
        "        self.idx_to_char = {i: c for i, c in enumerate(self.unique_chars)}\n",
        "        self.data = self._process_text()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _process_text(self):\n",
        "        sequences = []\n",
        "        for i in range(0, len(self.text) - self.chunk_len, self.stride):\n",
        "            chunk = self.text[i:i+self.chunk_len+1]\n",
        "            sequences.append(chunk)\n",
        "        return sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.data[idx]\n",
        "        input_seq = [self.char_to_idx[c] for c in chunk[:-1]]\n",
        "        target_seq = [self.char_to_idx[c] for c in chunk[1:]]\n",
        "        return torch.LongTensor(input_seq), torch.LongTensor(target_seq)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.unique_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sample(model, dataset, device, prompt=\"The\", max_length=500, temperature=1.0, top_k=10, top_p=0.9):\n",
        "    model.eval()\n",
        "    generated = []\n",
        "    input_seq = torch.LongTensor([dataset.char_to_idx[c] for c in prompt]).unsqueeze(0).to(device)  # (batch=1, seq_len)\n",
        "    hidden = model.init_hidden(1, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if len(prompt) > 0:\n",
        "            _, hidden = model(input_seq, hidden)\n",
        "\n",
        "        input_seq = input_seq[:, -1].unsqueeze(1)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            outputs, hidden = model(input_seq, hidden)\n",
        "            logits = outputs[:, -1, :] / temperature  # Берем последний выходной токен\n",
        "\n",
        "            if top_k > 0:\n",
        "                logits = _top_k_filter(logits, top_k)\n",
        "            if top_p > 0.0:\n",
        "                logits = _top_p_filter(logits, top_p)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            generated.append(next_token.item())\n",
        "            input_seq = next_token\n",
        "\n",
        "    generated_str = prompt + ''.join([dataset.idx_to_char[idx] for idx in generated])\n",
        "    print(\"\\nGenerated text:\")\n",
        "    print(generated_str)\n",
        "    return generated_str\n",
        "\n",
        "def _top_k_filter(logits, k):\n",
        "    values, _ = torch.topk(logits, k)\n",
        "    min_values = values[:, -1].unsqueeze(1)\n",
        "    return torch.where(logits < min_values, torch.ones_like(logits)*-float('inf'), logits)\n",
        "\n",
        "def _top_p_filter(logits, p):\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "    indices_to_remove = sorted_indices_to_remove.scatter(\n",
        "        1, sorted_indices, sorted_indices_to_remove\n",
        "    )\n",
        "    return logits.masked_fill(indices_to_remove, -float('inf'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CharRNN(10, 10, 10).hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def evaluate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            hidden = model.init_hidden(inputs.size(0), device)\n",
        "            outputs, _ = model(inputs, hidden)\n",
        "            loss = criterion(outputs.transpose(1, 2), targets)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "    return total_loss / len(val_loader.dataset)\n",
        "\n",
        "def train_model(model, dataset, epochs=50, batch_size=32, lr=3e-4, trial=None):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': model.encoder.parameters(), 'weight_decay': 0.01},\n",
        "        {'params': model.rnn.parameters()},\n",
        "        {'params': model.decoder.parameters(), 'weight_decay': 0.01}\n",
        "    ], lr=lr, fused=True)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        total_steps=epochs * len(loader),\n",
        "        pct_start=0.1\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    scaler = torch.amp.GradScaler()\n",
        "\n",
        "    writer = SummaryWriter(\n",
        "        log_dir=f\"runs/LR_{lr:.6f}-model_type_{model.model_type}-hidden_size_{model.hidden_size}-n_layers_{model.n_layers}-batch_size_{batch_size}\"\n",
        "        )\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    grad_norms = []\n",
        "\n",
        "    max_grad_norm = 1.0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        progress = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(progress):\n",
        "            current_batch_size = inputs.size(0)\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                hidden = model.init_hidden(current_batch_size, device)\n",
        "                outputs, _ = model(inputs, hidden)\n",
        "                loss = criterion(outputs.transpose(1, 2), targets)\n",
        "                l2_reg = sum(p.norm(2) for p in model.parameters())\n",
        "                loss += 0.001 * l2_reg\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                model.parameters(),\n",
        "                max_norm=max_grad_norm,\n",
        "                norm_type=2,\n",
        "                error_if_nonfinite=False\n",
        "            )\n",
        "\n",
        "            grad_norms.append(grad_norm.item())\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'grad': f\"{grad_norm:.2f}\",\n",
        "                'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
        "            })\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                writer.add_scalar('Train/Loss', loss.item(), epoch*len(loader)+batch_idx)\n",
        "                writer.add_scalar('Train/Grad_Norm', grad_norm.item(), epoch*len(loader)+batch_idx)\n",
        "                writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch*len(loader)+batch_idx)\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        writer.add_scalar('Epoch/Loss', avg_loss, epoch)\n",
        "\n",
        "        logging.info(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "                    f\"Loss: {avg_loss:.4f} - \"\n",
        "                    f\"Grad Norm: {grad_norm:.2f} - \"\n",
        "                    f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "        # Optuna integration\n",
        "        if trial is not None:\n",
        "            trial.report(avg_loss, epoch)\n",
        "            if trial.should_prune():\n",
        "                raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        if avg_loss < best_loss and not torch.isnan(torch.tensor(avg_loss)):\n",
        "            best_loss = avg_loss\n",
        "            torch.save({\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'loss': avg_loss\n",
        "            }, \"best_model.pth\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    writer.close()\n",
        "    return best_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        'model_type': trial.suggest_categorical('model_type', ['lstm', 'gru']),\n",
        "        'hidden_size': trial.suggest_int('hidden_size', 128, 512),\n",
        "        'n_layers': trial.suggest_int('n_layers', 1, 4),\n",
        "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
        "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
        "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256])\n",
        "    }\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv('data/arxiv.csv')\n",
        "    text = ' '.join(df['summary'].dropna().values)\n",
        "    dataset = TextDataset(text, chunk_len=250, stride=100)\n",
        "\n",
        "    # Create model\n",
        "    model = CharRNN(\n",
        "        input_size=dataset.vocab_size,\n",
        "        hidden_size=params['hidden_size'],\n",
        "        output_size=dataset.vocab_size,\n",
        "        model=params['model_type'],\n",
        "        n_layers=params['n_layers'],\n",
        "        dropout=params['dropout']\n",
        "    )\n",
        "\n",
        "    best_loss = train_model(\n",
        "        model,\n",
        "        dataset,\n",
        "        epochs=10,\n",
        "        batch_size=params['batch_size'],\n",
        "        lr=params['lr'],\n",
        "        trial=trial\n",
        "    )\n",
        "\n",
        "    return best_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXnbcOYT36sN"
      },
      "outputs": [],
      "source": [
        "find_best = False\n",
        "\n",
        "if find_best:\n",
        "\tstudy = optuna.create_study(direction='minimize')\n",
        "\tstudy.optimize(objective, n_trials=50, timeout=3600)\n",
        "\n",
        "\tprint(\"Best trial:\")\n",
        "\ttrial = study.best_trial\n",
        "\n",
        "\tbest_trial = study.best_trial\n",
        "\tpd.DataFrame([best_trial.params])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_best_model(params):\n",
        "\tdf = pd.read_csv('data/arxiv.csv')\n",
        "\ttext = ' '.join(df['summary'].dropna().values)\n",
        "\tdataset = TextDataset(text, chunk_len=250, stride=100)\n",
        "\n",
        "\tmodel = CharRNN(\n",
        "\t\tinput_size=dataset.vocab_size,\n",
        "\t\thidden_size=params['hidden_size'],\n",
        "\t\toutput_size=dataset.vocab_size,\n",
        "\t\tmodel=params['model_type'],\n",
        "\t\tn_layers=params['n_layers'],\n",
        "\t\tdropout=params['dropout']\n",
        "\t)\n",
        "\n",
        "\tbest_loss = train_model(\n",
        "\t\tmodel,\n",
        "\t\tdataset,\n",
        "\t\tepochs=params['epochs'],\n",
        "\t\tbatch_size=params['batch_size'],\n",
        "\t\tlr=params['lr'],\n",
        "\t)\n",
        "\n",
        "\treturn model, best_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8d061687aee44ccb20590f91376662e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:10:16] 2888956552:99 - INFO - Epoch 1/25 - Loss: 2.2961 - Grad Norm: 0.19 - LR: 1.28e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33ed31626b7742f7b65fc6d830820486",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:11:39] 2888956552:99 - INFO - Epoch 2/25 - Loss: 1.9630 - Grad Norm: 0.18 - LR: 3.12e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19a8c70a95734dfebf69f31e95dc048f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:13:01] 2888956552:99 - INFO - Epoch 3/25 - Loss: 1.9457 - Grad Norm: 0.12 - LR: 3.43e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8200761c048412b8c693c61e59d53ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 4:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:14:23] 2888956552:99 - INFO - Epoch 4/25 - Loss: 1.9325 - Grad Norm: 0.09 - LR: 3.40e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "499272d021044082b6b5ee0f5a538331",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 5:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:15:45] 2888956552:99 - INFO - Epoch 5/25 - Loss: 1.9237 - Grad Norm: 0.14 - LR: 3.33e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4640e3a68fa9457b95f289466f78961c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 6:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:17:07] 2888956552:99 - INFO - Epoch 6/25 - Loss: 1.9172 - Grad Norm: 0.10 - LR: 3.24e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bac6fb9cb0fa422bafdcf978fd01543b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 7:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:18:28] 2888956552:99 - INFO - Epoch 7/25 - Loss: 1.9125 - Grad Norm: 0.10 - LR: 3.11e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ca1e24fcfaa46a2a291e309a66216c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 8:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:19:51] 2888956552:99 - INFO - Epoch 8/25 - Loss: 1.9077 - Grad Norm: 0.10 - LR: 2.96e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98d927ae16b44f8296cba4000eab569f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 9:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:21:13] 2888956552:99 - INFO - Epoch 9/25 - Loss: 1.9031 - Grad Norm: 0.15 - LR: 2.78e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74363954a57144ac8b7f6303d38e4145",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 10:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:22:35] 2888956552:99 - INFO - Epoch 10/25 - Loss: 1.8987 - Grad Norm: 0.14 - LR: 2.58e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e487e752e06046da942ccf398773881d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 11:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:23:57] 2888956552:99 - INFO - Epoch 11/25 - Loss: 1.8940 - Grad Norm: 0.13 - LR: 2.36e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05b47f7858774cb6baeb8ea0e1106f23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 12:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:25:19] 2888956552:99 - INFO - Epoch 12/25 - Loss: 1.8896 - Grad Norm: 0.24 - LR: 2.13e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9db51ed64aaa43c0bbdf1127ea3c472d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 13:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:26:41] 2888956552:99 - INFO - Epoch 13/25 - Loss: 1.8845 - Grad Norm: 0.16 - LR: 1.90e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27594178e02a4f1c899a4f6af2a1920d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 14:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:28:05] 2888956552:99 - INFO - Epoch 14/25 - Loss: 1.8796 - Grad Norm: 0.13 - LR: 1.66e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d26eac278b9d448c99c65134c4269a36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 15:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:29:27] 2888956552:99 - INFO - Epoch 15/25 - Loss: 1.8747 - Grad Norm: 0.17 - LR: 1.42e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df303081ceb448b5826c1b29fa8d55c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 16:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:30:50] 2888956552:99 - INFO - Epoch 16/25 - Loss: 1.8696 - Grad Norm: 0.13 - LR: 1.19e-03\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78272df04f2643dcb952fdf4fdf2ef80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 17:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:32:12] 2888956552:99 - INFO - Epoch 17/25 - Loss: 1.8645 - Grad Norm: 0.13 - LR: 9.65e-04\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c591172e29649d280cf8c6296467daa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 18:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:33:34] 2888956552:99 - INFO - Epoch 18/25 - Loss: 1.8596 - Grad Norm: 0.15 - LR: 7.58e-04\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4f903a9833140239a9f620661e4a9fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 19:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:34:57] 2888956552:99 - INFO - Epoch 19/25 - Loss: 1.8550 - Grad Norm: 0.14 - LR: 5.69e-04\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f3adfe4a0eb42228938787b950c8d1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 20:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:36:19] 2888956552:99 - INFO - Epoch 20/25 - Loss: 1.8507 - Grad Norm: 0.19 - LR: 4.02e-04\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d071dca32f745d98573f056f8158c4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 21:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:37:40] 2888956552:99 - INFO - Epoch 21/25 - Loss: 1.8468 - Grad Norm: 0.18 - LR: 2.61e-04\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a361d0cec76148e597b5fca6389826fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 22:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:39:04] 2888956552:99 - INFO - Epoch 22/25 - Loss: 1.8435 - Grad Norm: 0.22 - LR: 1.49e-04\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33ebf45d439f4480a78e369883ab7ed1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 23:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:40:27] 2888956552:99 - INFO - Epoch 23/25 - Loss: 1.8408 - Grad Norm: 0.16 - LR: 6.66e-05\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72d9e31041474a9b95dd15740cc8a66d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 24:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:41:51] 2888956552:99 - INFO - Epoch 24/25 - Loss: 1.8390 - Grad Norm: 0.18 - LR: 1.67e-05\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8754118905654c93b27e17b559b5a5c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 25:   0%|          | 0/4591 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m>>> [2025-02-17 | 04:43:14] 2888956552:99 - INFO - Epoch 25/25 - Loss: 1.8379 - Grad Norm: 0.18 - LR: 1.38e-08\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "best_params = {\n",
        "\t'model_type': 'lstm',\n",
        "\t'hidden_size': 474,\n",
        "\t'n_layers': 1,\n",
        "\t'dropout': 0.348681,\n",
        "\t'lr': 0.003438,\n",
        "\t'batch_size': 64,\n",
        "\t'epochs': 25,\n",
        "}\n",
        "\n",
        "best_model, best_loss = train_best_model(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Loss: 1.8379204218955218\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best Loss: {best_loss}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
